{
  "documents": [
    "--- Page 1 --- Deep Feedforward Networks Fundamentals and Challenges Day 10: Deep Feedforward Networks --- Page 2 --- Activation Functions What are Activation Functions? Mathematical gates that introduce non-linearity into neural networks. Crucial for learning complex patterns and relationships in data. Enable backpropagation by providing gradients for weight updates. Common Types: **Sigmoid**: Squashes values between 0 and 1, useful for binary classiﬁcation.",
    "pdates. Common Types: **Sigmoid**: Squashes values between 0 and 1, useful for binary classiﬁcation. **Tanh**: Squashes values between -1 and 1, zero- centered output aids learning. **ReLU (Rectiﬁed Linear Unit)**: Outputs x for positive input, 0 for negative. Computationally eﬃcient and mitigates vanishing gradients. --- Page 3 --- Loss Functions What are Loss Functions? Quantify the error between predicted and actual outputs. Guide the learning process by indicating how well the model performs.",
    "predicted and actual outputs. Guide the learning process by indicating how well the model performs. Minimize during training to improve model accuracy. Common Types: **Mean Squared Error (MSE)**: For regression tasks (continuous values). **Cross-Entropy**: For classiﬁcation tasks (probabilities). **Mean Absolute Percentage Error (MAPE)**: For forecasting, provides percentage error. --- Page 4 --- Network Architectures What are Deep Feedforward Networks?",
    "provides percentage error. --- Page 4 --- Network Architectures What are Deep Feedforward Networks? Information ﬂows unidirectionally from input to output. Also known as Multi-layer Perceptrons (MLPs). Composed of input, hidden, and output layers. Key Components: **Input Layer**: Receives initial data. **Hidden Layers**: Transform data, learn complex patterns (one or more). **Output Layer**: Produces the ﬁnal prediction.",
    "ransform data, learn complex patterns (one or more). **Output Layer**: Produces the ﬁnal prediction. How They Work: **Forward Propagation**: Data moves through layers, applying weights and activation functions. **Learning**: Adjusts weights and biases to minimize error (e.g., via backpropagation). --- Page 5 --- Backpropagation Algorithm What is Backpropagation? Method to train neural networks by adjusting weights and biases. Minimizes the diﬀerence between predicted and actual outputs.",
    "works by adjusting weights and biases. Minimizes the diﬀerence between predicted and actual outputs. How it Works: **Forward Pass**: Input data ﬂows through the network to produce a prediction. **Error Calculation**: Compares prediction with actual output using a loss function. **Backward Pass**: Error is propagated backward through the network. **Gradient Computation**: Calculates gradients of the loss with respect to weights and biases.",
    "work. **Gradient Computation**: Calculates gradients of the loss with respect to weights and biases. **Weight Adjustment**: Updates weights and biases using optimization algorithms (e.g., Gradient Descent). --- Page 6 --- Vanishing and Exploding Gradients Vanishing Gradients: Gradients become extremely small during backpropagation. Earlier layers learn very slowly or stop learning. Caused by saturating activation functions (e.g., sigmoid, tanh) and deep networks.",
    "or stop learning. Caused by saturating activation functions (e.g., sigmoid, tanh) and deep networks. Exploding Gradients: Gradients grow uncontrollably large during backpropagation. Leads to unstable learning and divergence. Caused by large initial weights or accumulation of gradients. Solutions: Proper Weight Initialization (Xavier, He). Non-Saturating Activation Functions (ReLU, Leaky ReLU). Batch Normalization. Gradient Clipping. Residual Connections.",
    "--- Page 1 --- Introduction to Neural Networks Day 10 Presenter: Mihir Inamdar June 2025 --- Page 2 --- What are Neural Networks? Neural networks are machine learning models that mimic the complex functions of the human brain. These models consist of interconnected nodes or neurons that process data, learn patterns, and enable tasks such as pattern recognition and decision- making. Key Components: ●Neurons: Basic units that receive inputs, governed by a threshold and activation function.",
    "ponents: ●Neurons: Basic units that receive inputs, governed by a threshold and activation function. Connections: Links between neurons that carry information, regulated by weights and biases. ⚖Weights & Biases: Parameters that determine the strength and influence of connections. Propagation Functions: Mechanisms that help process and transfer data across layers. Learning Rule: Method that adjusts weights and biases over time to improve accuracy.",
    "across layers. Learning Rule: Method that adjusts weights and biases over time to improve accuracy. Brief History: From the first mathematical model in the 1940s to today's deep learning systems, neural networks have evolved significantly. Key milestones include perceptrons (1960s), backpropagation (1980s), and the rise of deep learning and transformer models (2010s). --- Page 3 --- How Neural Networks Learn (Theory) Three-stage Learning Process: →Input Computation: Data is fed into the network.",
    "works Learn (Theory) Three-stage Learning Process: →Input Computation: Data is fed into the network. →Output Generation: Network generates output based on current parameters. →Iterative Refinement: Adjusting weights and biases to improve performance. Adaptive Learning Environment: Network is exposed to a dataset or scenario.",
    "to improve performance. Adaptive Learning Environment: Network is exposed to a dataset or scenario. Parameters update in response to new data.Forward Propagation: Linear Transformation:Y = WX + B Activation Functions: Sigmoid:σ(x) = 1/(1+e^(-x)) ReLU:f(x) = max(0, x) Backpropagation: Loss Calculation: Error between actual and predicted output. Gradient Calculation: Computing gradients of loss function. Weight Update: Adjusting weights using optimization algorithms.",
    "mputing gradients of loss function. Weight Update: Adjusting weights using optimization algorithms. 3 --- Page 4 --- Neural Network Architecture Key Layers in Neural Networks: →Input Layer: Receives input data; each neuron corresponds to a feature in the input data. This is where the network gets its initial information. →Hidden Layers: Perform most of the computational heavy lifting. A neural network can have one or multiple hidden layers.",
    "rm most of the computational heavy lifting. A neural network can have one or multiple hidden layers. Each layer consists of units (neurons) that transform the inputs into something that the output layer can use. →Output Layer: Produces the final output of the model. The format of these outputs varies depending on the specific task (e.g., classification, regression).",
    "he format of these outputs varies depending on the specific task (e.g., classification, regression). Data Flow: Data flows in one direction from input to output through the hidden layers in a process called forward propagation.Input Layer FeaturesHidden Layers Computational ProcessingOutput Laye Predictions 4 --- Page 5 --- Types of Neural Networks →Feedforward Neural Network The most basic type, where data flows in one direction from input to output. No loops or cycles.",
    "ork The most basic type, where data flows in one direction from input to output. No loops or cycles. Used for simple pattern recognition and classification tasks. Convolutional Neural Network (CNN) Specialized for processing grid-like data such as images. Uses convolutional layers to extract features. Widely used in computer vision tasks like image classification and object detection. Recurrent Neural Network (RNN) Designed for sequential data (e.g., time series, natural language).",
    "Recurrent Neural Network (RNN) Designed for sequential data (e.g., time series, natural language). They have internal memory to process sequences. LSTMs and GRUs are popular variants used for speech recognition and text generation. Modular Neural Network A collection of independent neural networks, each performing a sub- task. This approach breaks down complex problems into smaller, manageable components.",
    "forming a sub- task. This approach breaks down complex problems into smaller, manageable components. ●Radial Basis Function (RBF) Network Uses radial basis functions as activation functions, often used for function approximation and pattern recognition.",
    "is functions as activation functions, often used for function approximation and pattern recognition. 5 --- Page 6 --- Applications of Neural Networks Computer Vision Image classification, object detection, facial recognition, and image segmentation.Speech Recognition Converting spoken language into text, used in virtual assistants and transcription services.",
    "gnition Converting spoken language into text, used in virtual assistants and transcription services. Natural Language Processing Machine translation, text summarization, sentiment analysis, and chatbots.Recommendation Systems Suggesting products, movies, or music based on user preferences and behavior. Healthcare Disease diagnosis, drug discovery, medical image analysis, and personalized treatment plans.Finance Fraud detection, algorithmic trading, risk assessment, and credit scoring.",
    "treatment plans.Finance Fraud detection, algorithmic trading, risk assessment, and credit scoring. Automotive Self-driving cars (object detection, path planning), predictive maintenance.⛅Weather Forecasting Predicting weather patterns based on historical data and current conditions. 6 --- Page 7 --- Simple Neural Network Example (Code) The XOR Problem The XOR (exclusive OR) problem is a classic example used to demonstrate neural networks.",
    "XOR Problem The XOR (exclusive OR) problem is a classic example used to demonstrate neural networks. It's a simple binary classification problem that can't be solved by a linear classifier, making it perfect for showing the power of neural networks. Input and Output Data # Input dataset (XOR problem) X = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]]) # Output dataset y = np.array([[0],[1],[1],[0]]) Network Structure Our neural network has 3 inputs, 4 hidden neurons, and 1 output neuron.",
    ",[1],[0]]) Network Structure Our neural network has 3 inputs, 4 hidden neurons, and 1 output neuron. It uses the sigmoid activation function and implements backpropagation for learning.Key Methods defsigmoid(self, x): # Sigmoid activation function return 1 / (1 + np.exp(-x)) defforward(self, X): # Forward propagation self.hidden_layer_input = np.dot(X, self.W1) + self.b1 self.hidden_layer_output = self.sigmoid(self.hidden_layer_input) self.output_layer_input = np.dot(self.hidden_layer_output, self.W2) + self.b2 self.predicted_output = self.sigmoid(self.output_layer_input) return self.predicted_output defbackward(self, X, y, output): # Backpropagation self.output_error = y - output self.output_delta = self.output_error * self.sigmoid_derivative(output) self.hidden_layer_error = self.output_delta.dot(self.W2.T) self.hidden_layer_delta = self.hidden_layer_error * self.sigmoid_derivative(self.hidden_layer_output) # Update weights and biases lfW2+lfhiddlttTdt(lfttdlt)*01 Training the Network deftrain(self, X, y, epochs): # Training loop for epoch in range(epochs): output = self.forward(X) self.backward(X, y, output) if epoch % 1000 == 0: loss = np.mean(np.square(y - output)) print(f\"Epoch {epoch}, Loss: {loss:.4f}\") 7 --- Page 8 --- Running the Code and Results Execution Output Epoch 0, Loss: 0.3177 Epoch 1000, Loss: 0.1560 Epoch 2000, Loss: 0.0325 Epoch 5000, Loss: 0.0047 Epoch 9000, Loss: 0.0019 Predictions after training: [[0.027] [0.961] [0.956] [0.049]] Results Comparison Input 1 Input 2 Expected Predicted 0 0 0 0.027 0 1 1 0.961 1 0 1 0.956 1 1 0 0.049 The final predictions closely match our expected outputs for the XOR problem, demonstrating successful learning.Loss Over Training Key Observations Rapid learning in first 2000 epochs Gradual fine-tuning after 5000 epochs Predictions close to binary outputs Successfully learned non-linear XOR pattern Why This Matters The XOR problem demonstrates neural networks' ability to learn complex, non-linear relationships in data, which is the foundation for their success in more advanced tasks like image recognition and natural language processing.",
    "ion for their success in more advanced tasks like image recognition and natural language processing. 8 --- Page 9 --- Conclusion and Q&A Key Concepts Recap Neural networks are machine learning models inspired by the human brain, consisting of interconnected neurons that process data and learn patterns. The architecture includes input, hidden, and output layers, with weights and biases determining connection strengths.",
    "includes input, hidden, and output layers, with weights and biases determining connection strengths. Learning occurs through forward propagation and backpropagation, gradually adjusting weights to minimize error. Future of Neural Networks Advancements in transformer models and attention mechanisms Specialized hardware for neural network acceleration Energy-efficient neural networks for edge devices Questions? Feel free to ask about any aspect of neural networks we've covered today.",
    "r edge devices Questions? Feel free to ask about any aspect of neural networks we've covered today. 9 --- Page 10 --- Thank You! Mihir Inamdar June 2025 Further Resources Deep Learning by Ian Goodfellow TensorFlow & PyTorch Documentation Coursera: Deep Learning Specialization",
    "--- Page 1 --- Introduction to Convolutional Neural Networks (CNNs) Understanding the Basic Components, Theory , and Code June 30, 2025 --- Page 2 --- What are Convolutional Neural Networks? CNNs are specialized neural networks designed primarily for processing grid-like data, such as images. Inspired by the organization of the animal visual cortex, where individual neurons respond to stimuli in restricted regions.",
    "tion of the animal visual cortex, where individual neurons respond to stimuli in restricted regions. Automatically extract features from data without manual engineering, making them highly ef ficient for image analysis. Revolutionized computer vision tasks like image classification, object detection, and facial recognition.",
    "utionized computer vision tasks like image classification, object detection, and facial recognition. --- Page 3 --- CNN Architecture Overview Input Layer Raw image data (pixels)Convolutional Layers Feature extraction using filtersPooling Layers Dimensionality reductionFully Connected Classification based on featuresOutput Layer Final predictions     --- Page 4 --- Convolutional Layers Apply filters (kernels) to detect features like edges, textures, and patterns Sliding window operation that performs element- wise multiplication and summation Key parameters: filter size, stride, padding, and number of filters Early layers detect simple features (edges), deeper layers detect complex features (shapes, objects) Output = (Input - Filter + 2*Padding)/Stride + 1 --- Page 5 --- Activation Functions ⚡Introduce non-linearity into the network, allowing it to learn complex patterns ReLU (Rectified Linear Unit) is most commonly used in CNNs ReLU helps mitigate the vanishing gradient problem and speeds up training Variants include Leaky ReLU, Parametric ReLU, and ELU for improved performance ReLU: f(x) = max(0, x) Leaky ReLU: f(x) = max(0.01x, x) --- Page 6 --- Pooling Layers Reduces spatial dimensions (width and height) of the feature maps Decreases computational load and helps prevent overfitting Provides a form of translation invariance to the network ⚙Key parameters: pool size (typically 2×2) and stride (typically 2) Max Pooling Takes the maximum value from each window , preserving the most prominent featuresAverage Pooling Takes the average of all values in each window , smoothing the features --- Page 7 --- Fully Connected Layers Connects every neuron from previous layer to every neuron in the next layer Flattens the 3D feature maps from convolutional layers into a 1D vector Performs high-level reasoning based on features extracted by convolutional layers Final layer uses softmax activation for classification tasks, outputting probabilities Dropout T echnique Randomly deactivates neurons during training to prevent overfitting model.add(layers.Dense(512, activation='relu')) model.add(layers.Dropout(0.5)) model.add(layers.Dense(10, activation='softmax')) --- Page 8 --- CNN Implementation Example import tensorflow as tf from tensorflow.keras import datasets, layers, models import matplotlib.pyplot as plt # Load and prepare the CIFAR-10 dataset (train_images, train_labels), (test_images, test_labels) = datasets.c # Normalize pixel values to be between 0 and 1 train_images, test_images = train_images / 255.0, test_images / 255.0 # Define the CNN model model = models.Sequential() # Add convolutional layers model.add(layers.Conv2D( 32, ( 3, 3), activation= 'relu', input_shape=( 3 model.add(layers.MaxPooling2D(( 2, 2))) model.add(layers.Conv2D( 64, ( 3, 3), activation= 'relu')) model.add(layers.MaxPooling2D(( 2, 2))) model.add(layers.Conv2D( 64, ( 3, 3), activation= 'relu')) #Addfullyconnected layersModel Architecture This example creates a CNN with three convolutional layers, each followed by ReLU activation.",
    "ecture This example creates a CNN with three convolutional layers, each followed by ReLU activation. Max pooling is applied after the first two convolutional layers to reduce dimensionality . Dataset CIFAR-10 contains 60,000 32x32 color images in 10 classes (airplanes, cars, birds, cats, etc.) with 6,000 images per class. Training Process The model is trained for 10 epochs using the Adam optimizer and categorical cross-entropy loss function, which is appropriate for classification tasks.",
    "ptimizer and categorical cross-entropy loss function, which is appropriate for classification tasks. Framework This example uses TensorFlow and Keras, but similar implementations can be done with PyT orch, MXNet, or other deep learning frameworks.",
    "as, but similar implementations can be done with PyT orch, MXNet, or other deep learning frameworks. --- Page 9 --- CNN Applications  Image Classification Categorizing images into predefined classes (e.g., ImageNet classification) Object Detection Identifying and locating multiple objects within an image (e.g., YOLO, SSD) Image Segmentation Pixel-level classification for precise object boundaries (e.g., U-Net)  Facial Recognition Identifying or verifying individuals based on facial features Medical Imaging Detecting diseases from X-rays, MRIs, and other medical scans Autonomous V ehicles Processing visual data for navigation and obstacle detection --- Page 10 --- Summary and Key T akeaways Convolutional Neural Networks have revolutionized computer vision by automatically learning hierarchical features from data.",
    "works have revolutionized computer vision by automatically learning hierarchical features from data. Their architecture mimics the human visual system, with specialized layers working together to process and understand visual information. Further Learning Resources TensorFlow Documentation PyTorch Tutorials CS231n: CNN for Visual RecognitionCNNs use convolutional layers to extract features, pooling layers to reduce dimensions, and fully connected layers for classification.",
    "xtract features, pooling layers to reduce dimensions, and fully connected layers for classification. Activation functions like ReLU introduce non-linearity , enabling the network to learn complex patterns. Modern frameworks like TensorFlow and PyT orch make CNN implementation accessible for various applications. CNNs continue to evolve with architectures like ResNet, Inception, and Ef ficientNet pushing the boundaries of performance.",
    "--- Page 2 --- LINEAR MODELS IN STATISTICS --- Page 4 --- LINEAR MODELS IN STATISTICS Second Edition Alvin C. Rencher and G. Bruce Schaalje Department of Statistics, Brigham Young University, Provo ,Utah --- Page 5 --- Copyright #2008 by John Wiley & Sons, Inc.",
    "ics, Brigham Young University, Provo ,Utah --- Page 5 --- Copyright #2008 by John Wiley & Sons, Inc. All rights reserved Published by John Wiley & Sons, Inc., Hoboken, New Jersey Published simultaneously in Canada No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, exceptas permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978)750-8400, fax (978) 750-4470, or on the web at www.copyright.com.",
    "ood Drive, Danvers, MA 01923, (978)750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http: //www.wiley. com/go/permission.",
    "en, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http: //www.wiley. com/go/permission. Limit of Liability /Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy orcompleteness of the contents of this book and speciﬁcally disclaim any implied warranties of merchantability or ﬁtness for a particular purpose.",
    "d speciﬁcally disclaim any implied warranties of merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by sales representatives or written sales materials. The advice and strategies contained herein may not be suitable for your situation. You should consult with a professional where appropriate.",
    "in may not be suitable for your situation. You should consult with a professional where appropriate. Neither the publisher nor author shall be liable for any loss of proﬁt or any other commercial damages, includingbut not limited to special, incidental, consequential, or other damages.",
    "ommercial damages, includingbut not limited to special, incidental, consequential, or other damages. For general information on our other products and services or for technical support, please contact our Customer Care Department within the United States at (800) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002. Wiley also publishes its books in variety of electronic formats. Some content that appears in print may not be available in electronic formats.",
    "f electronic formats. Some content that appears in print may not be available in electronic formats. For more information about Wiley products, visit our web site at www.wiley.com. Wiley Bicentennial Logo: Richard J. PaciﬁcoLibrary of Congress Cataloging-in-Publication Data: Rencher, Alvin C., 1934- Linear models in statistics /Alvin C. Rencher, G. Bruce Schaalje. – 2nd ed. p. cm. Includes bibliographical references. ISBN 978-0-471-75498-5 (cloth) 1. Linear models (Statistics) I. Schaalje, G.",
    "iographical references. ISBN 978-0-471-75498-5 (cloth) 1. Linear models (Statistics) I. Schaalje, G. Bruce. II. Title.",
    "nces. ISBN 978-0-471-75498-5 (cloth) 1. Linear models (Statistics) I. Schaalje, G. Bruce. II. Title. QA276.R425 2007519.5 035–dc22 2007024268 Printed in the United States of America 1 0987654321 --- Page 6 --- CONTENTS Preface xiii 1 Introduction 1 1.1 Simple Linear Regression Model 1 1.2 Multiple Linear Regression Model 21.3 Analysis-of-Variance Models 3 2 Matrix Algebra 5 2.1 Matrix and Vector Notation 5 2.1.1 Matrices, Vectors, and Scalars 52.1.2 Matrix Equality 6 2.1.3 Transpose 7 2.1.4 Matrices of Special Form 7 2.2 Operations 9 2.2.1 Sum of Two Matrices or Two Vectors 9 2.2.2 Product of a Scalar and a Matrix 10 2.2.3 Product of Two Matrices or Two Vectors 102.2.4 Hadamard Product of Two Matrices or Two Vectors 16 2.3 Partitioned Matrices 162.4 Rank 192.5 Inverse 21 2.6 Positive Deﬁnite Matrices 24 2.7 Systems of Equations 282.8 Generalized Inverse 32 2.8.1 Deﬁnition and Properties 33 2.8.2 Generalized Inverses and Systems of Equations 36 2.9 Determinants 37 2.10 Orthogonal Vectors and Matrices 412.11 Trace 44 2.12 Eigenvalues and Eigenvectors 46 2.12.1 Deﬁnition 46 2.12.2 Functions of a Matrix 49 v --- Page 7 --- 2.12.3 Products 50 2.12.4 Symmetric Matrices 51 2.12.5 Positive Deﬁnite and Semideﬁnite Matrices 53 2.13 Idempotent Matrices 54 2.14 Vector and Matrix Calculus 56 2.14.1 Derivatives of Functions of Vectors and Matrices 56 2.14.2 Derivatives Involving Inverse Matrices and Determinants 58 2.14.3 Maximization or Minimization of a Function of a Vector 60 3 Random Vectors and Matrices 69 3.1 Introduction 69 3.2 Means, Variances, Covariances, and Correlations 703.3 Mean Vectors and Covariance Matrices for Random Vectors 75 3.3.1 Mean Vectors 75 3.3.2 Covariance Matrix 75 3.3.3 Generalized Variance 773.3.4 Standardized Distance 77 3.4 Correlation Matrices 773.5 Mean Vectors and Covariance Matrices for Partitioned Random Vectors 78 3.6 Linear Functions of Random Vectors 79 3.6.1 Means 803.6.2 Variances and Covariances 81 4 Multivariate Normal Distribution 87 4.1 Univariate Normal Density Function 87 4.2 Multivariate Normal Density Function 884.3 Moment Generating Functions 90 4.4 Properties of the Multivariate Normal Distribution 92 4.5 Partial Correlation 100 5 Distribution of Quadratic Forms in y 105 5.1 Sums of Squares 1055.2 Mean and Variance of Quadratic Forms 1075.3 Noncentral Chi-Square Distribution 112 5.4 Noncentral FandtDistributions 114 5.4.1 Noncentral FDistribution 114 5.4.2 Noncentral tDistribution 116 5.5 Distribution of Quadratic Forms 117 5.6 Independence of Linear Forms and Quadratic Forms 119CONTENTS vi --- Page 8 --- 6 Simple Linear Regression 127 6.1 The Model 127 6.2 Estimation of b0,b1, ands2128 6.3 Hypothesis Test and Conﬁdence Interval for b1132 6.4 Coefﬁcient of Determination 133 7 Multiple Regression: Estimation 137 7.1 Introduction 137 7.2 The Model 137 7.3 Estimation of bands2141 7.3.1 Least-Squares Estimator for b145 7.3.2 Properties of the Least-Squares Estimator bˆ141 7.3.3 An Estimator for s2149 7.4 Geometry of Least-Squares 151 7.4.1 Parameter Space, Data Space, and Prediction Space 1527.4.2 Geometric Interpretation of the Multiple Linear Regression Model 153 7.5 The Model in Centered Form 1547.6 Normal Model 157 7.6.1 Assumptions 157 7.6.2 Maximum Likelihood Estimators for bands2158 7.6.3 Properties of bˆandsˆ2159 7.7 R2in Fixed- xRegression 161 7.8 Generalized Least-Squares: cov( y)¼s2V164 7.8.1 Estimation of bands2when cov( y)¼s2V164 7.8.2 Misspeciﬁcation of the Error Structure 167 7.9 Model Misspeciﬁcation 169 7.10 Orthogonalization 174 8 Multiple Regression: Tests of Hypotheses and Conﬁdence Intervals 185 8.1 Test of Overall Regression 185 8.2 Test on a Subset of the bValues 189 8.3 FTest in Terms of R2196 8.4 The General Linear Hypothesis Tests for H0: Cb¼0andH0:Cb¼t198 8.4.1 The Test for H0:Cb¼0 198 8.4.2 The Test for H0:Cb¼t203 8.5 Tests on bjanda0b204 8.5.1 Testing One bjor One a0b204 8.5.2 Testing Several bjora0ibValues 205CONTENTS vii --- Page 9 --- 8.6 Conﬁdence Intervals and Prediction Intervals 209 8.6.1 Conﬁdence Region for b209 8.6.2 Conﬁdence Interval for bj210 8.6.3 Conﬁdence Interval for a0b211 8.6.4 Conﬁdence Interval for E(y) 211 8.6.5 Prediction Interval for a Future Observation 213 8.6.6 Conﬁdence Interval for s2215 8.6.7 Simultaneous Intervals 215 8.7 Likelihood Ratio Tests 217 9 Multiple Regression: Model Validation and Diagnostics 227 9.1 Residuals 227 9.2 The Hat Matrix 230 9.3 Outliers 232 9.4 Inﬂuential Observations and Leverage 235 10 Multiple Regression: Random x’s 243 10.1 Multivariate Normal Regression Model 24410.2 Estimation and Testing in Multivariate Normal Regression 24510.3 Standardized Regression Coefﬁcents 249 10.4 R 2in Multivariate Normal Regression 254 10.5 Tests and Conﬁdence Intervals for R2258 10.6 Effect of Each Variable on R2262 10.7 Prediction for Multivariate Normal or Nonnormal Data 265 10.8 Sample Partial Correlations 266 11 Multiple Regression: Bayesian Inference 277 11.1 Elements of Bayesian Statistical Inference 277 11.2 A Bayesian Multiple Linear Regression Model 279 11.2.1 A Bayesian Multiple Regression Model with a Conjugate Prior 280 11.2.2 Marginal Posterior Density of b282 11.2.3 Marginal Posterior Densities of tands2284 11.3 Inference in Bayesian Multiple Linear Regression 285 11.3.1 Bayesian Point and Interval Estimates of Regression Coefﬁcients 285 11.3.2 Hypothesis Tests for Regression Coefﬁcients in Bayesian Inference 286 11.3.3 Special Cases of Inference in Bayesian Multiple Regression Models 286 11.3.4 Bayesian Point and Interval Estimation of s2287CONTENTS viii --- Page 10 --- 11.4 Bayesian Inference through Markov Chain Monte Carlo Simulation 288 11.5 Posterior Predictive Inference 290 12 Analysis-of-Variance Models 295 12.1 Non-Full-Rank Models 295 12.1.1 One-Way Model 295 12.1.2 Two-Way Model 299 12.2 Estimation 301 12.2.1 Estimation of b302 12.2.2 Estimable Functions of b305 12.3 Estimators 309 12.3.1 Estimators of l0b309 12.3.2 Estimation of s2313 12.3.3 Normal Model 314 12.4 Geometry of Least-Squares in the Overparameterized Model 316 12.5 Reparameterization 31812.6 Side Conditions 320 12.7 Testing Hypotheses 323 12.7.1 Testable Hypotheses 323 12.7.2 Full-Reduced-Model Approach 32412.7.3 General Linear Hypothesis 326 12.8 An Illustration of Estimation and Testing 329 12.8.1 Estimable Functions 330 12.8.2 Testing a Hypothesis 331 12.8.3 Orthogonality of Columns of X333 13 One-Way Analysis-of-Variance: Balanced Case 339 13.1 The One-Way Model 33913.2 Estimable Functions 34013.3 Estimation of Parameters 341 13.3.1 Solving the Normal Equations 34113.3.2 An Estimator for s2343 13.4 Testing the Hypothesis H0:m1¼m2¼...¼mk344 13.4.1 Full–Reduced-Model Approach 344 13.4.2 General Linear Hypothesis 348 13.5 Expected Mean Squares 351 13.5.1 Full-Reduced-Model Approach 352 13.5.2 General Linear Hypothesis 354CONTENTS ix --- Page 11 --- 13.6 Contrasts 357 13.6.1 Hypothesis Test for a Contrast 357 13.6.2 Orthogonal Contrasts 358 13.6.3 Orthogonal Polynomial Contrasts 363 14 Two-Way Analysis-of-Variance: Balanced Case 377 14.1 The Two-Way Model 377 14.2 Estimable Functions 37814.3 Estimators of l0bands2382 14.3.1 Solving the Normal Equations and Estimating l0b382 14.3.2 An Estimator for s2384 14.4 Testing Hypotheses 385 14.4.1 Test for Interaction 385 14.4.2 Tests for Main Effects 395 14.5 Expected Mean Squares 403 14.5.1 Sums-of-Squares Approach 403 14.5.2 Quadratic Form Approach 405 15 Analysis-of-Variance: The Cell Means Model for Unbalanced Data 413 15.1 Introduction 413 15.2 One-Way Model 415 15.2.1 Estimation and Testing 415 15.2.2 Contrasts 417 15.3 Two-Way Model 421 15.3.1 Unconstrained Model 421 15.3.2 Constrained Model 428 15.4 Two-Way Model with Empty Cells 432 16 Analysis-of-Covariance 443 16.1 Introduction 443 16.2 Estimation and Testing 444 16.2.1 The Analysis-of-Covariance Model 44416.2.2 Estimation 446 16.2.3 Testing Hypotheses 448 16.3 One-Way Model with One Covariate 449 16.3.1 The Model 449 16.3.2 Estimation 44916.3.3 Testing Hypotheses 450CONTENTS x --- Page 12 --- 16.4 Two-Way Model with One Covariate 457 16.4.1 Tests for Main Effects and Interactions 458 16.4.2 Test for Slope 462 16.4.3 Test for Homogeneity of Slopes 463 16.5 One-Way Model with Multiple Covariates 464 16.5.1 The Model 464 16.5.2 Estimation 465 16.5.3 Testing Hypotheses 468 16.6 Analysis-of-Covariance with Unbalanced Models 473 17 Linear Mixed Models 479 17.1 Introduction 47917.2 The Linear Mixed Model 47917.3 Examples 481 17.4 Estimation of Variance Components 486 17.5 Inference for b490 17.5.1 An Estimator for b490 17.5.2 Large-Sample Inference for Estimable Functions of b491 17.5.3 Small-Sample Inference for Estimable Functions of b491 17.6 Inference for the aiTerms 497 17.7 Residual Diagnostics 501 18 Additional Models 507 18.1 Nonlinear Regression 507 18.2 Logistic Regression 508 18.3 Loglinear Models 511 18.4 Poisson Regression 512 18.5 Generalized Linear Models 513 Appendix A Answers and Hints to the Problems 517 References 653 Index 663CONTENTS xi --- Page 13 --- PREFACE In the second edition, we have added chapters on Bayesian inference in linear models (Chapter 11) and linear mixed models (Chapter 17), and have upgraded the materialin all other chapters.",
    "apter 11) and linear mixed models (Chapter 17), and have upgraded the materialin all other chapters. Our continuing objective has been to introduce the theory of linear models in a clear but rigorous format. In spite of the availability of highly innovative tools in statistics, the main tool of the applied statistician remains the linear model.",
    "innovative tools in statistics, the main tool of the applied statistician remains the linear model. The linear model involves the sim-plest and seemingly most restrictive statistical properties: independence, normality, constancy of variance, and linearity. However, the model and the statistical methods associated with it are surprisingly versatile and robust.",
    "ver, the model and the statistical methods associated with it are surprisingly versatile and robust. More importantly, mastery of the linear model is a prerequisite to work with advanced statistical tools because most advanced tools are generalizations of the linear model. The linearmodel is thus central to the training of any statistician, applied or theoretical.",
    "model. The linearmodel is thus central to the training of any statistician, applied or theoretical. This book develops the basic theory of linear models for regression, analysis-of- variance, analysis–of–covariance, and linear mixed models. Chapter 18 brieﬂy intro-duces logistic regression, generalized linear models, and nonlinear models. Applications are illustrated by examples and problems using real data.",
    "models, and nonlinear models. Applications are illustrated by examples and problems using real data. This combination of theory and applications will prepare the reader to further explore the literature and tomore correctly interpret the output from a linear models computer package. This introductory linear models book is designed primarily for a one-semester course for advanced undergraduates or MS students.",
    "els book is designed primarily for a one-semester course for advanced undergraduates or MS students. It includes more material thancan be covered in one semester so as to give an instructor a choice of topics and to serve as a reference book for researchers who wish to gain a better understanding of regression and analysis-of-variance.",
    "book for researchers who wish to gain a better understanding of regression and analysis-of-variance. The book would also serve well as a textfor PhD classes in which the instructor is looking for a one-semester introduction, and it would be a good supplementary text or reference for a more advanced PhD class for which the students need to review the basics on their own. Our overriding objective in the preparation of this book has been clarity of expo- sition .",
    "eir own. Our overriding objective in the preparation of this book has been clarity of expo- sition . We hope that students, instructors, researchers, and practitioners will ﬁnd this linear models text more comfortable than most. In the ﬁnal stages of development, we asked students for written comments as they read each day’s assignment. They made many suggestions that led to improvements in readability of the book.",
    "ch day’s assignment. They made many suggestions that led to improvements in readability of the book. We are grateful to readers who have notiﬁed us of errors and other suggestions for improvements ofthe text, and we will continue to be very grateful to readers who take the time to do so for this second edition. xiii --- Page 14 --- Another objective of the book is to tie up loose ends. There are many approaches to teaching regression, for example.",
    "of the book is to tie up loose ends. There are many approaches to teaching regression, for example. Some books present estimation of regression coefﬁcients for ﬁxed x’s only, other books use random x’s, some use centered models, and others deﬁne estimated regression coefﬁcients in terms of variances and covariances or in terms of correlations. Theory for linear models has been pre- sented using both an algebraic and a geometric approach.",
    "ons. Theory for linear models has been pre- sented using both an algebraic and a geometric approach. Many books present clas- sical (frequentist) inference for linear models, while increasingly the Bayesian approach is presented. We have tried to cover all these approaches carefully and to show how they relate to each other. We have attempted to do something similarfor various approaches to analysis-of-variance. We believe that this will make the book useful as a reference as well as a textbook.",
    "is-of-variance. We believe that this will make the book useful as a reference as well as a textbook. An instructor can choose the approach he or she prefers, and a student or researcher has access to other methodsas well. The book includes a large number of theoretical problems and a smaller number of applied problems using real datasets.",
    "a large number of theoretical problems and a smaller number of applied problems using real datasets. The problems, along with the extensive set of answers in Appendix A, extend the book in two signiﬁcant ways: (1) the theoretical problems and answers ﬁll in nearly all gaps in derivations and proofs and also extend the coverage of material in the text, and (2) the applied problems and answers becomeadditional examples illustrating the theory.",
    "he text, and (2) the applied problems and answers becomeadditional examples illustrating the theory. As instructors, we ﬁnd that having answers available for the students saves a great deal of class time and enables us to cover more material and cover it better. The answers would be especially useful toa reader who is engaging this material outside the formal classroom setting. The mathematical prerequisites for this book are multivariable calculus and matrix algebra.",
    "setting. The mathematical prerequisites for this book are multivariable calculus and matrix algebra. The review of matrix algebra in Chapter 2 is intended to be sufﬁciently com-plete so that the reader with no previous experience can master matrix manipulation up to the level required in this book.",
    "er with no previous experience can master matrix manipulation up to the level required in this book. Statistical prerequisites include some exposure to statistical theory, with coverage of topics such as distributions of random variables,expected values, moment generating functions, and an introduction to estimation and testing hypotheses. These topics are brieﬂy reviewed as each is introduced.",
    "uction to estimation and testing hypotheses. These topics are brieﬂy reviewed as each is introduced. One or two statistical methods courses would also be helpful, with coverage oftopics such as ttests, regression, and analysis-of-variance. We have made considerable effort to maintain consistency of notation throughout the book. We have also attempted to employ standard notation as far as possible andto avoid exotic characters that cannot be readily reproduced on the chalkboard.",
    "s far as possible andto avoid exotic characters that cannot be readily reproduced on the chalkboard. With a few exceptions, we have refrained from the use of abbreviations and mnemonic devices. We often ﬁnd these annoying in a book or journal article. Equations are numbered sequentially throughout each chapter; for example, (3.29) indicates the twenty-ninth numbered equation in Chapter 3.",
    "oughout each chapter; for example, (3.29) indicates the twenty-ninth numbered equation in Chapter 3. Tables and ﬁgures arealso numbered sequentially throughout each chapter in the form “Table 7.4” or“Figure 3.2.” On the other hand, examples and theorems are numbered sequentially within a section, for example, Theorems 2.2a and 2.2b. The solution of most of the problems with real datasets requires the use of the com- puter.",
    "nd 2.2b. The solution of most of the problems with real datasets requires the use of the com- puter. We have not discussed command ﬁles or output of any particular program,because there are so many good packages available. Computations for the numerical examples and numerical problems were done with SAS. The datasets and SASxiv PREFACE --- Page 15 --- command ﬁles for all the numerical examples and problems in the text are available on the Internet; see Appendix B.",
    "r all the numerical examples and problems in the text are available on the Internet; see Appendix B. The references list is not intended to be an exhaustive survey of the literature. We have provided original references for some of the basic results in linear models andhave also referred the reader to many up-to-date texts and reference books useful for further reading. When citing references in the text, we have used the standard format involving the year of publication.",
    "n citing references in the text, we have used the standard format involving the year of publication. For journal articles, the year alone sufﬁces, for example, Fisher (1921). But for a speciﬁc reference in a book, we have included a page number or section, as in Hocking (1996, p. 216). Our selection of topics is intended to prepare the reader for a better understanding of applications and for further reading in topics such as mixed models, generalizedlinear models, and Bayesian models.",
    "d for further reading in topics such as mixed models, generalizedlinear models, and Bayesian models. Following a brief introduction in Chapter 1,Chapter 2 contains a careful review of all aspects of matrix algebra needed to read the book. Chapters 3, 4, and 5 cover properties of random vectors, matrices, and quadratic forms. Chapters 6, 7, and 8 cover simple and multiple linear regression,including estimation and testing hypotheses and consequences of misspeciﬁcation of the model.",
    "ession,including estimation and testing hypotheses and consequences of misspeciﬁcation of the model. Chapter 9 provides diagnostics for model validation and detection of inﬂuential observations. Chapter 10 treats multiple regression with random x’s. Chapter 11 covers Bayesian multiple linear regression models along with Bayesian inferences based on those models.",
    "ers Bayesian multiple linear regression models along with Bayesian inferences based on those models. Chapter 12 covers the basic theory of analysis- of-variance models, including estimability and testability for the overparameterizedmodel, reparameterization, and the imposition of side conditions. Chapters 13 and 14 cover balanced one-way and two-way analysis-of-variance models using an over- parameterized model.",
    "4 cover balanced one-way and two-way analysis-of-variance models using an over- parameterized model. Chapter 15 covers unbalanced analysis-of-variance modelsusing a cell means model, including a section on dealing with empty cells in two- way analysis-of-variance. Chapter 16 covers analysis of covariance models.",
    "with empty cells in two- way analysis-of-variance. Chapter 16 covers analysis of covariance models. Chapter 17 covers the basic theory of linear mixed models, including residualmaximum likelihood estimation of variance components, approximate small- sample inferences for ﬁxed effects, best linear unbiased prediction of random effects, and residual analysis.",
    "ferences for ﬁxed effects, best linear unbiased prediction of random effects, and residual analysis. Chapter 18 introduces additional topics such asnonlinear regression, logistic regression, loglinear models, Poisson regression, and generalized linear models. In our class for ﬁrst-year master’s-level students, we cover most of the material in Chapters 2–5, 7–8, 10–12, and 17. Many other sequences are possible.",
    "e cover most of the material in Chapters 2–5, 7–8, 10–12, and 17. Many other sequences are possible. For example,a thorough one-semester regression and analysis-of-variance course could cover Chapters 1–10, and 12–15. Al’s introduction to linear models came in classes taught by Dale Richards and Rolf Bargmann. He also learned much from the books by Graybill, Scheffe ´, and Rao.",
    "ale Richards and Rolf Bargmann. He also learned much from the books by Graybill, Scheffe ´, and Rao. Al expresses thanks to the following for reading the ﬁrst edition manuscriptand making many valuable suggestions: David Turner, John Walker, Joel Reynolds, and Gale Rex Bryce.",
    "tand making many valuable suggestions: David Turner, John Walker, Joel Reynolds, and Gale Rex Bryce. Al thanks the following students at Brigham Young University (BYU) who helped with computations, graphics, and typing ofthe ﬁrst edition: David Fillmore, Candace Baker, Scott Curtis, Douglas Burton, David Dahl, Brenda Price, Eric Hintze, James Liechty, and Joy Willbur.",
    "Scott Curtis, Douglas Burton, David Dahl, Brenda Price, Eric Hintze, James Liechty, and Joy Willbur. The studentsPREFACE xv --- Page 16 --- in Al’s Linear Models class went through the manuscript carefully and spotted many typographical errors and passages that needed additional clariﬁcation. Bruce’s education in linear models came in classes taught by Mel Carter, Del Scott, Doug Martin, Peter Bloomﬁeld, and Francis Giesbrecht, and inﬂuential shortcourses taught by John Nelder and Russ Wolﬁnger.",
    "omﬁeld, and Francis Giesbrecht, and inﬂuential shortcourses taught by John Nelder and Russ Wolﬁnger. We thank Bruce’s Linear Models classes of 2006 and 2007 for going through the book and new chapters. They made valuable suggestions for improvement of the text. We thank Paul Martin and James Hattaway for invaluable help with LaTex. The Department of Statistics, Brigham Young University provided ﬁnancial supportand encouragement throughout the project.",
    "tistics, Brigham Young University provided ﬁnancial supportand encouragement throughout the project. Second Edition For the second edition we added Chapter 11 on Bayesian inference in linear models (including Gibbs sampling) and Chapter 17 on linear mixed models. We also added a section in Chapter 2 on vector and matrix calculus, adding several new theorems and covering the Lagrange multiplier method.",
    "vector and matrix calculus, adding several new theorems and covering the Lagrange multiplier method. In Chapter 4, we pre-sented a new proof of the conditional distribution of a subvector of a multivariatenormal vector. In Chapter 5, we provided proofs of the moment generating function and variance of a quadratic form of a multivariate normal vector.",
    "of the moment generating function and variance of a quadratic form of a multivariate normal vector. The section on the geometry of least squares was completely rewritten in Chapter 7, and a section on thegeometry of least squares in the overparameterized linear model was added to Chapter 12. Chapter 8 was revised to provide more motivation for hypothesis testing and simultaneous inference. A new section was added to Chapter 15dealing with two-way analysis-of-variance when there are empty cells.",
    "section was added to Chapter 15dealing with two-way analysis-of-variance when there are empty cells. This material is not available in any other textbook that we are aware of. This book would not have been possible without the patience, support, and encouragement of Al’s wife LaRue and Bruce’s wife Lois. Both have helped and sup- ported us in more ways than they know. This book is dedicated to them. A LVIN C. R ENCHER AND G.",
    "up- ported us in more ways than they know. This book is dedicated to them. A LVIN C. R ENCHER AND G. B RUCE SCHAALJE Department of Statistics Brigham Young University Provo, Utahxvi PREFACE --- Page 17 --- 1Introduction The scientiﬁc method is frequently used as a guided approach to learning. Linear statistical methods are widely used as part of this learning process.",
    "d approach to learning. Linear statistical methods are widely used as part of this learning process. In the biological, physical, and social sciences, as well as in business and engineering, linear models are useful in both the planning stages of research and analysis of the resulting data. In Sections 1.1–1.3, we give a brief introduction to simple and multiple linear regression models, and analysis-of-variance (ANOVA) models.",
    "troduction to simple and multiple linear regression models, and analysis-of-variance (ANOVA) models. 1.1 SIMPLE LINEAR REGRESSION MODEL In simple linear regression, we attempt to model the relationship between two vari- ables, for example, income and number of years of education, height and weight of people, length and width of envelopes, temperature and output of an industrialprocess, altitude and boiling point of water, or dose of a drug and response.",
    "output of an industrialprocess, altitude and boiling point of water, or dose of a drug and response. For a linear relationship, we can use a model of the form y¼ b0þb1xþ1, (1:1) where yis the dependent orresponse variable and xis the independent orpredictor variable. The random variable 1is the error term in the model. In this context, error does not mean mistake but is a statistical term representing random ﬂuctuations, measurement errors, or the effect of factors outside of our control.",
    "epresenting random ﬂuctuations, measurement errors, or the effect of factors outside of our control. The linearity of the model in (1.1) is an assumption. We typically add other assumptions about the distribution of the error terms, independence of the observedvalues of y, and so on. Using observed values of xandy, we estimate b0andb1and make inferences such as conﬁdence intervals and tests of hypotheses for b0andb1.",
    "estimate b0andb1and make inferences such as conﬁdence intervals and tests of hypotheses for b0andb1. We may also use the estimated model to forecast or predict the value of yfor a particular value of x, in which case a measure of predictive accuracy may also be of interest. Estimation and inferential procedures for the simple linear regression model are developed and illustrated in Chapter 6. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G.",
    "d illustrated in Chapter 6. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 1 --- Page 18 --- 1.2 MULTIPLE LINEAR REGRESSION MODEL The response yis often inﬂuenced by more than one predictor variable. For example, the yield of a crop may depend on the amount of nitrogen, potash, and phosphate fer- tilizers used.",
    "the yield of a crop may depend on the amount of nitrogen, potash, and phosphate fer- tilizers used. These variables are controlled by the experimenter, but the yield may also depend on uncontrollable variables such as those associated with weather. A linear model relating the response yto several predictors has the form y¼b0þb1x1þb2x2þ/C1/C1/C1þbkxkþ1: (1:2) The parameters b0,b1,... ,bkare called regression coefﬁcients .",
    "b0þb1x1þb2x2þ/C1/C1/C1þbkxkþ1: (1:2) The parameters b0,b1,... ,bkare called regression coefﬁcients . As in (1.1), 1 provides for random variation in ynot explained by the xvariables. This random variation may be due partly to other variables that affect ybut are not known or not observed. The model in (1.2) is linear in the bparameters; it is not necessarily linear in the x variables. Thus models such as y¼b0þb1x1þb2x2 1þb3x2þb4sinx2þ1 are included in the designation linear model .",
    ". Thus models such as y¼b0þb1x1þb2x2 1þb3x2þb4sinx2þ1 are included in the designation linear model . A model provides a theoretical framework for better understanding of a pheno- menon of interest. Thus a model is a mathematical construct that we believe may represent the mechanism that generated the observations at hand.",
    "tical construct that we believe may represent the mechanism that generated the observations at hand. The postulated model may be an idealized oversimpliﬁcation of the complex real-world situation, but in many such cases, empirical models provide useful approximations of the relationships among variables. These relationships may be either associative orcausative. Regression models such as (1.2) are used for various purposes, including the following: 1.Prediction .",
    "gression models such as (1.2) are used for various purposes, including the following: 1.Prediction . Estimates of the individual parameters b0,b1,... ,bkare of less importance for prediction than the overall inﬂuence of the xvariables on y. However, good estimates are needed to achieve good prediction performance. 2.Data Description or Explanation . The scientist or engineer uses the estimated model to summarize or describe the observed data. 3.Parameter Estimation .",
    "gineer uses the estimated model to summarize or describe the observed data. 3.Parameter Estimation . The values of the estimated parameters may have theoretical implications for a postulated model. 4.Variable Selection or Screening . The emphasis is on determining the import- ance of each predictor variable in modeling the variation in y. The predictors that are associated with an important amount of variation in yare retained; those that contribute little are deleted. 5.Control of Output .",
    "mount of variation in yare retained; those that contribute little are deleted. 5.Control of Output . A cause-and-effect relationship between yand the x variables is assumed. The estimated model might then be used to control the2 INTRODUCTION --- Page 19 --- output of a process by varying the inputs. By systematic experimentation, it may be possible to achieve the optimal output. There is a fundamental difference between purposes 1 and 5.",
    "possible to achieve the optimal output. There is a fundamental difference between purposes 1 and 5. For prediction, we need only assume that the same correlations that prevailed when the data were collected also continue in place when the predictions are to be made. Showing that there is asigniﬁcant relationship between yand the xvariables in (1.2) does not necessarily prove that the relationship is causal.",
    "hip between yand the xvariables in (1.2) does not necessarily prove that the relationship is causal. To establish causality in order to controloutput, the researcher must choose the values of the xvariables in the model and use randomization to avoid the effects of other possible variables unaccounted for. In other words, to ascertain the effect of the xvariables on ywhen the xvariables are changed, it is necessary to change them.",
    "in the effect of the xvariables on ywhen the xvariables are changed, it is necessary to change them. Estimation and inferential procedures that contribute to the ﬁve purposes listed above are discussed in Chapters 7–11. 1.3 ANALYSIS-OF-VARIANCE MODELS In analysis-of-variance (ANOVA) models, we are interested in comparing several populations or several conditions in a study. Analysis-of-variance models can beexpressed as linear models with restrictions on the xvalues.",
    "tudy. Analysis-of-variance models can beexpressed as linear models with restrictions on the xvalues. Typically the x’s are 0s or 1s. For example, suppose that a researcher wishes to compare the mean yield forfour types of catalyst in an industrial process. If nobservations are to be obtained for each catalyst, one model for the 4 nobservations can be expressed as y ij¼miþ1ij,i¼1,2,3,4,j¼1,2,... ,n, (1:3) wheremiis the mean corresponding to the ith catalyst.",
    "d as y ij¼miþ1ij,i¼1,2,3,4,j¼1,2,... ,n, (1:3) wheremiis the mean corresponding to the ith catalyst. A hypothesis of interest is H0:m1¼m2¼m3¼m4. The model in (1.3) can be expressed in the alternative form yij¼mþaiþ1ij,i¼1,2,3,4,j¼1,2,... ,n: (1:4) In this form, aiis the effect of the ith catalyst, and the hypothesis can be expressed as H0:a1¼a2¼a3¼a4.",
    "is form, aiis the effect of the ith catalyst, and the hypothesis can be expressed as H0:a1¼a2¼a3¼a4. Suppose that the researcher also wishes to compare the effects of three levels of temperature and that nobservations are taken at each of the 12 catalyst–temperature combinations. Then the model can be expressed as yijk¼mijþ1ijk¼mþaiþbjþgijþ1ijk (1:5) i¼1,2,3,4;j¼1,2,3;k¼1,2,...",
    "Then the model can be expressed as yijk¼mijþ1ijk¼mþaiþbjþgijþ1ijk (1:5) i¼1,2,3,4;j¼1,2,3;k¼1,2,... ,n, wheremijis the mean for the ijth catalyst–temperature combination, aiis the effect of theith catalyst, bjis the effect of the jth level of temperature, and gijis the interaction or joint effect of the ith catalyst and jth level of temperature.1.3 ANALYSIS-OF-VARIANCE MODELS 3 --- Page 20 --- In the examples leading to models (1.3)–(1.5), the researcher chooses the type of catalyst or level of temperature and thus applies different treatments to the objects or experimental units under study.",
    "temperature and thus applies different treatments to the objects or experimental units under study. In other settings, we compare the means of variables measured on natural groupings of units, for example, males and females or various geographic areas. Analysis-of-variance models can be treated as a special case of regression models, but it is more convenient to analyze them separately. This is done in Chapters 12–15.",
    "ession models, but it is more convenient to analyze them separately. This is done in Chapters 12–15. Related topics, such as analysis-of-covariance and mixed models, are covered in Chapters 16–17.4 INTRODUCTION --- Page 21 --- 2Matrix Algebra If we write a linear model such as (1.2) for each of nobservations in a dataset, the n resulting models can be expressed in a single compact matrix expression. Then the estimation and testing results can be more easily obtained using matrix theory.",
    "expression. Then the estimation and testing results can be more easily obtained using matrix theory. In the present chapter, we review the elements of matrix theory needed in the remainder of the book. Proofs that seem instructive are included or called for inthe problems. For other proofs, see Graybill (1969), Searle (1982), Harville (1997), Schott (1997), or any general text on matrix theory. We begin with some basic deﬁ- nitions in Section 2.1.",
    "(1997), or any general text on matrix theory. We begin with some basic deﬁ- nitions in Section 2.1. 2.1 MATRIX AND VECTOR NOTATION 2.1.1 Matrices, Vectors, and Scalars Amatrix is a rectangular or square array of numbers or variables. We use uppercase boldface letters to represent matrices. In this book, all elements of matrices will be real numbers or variables representing real numbers.",
    "In this book, all elements of matrices will be real numbers or variables representing real numbers. For example, the height (in inches) and weight (in pounds) for three students are listed in the following matrix: A¼65 154 73 18268 1670 @1A: (2:1) To represent the elements of Aas variables, we use A¼(a ij)¼a11a12 a21a22 a31a320 @1A: (2:2) The ﬁrst subscript in a ijindicates the row; the second identiﬁes the column. The nota- tionA¼(aij) represents a matrix by means of a typical element.",
    "econd identiﬁes the column. The nota- tionA¼(aij) represents a matrix by means of a typical element. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 5 --- Page 22 --- The matrix Ain (2.1) or (2.2) has three rows and two columns, and we say that Ais 3/C22, or that the sizeofAis 3/C22. Avector is a matrix with a single row or column.",
    "we say that Ais 3/C22, or that the sizeofAis 3/C22. Avector is a matrix with a single row or column. Elements in a vector are often identiﬁed by a single subscript; for example x¼x1 x2 x30 @1A: As a convention, we use lowercase boldface letters for column vectors and lowercase boldface letters followed by the prime symbol ( 0) for row vectors; for example x0¼(x1,x2,x3)¼(x1x2x3): (Row vectors are regarded as transposes of column vectors. The transpose is deﬁned in Section 2.1.3 below).",
    "ctors are regarded as transposes of column vectors. The transpose is deﬁned in Section 2.1.3 below). We use either commas or spaces to separate elements of arow vector. Geometrically, a row or column vector with pelements can be associated with a point in a p-dimensional space. The elements in the vector are the coordinates of the point.",
    "with a point in a p-dimensional space. The elements in the vector are the coordinates of the point. Sometimes we are interested in the distance from the origin to the point (vector), the distance between two points (vectors), or the angle between thearrows drawn from the origin to the two points. In the context of matrices and vectors, a single real number is called a scalar . Thus 2.5,29, and 7.26 are scalars.",
    "f matrices and vectors, a single real number is called a scalar . Thus 2.5,29, and 7.26 are scalars. A variable representing a scalar will be denoted by a lightface letter (usually lowercase), such as c. A scalar is technically distinct from a 1/C21 matrix in terms of its uses and properties in matrix algebra. The same notation is often used to represent a scalar and a 1 /C21 matrix, but the meaning is usually obvious from the context.",
    "used to represent a scalar and a 1 /C21 matrix, but the meaning is usually obvious from the context. 2.1.2 Matrix Equality Two matrices or two vectors are equal if they are of the same size and if the elements in corresponding positions are equal; for example 3/C024 137/C18/C19 ¼3/C024 137/C18/C19 , but 52 /C09 8/C046/C18/C19 =53 /C09 8/C046/C18/C19 :6 MATRIX ALGEBRA --- Page 23 --- 2.1.3 Transpose If we interchange the rows and columns of a matrix A, the resulting matrix is known as the transpose ofAand is denoted by A0; for example A¼6/C02 47 130 @1A, A 0¼641 /C0273/C18/C19 : Formally, if Ais denoted by A¼(aij), then A0is deﬁned as A0¼(aij)0¼(aji): (2:3) This notation indicates that the element in the ith row and jth column of Ais found in thejth row and ith column of A0.",
    "ates that the element in the ith row and jth column of Ais found in thejth row and ith column of A0. If the matrix Aisn/C2p,t h e n A0isp/C2n. If a matrix is transposed twice, the result is the original matrix. Theorem 2.1. IfAis any matrix, then (A0)0¼A: (2:4) PROOF. By (2.3), A0¼(aij)0¼(aji):Then (A0)0¼(aji)0¼(aij)¼A.",
    "1. IfAis any matrix, then (A0)0¼A: (2:4) PROOF. By (2.3), A0¼(aij)0¼(aji):Then (A0)0¼(aji)0¼(aij)¼A. A (The notation Ais used to indicate the end of a theorem proof, corollary proof or example.) 2.1.4 Matrices of Special Form If the transpose of a matrix Ais the same as the original matrix, that is, if A0¼Aor equivalently ( aji)¼(aij), then the matrix Ais said to be symmetric . For example A¼32 6 21 0 /C07 6/C0790 @1A is symmetric. Clearly, all symmetric matrices are square.",
    ". For example A¼32 6 21 0 /C07 6/C0790 @1A is symmetric. Clearly, all symmetric matrices are square. The diagonal of a p/C2psquare matrix A¼(aij) consists of the elements a11,a22,...,app.",
    "s are square. The diagonal of a p/C2psquare matrix A¼(aij) consists of the elements a11,a22,...,app. If a matrix contains zeros in all off-diagonal positions, it is said2.1 MATRIX AND VECTOR NOTATION 7 --- Page 24 --- to be a diagonal matrix ; for example, consider the matrix D¼80 0 0 0/C0300 00 0 0 00 0 40 BB@1 CCA, which can also be denoted as D¼diag(8 ,/C03,0,4): We also use the notation diag( A) to indicate a diagonal matrix with the same diagonal elements as A; for example A¼32 6 21 0 /C07 6/C0790 @1A,diag(A)¼300 01 00 0090 @1A: A diagonal matrix with a 1 in each diagonal position is called an identity matrix, and is denoted by I; for example I¼100 010 0010 @1A: (2:5) Anupper triangular matrix is a square matrix with zeros below the diagonal; for example, T¼723 /C05 00 /C026 004 1 000 80 BB@ 1 CCA: Alower triangular matrix is deﬁned similarly.",
    "r example, T¼723 /C05 00 /C026 004 1 000 80 BB@ 1 CCA: Alower triangular matrix is deﬁned similarly. A vector of 1s is denoted by j: j¼1 1 ...",
    "0 BB@ 1 CCA: Alower triangular matrix is deﬁned similarly. A vector of 1s is denoted by j: j¼1 1 ... 10 BBB@1 CCCA: (2:6)8 MATRIX ALGEBRA --- Page 25 --- A square matrix of 1s is denoted by J; for example J¼111 111 1110 @1 A: (2:7) We denote a vector of zeros by 0and a matrix of zeros by O; for example 0¼0 0 00 @1A, O¼0000 0000 00000 @1A: (2:8) 2.2 OPERATIONS We now deﬁne sums and products of matrices and vectors and consider some pro- perties of these sums and products.",
    "sums and products of matrices and vectors and consider some pro- perties of these sums and products. 2.2.1 Sum of Two Matrices or Two Vectors If two matrices or two vectors are the same size, they are said to be conformal for addition . Their sum is found by adding corresponding elements.",
    "e, they are said to be conformal for addition . Their sum is found by adding corresponding elements. Thus, if Ais n/C2pand Bisn/C2p, then C¼AþBis also n/C2pand is found as C¼(c ij)¼(aijþbij); for example 7/C034 28 /C05/C18/C19 þ11 5 /C06 342/C18/C19 ¼18 2 /C02 51 2 /C03/C18/C19 : Thedifference D¼A/C0Bbetween two conformal matrices AandBis deﬁned simi- larly: D¼(dij)¼(aij/C0bij). Two properties of matrix addition are given in the following theorem. Theorem 2.2a.",
    "ij)¼(aij/C0bij). Two properties of matrix addition are given in the following theorem. Theorem 2.2a. IfAandBare both n/C2m, then (i)AþB¼BþA: (2.9) (ii) (AþB)0¼A0þB0: (2.10) A2.2 OPERATIONS 9 --- Page 26 --- 2.2.2 Product of a Scalar and a Matrix Any scalar can be multiplied by any matrix. The product of a scalar and a matrix is deﬁned as the product of each element of the matrix and the scalar: cA¼(caij)¼ca11ca12/C1/C1/C1 ca1m ca21ca22/C1/C1/C1 ca2m .........",
    "ment of the matrix and the scalar: cA¼(caij)¼ca11ca12/C1/C1/C1 ca1m ca21ca22/C1/C1/C1 ca2m ......... can1can2/C1/C1/C1 canm0 BBB@1 CCCA: (2:11) Since ca ij¼aijc, the product of a scalar and a matrix is commutative: cA¼Ac: (2:12) 2.2.3 Product of Two Matrices or Two Vectors In order for the product ABto be deﬁned, the number of columns in Amust equal the number of rows in B, in which case AandBare said to be conformal for multipli- cation .",
    "st equal the number of rows in B, in which case AandBare said to be conformal for multipli- cation . Then the ( ij)th element of the product C¼ABis deﬁned as cij¼X kaikbkj, (2:13) which is the sum of products of the elements in the ith row of Aand the elements in thejth column of B. Thus we multiply every row of Aby every column of B.I fAis n/C2mandBism/C2p,t h e n C¼ABisn/C2p. We illustrate matrix multiplication in the following example. Example 2.2.3.",
    "2p,t h e n C¼ABisn/C2p. We illustrate matrix multiplication in the following example. Example 2.2.3. Let A¼213 465/C18/C19 and B¼14 26 380 @1A: Then AB¼2/C11þ1/C12þ3/C132 /C14þ1/C16þ3/C18 4/C11þ6/C12þ5/C134 /C14þ6/C16þ5/C1 8/C18/C19 ¼13 38 31 92/C18/C19 , BA¼18 25 2328 38 36 38 51 490 B@1 CA: A Note that a 1 /C21matrix Acan only be multiplied on the right by a 1 /C2nmatrix Bor on the left by an n/C21 matrix C, whereas a scalar can be multiplied on the right or left by a matrix of any size.10 MATRIX ALGEBRA --- Page 27 --- IfAisn/C2mandBism/C2p, where n=p, then ABis deﬁned, but BAis not deﬁned.",
    "RIX ALGEBRA --- Page 27 --- IfAisn/C2mandBism/C2p, where n=p, then ABis deﬁned, but BAis not deﬁned. If Aisn/C2pandBisp/C2n,t h e n ABisn/C2nandBAisp/C2p. In this case, of course, AB=BA, as illustrated in Example 2.2.3.",
    "/C2n,t h e n ABisn/C2nandBAisp/C2p. In this case, of course, AB=BA, as illustrated in Example 2.2.3. If AandBare both n/C2n, then ABandBAare the same size, but, in general AB=BA: (2:14) [There are a few exceptions to (2.14), for example, two diagonal matrices or a square matrix and an identity.] Thus matrix multiplication is not commutative, and certain familiar manipulations with real numbers cannot be done with matrices.",
    "not commutative, and certain familiar manipulations with real numbers cannot be done with matrices. However, matrix multiplication is distributive over addition or subtraction: A(B+C)¼AB+AC, (2:15) (A+B)C¼AC+BC: (2:16) Using (2.15) and (2.16), we can expand products such as ( A/C0B)(C/C0D): (A/C0B)(C/C0D)¼(A/C0B)C/C0(A/C0B)D [by (2 :15)] ¼AC/C0BC/C0ADþBD [by (2 :16)]: (2:17) Multiplication involving vectors follows the same rules as for matrices.",
    "0ADþBD [by (2 :16)]: (2:17) Multiplication involving vectors follows the same rules as for matrices. Suppose thatAisn/C2p,bisp/C21,cisp/C21, and disn/C21. Then Abis a column vector of size n/C21,d0Ais a row vector of size 1 /C2p,b0cis a sum of products (1 /C21), bc0is ap/C2pmatrix, and cd0is ap/C2nmatrix.",
    "vector of size 1 /C2p,b0cis a sum of products (1 /C21), bc0is ap/C2pmatrix, and cd0is ap/C2nmatrix. Since b0cis a 1 /C21 sum of products, it is equal to c0b: b0c¼b1c1þb2c2þ/C1/C1/C1þ bpcp, c0b¼c1b1þc2b2þ/C1/C1/C1þ cpbp, b0c¼c0b: (2:18) The matrix cd0is given by cd0¼c1d1c1d2/C1/C1/C1 c1dn c2d1c2d2/C1/C1/C1 c2dn .........",
    "0c¼c0b: (2:18) The matrix cd0is given by cd0¼c1d1c1d2/C1/C1/C1 c1dn c2d1c2d2/C1/C1/C1 c2dn ......... cpd1cpd2/C1/C1/C1 cpdn0 BBB@1 CCCA: (2:19)2.2 OPERATIONS 11 --- Page 28 --- Similarly b0b¼b2 1þb22þ/C1/C1/C1þ b2p, (2:20) bb0¼b2 1b1b2/C1/C1/C1 b1bp b2b1b22/C1/C1/C1 b2bp ......... bpb1bpb2/C1/C1/C1 b2 p0 BBBBB@1 CCCCCA: (2:21) Thus, b 0bis a sum of squares and bb0is a (symmetric) square matrix.",
    "C1 b2 p0 BBBBB@1 CCCCCA: (2:21) Thus, b 0bis a sum of squares and bb0is a (symmetric) square matrix. The square root of the sum of squares of the elements of a p/C21 vector bis the distance from the origin to the point band is also referred to as the length ofb: Length of b¼ﬃﬃﬃﬃﬃﬃﬃ b0bp ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Xp i¼1b2 is : (2:22) Ifjis an n/C21 vector of 1s as deﬁned in (2.6), then by (2.20) and (2.21), we have j0j¼n,jj0¼11 /C1/C1/C1 1 11 /C1/C1/C1 1 .........",
    "eﬁned in (2.6), then by (2.20) and (2.21), we have j0j¼n,jj0¼11 /C1/C1/C1 1 11 /C1/C1/C1 1 ......... 11 /C1/C1/C1 10 BBB@1 CCCA¼J, (2:23) where Jis an n/C2nsquare matrix of 1s as illustrated in (2.7). If aisn/C21 and Ais n/C2p, then a0j¼j0a¼Xn i¼1ai, (2:24) j0A¼/C18X iai1,X iai2,...,X iaip/C19 , Aj¼P ja1jP ja2j ... P janj0 BBBBB@1 CCCCCA: (2:25) Thus a 0jis the sum of the elements in a,j0Acontains the column sums of A, and Aj contains the row sums of A.",
    "is the sum of the elements in a,j0Acontains the column sums of A, and Aj contains the row sums of A. Note that in a0j, the vector jisn/C21; in j0A, the vector j isn/C21; and in Aj, the vector jisp/C21.12 MATRIX ALGEBRA --- Page 29 --- The transpose of the product of two matrices is the product of the transposes in reverse order. Theorem 2.2b. IfAisn/C2pandBisp/C2m, then (AB)0¼B0A0: (2:26) PROOF.LetC¼AB:Then by (2.13) C¼(cij)¼Xp k¼1aikbkj !",
    ". IfAisn/C2pandBisp/C2m, then (AB)0¼B0A0: (2:26) PROOF.LetC¼AB:Then by (2.13) C¼(cij)¼Xp k¼1aikbkj ! : By (2.3), the transpose of C¼ABbecomes (AB)0¼C0¼(cij)0¼(cji) ¼Xp k¼1ajkbki ! ¼Xp k¼1bkiajk !",
    "bkj ! : By (2.3), the transpose of C¼ABbecomes (AB)0¼C0¼(cij)0¼(cji) ¼Xp k¼1ajkbki ! ¼Xp k¼1bkiajk ! ¼B0A0: A We illustrate the steps in the proof of Theorem 2.2b using a 2 /C23 matrix Aand a 3/C22 matrix B: AB¼a11a12a13 a21a22a23/C18/C19 b11b12 b21b22 b31b320 B@1 CA ¼a11b11þa12b21þa13b31a11b12þa12b22þa13b32 a21b11þa22b21þa23b31a21b12þa22b22þa23b32/C18/C19 , (AB)0¼a11b11þa12b21þa13b31a21b11þa22b21þa23b31 a11b12þa12b22þa13b32a21b12þa22b22þa23b32/C18/C19 ¼b11a11þb21a12þb31a13b11a21þb21a22þb31a23 b12a11þb22a12þb32a13b12a21þb22a22þb32a23/C18/C19 ¼b11b21b31 b12b22b32/C18/C19 a11a21 a12a22 a13a230 B@1 CA ¼B0A0:2.2 OPERATIONS 13 --- Page 30 --- The following corollary to Theorem 2.2b gives the transpose of the product of three matrices.",
    "30 --- The following corollary to Theorem 2.2b gives the transpose of the product of three matrices. Corollary 1. IfA, B, and Care conformal so that ABC is deﬁned, then (ABC )0¼C0B0A0. A Suppose that Aisn/C2mandBism/C2p. Leta0 ibe the ithrowofAandbjbe the jth column ofB, so that A¼a01 a02 ... a0 n0 BBB@1 CCCA, B¼(b 1,b2,...,bp): Then, by deﬁnition, the ( ij)th element of ABisa0 ibj: AB¼a01b1a01b2/C1/C1/C1a01bp a02b1a02b2/C1/C1/C1a02bp .........",
    "n, the ( ij)th element of ABisa0 ibj: AB¼a01b1a01b2/C1/C1/C1a01bp a02b1a02b2/C1/C1/C1a02bp ......... a0 nb1a0nb2/C1/C1/C1a0nbp0 BBB@1 CCCA: This product can be written in terms of the rows of A: AB¼a 0 1(b1,b2,...,bp) a02(b1,b2,...,bp) ... a0 n(b1,b2,...,bp)0 BBB@1 CCCA¼a0 1B a02B ... a0 nB0 BBB@1 CCCA¼a0 1 a02 ... a0 n0 BBB@1 CCCAB: (2:27) The ﬁrst column of ABcan be expressed in terms of Aas a0 1b1 a02b1 ... a0 nb10 BBB@1 CCCA¼a0 1 a02 ...",
    "e ﬁrst column of ABcan be expressed in terms of Aas a0 1b1 a02b1 ... a0 nb10 BBB@1 CCCA¼a0 1 a02 ... a0 n0 BBB@1 CCCAb1¼Ab1: Likewise, the second column is Ab2, and so on. Thus ABcan be written in terms of the columns of B: AB¼A(b1,b2,...,bp)¼(Ab1,Ab2,...,Abp): (2:28)14 MATRIX ALGEBRA --- Page 31 --- Any matrix Acan be multiplied by its transpose to form A0AorAA0. Some pro- perties of these two products are given in the following theorem. Theorem 2.2c. LetAbe any n/C2pmatrix.",
    "ties of these two products are given in the following theorem. Theorem 2.2c. LetAbe any n/C2pmatrix. Then A0AandAA0have the following properties. (i)A0Aisp/C2pand its elements are products of the columns ofA. (ii)AA0isn/C2nand its elements are products of the rows ofA. (iii) Both A0AandAA0are symmetric. (iv) If A0A¼O, then A¼O. A LetAbe an n/C2nmatrix and let D¼diag( d1,d2,...,dn). In the product DA, the ith row of Ais multiplied by di, and in AD, the jth column of Ais multiplied by dj.",
    "product DA, the ith row of Ais multiplied by di, and in AD, the jth column of Ais multiplied by dj. For example, if n¼3, we have DA¼d100 0d20 00 d30 BBB@1 CCCAa11a12a13 a21a22a23 a31a32a330 BBB@1 CCCA ¼d1a11d1a12d1a13 d2a21d2a22d2a23 d3a31d3a32d3a330 BBB@1 CCCA, (2:29) AD¼a11a12a13 a21a22a23 a31a32a330 BBB@1 CCCAd100 0d20 00 d30 BBB@1 CCCA ¼d1a11d2a12d3a13 d1a21d2a22d3a23 d1a31d2a32d3a330 BBB@1 CCCA, (2:30) DAD ¼d 2 1a11 d1d2a12d1d3a13 d2d1a21 d2 2a22 d2d3a23 d3d1a31d3d2a32 d2 3a330 BBB@1 CCCA: (2:31)2.2 OPERATIONS 15 --- Page 32 --- Note that DA=AD.",
    "d2d3a23 d3d1a31d3d2a32 d2 3a330 BBB@1 CCCA: (2:31)2.2 OPERATIONS 15 --- Page 32 --- Note that DA=AD. However, in the special case where the diagonal matrix is the identity, (2.29) and (2.30) become IA¼AI¼A: (2:32) IfAis rectangular, (2.32) still holds, but the two identities are of different sizes.",
    "A¼AI¼A: (2:32) IfAis rectangular, (2.32) still holds, but the two identities are of different sizes. IfAis a symmetric matrix and yis a vector, the product y0Ay¼X iaiiy2 iþX i=jaijyiyj (2:33) is called a quadratic form .I fxisn/C21,yisp/C21, and Aisn/C2p, the product x0Ay¼X ijaijxiyj (2:34) is called a bilinear form . 2.2.4 Hadamard Product of Two Matrices or Two Vectors Sometimes a third type of product, called the elementwise orHadamard product , is useful.",
    "wo Vectors Sometimes a third type of product, called the elementwise orHadamard product , is useful. If two matrices or two vectors are of the same size (conformal for addition), the Hadamard product is found by simply multiplying corresponding elements: (aijbij)¼a11b11a12b12/C1/C1/C1 a1pb1p a21b21a22b22/C1/C1/C1 a2pb2p ......... an1bn1an2bn2/C1/C1/C1 anpbnp0 BBB@1 CCCA: 2.3 PARTITIONED MATRICES It is sometimes convenient to partition a matrix into submatrices.",
    "@1 CCCA: 2.3 PARTITIONED MATRICES It is sometimes convenient to partition a matrix into submatrices. For example, a par- titioning of a matrix Ainto four (square or rectangular) submatrices of appropriate sizes can be indicated symbolically as follows: A¼A 11A12 A21A22/C18/C19 :16 MATRIX ALGEBRA --- Page 33 --- To illustrate, let the 4 /C25 matrix Abe partitioned as A¼72 5 84 /C0340 27 93 6 5/C02 31 2 160 BBBBBBB@1 CCCCCCCA¼A 11A12 A21A22/C18/C19 , where A11¼725 /C0340/C18/C19 ,A12¼84 27/C18/C19 , A21¼936 312/C18/C19 ,A22¼5/C02 16/C18/C19 : If two matrices AandBare conformal for multiplication, and if AandBare parti- tioned so that the submatrices are appropriately conformal, then the product ABcan be found using the usual pattern of row by column multiplication with the subma- trices as if they were single elements; for example AB¼A11A12 A21A22/C18/C19B11B12 B21B22/C18/C19 ¼A11B11þA12B21A11B12þA12B22 A21B11þA22B21A21B12þA22B22/C18/C19 : (2:35) IfBis replaced by a vector bpartitioned into two sets of elements, and if Ais correspondingly partitioned into two sets of columns, then (2.35) becomes Ab¼(A1,A2)b1 b2/C18/C19 ¼A1b1þA2b2, (2:36) where the number of columns of A1is equal to the number of elements of b1, and A2 andb2are similarly conformal.",
    "umber of columns of A1is equal to the number of elements of b1, and A2 andb2are similarly conformal. Note that the partitioning in A¼(A1,A2) is indicated by a comma. The partitioned multiplication in (2.36) can be extended to individual columns of Aand individual elements of b: Ab¼(a1,a2,...,ap)b1 b2 ...",
    ") can be extended to individual columns of Aand individual elements of b: Ab¼(a1,a2,...,ap)b1 b2 ... bp0 BBB@1 CCCA¼b1a1þb2a2þ/C1/C1/C1þ bpap: (2:37)2.3 PARTITIONED MATRICES 17 --- Page 34 --- Thus Abis expressible as a linear combination of the columns of A, in which the coefﬁcients are elements of b. We illustrate (2.37) in the following example. Example 2.3.",
    "which the coefﬁcients are elements of b. We illustrate (2.37) in the following example. Example 2.3. Let A¼6/C023 21 0 43 20 @1A, b¼4 2 /C010 @1A: Then Ab¼17 10 200 @1A: Using a linear combination of columns of Aas in (2.37), we obtain Ab¼b 1a1þb2a2þb2a3 ¼46 2 40 B@1 CAþ2/C02 1 30 B@1 CA/C03 0 20 B@1 CA ¼24 8 160 B@1 CAþ/C04 260 B@1 CA/C03 020 B@1 CA¼17 10200 B@1 CA: A By (2.28) and (2.37), the columns of the product ABare linear combinations of the columns of A.",
    "A: A By (2.28) and (2.37), the columns of the product ABare linear combinations of the columns of A. The coefﬁcients for the jth column of ABare the elements of the jth column of B. The product of a row vector and a matrix, a 0B, can be expressed as a linear com- bination of the rows of B, in which the coefﬁcients are elements of a0: a0B¼(a1,a2,...,an)b0 1 b02 ...",
    "nation of the rows of B, in which the coefﬁcients are elements of a0: a0B¼(a1,a2,...,an)b0 1 b02 ... b0 n0 BBB@1 CCCA¼a1b0 1þa2b02þ/C1/C1/C1þ anb0n: (2:38) By (2.27) and (2.38), the rows of the matrix product ABare linear combinations of the rows of B.",
    "38) By (2.27) and (2.38), the rows of the matrix product ABare linear combinations of the rows of B. The coefﬁcients for the ith row of ABare the elements of the ith row of A.18 MATRIX ALGEBRA --- Page 35 --- Finally, we note that if a matrix Ais partitioned as A¼(A1,A2), then A0¼(A1,A2)0¼A0 1 A02/C18/C19 : (2:39) 2.4 RANK Before deﬁning the rank of a matrix, we ﬁrst introduce the notion of linear indepen- dence and dependence.",
    "deﬁning the rank of a matrix, we ﬁrst introduce the notion of linear indepen- dence and dependence. A set of vectors a1,a2,...,anis said to be linearly dependent if scalars c1,c2,...,cn(not all zero) can be found such that c1a1þc2a2þ/C1/C1/C1þ cnan¼0: (2:40) If no coefﬁcients c1,c2,...,cncan be found that satisfy (2.40), the set of vectors a1,a2,...,anis said to be linearly independent . By (2.37) this can be restated as follows. The columns of Aare linearly independent if Ac¼0implies c¼0.",
    "(2.37) this can be restated as follows. The columns of Aare linearly independent if Ac¼0implies c¼0. (If a set of vectors includes 0, the set is linearly dependent.) If (2.40) holds, then at least one of the vectors aican be expressed as a linear combination of the other vectors in the set. Among linearly independent vectors there is no redundancy of this type.",
    "he other vectors in the set. Among linearly independent vectors there is no redundancy of this type. Therank of any square or rectangular matrix Ais deﬁned as rank(A)¼number of linearly independent columns of A ¼number of linearly independent rows of A: It can be shown that the number of linearly independent columns of any matrix is always equal to the number of linearly independent rows. If a matrix Ahas a single nonzero element, with all other elements equal to 0, then rank(A)¼1.",
    "rows. If a matrix Ahas a single nonzero element, with all other elements equal to 0, then rank(A)¼1. The vector 0and the matrix Ohave rank 0. Suppose that a rectangular matrix Aisn/C2pof rank p, where p,n. (We typically shorten this statement to “ Aisn/C2pof rank p,n.”) Then Ahas maximum possible rank and is said to be of full rank . In general, the maximum possible rank of an n/C2p matrix Ais min( n, p). Thus, in a rectangular matrix, the rows or columns (or both) are linearly dependent.",
    "Ais min( n, p). Thus, in a rectangular matrix, the rows or columns (or both) are linearly dependent. We illustrate this in the following example. Example 2.4a. The rank of A¼1/C023 52 4/C18/C192.4 RANK 19 --- Page 36 --- is 2 because the two rows are linearly independent (neither row is a multiple of the other). Hence, by the deﬁnition of rank, the number of linearly independent columns is also 2.",
    "f the other). Hence, by the deﬁnition of rank, the number of linearly independent columns is also 2. Therefore, the columns are linearly dependent, and by (2.40) there existconstants c 1,c2, and c3such that c11 5/C18/C19 þc2/C022/C18/C19 þc 334/C18/C19 ¼00/C18/C19 : (2:41) By (2.37), we can write (2.41) in the form 1/C023 52 4/C18/C19 c 1 c2 c30 @1A¼0 0/C18/C19 orAc¼0: (2:42) The solution to (2.42) is given by any multiple of c¼(14,/C011,/C012) 0.",
    "¼0 0/C18/C19 orAc¼0: (2:42) The solution to (2.42) is given by any multiple of c¼(14,/C011,/C012) 0. In this case, the product Acis equal to 0, even though A=Oandc=0. This is possible because of the linear dependence of the column vectors of A. A We can extend (2.42) to products of matrices.",
    "of the linear dependence of the column vectors of A. A We can extend (2.42) to products of matrices. It is possible to ﬁnd A=Oand B=Osuch that AB¼O; (2:43) for example 12 24/C18/C19 26 /C01/C03/C18/C19 ¼0000/C18/C19 : We can also exploit the linear dependence of rows or columns of a matrix to create expressions such as AB¼CB, where A=C. Thus in a matrix equation, we cannot, in general, cancel a matrix from both sides of the equation.",
    ". Thus in a matrix equation, we cannot, in general, cancel a matrix from both sides of the equation. There are two exceptions to this rule: (1) if Bis a full-rank square matrix, then AB¼CBimplies A¼C; (2) the other special case occurs when the expression holds for all possible values of the matrix common to both sides of the equation; for example ifAx¼Bxfor all possible values of x, (2:44) thenA¼B. To see this, let x¼(1,0,...,0) 0. Then, by (2.37) the ﬁrst column of A equals the ﬁrst column of B.",
    "To see this, let x¼(1,0,...,0) 0. Then, by (2.37) the ﬁrst column of A equals the ﬁrst column of B. Now let x¼(0,1,0,...,0)0, and the second column of Aequals the second column of B. Continuing in this fashion, we obtain A¼B.20 MATRIX ALGEBRA --- Page 37 --- Example 2.4b. We illustrate the existence of matrices A,B, and Csuch that AB¼CB, where A=C.",
    "e 37 --- Example 2.4b. We illustrate the existence of matrices A,B, and Csuch that AB¼CB, where A=C. Let A¼13 2 20 /C01/C18/C19 ,B¼1201 100 @1A,C¼211 5/C06/C04/C18/C19 : Then AB¼CB¼35 14/C18/C19 :A The following theorem gives a general case and two special cases for the rank of a product of two matrices. Theorem 2.4 (i) If the matrices AandBare conformal for multiplication, then rank( AB)/C20 rank( A) and rank( AB)/C20rank( B).",
    "trices AandBare conformal for multiplication, then rank( AB)/C20 rank( A) and rank( AB)/C20rank( B). (ii) Multiplication by a full–rank square matrix does not change the rank; that is, ifBandCare full–rank square matrices, rank( AB)¼rank(CA)¼rank(A). (iii) For any matrix A, rank( A 0A)¼rank(AA0)¼rank(A0)¼rank(A). PROOF (i) All the columns of ABare linear combinations of the columns of A(see a comment following Example 2.3).",
    "l the columns of ABare linear combinations of the columns of A(see a comment following Example 2.3). Consequently, the number of linearly independent columns of ABis less than or equal to the number of linearly independent columns of A, and rank( AB)/C20rank(A). Similarly, all the rows of ABare linear combinations of the rows of B[see a comment follow- ing (2.38)], and therefore rank( AB)/C20rank(B). (ii) This will be proved later. (iii) This will also be proved later.",
    "herefore rank( AB)/C20rank(B). (ii) This will be proved later. (iii) This will also be proved later. A 2.5 INVERSE A full-rank square matrix is said to be nonsingular . A nonsingular matrix Ahas a unique inverse , denoted by A/C01, with the property that AA/C01¼A/C01A¼I: (2:45)2.5 INVERSE 21 --- Page 38 --- IfAis square and less than full rank, then it does not have an inverse and is said to be singular . Note that full-rank rectangular matrices do not have inverses as in (2.45).",
    "is said to be singular . Note that full-rank rectangular matrices do not have inverses as in (2.45). From the deﬁnition in (2.45), it is clear that Ais the inverse of A/C01: (A/C01)/C01¼A: (2:46) Example 2.5. Let A¼47 26/C18/C19 : Then A/C01¼:6/C0:7 /C0:2 :4/C18/C19 and 47 26/C18/C19 :6/C0:7 /C0:2 :4/C18/C19 ¼:6/C0:7 /C0:2 :4/C18/C19 47 26/C18/C19 ¼1001/C18/C19 : A We can now prove Theorem 2.4(ii).",
    "/C18/C19 ¼:6/C0:7 /C0:2 :4/C18/C19 47 26/C18/C19 ¼1001/C18/C19 : A We can now prove Theorem 2.4(ii). P ROOF.I fBis a full-rank square (nonsingular) matrix, there exists a matrix B/C01such thatBB/C01¼I. Then, by Theorem 2.4(i), we have rank(A)¼rank(ABB/C01)/C20rank(AB)/C20rank(A): Thus both inequalities become equalities, and rank( A)¼rank( AB). Similarly, rank( A)¼rank( CA) for Cnonsingular. A In applications, inverses are typically found by computer. Many calculators also compute inverses.",
    "A In applications, inverses are typically found by computer. Many calculators also compute inverses. Algorithms for hand calculation of inverses of small matrices can be found in texts on matrix algebra. IfBis nonsingular and AB¼CB, then we can multiply on the right by B/C01 to obtain A¼C.",
    "trix algebra. IfBis nonsingular and AB¼CB, then we can multiply on the right by B/C01 to obtain A¼C. (If Bis singular or rectangular, we can’t cancel it from both sides of AB¼CB; see Example 2.4b and the paragraph preceding the example.) Similarly, if Ais nonsingular, the system of equations Ax¼chas the unique solution x¼A/C01c, (2:47)22 MATRIX ALGEBRA --- Page 39 --- since we can multiply on the left by A/C01to obtain A/C01Ax¼A/C01c Ix¼A/C01c: Two properties of inverses are given in the next two theorems.",
    "01to obtain A/C01Ax¼A/C01c Ix¼A/C01c: Two properties of inverses are given in the next two theorems. Theorem 2.5a. IfAis nonsingular, then A0is nonsingular and its inverse can be found as (A0)/C01¼(A/C01)0: (2:48) A Theorem 2.5b. IfAandBare nonsingular matrices of the same size, then ABis nonsingular and (AB)/C01¼B/C01A/C01: (2:49) A We now give the inverses of some special matrices.",
    "Bis nonsingular and (AB)/C01¼B/C01A/C01: (2:49) A We now give the inverses of some special matrices. If Ais symmetric and nonsin- gular and is partitioned as A¼A11A12 A21A22/C18/C19 , and if B¼A22/C0A21A/C01 11A12, then, provided A/C01 11andB/C01exist, the inverse of Ais given by A/C01¼A/C01 11þA/C01 11A12B/C01A21A/C01 11/C0A/C01 11A12B/C01 /C0B/C01A21A/C01 11 B/C01/C18/C19 : (2:50) As a special case of (2.50), consider the symmetric nonsingular matrix A¼A11a12 a0 12a22/C18/C19 , in which A11is square, a22is a 1 /C21 matrix, and a12is a vector.",
    "matrix A¼A11a12 a0 12a22/C18/C19 , in which A11is square, a22is a 1 /C21 matrix, and a12is a vector. Then if A/C01 11exists, A/C01can be expressed as A/C01¼1 bbA/C01 11þA/C01 11a12a0 12A/C01 11/C0A/C01 11a12 /C0a012A/C01 11 1/C18/C19 , (2:51)2.5 INVERSE 23 --- Page 40 --- where b¼a22/C0a0 12A/C01 11a12.",
    "a12 /C0a012A/C01 11 1/C18/C19 , (2:51)2.5 INVERSE 23 --- Page 40 --- where b¼a22/C0a0 12A/C01 11a12. As another special case of (2.50), we have A11O OA 22/C18/C19/C01 ¼A/C01 11 O OA/C01 22/C18/C19 : (2:52) If a square matrix of the form Bþcc0is nonsingular, where cis a vector and Bis a nonsingular matrix, then (Bþcc0)/C01¼B/C01/C0B/C01cc0B/C01 1þc0B/C01c: (2:53) In more generality, if A,B, and AþPBQ are nonsingular, then (AþPBQ )/C01¼A/C01/C0A/C01PB(BþBQA/C01PB)/C01BQA/C01: (2:54) Both (2.53) and (2.54) can be easily veriﬁed (Problems 2.33 and 2.34).",
    "BQA/C01PB)/C01BQA/C01: (2:54) Both (2.53) and (2.54) can be easily veriﬁed (Problems 2.33 and 2.34). 2.6 POSITIVE DEFINITE MATRICES Quadratic forms were introduced in (2.33).",
    "(Problems 2.33 and 2.34). 2.6 POSITIVE DEFINITE MATRICES Quadratic forms were introduced in (2.33). For example, the quadratic form 3y2 1þy22þ2y23þ4y1y2þ5y1y3/C06y2y3can be expressed as 3y2 1þy22þ2y23þ4y1y2þ5y1y3/C06y2y3¼y0Ay, where y¼y1 y2 y30 @1A, A¼34 5 01 /C06 00 20 @1A: However, the same quadratic form can also be expressed in terms of the symmetric matrix 1 2(AþA0)¼325 2 21 /C03 5 2/C0320 @1A:24 MATRIX ALGEBRA --- Page 41 --- In general, any quadratic form y0Aycan be expressed as y0Ay¼y0AþA0 2/C18/C19 y, (2:55) and thus the matrix of a quadratic form can always be chosen to be symmetric (and thereby unique).",
    ") and thus the matrix of a quadratic form can always be chosen to be symmetric (and thereby unique). The sums of squares we will encounter in regression (Chapters 6–11) and analysis–of–variance (Chapters 12–15) can be expressed in the form y0Ay, where yis an observation vector. Such quadratic forms remain positive (or at least nonne- gative) for all possible values of y. We now consider quadratic forms of this type.",
    "at least nonne- gative) for all possible values of y. We now consider quadratic forms of this type. If the symmetric matrix Ahas the property y0Ay.0 for all possible yexcept y¼0, then the quadratic form y0Ayis said to be positive deﬁnite , and Ais said to be a positive deﬁnite matrix. Similarly, if y0Ay/C210 for all yand there is at least oney=0such that y0Ay¼0, then y0AyandAare said to be positive semideﬁnite . Both types of matrices are illustrated in the following example. Example 2.6.",
    "positive semideﬁnite . Both types of matrices are illustrated in the following example. Example 2.6. To illustrate a positive deﬁnite matrix, consider A¼2/C01 /C013/C18/C19 and the associated quadratic form y0Ay¼2y2 1/C02y1y2þ3y22¼2(y1/C01 2y2)2þ52y2 2, which is clearly positive as long as y1andy2are not both zero.",
    "1/C02y1y2þ3y22¼2(y1/C01 2y2)2þ52y2 2, which is clearly positive as long as y1andy2are not both zero. To illustrate a positive semideﬁnite matrix, consider (2y1/C0y2)2þ(3y1/C0y3)2þ(3y2/C02y3)2, which can be expressed as y0Ay, with A¼13/C02/C03 /C021 0 /C06 /C03/C0650 @1 A: If 2 y1¼y2,3y1¼y3, and 3 y2¼2y3, then (2 y1/C0y2)2þ(3y1/C0y3)2þ (3y2/C02y3)2¼0. Thus y0Ay¼0 for any multiple of y¼(1,2,3)0. Otherwise y0Ay>0 (except for y¼0).",
    "y3)2þ (3y2/C02y3)2¼0. Thus y0Ay¼0 for any multiple of y¼(1,2,3)0. Otherwise y0Ay>0 (except for y¼0). A2.6 POSITIVE DEFINITE MATRICES 25 --- Page 42 --- In the matrices in Example 2.6, the diagonal elements are positive. For positive deﬁnite matrices, this is true in general. Theorem 2.6a (i) If Ais positive deﬁnite, then all its diagonal elements aiiare positive. (ii) If Ais positive semideﬁnite, then all aii/C210.",
    "then all its diagonal elements aiiare positive. (ii) If Ais positive semideﬁnite, then all aii/C210. PROOF (i) Let y0¼(0,...,0,1,0,...,0) with a 1 in the ith position and 0’s elsewhere. Then y0Ay¼aii.0. (ii) Let y0¼(0,...,0,1,0,...,0) with a 1 in the ith position and 0’s elsewhere. Then y0Ay¼aii/C210. A Some additional properties of positive deﬁnite and positive semideﬁnite matrices are given in the following theorems. Theorem 2.6b. LetPbe a nonsingular matrix.",
    "semideﬁnite matrices are given in the following theorems. Theorem 2.6b. LetPbe a nonsingular matrix. (i) If Ais positive deﬁnite, then P0APis positive deﬁnite. (ii) If Ais positive semideﬁnite, then P0APis positive semideﬁnite. PROOF (i) To show that y0P0APy.0 for y=0, note that y0(P0AP)y¼(Py)0A(Py). Since Ais positive deﬁnite, ( Py)0A(Py).0 provided that Py=0.B y (2.47), Py¼0only if y¼0, since P/C01Py¼P/C010¼0. Thus y0P0APy.0i fy=0. (ii) See problem 2.36. A Corollary 1.",
    "Py¼0only if y¼0, since P/C01Py¼P/C010¼0. Thus y0P0APy.0i fy=0. (ii) See problem 2.36. A Corollary 1. LetAbe ap/C2ppositive deﬁnite matrix and let Bbe ak/C2pmatrix of rank k/C20p. Then BAB0is positive deﬁnite. A Corollary 2. LetAbe a p/C2ppositive deﬁnite matrix and let Bbe a k/C2pmatrix. Ifk.por if rank( B)¼r, where r,kand r,p, then BAB0is positive semideﬁnite. A Theorem 2.6c.",
    "2pmatrix. Ifk.por if rank( B)¼r, where r,kand r,p, then BAB0is positive semideﬁnite. A Theorem 2.6c. A symmetric matrix Ais positive deﬁnite if and only if there exists a nonsingular matrix Psuch that A¼P0P.26 MATRIX ALGEBRA --- Page 43 --- PROOF. We prove the “if” part only. Suppose A¼P0Pfor nonsingular P. Then y0Ay¼y0P0Py¼(Py)0(Py): This is a sum of squares [see (2.20)] and is positive unless Py¼0. By (2.47), Py¼0 only if y¼0. A Corollary 1. A positive deﬁnite matrix is nonsingular.",
    "e unless Py¼0. By (2.47), Py¼0 only if y¼0. A Corollary 1. A positive deﬁnite matrix is nonsingular. A One method of factoring a positive deﬁnite matrix Ainto a product P0Pas in Theorem 2.6c is provided by the Cholesky decomposition (Seber and Lee 2003, pp. 335–337), by which Acan be factored uniquely into A¼T0T, where Tis a non- singular upper triangular matrix. For any square or rectangular matrix B, the matrix B0Bis positive deﬁnite or posi- tive semideﬁnite. Theorem 2.6d.",
    "or rectangular matrix B, the matrix B0Bis positive deﬁnite or posi- tive semideﬁnite. Theorem 2.6d. LetBbe an n/C2pmatrix. (i) If rank( B)¼p, then B0Bis positive deﬁnite. (ii) If rank( B),p,t h e n B0Bis positive semideﬁnite. PROOF (i) To show that y0B0By.0 for y=0, we note that y0B0By¼(By)0(By), which is a sum of squares and is thereby positive unless By¼0.",
    "r y=0, we note that y0B0By¼(By)0(By), which is a sum of squares and is thereby positive unless By¼0. By (2.37), we can express Byin the form By¼y1b1þy2b2þ/C1/C1/C1þ ypbp: This linear combination is not 0(for any y=0) because rank( B)¼p, and the columns of Bare therefore linearly independent [see (2.40)]. (ii) If rank( B),p, then we can ﬁnd y=0such that By¼y1b1þy2b2þ/C1/C1/C1þ ypbp¼0 since the columns of Bare linearly dependent [see (2.40)]. Hence y0B0By/C210.",
    "y2b2þ/C1/C1/C1þ ypbp¼0 since the columns of Bare linearly dependent [see (2.40)]. Hence y0B0By/C210. A2.6 POSITIVE DEFINITE MATRICES 27 --- Page 44 --- Note that if Bis a square matrix, the matrix BB¼B2is not necessarily positive semideﬁnite. For example, let B¼1/C02 1/C02/C18/C19 : Then B2¼/C012 /C012/C18/C19 ,B0B¼2/C04 /C048/C18/C19 : In this case, B2is not positive semideﬁnite, but B0Bis positive semideﬁnite, since y0B0By¼2(y1/C02y2)2.",
    "this case, B2is not positive semideﬁnite, but B0Bis positive semideﬁnite, since y0B0By¼2(y1/C02y2)2. Two additional properties of positive deﬁnite matrices are given in the following theorems. Theorem 2.6e. IfAis positive deﬁnite, then A21is positive deﬁnite. PROOF. By Theorem 2.6c, A¼P0P, where Pis nonsingular. By Theorems 2.5a and 2.5b, A/C01¼(P0P)/C01¼P/C01(P0)/C01¼P/C01(P/C01)0, which is positive deﬁnite by Theorem 2.6c. A Theorem 2.6f.",
    "01¼(P0P)/C01¼P/C01(P0)/C01¼P/C01(P/C01)0, which is positive deﬁnite by Theorem 2.6c. A Theorem 2.6f. IfAis positive deﬁnite and is partitioned in the form A¼A11A12 A21A22/C18/C19 , where A11andA22are square, then A11andA22are positive deﬁnite. PROOF. We can write A11, for example, as A11¼(I,O)AI O/C18/C19 , where Iis the same size as A11. Then by Corollary 1 to Theorem 2.6b, A11is positive deﬁnite.",
    "8/C19 , where Iis the same size as A11. Then by Corollary 1 to Theorem 2.6b, A11is positive deﬁnite. A 2.7 SYSTEMS OF EQUATIONS The system of n(linear) equations in punknowns a11x1þa12x2þ/C1/C1/C1þ a1pxp¼c1 a21x1þa22x2þ/C1/C1/C1þ a2pxp¼c2 ... an1x1þan2x2þ/C1/C1/C1þ anpxp¼cn (2:56)28 MATRIX ALGEBRA --- Page 45 --- can be written in matrix form as Ax¼c, (2:57) where Aisn/C2p,xisp/C21, and cisn/C21. Note that if n=p,xandcare of differ- ent sizes.",
    "Ax¼c, (2:57) where Aisn/C2p,xisp/C21, and cisn/C21. Note that if n=p,xandcare of differ- ent sizes. If n¼pandAis nonsingular, then by (2.47), there exists a unique solution vector xobtained as x¼A/C01c.I fn.p, so that Ahas more rows than columns, then Ax¼ctypically has no solution. If n,p, so that Ahas fewer rows than columns, thenAx¼ctypically has an inﬁnite number of solutions. If the system of equations Ax¼chas one or more solution vectors, it is said to be consistent .",
    "ions. If the system of equations Ax¼chas one or more solution vectors, it is said to be consistent . If the system has no solution, it is said to be inconsistent . To illustrate the structure of a consistent system of equations Ax¼c, suppose that Aisp/C2pof rank r,p.",
    "illustrate the structure of a consistent system of equations Ax¼c, suppose that Aisp/C2pof rank r,p. Then the rows of Aare linearly dependent, and there exists some bsuch that [see (2.38)] b0A¼b1a0 1þb2a02þ/C1/C1/C1þ bpa0p¼00: Then we must also have b0c¼b1c1þb2c2þ/C1/C1/C1þ bpcp¼0, since multiplication of Ax¼cbyb0gives b0Ax¼b0c,o r00x¼b0c. Otherwise, if b0c=0, there is no x such that Ax¼c.",
    "tiplication of Ax¼cbyb0gives b0Ax¼b0c,o r00x¼b0c. Otherwise, if b0c=0, there is no x such that Ax¼c. Hence, in order for Ax¼cto be consistent, the same linear relationships, if any, that exist among the rows of Amust exist among the elements (rows) of c. This is formalized by comparing the rank of Awith the rank of the aug- mented matrix (A,c). The notation ( A,c) indicates that chas been appended to Aas an additional column.",
    "ted matrix (A,c). The notation ( A,c) indicates that chas been appended to Aas an additional column. Theorem 2.7 The system of equations Ax¼chas at least one solution vector xif and only if rank( A)¼rank( A,c). PROOF. Suppose that rank( A)¼rank( A,c), so that appending cdoes not change the rank. Then cis a linear combination of the columns of A; that is, there exists some x such that x1a1þx2a2þ/C1/C1/C1þ xpap¼c, which, by (2.37), can be written as Ax¼c:Thus xis a solution.",
    "such that x1a1þx2a2þ/C1/C1/C1þ xpap¼c, which, by (2.37), can be written as Ax¼c:Thus xis a solution. Conversely, suppose that there exists a solution vector xsuch that Ax¼c.I n general, rank ( A)/C20rank(A,c) (Harville 1997, p. 41). But since there exists an x such that Ax¼c,w eh a v e rank(A,c)¼rank(A,Ax)¼rank[A(I,x)] /C20rank(A) [by Theorem 2 :4(i)]:2.7 SYSTEMS OF EQUATIONS 29 --- Page 46 --- Hence rank(A)/C20rank(A,c)/C20rank(A), and we have rank( A)¼rank( A,c).",
    "EQUATIONS 29 --- Page 46 --- Hence rank(A)/C20rank(A,c)/C20rank(A), and we have rank( A)¼rank( A,c). A A consistent system of equations can be solved by the usual methods given in elementary algebra courses for eliminating variables, such as adding a multiple of one equation to another or solving for a variable and substituting into another equation. In the process, one or more variables may end up as arbitrary constants, thus generating an inﬁnite number of solutions.",
    "or more variables may end up as arbitrary constants, thus generating an inﬁnite number of solutions. A method of solution involving gen- eralized inverses is given in Section 2.8.2. Some illustrations of systems of equationsand their solutions are given in the following examples. Example 2.7a.",
    "ations of systems of equationsand their solutions are given in the following examples. Example 2.7a. Consider the system of equations x 1þ2x2¼4 x1/C0x2¼1 x1þx2¼3 or 12 1/C01 110 @1 Ax1 x2/C18/C19 ¼4 1 30 @1A: The augmented matrix is (A,c)¼12 4 1/C011 11 30 @1A, which has rank ¼2 because the third column is equal to twice the ﬁrst column plus the second: 21 1 10 @1Aþ2 /C01 10 @1A¼4 1 30 @1A: Since rank( A)¼rank( A,c)¼2, there is at least one solution.",
    ": 21 1 10 @1Aþ2 /C01 10 @1A¼4 1 30 @1A: Since rank( A)¼rank( A,c)¼2, there is at least one solution. If we add twice the ﬁrst equation to the second, the result is a multiple of the third equation. Thus the third equation is redundant, and the ﬁrst two can readily be solved to obtain the uniquesolution x¼(2, 1) 0.30 MATRIX ALGEBRA --- Page 47 --- The three lines representing the three equations are plotted in Figure 2.1.",
    "ALGEBRA --- Page 47 --- The three lines representing the three equations are plotted in Figure 2.1. Notice that the three lines intersect at the point (2, 1), which is the unique solution of the three equations. A Example 2.7b. If we change the 3 to 2 in the third equation in Example 2.7, the aug- mented matrix becomes (A,c)¼12 4 1/C011 11 20 @1A, which has rank ¼3, since no linear combination of columns is 0.",
    "becomes (A,c)¼12 4 1/C011 11 20 @1A, which has rank ¼3, since no linear combination of columns is 0. [Alternatively, j(A,c)j=0, and ( A,c) is nonsingular; see Theorem 2.9(iii)] Hence rank ( A,c)¼ 3=rank(A)¼2, and the system is inconsistent. The three lines representing the three equations are plotted in Figure 2.2, in which we see that the three lines do not have a common point of intersection.",
    "tted in Figure 2.2, in which we see that the three lines do not have a common point of intersection. [For the “best” approximate solution, one approach is to use least squares; that is, we ﬁnd the values ofx 1andx2that minimize ( x1þ2x2/C04)2þ(x1/C0x2/C01)2þ(x1þx2/C02)2.]A Example 2.7c.",
    "e ﬁnd the values ofx 1andx2that minimize ( x1þ2x2/C04)2þ(x1/C0x2/C01)2þ(x1þx2/C02)2.]A Example 2.7c. Consider the system x1þx2þx3¼1 2x1þx2þ3x3¼5 3x1þ2x2þ4x3¼6:Figure 2.1 Three lines representing the three equations in Example 2.7a.2.7 SYSTEMS OF EQUATIONS 31 --- Page 48 --- The third equation is the sum of the ﬁrst two, but the second is not a multiple of the ﬁrst. Thus, rank( A,c)¼rank(A)¼2, and the system is consistent.",
    "the second is not a multiple of the ﬁrst. Thus, rank( A,c)¼rank(A)¼2, and the system is consistent. By solving the ﬁrst two equations for x1andx2in terms of x3, we obtain x1¼/C02x3þ4 x2¼x3/C03: The solution vector can be expressed as x¼/C02x3þ4 x3/C03 x30 @1A¼x 3/C02 1 10 @1Aþ4 /C03 00 @1A, where x 3is an arbitrary constant. Geometrically, xis the line representing the inter- section of the two planes corresponding to the ﬁrst two equations.",
    "the line representing the inter- section of the two planes corresponding to the ﬁrst two equations. A 2.8 GENERALIZED INVERSE We now consider generalized inverses of those matrices that do not have inverses in the usual sense [see (2.45)].",
    "er generalized inverses of those matrices that do not have inverses in the usual sense [see (2.45)]. A solution of a consistent system of equations Ax¼c can be expressed in terms of a generalized inverse of A.Figure 2.2 Three lines representing the three equations in Example 2.7b.32 MATRIX ALGEBRA --- Page 49 --- 2.8.1 Deﬁnition and Properties Ageneralized inverse of an n/C2pmatrix Ais any matrix A2that satisﬁes AA/C0A¼A: (2:58) A generalized inverse is not unique except when Ais nonsingular, in which case A/C0¼A/C01.",
    "A: (2:58) A generalized inverse is not unique except when Ais nonsingular, in which case A/C0¼A/C01. A generalized inverse is also called a conditional inverse . Every matrix, whether square or rectangular, has a generalized inverse. This holds even for vectors. For example, let x¼1 2 3 40 BB@1 CCA: Then x /C0 1¼(1,0,0,0) is a generalized inverse of xsatisfying (2.58). Other examples arex/C02¼(0,1 2,0,0),x/C0 3¼(0,0,1 3,0),andx/C0 4¼(0,0,0,1 4).",
    "of xsatisfying (2.58). Other examples arex/C02¼(0,1 2,0,0),x/C0 3¼(0,0,1 3,0),andx/C0 4¼(0,0,0,1 4). For each x/C0 i,w eh a v e xx/C0 ix¼x1¼x,i¼1,2,3,4: In this illustration, xis a column vector and x/C0 iis a row vector. This pattern is generalized in the following theorem. Theorem 2.8a. IfAisn/C2p, any generalized inverse A2isp/C2n. A In the following example we give two illustrations of generalized inverses of a singular matrix. Example 2.8.1.",
    "owing example we give two illustrations of generalized inverses of a singular matrix. Example 2.8.1. Let A¼223 101 3240 @1A: (2:59) The third row of Ais the sum of the ﬁrst two rows, and the second row is not a mul- tiple of the ﬁrst; hence Ahas rank 2. Let A /C0 1¼01 0 1 2/C010 00 00 @1A,A /C0 2¼01 0 0/C03 212 00 00 @1A: (2:60) It is easily veriﬁed that AA /C0 1A¼AandAA/C02A¼A.",
    "0 00 @1A,A /C0 2¼01 0 0/C03 212 00 00 @1A: (2:60) It is easily veriﬁed that AA /C0 1A¼AandAA/C02A¼A. A2.8 GENERALIZED INVERSE 33 --- Page 50 --- The methods used to obtain A/C0 1andA/C02in (2.60) are described in Theorem 2.8b and the ﬁve-step algorithm following the theorem. Theorem 2.8b. Suppose Aisn/C2pof rank rand that Ais partitioned as A¼A11A12 A21A22/C18/C19 ; where A11isr/C2rof rank r.",
    "se Aisn/C2pof rank rand that Ais partitioned as A¼A11A12 A21A22/C18/C19 ; where A11isr/C2rof rank r. Then a generalized inverse of Ais given by A/C0¼A/C01 11O OO/C18/C19 , where the three Omatrices are of appropriate sizes so that A2isp/C2n. PROOF.",
    "¼A/C01 11O OO/C18/C19 , where the three Omatrices are of appropriate sizes so that A2isp/C2n. PROOF. By multiplication of partitioned matrices, as in (2.35), we obtain AA/C0A¼IO A21A/C01 11O/C18/C19 A¼A11 A12 A21A21A/C01 11A12/C18/C19 : To show that A21A/C01 11A12¼A22, multiply Aby B¼IO /C0A21A/C01 11I/C18/C19 , where OandIare of appropriate sizes, to obtain BA¼A11 A12 OA 22/C0A21A/C01 11A12/C18/C19 : The matrix Bis nonsingular, and the rank of BAis therefore r¼rank( A) [see Theorem 2.4(ii)].",
    "8/C19 : The matrix Bis nonsingular, and the rank of BAis therefore r¼rank( A) [see Theorem 2.4(ii)]. In BA, the submatrixA11 O/C18/C19 is of rank r, and the columns headed by A12are therefore linear combinations of the columns headed by A11.B y a comment following Example 2.3, this relationship can be expressed as A12 A22/C0A21A/C01 11A12/C18/C19 ¼A11 O/C18/C19 Q (2:61)34 MATRIX ALGEBRA --- Page 51 --- for some matrix Q.",
    "C0A21A/C01 11A12/C18/C19 ¼A11 O/C18/C19 Q (2:61)34 MATRIX ALGEBRA --- Page 51 --- for some matrix Q. By (2.27), the right side of (2.61) becomes A11 O/C18/C19 Q¼A11Q OQ/C18/C19 ¼A11Q O/C18/C19 : Thus A22/C0A21A/C01 11A12¼O,o r A22¼A21A/C01 11A12:A Corollary 1. Suppose that Aisn/C2pof rank rand that Ais partitioned as in Theorem 2.8b, where A22isr/C2rof rank r.",
    "uppose that Aisn/C2pof rank rand that Ais partitioned as in Theorem 2.8b, where A22isr/C2rof rank r. Then a generalized inverse of Ais given by A/C0¼OO OA/C01 22/C18/C19 , where the three Omatrices are of appropriate sizes so that A2isp/C2n. A The nonsingular submatrix need not be in the A11orA22position, as in Theorem 2.8b or its corollary. Theorem 2.8b can be extended to the following algorithm for ﬁnding a conditional inverse A2for any n/C2pmatrix Aof rank r(Searle 1982, p. 218): 1.",
    "algorithm for ﬁnding a conditional inverse A2for any n/C2pmatrix Aof rank r(Searle 1982, p. 218): 1. Find any nonsingular r/C2rsubmatrix C. It is not necessary that the elements ofCoccupy adjacent rows and columns in A. 2. Find C21and (C/C01)0. 3. Replace the elements of Cby the elements of ( C/C01)0. 4. Replace all other elements in Aby zeros. 5. Transpose the resulting matrix.",
    "lements of ( C/C01)0. 4. Replace all other elements in Aby zeros. 5. Transpose the resulting matrix. Some properties of generalized inverses are given in the following theorem, which is the theoretical basis for many of the results in Chapter 11. Theorem 2.8c. LetAben/C2pof rank r, letA2be any generalized inverse of A, and let (A0A)2be any generalized inverse of A0A. Then (i) rank( A2A)¼rank( AA2)¼rank( A)¼r. (ii) ( A2)0is a generalized inverse of A0; that is, ( A0)2¼(A2)0.",
    "rank( A2A)¼rank( AA2)¼rank( A)¼r. (ii) ( A2)0is a generalized inverse of A0; that is, ( A0)2¼(A2)0. (iii)A¼A(A0A)/C0A0AandA0¼A0A(A0A)/C0A0. (iv) ( A0A)/C0A0is a generalized inverse of A; that is, A/C0¼(A0A)/C0A0.2.8 GENERALIZED INVERSE 35 --- Page 52 --- (v)A(A0A)/C0A0is symmetric, has rank ¼r, and is invariant to the choice of (A0A)/C0; that is, A(A0A)/C0A0remains the same, no matter what value of (A0A)/C0is used. A A generalized inverse of a symmetric matrix is not necessarily symmetric.",
    "alue of (A0A)/C0is used. A A generalized inverse of a symmetric matrix is not necessarily symmetric. However, it is also true that a symmetric generalized inverse can always be found for a symmetric matrix; see Problem 2.46. In this book, we will assume that gener- alized inverses of symmetric matrices are symmetric. 2.8.2 Generalized Inverses and Systems of Equations Generalized inverses can be used to ﬁnd solutions to a system of equations. Theorem 2.8d.",
    "Equations Generalized inverses can be used to ﬁnd solutions to a system of equations. Theorem 2.8d. If the system of equations Ax¼cis consistent and if A2is any generalized inverse for A, then x¼A/C0cis a solution. PROOF. Since AA2A¼A,w eh a v e AA/C0Ax¼Ax: Substituting Ax¼con both sides, we obtain AA/C0c¼c: Writing this in the form A(A/C0c)¼c, we see that A2cis a solution to Ax¼c.A Different choices of A2will result in different solutions for Ax¼c. Theorem 2.8e.",
    "solution to Ax¼c.A Different choices of A2will result in different solutions for Ax¼c. Theorem 2.8e. If the system of equations Ax¼cis consistent, then all possible sol- utions can be obtained in the following two ways: (i) Use a speciﬁc A2inx¼A/C0cþ(I/C0A/C0A)h, and use all possible values of the arbitrary vector h. (ii) Use all possible values of A2inx¼A/C0cifc=0. PROOF. See Searle (1982, p. 238).",
    "itrary vector h. (ii) Use all possible values of A2inx¼A/C0cifc=0. PROOF. See Searle (1982, p. 238). A A necessary and sufﬁcient condition for the system of equations Ax¼cto be consistent can be given in terms of a generalized inverse of A(Graybill 1976, p. 36).36 MATRIX ALGEBRA --- Page 53 --- Theorem 2.8f. The system of equations Ax¼chas a solution if and only if for any generalized inverse A2ofA AA/C0c¼c: PROOF. Suppose that Ax¼cis consistent. Then, by Theorem 2.8d, x¼A/C0cis a solution.",
    "A2ofA AA/C0c¼c: PROOF. Suppose that Ax¼cis consistent. Then, by Theorem 2.8d, x¼A/C0cis a solution. Multiply c¼AxbyAA/C0to obtain AA/C0c¼AA/C0Ax¼Ax¼c: Conversely, suppose AA/C0c¼c. Multiply x¼A/C0cbyAto obtain Ax¼AA/C0c¼c: Hence, a solution exists, namely, x¼A2c. A Theorem 2.8f provides an alternative to Theorem 2.7a for determining whether a system of equations is consistent. 2.9 DETERMINANTS Thedeterminant of an n/C2nmatrix Ais a scalar function of Adeﬁned as the sum of alln!",
    "9 DETERMINANTS Thedeterminant of an n/C2nmatrix Ais a scalar function of Adeﬁned as the sum of alln! possible products of nelements such that 1. each product contains one element from every row and every column of A. 2. the factors in each product are written so that the column subscripts appear in order of magnitude and each product is then preceded by a plus or minus sign according to whether the number of inversions in the row sub-scripts is even or odd.",
    "s or minus sign according to whether the number of inversions in the row sub-scripts is even or odd. (An inversion occurs whenever a larger number pre- cedes a smaller one.) The determinant of Ais denoted by jAjor det( A). The preceding deﬁnition is not very useful in evaluating determinants, except in the case of 2 /C22o r3 /C23 matrices. For larger matrices, determinants are typically found by computer. Some calculatorsalso evaluate determinants.",
    "matrices, determinants are typically found by computer. Some calculatorsalso evaluate determinants. The determinants of some special square matrices are given in the following theorem. Theorem 2.9a. (i) If D¼diag( d 1,d2,...,dn),jDj¼Qn i¼1di:2.9 DETERMINANTS 37 --- Page 54 --- (ii) The determinant of a triangular matrix is the product of the diagonal elements.",
    "--- Page 54 --- (ii) The determinant of a triangular matrix is the product of the diagonal elements. (iii) If Ais singular ,jAj¼0: (iv) If Ais nonsingular ,jAj=0: (v) If Ais positive definite ,jAj.0: (vi)jA0j¼jAj: (vii) If Ais nonsingular ,jA/C01j¼1 jAj: A Example 2.9a. We illustrate each of the properties in Theorem 2.9a. (i) diagonal:20 03/C12/C12/C12/C12/C12/C12/C12/C12¼(2) (3) /C0(0) (0) ¼(2) (3). (ii) triangular:21 03/C12/C12/C12/C12/C12/C12/C12/C12¼(2) (3) /C0(0) (1) ¼(2) (3).",
    "(0) (0) ¼(2) (3). (ii) triangular:21 03/C12/C12/C12/C12/C12/C12/C12/C12¼(2) (3) /C0(0) (1) ¼(2) (3). (iii) singular:12 36/C12/C12/C12/C12/C12/C12/C12/C12¼(1) (6) /C0(3) (2) ¼0, nonsingular:12 34/C12/C12/C12/C12/C12/C12/C12/C12¼(1) (4) /C0(3) (2) ¼/C02. (iv) positive deﬁnite:3/C02 /C024/C12/C12/C12/C12/C12/C12/C12/C12¼(3) (4) /C0(/C02) (/C02)¼8.0. (v) transpose: 3/C07 21/C12/C12/C12/C12/C12/C12/C12/C12¼(3)(1) /C0(2)(/C07)¼17, 32 /C071/C12/C12/C12/C12/C12/C12/C12/C12¼(3)(1) /C0(/C07)(2)¼17.",
    "C12/C12/C12¼(3)(1) /C0(2)(/C07)¼17, 32 /C071/C12/C12/C12/C12/C12/C12/C12/C12¼(3)(1) /C0(/C07)(2)¼17. (vi) inverse: 32 14/C18/C19 /C01 ¼:4/C0:2 /C0:1 :3/C18/C19 ,32 14/C12/C12/C12/C12/C12/C12/C12/C12¼10,:4/C0:2 /C0:1 :3/C12/C12/C12/C12/C12/C12/C12/C12¼:1. A As a special case of (62), suppose that all diagonal elements are equal, say, D¼diag( c,c,...,c)¼cI.",
    "As a special case of (62), suppose that all diagonal elements are equal, say, D¼diag( c,c,...,c)¼cI. Then jDj¼jcIj¼Y n i¼1c¼cn: (2:68)38 MATRIX ALGEBRA --- Page 55 --- By extension, if an n/C2nmatrix is multiplied by a scalar, the determinant becomes jcAj¼cnjAj: (2:69) The determinant of certain partitioned matrices is given in the following theorem. Theorem 2.9b.",
    "69) The determinant of certain partitioned matrices is given in the following theorem. Theorem 2.9b. If the square matrix Ais partitioned as A¼A11A12 A21A22/C18/C19 , (2:70) and if A11andA22are square and nonsingular (but not necessarily the same size), then jAj¼jA11jjA22/C0A21A/C01 11A12jð 2:71Þ ¼jA22jjA11/C0A12A/C01 22A21j: (2:72) A Note the analogy of (2.71) and (2.72) to the case of the determinant of a 2 /C22 matrix: a11a12 a21a22/C12/C12/C12/C12/C12/C12/C12/C12¼a11a22/C0a21a12 ¼a11a22/C0a21a12 a11/C18/C19 ¼a22a11/C0a12a21 a22/C18/C19 : Corollary 1.",
    "C12/C12/C12¼a11a22/C0a21a12 ¼a11a22/C0a21a12 a11/C18/C19 ¼a22a11/C0a12a21 a22/C18/C19 : Corollary 1. Suppose A¼A11O A21A22/C18/C19 orA¼A11A12 OA 22/C18/C19 , where A11andA22are square (but not necessarily the same size). Then in either case jAj¼jA11jjA22j: (2:73) A2.9 DETERMINANTS 39 --- Page 56 --- Corollary 2. Let A¼A11O OA 22/C18/C19 , where A11andA22are square (but not necessarily the same size). Then jAj¼jA11jjA22j: (2:74) A Corollary 3.",
    "A11andA22are square (but not necessarily the same size). Then jAj¼jA11jjA22j: (2:74) A Corollary 3. IfAhas the form A¼A11a12 a0 12a22/C18/C19 , where A11is a nonsingular matrix, a12is a vector, and a22is a 1 /C21 matrix, then jAj¼A11a12 a012a22/C12/C12/C12/C12/C12/C12/C12/C12¼jA 11j(a22/C0a0 12A/C01 11a12): (2:75) A Corollary 4.",
    "A11a12 a012a22/C12/C12/C12/C12/C12/C12/C12/C12¼jA 11j(a22/C0a0 12A/C01 11a12): (2:75) A Corollary 4. IfAhas the form A¼Bc /C0c01/C18/C19 , where cis a vector and Bis a nonsingular matrix, then jBþcc0j¼jBj(1þc0B/C01c): (2:76) A The determinant of the product of two square matrices is given in the following theorem. Theorem 2.9c.",
    "e determinant of the product of two square matrices is given in the following theorem. Theorem 2.9c. IfAandBare square and the same size, then the determinant of the product is the product of the determinants: jABj¼jAjjBj: (2:77) A Corollary 1 jABj¼jBAj: (2:78) A Corollary 2 jA2j¼jAj2: (2:79) A40 MATRIX ALGEBRA --- Page 57 --- Example 2.9b.",
    "1 jABj¼jBAj: (2:78) A Corollary 2 jA2j¼jAj2: (2:79) A40 MATRIX ALGEBRA --- Page 57 --- Example 2.9b. To illustrate Theorem 2.9c, let A¼12 34/C18/C19 and B¼3/C02 12/C18/C19 : Then AB¼52 13 2/C18/C19 ,jABj¼/C0 16, jAj¼/C0 2,jBj¼8,jAjjBj¼/C0 16:A 2.10 ORTHOGONAL VECTORS AND MATRICES Two n/C21 vectors bandbare said to be orthogonal if a0b¼a1b1þa2b2þ/C1/C1/C1þ anbn¼0: (2:80) Note that the term orthogonal applies to twovectors, not to a single vector.",
    "1/C1/C1þ anbn¼0: (2:80) Note that the term orthogonal applies to twovectors, not to a single vector. Geometrically, two orthogonal vectors are perpendicular to each other. This is illustrated in Figure 2.3 for the vectors x1¼(4,2)0andx2¼(/C01,2)0. Note that x0 1x2¼(4) (/C01)þ(2) (2) ¼0. To show that two orthogonal vectors are perpendicular, let ube the angle between vectors aandbin Figure 2.4. The vector from the terminal point of ato the terminal point of bcan be represented as c¼b/C0a.",
    "2.4. The vector from the terminal point of ato the terminal point of bcan be represented as c¼b/C0a. The law of cosines for the relationship of Figure 2.3 Two orthogonal (perpendicular) vectors.2.10 ORTHOGONAL VECTORS AND MATRICES 41 --- Page 58 --- uto the sides of the triangle can be stated in vector form as cosu¼a0aþb0b/C0(b/C0a)0(b/C0a) 2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (a0a)(b0b)p ¼a0aþb0b/C0(b0bþa0a/C02a0b) 2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (a0a)(b0b)p ¼a0bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (a0a)(b0b)p : (2:81) Whenu¼908,a0b¼0 since cos(90 8)¼0.",
    "ﬃﬃﬃﬃﬃﬃﬃﬃ (a0a)(b0b)p ¼a0bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (a0a)(b0b)p : (2:81) Whenu¼908,a0b¼0 since cos(90 8)¼0. Thus aandbareperpendicular when a0b¼0. Ifa0a¼1, the vector ais said to be normalized . A vector bcan be normalized by dividing by its length,ﬃﬃﬃﬃﬃﬃﬃ b0bp . Thus c¼bﬃﬃﬃﬃﬃﬃﬃ b0bp (2:82) is normalized so that c0c¼1. A set of p/C21 vectors c1,c2,...,cpthat are normalized ( c0 ici¼1 for all i) and mutually orthogonal ( c0icj¼0 for all i=j) is said to be an orthonormal set of vectors.",
    "r all i) and mutually orthogonal ( c0icj¼0 for all i=j) is said to be an orthonormal set of vectors. If the p/C2pmatrix C¼(c1,c2,...,cp) has orthonormal columns, Cis called an orthogonal matrix.",
    "ctors. If the p/C2pmatrix C¼(c1,c2,...,cp) has orthonormal columns, Cis called an orthogonal matrix. Since the elements of C0Care products of columns ofFigure 2.4 Vectors aandbin 3-space.42 MATRIX ALGEBRA --- Page 59 --- C[see Theorem 2.2c(i)], an orthogonal matrix Chas the property C0C¼I: (2:83) It can be shown that an orthogonal matrix Calso satisﬁes CC0¼I: (2:84) Thus an orthogonal matrix Chas orthonormal rows as well as orthonormal columns.",
    "tisﬁes CC0¼I: (2:84) Thus an orthogonal matrix Chas orthonormal rows as well as orthonormal columns. It is also clear from (2.83) and (2.84) that C0¼C21ifCis orthogonal. Example 2.10. To illustrate an orthogonal matrix, we start with A¼111 1/C020 11 /C010 @1A, whose columns are mutually orthogonal but not orthonormal.",
    "we start with A¼111 1/C020 11 /C010 @1A, whose columns are mutually orthogonal but not orthonormal. To normalize the three columns, we divide by their respective lengths,ﬃﬃﬃ 3p ,ﬃﬃﬃ 6p , andﬃﬃﬃ2p , to obtain the matrix C¼1=ﬃﬃﬃ 3p 1=ﬃﬃﬃ 6p 1=ﬃﬃﬃ2p 1=ﬃﬃﬃ 3p /C02=ﬃﬃﬃ 6p 0 1=ﬃﬃﬃ 3p 1=ﬃﬃﬃ 6p /C01=ﬃﬃﬃ2p0 @1A, whose columns are orthonormal. Note that the rows of Care also orthonormal, so that Csatisﬁes (2.84) as well as (2.83).",
    "rthonormal. Note that the rows of Care also orthonormal, so that Csatisﬁes (2.84) as well as (2.83). A Multiplication of a vector by an orthogonal matrix has the effect of rotating axes; that is, if a point xis transformed to z¼Cx, where Cis orthogonal, then the distance from the origin to zis the same as the distance to x: z0z¼(Cx)0(Cx)¼x0C0Cx¼x0Ix¼x0x: (2:85) Hence, the transformation from xtozis a rotation. Some properties of orthogonal matrices are given in the following theorem.",
    "n from xtozis a rotation. Some properties of orthogonal matrices are given in the following theorem. Theorem 2.10. If the p/C2pmatrix Cis orthogonal and if Ais any p/C2pmatrix, then (i)jCj¼þ 1o r21.2.10 ORTHOGONAL VECTORS AND MATRICES 43 --- Page 60 --- (ii)jC0ACj¼jAj: (iii)/C01/C20cij/C201, where cijis any element of C. 2.11 TRACE Thetrace of an n/C2nmatrix A¼(aij) is a scalar function deﬁned as the sum of the diagonal elements of A; that is, tr( A)¼Pn i¼1aii.",
    "ij) is a scalar function deﬁned as the sum of the diagonal elements of A; that is, tr( A)¼Pn i¼1aii. For example, suppose A¼84 2 2/C036 35 90 @1A: Then tr(A)¼8/C03þ9¼14: Some properties of the trace are given in the following theorem. Theorem 2.11 (i) If AandBaren/C2n, then tr(A+B)¼tr(A)+tr(B): (2:86) (ii) If Aisn/C2pandBisp/C2n, then tr(AB)¼tr(BA): ð2:87Þ Note that in (2.87) ncan be less than, equal to, or greater than p.",
    "/C2n, then tr(AB)¼tr(BA): ð2:87Þ Note that in (2.87) ncan be less than, equal to, or greater than p. (iii) If Aisn/C2p, then tr(A 0A)¼Xp i¼1a0 iai, (2:88) where aiis the ithcolumn ofA.",
    "greater than p. (iii) If Aisn/C2p, then tr(A 0A)¼Xp i¼1a0 iai, (2:88) where aiis the ithcolumn ofA. (iv) If Aisn/C2p, then tr(AA0)¼Xn i¼1a0iai, (2:89) where a0 iis the ithrowofA.44 MATRIX ALGEBRA --- Page 61 --- (v) If A¼(aij)i sa n n/C2pmatrix with representative element aij, then tr(A0A)¼tr(AA0)¼Xn i¼1Xp j¼1a2 ij: (2:90) (vi) If Ais any n/C2nmatrix and Pis any n/C2nnonsingular matrix, then tr(P/C01AP)¼tr(A): (2:91) (vii) If Ais any n/C2nmatrix and Cis any n/C2northogonal matrix, then tr(C0AC)¼tr(A): (2:92) (viii) If Aisn/C2pof rank randA2is a generalized inverse of A, then tr(A/C0A)¼tr(AA/C0)¼r: (2:93) PROOF.",
    "i) If Aisn/C2pof rank randA2is a generalized inverse of A, then tr(A/C0A)¼tr(AA/C0)¼r: (2:93) PROOF. We prove parts (ii), (iii), and (vi). (ii) By (2.13), the ith diagonal element of E¼ABiseii¼P kaikbki.",
    "prove parts (ii), (iii), and (vi). (ii) By (2.13), the ith diagonal element of E¼ABiseii¼P kaikbki. Then tr(AB)¼tr(E)¼X ieii¼X iX kaikbki: Similarly, the ith diagonal element of F¼BAisfii¼P kbikaki, and tr(BA)¼tr(F)¼X ifii¼X iX kbikaki ¼X kX iakibik¼tr(E)¼tr(AB): (iii) By Theorem 2.2c(i), A0Ais obtained as products of columns of A.I faiis theith column of A, then the ith diagonal element of A0Aisa0 iai. (vi) By (2.87) we obtain tr(P/C01AP)¼tr(APP/C01)¼tr(A):A Example 2.11.",
    "gonal element of A0Aisa0 iai. (vi) By (2.87) we obtain tr(P/C01AP)¼tr(APP/C01)¼tr(A):A Example 2.11. We illustrate parts (ii) and (viii) of Theorem 2.11.",
    "n tr(P/C01AP)¼tr(APP/C01)¼tr(A):A Example 2.11. We illustrate parts (ii) and (viii) of Theorem 2.11. (ii) Let A¼13 2/C01 460 @1A and B¼3/C021 24 5/C18/C19 :2.11 TRACE 45 --- Page 62 --- Then AB¼91 0 1 6 4/C08/C03 24 16 340 B@1 CA,BA¼31 7 30 32/C18/C19 , tr(AB)¼9/C08þ34¼35,tr(BA)¼3þ32¼35: (viii) Using Ain (2.59) and A/C0 1in (2.60), we obtain A/C0A¼101 011 2 0000 B@1 CA,AA/C0¼100 010 1100 B@1 CA, tr(A/C0A)¼1þ1þ0¼2¼rank(A), tr(AA/C0)¼1þ1þ0¼2¼rank(A):A 2.12 EIGENVALUES AND EIGENVECTORS 2.12.1 Deﬁnition For every square matrix A, a scalar land a nonzero vector xcan be found such that Ax¼lx, (2:94) Figure 2.5 An eigenvector xis transformed to lx.46 MATRIX ALGEBRA --- Page 63 --- wherelis an eigenvalue ofAandxis an eigenvector .",
    "ransformed to lx.46 MATRIX ALGEBRA --- Page 63 --- wherelis an eigenvalue ofAandxis an eigenvector . (These terms are sometimes referred to as characteristic root and characteristic vector, respectively.) Note that in (2.94), the vector xis transformed by Aonto a multiple of itself, so that the point Axis on the line passing through xand the origin. This is illustrated in Figure 2.5.",
    "that the point Axis on the line passing through xand the origin. This is illustrated in Figure 2.5. To ﬁndlandxfor a matrix A, we write (2.94) as (A/C0lI)x¼0: (2:95) By (2.37), ( A/C0lI)xis a linear combination of the columns of A/C0lI, and by (2.40) and (2.95), these columns are linearly dependent. Thus the square matrix ( A/C0lI)i s singular, and by Theorem 2.9a(iii), we can solve for lusing jA/C0lIj¼0, (2:96) which is known as the characteristic equation .",
    ".9a(iii), we can solve for lusing jA/C0lIj¼0, (2:96) which is known as the characteristic equation . IfAisn/C2n, the characteristic equation (2.96) will have nroots; that is, Awill have neigenvalues l1,l2,...,ln. Thel’s will not necessarily all be distinct, or all nonzero, or even all real. (However, the eigenvalues of a symmetric matrix arereal; see Theorem 2.12c.) After ﬁnding l1,l2,...,lnusing (2.96), the accompanying eigenvectors x1,x2,...,xncan be found using (2.95).",
    "nding l1,l2,...,lnusing (2.96), the accompanying eigenvectors x1,x2,...,xncan be found using (2.95). If an eigenvalue is 0, the corresponding eigenvector is not 0. To see this, note that ifl¼0, then ( A/C0lI)x¼0becomes Ax¼0, which has solutions for xbecause A is singular, and the columns are therefore linearly dependent.",
    "0, which has solutions for xbecause A is singular, and the columns are therefore linearly dependent. [The matrix Ais singu- lar because it has a zero eigenvalue; see (63) and (2.107).] If we multiply both sides of (2.95) by a scalar k, we obtain k(A/C0lI)x¼k0¼0, which can be rewritten as (A/C0lI)kx¼0 [by (2 :12)]: Thus if xis an eigenvector of A,kxis also an eigenvector. Eigenvectors are therefore unique only up to multiplication by a scalar.",
    "A,kxis also an eigenvector. Eigenvectors are therefore unique only up to multiplication by a scalar. (There are many solution vectors x because A/C0lIis singular; see Section 2.8) Hence, the length of xis arbitrary, but its direction from the origin is unique; that is, the relative values of (ratios of) the elements of x¼(x1,x2,...,xn)0are unique. Typically, an eigenvector xis scaled to normalized form as in (2.82), x0x¼1.2.12 EIGENVALUES AND EIGENVECTORS 47 --- Page 64 --- Example 2.12.1.",
    "alized form as in (2.82), x0x¼1.2.12 EIGENVALUES AND EIGENVECTORS 47 --- Page 64 --- Example 2.12.1. To illustrate eigenvalues and eigenvectors, consider the matrix A¼12 /C014/C18/C19 : By (2.96), the characteristic equation is jA/C0lIj¼1/C0l 2 /C014 /C0l/C12/C12/C12/C12/C12/C12/C12/C12¼(1/C0l)(4/C0l)þ2¼0, which becomes l2/C05lþ6¼(l/C03)(l/C02)¼0, with roots l1¼3 andl2¼2.",
    "2/C12/C12/C12¼(1/C0l)(4/C0l)þ2¼0, which becomes l2/C05lþ6¼(l/C03)(l/C02)¼0, with roots l1¼3 andl2¼2. To ﬁnd the eigenvector x1corresponding to l1¼3, we use (2.95) (A/C0l1I)x1¼0, 1/C032 /C014 /C03/C18/C19x1 x2/C18/C19 ¼0 0/C18/C19 , which can be written as /C02x1þ2x2¼0 /C0x1þx2¼0: The second equation is a multiple of the ﬁrst, and either equation yields x1¼x2.",
    "1þ2x2¼0 /C0x1þx2¼0: The second equation is a multiple of the ﬁrst, and either equation yields x1¼x2. The solution vector can be written with x1¼x2¼cas an arbitrary constant: x1¼x1 x2/C18/C19 ¼x1 x1/C18/C19 ¼x11 1/C18/C19 ¼c11/C18/C19 : Ifcis set equal to 1 =ﬃﬃﬃ 2p to normalize the eigenvector, we obtain x 1¼1=ﬃﬃﬃ 2p 1=ﬃﬃﬃ 2p/C18/C19 : Similarly, corresponding to l2¼2, we obtain x2¼2=ﬃﬃﬃ 5p 1=ﬃﬃﬃ 5p/C18/C19 : A48 MATRIX ALGEBRA --- Page 65 --- 2.12.2 Functions of a Matrix Iflis an eigenvalue of Awith corresponding eigenvector x, then for certain functions g(A), an eigenvalue is given by g(l) and xis the corresponding eigenvector of g(A) as well as of A.",
    "g(A), an eigenvalue is given by g(l) and xis the corresponding eigenvector of g(A) as well as of A. We illustrate some of these cases: 1. Iflis an eigenvalue of A, then clis an eigenvalue of cA,w h e r e cis an arbi- trary constant such that c=0. This is easily demonstrated by multiplying the deﬁning relationship Ax¼lxbyc: cAx¼clx: (2:97) Note that xis an eigenvector of Acorresponding to l, and xis also an eigen- vector of cAcorresponding to cl. 2.",
    "is an eigenvector of Acorresponding to l, and xis also an eigen- vector of cAcorresponding to cl. 2. Iflis an eigenvalue of the Aandxis the corresponding eigenvector of A, then clþkis an eigenvalue of the matrix cAþkIandxis an eigenvector ofcAþkI, where candkare scalars. To show this, we add kxto (2.97): cAxþkx¼clxþkx, (cAþkI)x¼(clþk)x: (2:98) Thus clþkis an eigenvalue of cAþkIandxis the corresponding eigen- vector of cAþkI.",
    "x¼(clþk)x: (2:98) Thus clþkis an eigenvalue of cAþkIandxis the corresponding eigen- vector of cAþkI. Note that (2.98) does not extend to AþBfor arbitrary n/C2nmatrices AandB; that is, AþBdoes not have lAþlBfor an eigen- value, where lAis an eigenvalue of AandlBis an eigenvalue of B. 3. Iflis an eigenvalue of A, thenl2is an eigenvalue of A2.",
    "igenvalue of AandlBis an eigenvalue of B. 3. Iflis an eigenvalue of A, thenl2is an eigenvalue of A2. This can be demon- strated by multiplying the deﬁning relationship Ax¼lxbyA: A(Ax)¼A(lx), A2x¼lAx¼l(lx)¼l2x: (2:99) Thusl2is an eigenvalue of A2, and xis the corresponding eigenvector of A2. This can be extended to any power of A: Akx¼lkx; (2:100) that is,lkis an eigenvalue of Ak, and xis the corresponding eigenvector.2.12 EIGENVALUES AND EIGENVECTORS 49 --- Page 66 --- 4.",
    "of Ak, and xis the corresponding eigenvector.2.12 EIGENVALUES AND EIGENVECTORS 49 --- Page 66 --- 4. Iflis an eigenvalue of the nonsingular matrix A, then 1 /lis an eigenvalue ofA/C01. To demonstrate this, we multiply Ax¼lxbyA/C01to obtain A/C01Ax¼A/C01lx, x¼lA/C01x, A/C01x¼1 lx: (2:101) Thus 1 /lis an eigenvalue of A/C01, and xis an eigenvector of both AandA/C01. 5. The results in (2.97) and (2.100) can be used to obtain eigenvalues and eigen- vectors of a polynomial in A.",
    "lts in (2.97) and (2.100) can be used to obtain eigenvalues and eigen- vectors of a polynomial in A. For example, if lis an eigenvalue of A, then (A3þ4A2/C03Aþ5I)x¼A3xþ4A2x/C03Axþ5x ¼l3xþ4l2x/C03lxþ5x ¼(l3þ4l2/C03lþ5)x: Thusl3þ4l2/C03lþ5 is an eigenvalue of A3þ4A2/C03Aþ5I, and xis the corresponding eigenvector. For certain matrices, property 5 can be extended to an inﬁnite series.",
    "he corresponding eigenvector. For certain matrices, property 5 can be extended to an inﬁnite series. For example, iflis an eigenvalue of A, then, by (2.98), 1 /C0lis an eigenvalue of I/C0A.I f I/C0Ais nonsingular, then, by (2.101), 1 =(1/C0l) is an eigenvalue of ( I/C0A)/C01.",
    "lue of I/C0A.I f I/C0Ais nonsingular, then, by (2.101), 1 =(1/C0l) is an eigenvalue of ( I/C0A)/C01. If/C01,l,1, then 1 =(1/C0l) can be represented by the series 1 1/C0l¼1þlþl2þl3þ/C1/C1/C1 : Correspondingly, if all eigenvalues of Asatisfy /C01,l,1, then (I/C0A)/C01¼IþAþA2þA3þ/C1/C1/C1 : (2:102) 2.12.3 Products It was noted in a comment following (2.98) that the eigenvalues of AþBare not of the form lAþlB, wherelAis an eigenvalue of AandlBis an eigenvalue of B.",
    "eigenvalues of AþBare not of the form lAþlB, wherelAis an eigenvalue of AandlBis an eigenvalue of B. Similarly, the eigenvalues of ABare not products of the form lAlB. However, the eigenvalues of ABare the same as those of BA. Theorem 2.12a. IfAandBaren/C2nor if Aisn/C2pandBisp/C2n, then the (nonzero) eigenvalues of ABare the same as those of BA.I fxis an eigenvector ofAB, then Bxis an eigenvector of BA.",
    "nvalues of ABare the same as those of BA.I fxis an eigenvector ofAB, then Bxis an eigenvector of BA. A50 MATRIX ALGEBRA --- Page 67 --- Two additional results involving eigenvalues of products are given in the follow- ing theorem. Theorem 2.12b. LetAbe any n/C2nmatrix. (i) If Pis any n/C2nnonsingular matrix, then AandP21APhave the same eigenvalues.",
    "be any n/C2nmatrix. (i) If Pis any n/C2nnonsingular matrix, then AandP21APhave the same eigenvalues. (ii) If Cis any n/C2northogonal matrix, then Aand C0AC have the same eigenvalues.A 2.12.4 Symmetric Matrices Two properties of the eigenvalues and eigenvectors of a symmetric matrix are given in the following theorem. Theorem 2.12c. LetAbe an n/C2nsymmetric matrix. (i) The eigenvalues l1,l2,...,lnofAare real.",
    "heorem. Theorem 2.12c. LetAbe an n/C2nsymmetric matrix. (i) The eigenvalues l1,l2,...,lnofAare real. (ii) The eigenvectors x1,x2,...,xkofAcorresponding to distinct eigenvalues l1,l2,...,lkare mutually orthogonal; the eigenvectors xkþ1,xkþ2,...,xn corresponding to the nondistinct eigenvalues can be chosen to be mutually orthogonal to each other and to the other eigenvectors; that is, x0 ixj¼0 for i=j.",
    "en to be mutually orthogonal to each other and to the other eigenvectors; that is, x0 ixj¼0 for i=j. A If the eigenvectors of a symmetric matrix Aare normalized and placed as columns of a matrix C, then by Theorem 2.12c(ii), Cis an orthogonal matrix. This orthogonal matrix can be used to express Ain terms of its eigenvalues and eigenvectors. Theorem 2.12d.",
    "thogonal matrix can be used to express Ain terms of its eigenvalues and eigenvectors. Theorem 2.12d. IfAis an n/C2nsymmetric matrix with eigenvalues l1,l2,...,ln and normalized eigenvectors x1,x2,...,xn,t h e n Acan be expressed as A¼CDC0(2:103) ¼Xn i¼1lixix0 i, (2:104) where D¼diag(l1,l2,...,ln) and Cis the orthogonal matrix C¼(x1,x2,...,xn). The result in either (2.103) or (2.104) is often called the spectral decomposition ofA. PROOF. By Theorem 2.12c(ii), Cis orthogonal.",
    "(2.104) is often called the spectral decomposition ofA. PROOF. By Theorem 2.12c(ii), Cis orthogonal. Then by (2.84), I¼CC0, and multi- plication by Agives A¼ACC0:2.12 EIGENVALUES AND EIGENVECTORS 51 --- Page 68 --- We now substitute C¼(x1,x2,...,xn) to obtain A¼A(x1,x2,...,xn)C0 ¼(Ax1,Ax2,...,Axn)C0[by (2 :28)] ¼(l1x1,l2x2,...,lnxn)C0[by (2 :94)] ¼CDC0, (2:105) since multiplication on the right by D¼diag(l1,l2,...,ln) multiplies columns of C by elements of D[see (2.30)].",
    "plication on the right by D¼diag(l1,l2,...,ln) multiplies columns of C by elements of D[see (2.30)]. Now writing C0in the form C0¼(x1,x2,...,xn)0¼x0 1 x02 ... x0 n0 BBB@1 CCCA[by (2 :39)] , (2.105) becomes A¼( l1x1,l2x2,...,lnxn)x0 1 x02 ... x0 n0 BBBB@1 CCCCA ¼ l1x1x0 1þl2x2x02þ/C1/C1/C1þlnxnx0n: A Corollary 1. IfAis symmetric and CandDare deﬁned as in Theorem 2.12d, then C diagonalizes A: C0AC¼D: (2:106) A We can express the determinant and trace of a square matrix Ain terms of its eigenvalues.",
    "(2:106) A We can express the determinant and trace of a square matrix Ain terms of its eigenvalues. Theorem 2.12e. IfAis any n/C2nmatrix with eigenvalues l1,l2,...,ln, then (i)jAj¼Yn i¼1li: (2:107) (ii) tr(A)¼Xn i¼1li: (2:108) A We have included Theorem 2.12e here because it is easy to prove for a symmetric matrix Ausing Theorem 2.12d (see Problem 2.72). However, the theorem is true for any square matrix (Searle 1982, p. 278).52 MATRIX ALGEBRA --- Page 69 --- Example 2.12.4.",
    "s true for any square matrix (Searle 1982, p. 278).52 MATRIX ALGEBRA --- Page 69 --- Example 2.12.4. To illustrate Theorem 2.12e, consider the matrix Ain Example 2.12.1 A¼12 /C014/C18/C19 , which has eigenvalues l1¼3 andl2¼2. The product l1l2¼6 is the same as jAj¼4/C0(/C01) (2) ¼6. The sum l1þl2¼3þ2¼5 is the same as tr(A)¼1þ4¼5.",
    "product l1l2¼6 is the same as jAj¼4/C0(/C01) (2) ¼6. The sum l1þl2¼3þ2¼5 is the same as tr(A)¼1þ4¼5. A 2.12.5 Positive Deﬁnite and Semideﬁnite Matrices The eigenvalues l1,l2,...,lnof positive deﬁnite and positive semideﬁnite matrices (Section 2.6) are positive and nonnegative, respectively. Theorem 2.12f. LetAben/C2nwith eigenvalues l1,l2,...,ln. (i) If Ais positive deﬁnite, then li.0 for i¼1,2,...,n. (ii) If Ais positive semideﬁnite, then li/C210 for i¼1,2,...,n.",
    "deﬁnite, then li.0 for i¼1,2,...,n. (ii) If Ais positive semideﬁnite, then li/C210 for i¼1,2,...,n. The number of eigenvalues lifor which li.0 is the rank of A. PROOF. (i) For any li,w eh a v e Axi¼lixi. Multiplying by x0 i, we obtain x0 iAxi¼lix0ixi, li¼x0 iAxi x0 ixi.0: In the second expression, x0 iAxiis positive because Ais positive deﬁnite, and x0ixiis positive because xi=0. A If a matrix Ais positive deﬁnite, we can ﬁnd a square root matrix A1=2as follows.",
    "ve because xi=0. A If a matrix Ais positive deﬁnite, we can ﬁnd a square root matrix A1=2as follows. Since the eigenvalues of Aare positive, we can substitute the square rootsﬃﬃﬃﬃlipforli in the spectral decomposition of Ain (2.103), to obtain A1=2¼CD1=2C0, (2:109) where D1=2¼diag(ﬃﬃﬃﬃﬃl1p,ﬃﬃﬃﬃﬃl2p,...,ﬃﬃﬃﬃﬃlnp).",
    "ion of Ain (2.103), to obtain A1=2¼CD1=2C0, (2:109) where D1=2¼diag(ﬃﬃﬃﬃﬃl1p,ﬃﬃﬃﬃﬃl2p,...,ﬃﬃﬃﬃﬃlnp). The matrix A1=2is symmetric and has the property A1=2A1=2¼(A1=2)2¼A: (2:110)2.12 EIGENVALUES AND EIGENVECTORS 53 --- Page 70 --- 2.13 IDEMPOTENT MATRICES A square matrix Ais said to be idempotent ifA2¼A. Most idempotent matrices in this book are symmetric. Many of the sums of squares in regression (Chapters 6–11) and analysis of variance (Chapters 12–15) can be expressed as quadratic forms y0Ay.",
    "(Chapters 6–11) and analysis of variance (Chapters 12–15) can be expressed as quadratic forms y0Ay. The idempotence of Aor of a product involving Awill be used to establish that y0Ay (or a multiple of y0Ay) has a chi-square distribution. An example of an idempotent matrix is the identity matrix I. Theorem 2.13a. The only nonsingular idempotent matrix is the identity matrix I. PROOF.I fAis idempotent and nonsingular, then A2¼Aand the inverse A/C01exists.",
    "he identity matrix I. PROOF.I fAis idempotent and nonsingular, then A2¼Aand the inverse A/C01exists. If we multiply A2¼AbyA/C01, we obtain A/C01A2¼A/C01A, A¼I: AMany of the matrices of quadratic forms we will encounter in later chapters are singular idempotent matrices. We now give some properties of such matrices. Theorem 2.13b. IfAis singular, symmetric, and idempotent, then Ais positive semideﬁnite. PROOF.",
    "ces. Theorem 2.13b. IfAis singular, symmetric, and idempotent, then Ais positive semideﬁnite. PROOF. Since A¼A0andA¼A2,w eh a v e A¼A2¼AA¼A0A, which is positive semideﬁnite by Theorem 2.6d(ii). A Ifais a real number such that a2¼a, then ais either 0 or 1. The analogous prop- erty for matrices is that if A2¼A, then the eigenvalues of Aare 0s and 1s. Theorem 2.13c. IfAis an n/C2nsymmetric idempotent matrix of rank r, then Ahas reigenvalues equal to 1 and n/C0reigenvalues equal to 0. PROOF.",
    "empotent matrix of rank r, then Ahas reigenvalues equal to 1 and n/C0reigenvalues equal to 0. PROOF. By (2.99), if Ax¼lx, then A2x¼l2x. Since A2¼A,w eh a v e A2x¼Ax¼lx. Equating the right sides of A2x¼l2xandA2x¼lx,w eh a v e lx¼l2xor (l/C0l2)x¼0: Butx=0, and therefore l/C0l2¼0, from which, lis either 0 or 1. By Theorem 2.13b, Ais positive semideﬁnite, and therefore by Theorem 2.12f(ii), the number of nonzero eigenvalues is equal to rank( A).",
    "deﬁnite, and therefore by Theorem 2.12f(ii), the number of nonzero eigenvalues is equal to rank( A). Thus reigenvalues of Aare equal to 1 and the remaining n/C0reigenvalues are equal to 0. A54 MATRIX ALGEBRA --- Page 71 --- We can use Theorems 2.12e and 2.13c to ﬁnd the rank of a symmetric idempotent matrix. Theorem 2.13d. IfAis symmetric and idempotent of rank r, then rank( A)¼ tr(A)¼r. PROOF. By Theorem 2.12e(ii), tr( A)¼Pn i¼1li, and by Theorem 2.13c,Pni¼1li¼r.",
    "hen rank( A)¼ tr(A)¼r. PROOF. By Theorem 2.12e(ii), tr( A)¼Pn i¼1li, and by Theorem 2.13c,Pni¼1li¼r. A Some additional properties of idempotent matrices are given in the following four theorems. Theorem 2.13e. IfAis an n/C2nidempotent matrix, Pis an n/C2nnonsingular matrix, and Cis an n/C2northogonal matrix, then (i)I/C0Ais idempotent. (ii)A(I/C0A)¼Oand (I/C0A)A¼O. (iii)P/C01APis idempotent. (iv)C0ACis idempotent. (If Ais symmetric, C0ACis a symmetric idempotent matrix.) A Theorem 2.13f.",
    "t. (iv)C0ACis idempotent. (If Ais symmetric, C0ACis a symmetric idempotent matrix.) A Theorem 2.13f. LetAben/C2pof rank r, letA/C0be any generalized inverse of A, and let ( A0A)/C0be any generalized inverse of A0A. Then A/C0A,AA/C0, and A(A0A)/C0A0are all idempotent. A Theorem 2.13g. Suppose that the n/C2nsymmetric matrix Acan be written as A¼Pk i¼1Aifor some k, where each Aiis an n/C2nsymmetric matrix. Then any two of the following conditions implies the third condition. (i)Ais idempotent.",
    "ric matrix. Then any two of the following conditions implies the third condition. (i)Ais idempotent. (ii) Each of A1,A2,...,Akis idempotent. (iii)AiAj¼Ofori=j.A Theorem 2.13h. IfI¼Pki¼1Ai, where each n/C2nmatrix Aiis symmetric of rank ri, and if n¼Pki¼1ri, then both of the following are true: (i) Each of A1,A2,...,Akis idempotent.",
    "nk ri, and if n¼Pki¼1ri, then both of the following are true: (i) Each of A1,A2,...,Akis idempotent. (ii)AiAj¼Ofori=j.A2.13 IDEMPOTENT MATRICES 55 --- Page 72 --- 2.14 VECTOR AND MATRIX CALCULUS 2.14.1 Derivatives of Functions of Vectors and Matrices Letu¼f(x) be a function of the variables x1,x2,...,xpinx¼(x1,x2,...,xp)0, and let@u=@x1,@u=@x2,...,@u=@xpbe the partial derivatives. We deﬁne @u=@xas @u @x¼@u @x1 @u @x2 ...",
    "and let@u=@x1,@u=@x2,...,@u=@xpbe the partial derivatives. We deﬁne @u=@xas @u @x¼@u @x1 @u @x2 ... @u @xp0 BBBBBBBBB@1 CCCCCCCCCA: (2:111) Two speciﬁc functions of interest are u¼a 0xandu¼x0Ax. Their derivatives with respect to xare given in the following two theorems. Theorem 2.14a. Letu¼a0x¼x0a, where a0¼(a1,a2,...,ap) is a vector of con- stants. Then @u @x¼@(a0x) @x¼@(x0a) @x¼a: (2:112) PROOF @u @xi¼@(a1x1þa2x2þ/C1/C1/C1þ apxp) @xi¼ai: Thus by (2.111) we obtain @u @x¼a1 a2 ...",
    "(2:112) PROOF @u @xi¼@(a1x1þa2x2þ/C1/C1/C1þ apxp) @xi¼ai: Thus by (2.111) we obtain @u @x¼a1 a2 ... ap0 BBB@1 CCCA¼a: A Theorem 2.14b. Letu¼x 0Ax, where Ais a symmetric matrix of constants. Then @u @x¼@(x0Ax) @x¼2Ax: (2:113)56 MATRIX ALGEBRA --- Page 73 --- PROOF. We demonstrate that (2.113) holds for the special case in which Ais 3/C23. The illustration could be generalized to a symmetric Aof any size.",
    "special case in which Ais 3/C23. The illustration could be generalized to a symmetric Aof any size. Let x¼x1 x2 x30 @1A and A¼a 11a12a13 a12a22a23 a13a23a330@1A¼a 0 1 a02 a030 @1A: Then x 0Ax¼x2 1a11þ2x1x2a12þ2x1x3a13þx22a22þ2x2x3a23þx23a33, and we have @(x0Ax) @x1¼2x1a11þ2x2a12þ2x3a13¼2a0 1x @(x0Ax) @x2¼2x1a12þ2x2a22þ2x3a23¼2a02x @(x0Ax) @x3¼2x1a13þ2x2a23þ2x3a33¼2a03x: Thus by (2.11), (2.27), and (2.111), we obtain @(x0Ax) @x¼@(x0Ax) @x1 @(x0Ax) @x2 @(x0Ax) @x30 BBBBBB@1 CCCCCCA¼2a 0 1x a02x a0 3x0 @1A¼2Ax: A Now let u¼f(X) be a function of the variables x 11,x12,...,xppin the p/C2p matrix X, and let ( @u=@x11),(@u=@x12),...,(@u=@xpp) be the partial derivatives.",
    "...,xppin the p/C2p matrix X, and let ( @u=@x11),(@u=@x12),...,(@u=@xpp) be the partial derivatives. Similarly to (2.111), we deﬁne @u=@Xas @u @X¼@u @x11/C1/C1/C1@u @x1p ...... @u @xp1/C1/C1/C1@u @xpp0 BBBBB@1 CCCCCA: (2:114) Two functions of interest of this type are u¼tr(XA) and u¼lnjXjfor a positive deﬁnite matrix X. Theorem 2.14c. Letu¼tr(XA), where Xis ap/C2ppositive deﬁnite matrix and Ais ap/C2pmatrix of constants.",
    "eorem 2.14c. Letu¼tr(XA), where Xis ap/C2ppositive deﬁnite matrix and Ais ap/C2pmatrix of constants. Then @u @X¼@[tr(XA)] @X¼AþA0/C0diagA: (2:115)2.14 VECTOR AND MATRIX CALCULUS 57 --- Page 74 --- PROOF. Note that tr( XA)¼Pp i¼1Pp j¼1xijaji[see the proof of Theorem 2.11(ii)]. Since xij¼xji,[@tr(XA)]=@xij¼ajiþaijifi=j, and [ @tr(XA)]=@xii¼aii. The result follows. A Theorem 2.14d. Letu¼lnjXjwhere Xis ap/C2ppositive deﬁnite matrix. Then @lnjXj @X¼2X/C01/C0diag(X/C01): (2:116) PROOF.",
    "tu¼lnjXjwhere Xis ap/C2ppositive deﬁnite matrix. Then @lnjXj @X¼2X/C01/C0diag(X/C01): (2:116) PROOF. See Harville (1997, p. 306). See Problem 2.83 for a demonstration that this theorem holds for 2 /C22 matrices. A 2.14.2 Derivatives Involving Inverse Matrices and Determinants LetAbe an n/C2nnonsingular matrix with elements aijthat are functions of a scalar x. We deﬁne @A=@xas the n/C2nmatrix with elements @aij=@x. The related derivative @A/C01=@xis often of interest.",
    "@A=@xas the n/C2nmatrix with elements @aij=@x. The related derivative @A/C01=@xis often of interest. If Ais positive deﬁnite, the derivative (@=@x)logjAjis also often of interest. Theorem 2.14e. LetAbe nonsingular of order nwith derivative @A=@x. Then @A @x/C01 ¼/C0A/C01@A @xA/C01(2:117) PROOF. Because Ais nonsingular, we have A/C01A¼I: Thus @A/C01 @xAþA/C01@A @x¼O: Hence @A/C01 @xA¼/C0A/C01@A @x, and so @A/C01 @x¼/C0A/C01@A @xA/C01:A Theorem 2.14f. LetAbe an n/C2npositive deﬁne matrix.",
    "C01@A @x, and so @A/C01 @x¼/C0A/C01@A @xA/C01:A Theorem 2.14f. LetAbe an n/C2npositive deﬁne matrix. Then @logjAj @x¼trA/C01@A @x/C18/C19 : (2:118)58 MATRIX ALGEBRA --- Page 75 --- PROOF. Since Ais positive deﬁnite, its spectral decomposition (Theorem 2.12d) can be written as CDC0, where Cis an orthogonal matrix and Dis a diagonal matrix of positive eigenvalues, li.",
    "itten as CDC0, where Cis an orthogonal matrix and Dis a diagonal matrix of positive eigenvalues, li. Using Theorem 2.12e, we obtain @logjAj @x¼@logQn i¼1li @x ¼@Pni¼1logli @x ¼Xn i¼11 li@li @x ¼trD/C01@D @x/C18/C19 : Now A/C01@A @x¼CD/C01C0@CDC0 @x ¼CD/C01C0C@DC0 @xþ@C @xDC0/C20/C21 ¼CD/C01C0C@D @xC0þCD@C0 @xþ@C @xDC0/C20/C21 ¼CD/C01@D @xC0þC@C0 @xþCD/C01C0@C @xDC0: Using Theorem 2.11(i) and (ii), we have trA/C01@A @x/C18/C19 ¼trD/C01@D @xþC@C0 @xþC0@C @x/C18/C19 : Since Cis orthogonal, C0C¼Iwhich implies that @C0C @x¼C0@C @xþ@C0 @xC¼O and trC0@C @xþ@C0C @x/C18/C19 ¼trC0@C @xþC@C0 @x/C18/C19 ¼0: Thus tr[ A/C01(@A=@x)]¼tr[D/C01(@D=@x)] and the result follows.",
    "8/C19 ¼trC0@C @xþC@C0 @x/C18/C19 ¼0: Thus tr[ A/C01(@A=@x)]¼tr[D/C01(@D=@x)] and the result follows. A2.14 VECTOR AND MATRIX CALCULUS 59 --- Page 76 --- 2.14.3 Maximization or Minimization of a Function of a Vector Consider a function u¼f(x) of the pvariables in x. In many cases we can ﬁnd a maximum or minimum of uby solving the system of pequations @u @x¼0: (2:119) Occasionally the situation requires the maximization or minimization of the func- tion u, subject to qconstraints on x.",
    "tuation requires the maximization or minimization of the func- tion u, subject to qconstraints on x. We denote the constraints as h1(x)¼0,h2(x)¼0,...,hq(x)¼0 or, more succinctly, h(x)¼0. Maximization or minimization of usubject to h(x)¼0can often be carried out by the method of Lagrange multipliers. We denote a vector of qunknown constants (the Lagrange mul- tipliers )b yland let y0¼(x0,l0). We then let v¼uþl0h(x).",
    "r of qunknown constants (the Lagrange mul- tipliers )b yland let y0¼(x0,l0). We then let v¼uþl0h(x). The maximum or minimum of usubject to h(x)¼0is obtained by solving the equations @v @y¼0 or, equivalently @u @xþ@h @xl¼0and h(x)¼0, (2:120) where @h @x¼@h1 @x1/C1/C1/C1@hq @x1 ...... @h1 @xp/C1/C1/C1@hq @xp0 BBBBB@1 CCCCCA: PROBLEMS 2.1 Prove Theorem 2.2a. 2.2 LetA¼7/C032 49 5/C18/C19 : (a) Find A 0. (b) Verify that ( A0)0¼A, thus illustrating Theorem 2.1. (c) Find A0AandAA0.",
    "C18/C19 : (a) Find A 0. (b) Verify that ( A0)0¼A, thus illustrating Theorem 2.1. (c) Find A0AandAA0. 2.3 LetA¼24 /C013/C18/C19 andB¼13 2/C01/C18/C19 .60 MATRIX ALGEBRA --- Page 77 --- (a) Find ABandBA. (b) Find jAj,jBj, andjABj, and verify that Theorem 2.9c holds in this case. (c) Find jBAjand compare to jABj. (d) Find ( AB)0and compare to B0A0. (e) Find tr( AB) and compare to tr( BA). (f) Find the eigenvalues of ABand of BA, thus illustrating Theorem 2.12a.",
    "B) and compare to tr( BA). (f) Find the eigenvalues of ABand of BA, thus illustrating Theorem 2.12a. 2.4 LetA¼13 /C04 5/C072/C18/C19 andB¼3/C025 69 7/C18/C19 . (a) Find AþBandA/C0B. (b) Find A0andB0. (c) Find ( AþB)0andA0þB0, thus illustrating Theorem 2.2a(ii). 2.5 Verify the distributive law in (2.15), A(BþC)¼ABþAC. 2.6 LetA¼83 7 /C025 /C03/C18/C19 , B¼/C025 376/C040 @1 A,C¼12 /C031 240 @1 A. (a) Find ABandBA. (b) Find BþC,AC, andA(BþC). Compare A(BþC) with ABþAC, thus illustrating (2.15).",
    ". (a) Find ABandBA. (b) Find BþC,AC, andA(BþC). Compare A(BþC) with ABþAC, thus illustrating (2.15). (c) Compare ( AB) 0withB0A0, thus illustrating Theorem 2.2b. (d) Compare tr( AB) with tr( BA) and conﬁrm that (2.87) holds in this case. (e) Let a0 1anda02be the two rows of A. Finda01B a02B/C18/C19 and compare with ABin part (a), thus illustrating (2.27). (f) Let b1andb2be the two columns of B. Find ( Ab1,Ab2) and compare with ABin part (a), thus illustrating (2.28).",
    "b2be the two columns of B. Find ( Ab1,Ab2) and compare with ABin part (a), thus illustrating (2.28). 2.7 LetA¼32 1 64 2 12 8 40 @1A,B¼1/C012 /C011 /C02 /C011 /C020@1A. (a) Show that AB¼O. (b) Find a vector xsuch that Ax¼0. (c) What is the rank of Aand the rank of B? 2.8 Ifjis a vector of 1s, as deﬁned in (2.6), show that (a)j 0a¼a0j¼P iai, as in (2.24). (b)Ajis a column vector whose elements are the row sums of A, as in (2.25).",
    "a0j¼P iai, as in (2.24). (b)Ajis a column vector whose elements are the row sums of A, as in (2.25). (c)j0Ais a row vector whose elements are the column sums of A, as in (2.25).PROBLEMS 61 --- Page 78 --- 2.9 Prove Corollary 1 to Theorem 2.2b; that is, assuming that A,B, and Care conformal, show that ( ABC )0¼C0B0A0. 2.10 Prove Theorem 2.2c. 2.11 Use matrix Ain Problem 2.6 and let D1¼30 0/C02/C18/C19 , D2¼500 030 0060 @1A: Find D 1AandAD 2, thus illustrating (2.29) and (2.30).",
    "et D1¼30 0/C02/C18/C19 , D2¼500 030 0060 @1A: Find D 1AandAD 2, thus illustrating (2.29) and (2.30). 2.12 LetA¼123 4567890 @1 A, D¼a00 0b0 00 c0 @1 A. Find DA,AD, and DAD . 2.13 Fory 0¼(y1,y2,y3) and the symmetric matrix A¼a11a12a13 a12a22a23 a13a23a330 @1A, express y 0Ayin the form given in (2.33). 2.14 Let A¼5/C013 /C011 2 32 70 @1 A,B¼6/C023 71 0 2/C0350 @1 A,C¼2/C03 /C014 310 @1 A, x¼3 /C01 20 @1A,y¼3 240 @1A,z¼2 5/C18/C19 :.",
    "@1 A,B¼6/C023 71 0 2/C0350 @1 A,C¼2/C03 /C014 310 @1 A, x¼3 /C01 20 @1A,y¼3 240 @1A,z¼2 5/C18/C19 :. Find the following: (a)Bx (h)xy0 (b)y0B (i)B0B (c)x0Ax (j)yz0 (d)x0Cz (k)zy0 (e)x0x (l)ﬃﬃﬃﬃﬃﬃy0yp (f)x0y (m)C0C (g)xx0 2.15 Usex,y,A, and Bas deﬁned in Problem 2.14. (a) Find xþyandx/C0y.62 MATRIX ALGEBRA --- Page 79 --- (b) Find tr( A), tr(B),AþB, and tr( AþB). (c) Find ABandBA. (d) Find tr( AB) and tr( BA). (e) Find jABjandjBAj. (f) Find ( AB)0andB0A0.",
    "AþB). (c) Find ABandBA. (d) Find tr( AB) and tr( BA). (e) Find jABjandjBAj. (f) Find ( AB)0andB0A0. 2.16 Using Bandxin Problem 2.14, ﬁnd Bxas a linear combination of the columns ofB, as in (2.37), and compare with Bxas found in Problem 2.14(a). 2.17 LetA¼25 13/C18/C19 ,B¼1/C062 50 3/C18/C19 ,I¼1001/C18/C19 . (a) Show that ( AB) 0¼B0A0as in (2.26). (b) Show that AI¼Aand that IB¼B. (c) Find jAj. (d) Find A/C01. (e) Find ( A/C01)/C01and compare with A, thus verifying (2.46).",
    "IB¼B. (c) Find jAj. (d) Find A/C01. (e) Find ( A/C01)/C01and compare with A, thus verifying (2.46). (f) Find ( A0)/C01and verify that it is equal to ( A/C01)0as in Theorem 2.5a. 2.18 LetAandBbe deﬁned and partitioned as follows: A¼21 2 32 0 10 10 B@1 CA, B¼111 0 211 2 231 20 B@1 CA: (a) Find ABas in (2.35), using the indicated partitioning. (b) Check by ﬁnding ABin the usual way, ignoring the partitioning.",
    "using the indicated partitioning. (b) Check by ﬁnding ABin the usual way, ignoring the partitioning. 2.19 Partition the matrices AandBin Problem 2.18 as follows: A¼212 320 1010 @1A¼(a 1,A2), B¼1110 2112 23120 @1A¼b 0 1 B2/C18/C19 : Repeat parts (a) and (b) of Problem 2.18. Note that in this case, (2.35) becomes AB¼a1b0 1þA2B2.PROBLEMS 63 --- Page 80 --- 2.20 LetA¼5/C023 73 1/C18/C19 , b¼2 4 /C030 @1A.",
    "becomes AB¼a1b0 1þA2B2.PROBLEMS 63 --- Page 80 --- 2.20 LetA¼5/C023 73 1/C18/C19 , b¼2 4 /C030 @1A. Find Abas a linear combination of the columns of Aas in (2.37) and check the result by ﬁnding Abin the usual way. 2.21 Show that each column of the product ABcan be expressed as a linear com- bination of the columns of A, with coefﬁcients arising from the corresponding column of B, as noted following Example 2.3. 2.22 LetA¼30 2 1/C011 21 00 @1A, B¼/C02/C01 31 1/C010 @1A.",
    "umn of B, as noted following Example 2.3. 2.22 LetA¼30 2 1/C011 21 00 @1A, B¼/C02/C01 31 1/C010 @1A. Express the columns of ABas linear combinations of the columns of A. 2.23 Show that if a set of vectors includes 0, the set is linearly dependent, as noted following (2.40). 2.24 Suppose that AandBaren/C2nand that AB¼Oas in (2.43). Show that A andBare both singular or one of them is O. 2.25 LetA¼ 13 2 20 /C01/C18/C19 ,B¼1201100 @1A,C¼211 5/C06/C04/C18/C19 . Find ABandCB. Are they equal?",
    "25 LetA¼ 13 2 20 /C01/C18/C19 ,B¼1201100 @1A,C¼211 5/C06/C04/C18/C19 . Find ABandCB. Are they equal? What are the ranks of A,B, and C? 2.26 LetA¼31 2 10 /C01/C18/C19 , B¼21 02 100 @1A. (a) Find a matrix Csuch that AB¼CB.I sCunique? (b) Find a vector xsuch that Ax¼0. Can you do this for B? 2.27 LetA¼312 4/C023 10 /C010 @1A, x¼5 230 @1 A. (a) Find a matrix B=Asuch that Ax¼Bx. Why is this possible? Can A andBbe nonsingular? Can A/C0Bbe nonsingular? (b) Find a matrix C=Osuch that Cx¼0.",
    "is possible? Can A andBbe nonsingular? Can A/C0Bbe nonsingular? (b) Find a matrix C=Osuch that Cx¼0. Can Cbe nonsingular? 2.28 Prove Theorem 2.5a. 2.29 Prove Theorem 2.5b. 2.30 Use the matrix Ain Problem 2.17, and let B¼4/C02 31/C18/C19 . Find AB,B21, and ( AB)21. Verify that Theorem 2.5b holds in this case.64 MATRIX ALGEBRA --- Page 81 --- 2.31 Show that the partitioned matrix A¼A11A12 A21A22/C18/C19 has the inverse indicated in (2.50).",
    "- 2.31 Show that the partitioned matrix A¼A11A12 A21A22/C18/C19 has the inverse indicated in (2.50). 2.32 Show that the partitioned matrix A¼A11a12 a0 12a22/C18/C19 has the inverse given in (2.51). 2.33 Show that Bþcc0has the inverse indicated in (2.53). 2.34 Show that AþPBQ has the inverse indicated in (2.54). 2.35 Show that y0Ay¼y01 2(AþA0)/C2/C3 yas in (2.55). 2.36 Prove Theorem 2.6b(ii). 2.37 Prove Corollaries 1 and 2 of Theorem 2.6b. 2.38 Prove the “only if” part of Theorem 2.6c.",
    "(ii). 2.37 Prove Corollaries 1 and 2 of Theorem 2.6b. 2.38 Prove the “only if” part of Theorem 2.6c. 2.39 Prove Corollary 1 to Theorem 2.6c. 2.40 Compare the rank of the augmented matrix with the rank of the coefﬁcient matrix for each of the following systems of equations. Find solutions wherethey exist. ðaÞx 1þ2x2þ3x3¼6 x1/C0x2¼2 x1/C0x3¼/C01ðbÞx1/C0x2þ2x3¼2 x1/C0x2/C0x3¼/C01 2x1/C02x2þx3¼2 ðcÞx1þx2þx3þx4¼8 x1/C0x2/C0x3/C0x4¼6 3x1þx2þx3þx4¼22 2.41 Prove Theorem 2.8a.",
    "x3¼/C01 2x1/C02x2þx3¼2 ðcÞx1þx2þx3þx4¼8 x1/C0x2/C0x3/C0x4¼6 3x1þx2þx3þx4¼22 2.41 Prove Theorem 2.8a. 2.42 For the matrices A,A/C0 1, and A/C02in (2.59) and (2.60), show that AA/C01A¼A andAA/C0 2A¼A. 2.43 Show that A/C01in (2.60) can be obtained using Theorem 2.8b. 2.44 Show that A/C02in (2.60) can be obtained using the ﬁve-step algorithm follow- ing Theorem 2.8b. 2.45 Prove Theorem 2.8c.",
    ".60) can be obtained using the ﬁve-step algorithm follow- ing Theorem 2.8b. 2.45 Prove Theorem 2.8c. 2.46 Show that if Ais symmetric, there exists a symmetric generalized inverse for A, as noted following Theorem 2.8c. 2.47 LetA¼422 220 2020 @1A. (a) Find a symmetric generalized inverse for A.PROBLEMS 65 --- Page 82 --- (b) Find a nonsymmetric generalized inverse for A. 2.48 (a) Show that if Ais nonsingular, then A/C0¼A/C01.",
    "nd a nonsymmetric generalized inverse for A. 2.48 (a) Show that if Ais nonsingular, then A/C0¼A/C01. (b)Show that if Aisn/C2pof rank p,n, then A/C0is a “left inverse” of A, that is, A/C0A¼I. 2.49 Prove Theorem 2.9a parts (iv) and (vi). 2.50 UseA¼25 13/C18/C19 from Problem 2.17 to illustrate (64), (2.66), and (2.67) in Theorem 2.9a. 2.51 (a) Multiply Ain Problem 2.50 by 10 and verify that (2.69) holds in this case. (b)Verify that (2.69) holds in general.",
    "roblem 2.50 by 10 and verify that (2.69) holds in this case. (b)Verify that (2.69) holds in general. 2.52 Prove Corollaries 1, 2, 3, and 4 of Theorem 2.9b. 2.53 Prove Corollaries 1 and 2 of Theorem 2.9c. 2.54 UseAin Problem 2.50 and let B¼4/C02 31/C18/C19 . (a) Find jAj,jBj,AB, andjABjand illustrate (2.77). (b) Find jAj2andjA2jand illustrate (2.79). 2.55 Use Theorem 2.9c and Corollary 1 of Theorem 2.9b to prove Theorem 2.9b. 2.56 Show that if C0C¼I, then CC0¼Ias in (2.84).",
    "Corollary 1 of Theorem 2.9b to prove Theorem 2.9b. 2.56 Show that if C0C¼I, then CC0¼Ias in (2.84). 2.57 The columns of the following matrix are mutually orthogonal: A¼1/C011 /C010 2 11 10 @1 A: (a) Normalize the columns of Aby dividing each column by its length; denote the resulting matrix by C. (b) Show that C0C¼CC0¼I. 2.58 Prove Theorem 2.10a. 2.59 Prove Theorem 2.11 parts (i), (iv), (v), and (vii). 2.60 Use matrix Bin Problem 2.26 to illustrate Theorem 2.11 parts (iii) and (iv).",
    "), (v), and (vii). 2.60 Use matrix Bin Problem 2.26 to illustrate Theorem 2.11 parts (iii) and (iv). 2.61 Use matrix Ain Problem 2.26 to illustrate Theorem 2.11(v), that is, tr(A0A)¼tr(AA0)¼P ija2 ij. 2.62 Show that tr( A/C0A)¼tr(AA/C0)¼r¼rank(A), as in (2.93). 2.63 Use Ain (2.59) and A/C0 2in (2.60) to illustrate Theorem 2.11(viii), that is, tr(A/C0A)¼tr(AA/C0)¼r¼rank(A).66 MATRIX ALGEBRA --- Page 83 --- 2.64 Obtain x2¼(2=ﬃﬃﬃ 5p ,1=ﬃﬃﬃ5p )0in Example 2.12.1.",
    ")¼r¼rank(A).66 MATRIX ALGEBRA --- Page 83 --- 2.64 Obtain x2¼(2=ﬃﬃﬃ 5p ,1=ﬃﬃﬃ5p )0in Example 2.12.1. 2.65 Fork¼3, show that Akx¼lkxas in (2.100). 2.66 Show that lim k!1Ak¼Oin (2.102) if Ais symmetric and if all eigenvalues ofAsatisfy /C01,l,1. 2.67 Prove Theorem 2.12a. 2.68 Prove Theorem 2.12b. 2.69 Prove Theorem 2.12c(ii) for the case where the eigenvalues l1,l2,...,lnare distinct. 2.70 Prove Corollary 1 to Theorem 2.12d. 2.71 LetA¼311 102 1200 @1A. (a) The eigenvalues of Aare 1, 4, 22.",
    "rove Corollary 1 to Theorem 2.12d. 2.71 LetA¼311 102 1200 @1A. (a) The eigenvalues of Aare 1, 4, 22. Find the normalized eigenvectors and use them as columns in an orthogonal matrix C. (b) Show that A¼CDC 0, as in (2.103), where D¼diag(1 ,4,/C02). (c) Show that C0AC¼Das in (2.106). 2.72 Prove Theorem 2.12e for a symmetric matrix A. 2.73 LetA¼11 /C02 /C012 1 01 /C010 @1A. (a) Find the eigenvalues and associated normalized eigenvectors.",
    "LetA¼11 /C02 /C012 1 01 /C010 @1A. (a) Find the eigenvalues and associated normalized eigenvectors. (b) Find tr( A) and jAjand verify that tr( A)¼P 3 i¼1liandjAj¼Q3i¼1li,a s in Theorem 2.12e. 2.74 Prove Theorem 2.12f(ii). 2.75 LetA¼10 /C01 01 /C01 /C01/C0130 @1A. (a) Show that jAj.0. (b) Find the eigenvalues of A. Are they all positive? 2.76 LetA 1=2be deﬁned as in (2.109).PROBLEMS 67 --- Page 84 --- (a) Show that A1=2is symmetric. (b) Show that ( A1=2)2¼Aas in (2.110).",
    ").PROBLEMS 67 --- Page 84 --- (a) Show that A1=2is symmetric. (b) Show that ( A1=2)2¼Aas in (2.110). 2.77 For the positive deﬁnite matrix A¼2/C01 /C012/C18/C19 , calculate the eigenvalues and eigenvectors and ﬁnd the square root matrix A1=2as in (2.109). Check by showing ( A1=2)2¼A. 2.78 Prove Theorem 2.13e. 2.79 Prove Theorem 2.13f. 2.80 LetA¼2 30ﬃﬃ 2p 3 010ﬃﬃ 2p 301 30 @1A. (a) Find the rank of A. (b) Show that Ais idempotent. (c) Show that I/C0Ais dempotent. (d) Show that A(I/C0A)¼O.",
    "rank of A. (b) Show that Ais idempotent. (c) Show that I/C0Ais dempotent. (d) Show that A(I/C0A)¼O. (e) Find tr( A). (f) Find the eigenvalues of A. 2.81 Consider a p/C2pmatrix Awith eigenvalues l1,l2,...,lp. Show that [tr(A)]2¼tr(A2)þ2PP i=jlilj: 2.82 Consider a nonsingular n/C2nmatrix Awhose elements are functions of the scalar x. Also consider the full-rank p/C2nmatrix B. LetH¼B0(BAB0)/C01B.",
    "ements are functions of the scalar x. Also consider the full-rank p/C2nmatrix B. LetH¼B0(BAB0)/C01B. Show that @H @x¼/C0H@A @xH: 2.83 Show that @lnjXj @X¼2X/C01/C0diagX/C01 for a 2 /C22 positive deﬁnite matrix X. 2.84 Letu¼x0Axwhere xis a 3 /C21 vector and A¼100 020 0030 @1A.",
    "a 2 /C22 positive deﬁnite matrix X. 2.84 Letu¼x0Axwhere xis a 3 /C21 vector and A¼100 020 0030 @1A. Use the Lagrange multiplier method to ﬁnd the vector xthat minimizes usubject to the constraints x 1þx2¼2, and x2þx3¼3.68 MATRIX ALGEBRA --- Page 85 --- 3Random Vectors and Matrices 3.1 INTODUCTION As we work with linear models, it is often convenient to express the observed data (or data that will be observed) in the form of a vector or matrix.",
    "ient to express the observed data (or data that will be observed) in the form of a vector or matrix. A random vector orrandom matrix is a vector or matrix whose elements are random variables. Informally, a random variable is deﬁned as a variable whose value depends on the outcome of a chance experiment. (Formally, a random variable is a function deﬁned for each element of a sample space.) In terms of experimental structure, we can distinguish two kinds of random vectors: 1.",
    "ample space.) In terms of experimental structure, we can distinguish two kinds of random vectors: 1. A vector containing a measurement on each of ndifferent individuals or exper- imental units. In this case, where the same variable is observed on each of n units selected at random, the nrandom variables y1,y2,...,ynin the vector are typically uncorrelated and have the same variance. 2. A vector consisting of pdifferent measurements on one individual or exper- imental unit.",
    "ariance. 2. A vector consisting of pdifferent measurements on one individual or exper- imental unit. The prandom variables thus obtained are typically correlated and have different variances. To illustrate the ﬁrst type of random vector, consider the multiple regression model yi¼b0þb1xi1þb2xi2þ/C1/C1/C1þbkxikþ1i,i¼1,2,... ,n, as given in (1.2). In Chapters 7–9, we treat the xvariables as constants, in which case we have two random vectors: y¼y1 y2 ... yn0 BBB@1 CCCAand 1¼11 12 ...",
    "es as constants, in which case we have two random vectors: y¼y1 y2 ... yn0 BBB@1 CCCAand 1¼11 12 ... 1n0 BBB@1 CCCA: (3:1) Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 69 --- Page 86 --- The yivalues are observable, but the 1i’s are not observable unless the b’s are known.",
    "- Page 86 --- The yivalues are observable, but the 1i’s are not observable unless the b’s are known. To illustrate the second type of random vector, consider regression of yon several random xvariables (this regression case is discussed in Chapter 10). For the ith indi- vidual in the sample, we observe the kþ1 random variables yi,xi1,xi2,...,xik, which constitute the random vector ( yi,xi1,...,xik)0.",
    "the kþ1 random variables yi,xi1,xi2,...,xik, which constitute the random vector ( yi,xi1,...,xik)0. In some cases, the kþ1 variables yi,xi1,...,xikare all measured using the same units or scale of measurement, but typically the scales differ. 3.2 MEANS, VARIANCES, COVARIANCES, AND CORRELATIONS In this section, we review some properties of univariate and bivariate random vari- ables. We begin with a univariate random variable y.",
    "erties of univariate and bivariate random vari- ables. We begin with a univariate random variable y. We do not distinguish notation- ally between the random variable yand an observed value of y. In many texts, an uppercase letter is used for the random variable and the corresponding lowercaseletter represents a realization of the random variable, as in the expression P(Y/C20y).",
    "ing lowercaseletter represents a realization of the random variable, as in the expression P(Y/C20y). This practice is convenient in a univariate context but would be confusing in the present text where we use uppercase letters for matrices and lowercase letters for vectors. Iff(y) is the density of the random variable y, the mean orexpected value ofyis deﬁned as m¼E(y)¼ð1 /C01yf(y)dy: (3:2) This is the population mean.",
    "the mean orexpected value ofyis deﬁned as m¼E(y)¼ð1 /C01yf(y)dy: (3:2) This is the population mean. Later (beginning in Chapter 5), we also use the sample mean of y, obtained from a random sample of nobserved values of y. The expected value of a function of ysuch as y2can be found directly without ﬁrst ﬁnding the density of y2.",
    "ected value of a function of ysuch as y2can be found directly without ﬁrst ﬁnding the density of y2. In general, for a function u(y), we have E[u(y)]¼ð1 /C01u(y)f(y)dy: (3:3) For a constant aand functions u(y) and v(y), it follows from (3.3) that E(ay)¼aE(y), (3:4) E[u(y)þv(y)]¼E[u(y)]þE[v(y)]: (3:5) Thevariance of a random variable yis deﬁned as s2¼var(y)¼E(y/C0m)2, (3:6)70 RANDOM VECTORS AND MATRICES --- Page 87 --- This is the population variance.",
    "r(y)¼E(y/C0m)2, (3:6)70 RANDOM VECTORS AND MATRICES --- Page 87 --- This is the population variance. Later (beginning in Chapter 5), we also use the sample variance of y, obtained from a random sample of nobserved values of y.",
    "er 5), we also use the sample variance of y, obtained from a random sample of nobserved values of y. The square root of the variance is known as the standard deviation : s¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ var(y)p ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ E(y/C0m)2q : (3:7) Using (3.4) and (3.5), we can express the variance of yin the form s2¼var(y)¼E(y2)/C0m2: (3:8) Ifais a constant, we can use (3.4) and (3.6) to show that var(ay)¼a2var(y)¼a2s2: (3:9) For any two variables yiandyjin the random vector yin (3.1), we deﬁne the covariance as sij¼cov(yi,yj)¼E[(yi/C0mi)(yj/C0mj)], (3:10) wheremi¼E(yi) andmj¼E(yj).",
    ", we deﬁne the covariance as sij¼cov(yi,yj)¼E[(yi/C0mi)(yj/C0mj)], (3:10) wheremi¼E(yi) andmj¼E(yj). Using (3.4) and (3.5), we can express sijin the form sij¼cov(yi,yj)¼E(yiyj)/C0mimj: (3:11) Two random variables yiandyjare said to be independent if their joint density factors into the product of their marginal densities f(yi,yj)¼fi(yi)fj(yj), (3:12) where the marginal density fi(yi) is deﬁned as fi(yi)¼ð1 /C01f(yi,yj)dyj: (3:13) From the deﬁnition of independence in (3.12), we obtain the following properties: 1:E(yi,yj)¼E(yi)E(yj)i fyiandyjare independent.",
    "dence in (3.12), we obtain the following properties: 1:E(yi,yj)¼E(yi)E(yj)i fyiandyjare independent. (3 :14) 2:sij¼cov(yi,yj)¼0i fyiandyjare independent. (3 :15) The second property follows from the ﬁrst.3.2 MEANS, VARIANCES, COVARIANCES, AND CORRELATIONS 71 --- Page 88 --- In the ﬁrst type of random vector deﬁned in Section 3.1, the variables y1,y2,...,yn would typically be independent if obtained from a random sample, and we would thus have sij¼0 for all i=j.",
    "typically be independent if obtained from a random sample, and we would thus have sij¼0 for all i=j. However, for the variables in the second type of random vector, we would typically have sij=0 for at least some values of iandj. The converse of the property in (3.15) is not true; that is, sij¼0 does not imply independence. This is illustrated in the following example. Example 3.2.",
    "at is, sij¼0 does not imply independence. This is illustrated in the following example. Example 3.2. Suppose that the bivariate random variable ( x,y) is distributed uni- formly over the region 0 /C20x/C202, 2x2x2/C20y/C201þ2x2x2; see Figure 3.1. The area of the region is given by Area ¼ð2 0ð1þ2x/C0x2 2x/C0x2dy dx ¼2: Hence, for a uniform distribution over the region, we set f(x,y)¼1 2,0/C20x/C202,2x/C0x2/C20y/C201þ2x/C0x2, so thatÐÐ f(x,y)dx dy¼1.",
    "n over the region, we set f(x,y)¼1 2,0/C20x/C202,2x/C0x2/C20y/C201þ2x/C0x2, so thatÐÐ f(x,y)dx dy¼1. To ﬁndsxyusing (3.11), we need E(xy),E(x), and E(y).",
    "0x2/C20y/C201þ2x/C0x2, so thatÐÐ f(x,y)dx dy¼1. To ﬁndsxyusing (3.11), we need E(xy),E(x), and E(y). The ﬁrst of these is given by E(xy)¼ð2 0ð1þ2x/C0x2 2x/C0x2xy1 2/C0/C1 dy dx ¼ð2 0x 4(1þ4x/C02x2)dx¼7 6:Figure 3.1 Region for f(x,y) in Example 3.2.72 RANDOM VECTORS AND MATRICES --- Page 89 --- To ﬁnd E(x) and E(y), we ﬁrst ﬁnd the marginal distributions of xandy.F o r f1(x), we have, by (3.13), f1(x)¼ð1þ2x/C0x2 2x/C0x21 2dy¼12,0/C20x/C202: Forf2(y), we obtain different results for 0 /C20y/C201 and 1 /C20y/C202: f2(y)¼ð1/C0ﬃﬃﬃﬃﬃﬃ1/C0yp 01 2dxþð2 1þﬃﬃﬃﬃﬃﬃ1/C0yp1 2dx¼1/C0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0yp,0/C20y/C201,(3:16) f2(y)¼ð1þﬃﬃﬃﬃﬃﬃ2/C0yp 1/C0ﬃﬃﬃﬃﬃﬃ2/C0yp1 2dx¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ2/C0yp,1/C20y/C202: (3:17) Then E(x)¼ð2 0x1 2/C0/C1 dx¼1, E(y)¼ð1 0y(1/C0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1/C0yp )dyþð2 1yﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ2/C0yp dy¼7 6: Now by (3.11), we obtain sxy¼E(xy)/C0E(x)E(y) ¼7 6/C0(1)76/C0/C1 ¼0: However, xandyare clearly dependent since the range of yfor each xdepends on the value of x.",
    "/C1 ¼0: However, xandyare clearly dependent since the range of yfor each xdepends on the value of x. As a further indication of the dependence of yonx, we examine E(yjx), the expected value of yfor a given value of x, which is found as E(yjx)¼ð yf(yjx)dy: The conditional density f(yjx) is deﬁned as f(yjx)¼f(x,y) f1(x), (3:18)3.2 MEANS, VARIANCES, COVARIANCES, AND CORRELATIONS 73 --- Page 90 --- which becomes f(yjx)¼1 2 12¼1,2x/C0x2/C20y/C201þ2x/C0x2: Thus E(yjx)¼ð1þ2x/C0x2 2x/C0x2y(1)dy ¼12(1þ4x/C02x2): Since E(yjx) depends on x, the two variables are dependent.",
    "1þ2x/C0x2 2x/C0x2y(1)dy ¼12(1þ4x/C02x2): Since E(yjx) depends on x, the two variables are dependent. Note that E(yjx)¼12(1þ4x/C02x2) is the average of the two curves y¼2x2x2and y¼ 1þ2x2x2. This is illustrated in Figure 3.2. A In Example 3.2 we have two dependent random variables xandyfor which sxy¼0. In cases such as this, sxyis not a good measure of relationship.",
    "dom variables xandyfor which sxy¼0. In cases such as this, sxyis not a good measure of relationship. However, if xandyhave a bivariate normal distribution (see Section 4.2), then sxy¼0 implies independence of xandy(see Corollary 1 to Theorem 4.4c). In the bivariate normal case, E(yjx)i sa linear function of x(see Theorem 4.4d), and curves such as E(yjx)¼1 2(1þ4x/C02x2) do not occur. The covariance sijas deﬁned in (3.10) depends on the scale of measurement of both yiandyj.",
    "ot occur. The covariance sijas deﬁned in (3.10) depends on the scale of measurement of both yiandyj. To standardize sij, we divide it by (the product of) the standard devi- ations of yiandyjto obtain the correlation : rij¼corr( yi,yj)¼sij sisj:(3:19)Figure 3.2 E(yjx) in Example 3.2.74 RANDOM VECTORS AND MATRICES --- Page 91 --- 3.3 MEAN VECTORS AND COVARIANCE MATRICES FOR RANDOM VECTORS 3.3.1 Mean Vectors The expected value of a p/C21 random vector yis deﬁned as the vector of expected values of the prandom variables y1,y2,...,ypiny: EðyÞ¼Ey1 y2 ...",
    "is deﬁned as the vector of expected values of the prandom variables y1,y2,...,ypiny: EðyÞ¼Ey1 y2 ... yp0 BBB@1 CCCA¼E(y 1) E(y2) ... E(yp)0 BBB@1 CCCA¼m1 m2 ... mp0 BBB@1 CCCA¼ m, (3:20) where E(yi)¼miis obtained as E(yi)¼Ðyifi(yi)dyi, using fi(yi), the marginal density of yi.",
    "m, (3:20) where E(yi)¼miis obtained as E(yi)¼Ðyifi(yi)dyi, using fi(yi), the marginal density of yi. Ifxandyarep/C21 random vectors, it follows from (3.20) and (3.5) that the expected value of their sum is the sum of their expected values: E(xþy)¼E(x)þE(y): (3:21) 3.3.2 Covariance Matrix The variances s12,s22,...,sp2ofy1,y2,...,ypand the covariances sijfor all i=j can be conveniently displayed in the covariance matrix , which is denoted by S, the uppercase version of sij: S¼cov(y)¼s11s12 ...s1p s21s22 ...s2p .........",
    "which is denoted by S, the uppercase version of sij: S¼cov(y)¼s11s12 ...s1p s21s22 ...s2p ......... sp1sp2 ...spp0 BBB@1 CCCA: (3:22) Theith row of Scontains the variance of y iand the covariance of yiwith each of the other yvariables. To be consistent with the notation sij, we have used sii¼si2,i¼1, 2,...,p, for the variances. The variances are on the diagonal of S, and the covari- ances occupy off-diagonal positions.",
    "iances. The variances are on the diagonal of S, and the covari- ances occupy off-diagonal positions. There is a distinction in the font used for S as the covariance matrix andPas the summation symbol. Note also the distinction in meaning between the notation cov( y)¼Sand cov( yi,yj)¼sij. The covariance matrix Sis symmetric because sij¼sji[see (3.10)]. In many appli- cations,Sis assumed to be positive deﬁnite.",
    "is symmetric because sij¼sji[see (3.10)]. In many appli- cations,Sis assumed to be positive deﬁnite. This will ordinarily hold if the yvariables are continuous random variables and if there are no linear relationships among them.",
    "the yvariables are continuous random variables and if there are no linear relationships among them. (If there are linear relationships among the yvariables, Swill be positive semideﬁnite.)3.3 MEAN VECTORS AND COVARIANCE MATRICES FOR RANDOM VECTORS 75 --- Page 92 --- By analogy with (3.20), we deﬁne the expected value of a random matrix Zas the matrix of expected values: E(Z)¼Ez11z12 ... z1p z21z22 ... z2p ......... zn1zn2 ... znp0 BBB@1 CCCA¼E(z11)E(z12) ... E(z1p) E(z21)E(z22) ...",
    "... z1p z21z22 ... z2p ......... zn1zn2 ... znp0 BBB@1 CCCA¼E(z11)E(z12) ... E(z1p) E(z21)E(z22) ... E(z2p) ......... E(zn1)E(zn2) ... E(znp)0 BBB@1 CCCA:(3:23) We can express Sas the expected value of a random matrix. By (2.21), the ( ij)th element of the matrix ( y2m)(y2m)0is (yi2mi)(yj2mj). Thus, by (3.10) and (3.23), the (ij)th element of E[(y2m)(y2m)0]i sE[(yi2mi)(yj2mj)]¼sij.H e n c e E[(y/C0m)(y/C0m0)]¼s11s12 ...s1p s21s22 ...s2p .........",
    "m)(y2m)0]i sE[(yi2mi)(yj2mj)]¼sij.H e n c e E[(y/C0m)(y/C0m0)]¼s11s12 ...s1p s21s22 ...s2p ......... sp1sp2 ...spp0 BBB@1 CCCA¼S: (3:24) We illustrate (3.24) for p¼3: S¼E[(y/C0m)(y/C0m)0] ¼Ey1/C0m1 y2/C0m2 y3/C0m30 B@1 CA(y1/C0m1,y2/C0m2,y3/C0y3)2 643 75 ¼E(y1/C0m1)2(y1/C0m1)(y2/C0m2)(y1/C0m1)(y3/C0m3) (y2/C0m2)(y1/C0m1)( y2/C0m2)2(y2/C0m2)(y3/C0m3) (y3/C0m3)(y1/C0m1)(y3/C0m3)(y2/C0m2)( y3/C0m3)22 643 75 ¼E(y1/C0m1)2E[(y1/C0m1)(y2/C0m2)]E[(y1/C0m1)(y3/C0m3)] E[(y2/C0m2)(y1/C0m1)] E(y2/C0m2)2E[(y2/C0m2)(y3/C0m3)] E[(y3/C0m3)(y1/C0m1)]E[(y3/C0m3)(y2/C0m2)] E(y3/C0m3)22 643 75 ¼s2 1s12s13 s21s22s23 s31s32s230 B@1 CA: We can write (3.24) in the form S¼E[(/C0m)(y/C0m)0]¼E(yy0)/C0mm0, (3:25) which is analogous to (3.8) and (3.11).76 RANDOM VECTORS AND MATRICES --- Page 93 --- 3.3.3 Generalized Variance A measure of overall variability in the population of yvariables can be deﬁned as the determinant of S: Generalized variance ¼jSj: (3:26) IfjSjis small, the yvariables are concentrated closer to mthan if jSjis large.",
    "variance ¼jSj: (3:26) IfjSjis small, the yvariables are concentrated closer to mthan if jSjis large. A small value of jSjmay also indicate that the variables y1,y2,...,ypinyare highly inter- correlated, in which case the yvariables tend to occupy a subspace of the pdimen- sions [this corresponds to one or more small eigenvalues; see Rencher (1998, Section 2.1.3)].",
    "dimen- sions [this corresponds to one or more small eigenvalues; see Rencher (1998, Section 2.1.3)]. 3.3.4 Standardized Distance To obtain a useful measure of distance between yandm, we need to account for the variances and covariances of the yivariables in y.",
    "istance between yandm, we need to account for the variances and covariances of the yivariables in y. By analogy with the univariate stan- dardized variable ( y/C0m)=s, which has mean 0 and variance 1, the standardized dis- tance is deﬁned as Standardized distance ¼(y/C0m)0S/C01(y/C0m): (3:27) The use of S/C01standardizes the (transformed) yivariables so that they have means equal to 0 and variances equal to 1 and are also uncorrelated (see Problem 3.11).",
    "at they have means equal to 0 and variances equal to 1 and are also uncorrelated (see Problem 3.11). A distance such as (3.27) is often called a Mahalanobis distance (Mahalanobis 1936). 3.4 CORRELATION MATRICES By analogy with Sin (3.22), the correlation matrix is deﬁned as Pr¼(rij)¼1r12 ...r1p r21 1 ...r2p ......... rp1rp2 ... 10 BBB@1 CCCA, (3:28) where rij¼sij=sisjis the correlation of yiandyjdeﬁned in (3.19).",
    ".. rp1rp2 ... 10 BBB@1 CCCA, (3:28) where rij¼sij=sisjis the correlation of yiandyjdeﬁned in (3.19). The second row ofPr, for example, contains the correlation of y2with each of the other yvariables. We use the subscript rinPrto emphasize that Pis the uppercase version of r. If we deﬁne Ds¼[diag(S)]1=2¼diag(s1,s2,...",
    "pt rinPrto emphasize that Pis the uppercase version of r. If we deﬁne Ds¼[diag(S)]1=2¼diag(s1,s2,... ,sp), (3:29)3.4 CORRELATION MATRICES 77 --- Page 94 --- then by (2.31), we can obtain PrfromSand vice versa: Pr¼D/C01 sSD/C01 s, (3:30) S¼DsPrDs: (3:31) 3.5 MEAN VECTORS AND COVARIANCE MATRICES FOR PARTITIONED RANDOM VECTORS Suppose that the random vector vis partitioned into two subsets of variables, which we denote by yandx: v¼y x/C18/C19 ¼y1 ... yp x1 ...",
    "partitioned into two subsets of variables, which we denote by yandx: v¼y x/C18/C19 ¼y1 ... yp x1 ... xq0 BBBBBBBB@1 CCCCCCCCA: Thus there are pþqrandom variables in v. The mean vector and covariance matrix for vpartitioned as above can be expressed in the following form m¼E(v)¼Ey x/C18/C19 ¼E(y) E(x)/C18/C19 ¼my mx/C18/C19 , (3:32) S¼cov(v)¼covy x/C18/C19 ¼SyySyx SxySxx/C18/C19 , (3:33) whereSxy¼S0 yx. In (3.32), the submatrix my¼[E(y1),E(y2),... ,E(yp)]0 contains the means of y1,y2,... ,yp.",
    "xy¼S0 yx. In (3.32), the submatrix my¼[E(y1),E(y2),... ,E(yp)]0 contains the means of y1,y2,... ,yp. Similarly mxcontains the means of the x variables. In (3.33), the submatrix Syy¼cov(y)i sa p/C2pcovariance matrix for y containing the variances of y1,y2,...,ypon the diagonal and the covariance of78 RANDOM VECTORS AND MATRICES --- Page 95 --- each yiwith each yj(i=j) off the diagonal: Syy¼s2 y1sy1y2/C1/C1/C1sy1yp sy2y1s2y 2/C1/C1/C1sy2yp .........",
    "iwith each yj(i=j) off the diagonal: Syy¼s2 y1sy1y2/C1/C1/C1sy1yp sy2y1s2y 2/C1/C1/C1sy2yp ......... sypy1sypy2/C1/C1/C1s2 yp0 BBBB@1 CCCCA: Similarly, S xx¼cov(x) is the q/C2qcovariance matrix of x1,x2,...,xq. The matrix Syxin (3.33) is p/C2qand contains the covariance of each yiwith each xj: Syx¼sy1x1sy1x2/C1/C1/C1sy1xq sy2x1sy2x2/C1/C1/C1sy2xq ......... sypx1sypx2/C1/C1/C1sypxq0 BBB@1 CCCA: ThusSyxis rectangular unless p¼q.",
    "y2x2/C1/C1/C1sy2xq ......... sypx1sypx2/C1/C1/C1sypxq0 BBB@1 CCCA: ThusSyxis rectangular unless p¼q. The covariance matrix Syxis also denoted by cov( y,x) and can be deﬁned as Syx¼cov(y,x)¼E[(y/C0my)(x/C0mx)0]: (3:34) Note the difference in meaning between covy x/C18/C19 in (3.33) and cov( y,x)¼Syx in (3.34). We have now used the notation cov in three ways: (1) cov( yi,yj), (2) cov(y), and (3) cov( y,x).",
    "4). We have now used the notation cov in three ways: (1) cov( yi,yj), (2) cov(y), and (3) cov( y,x). The ﬁrst of these is a scalar, the second is a symmetric (usually positive deﬁnite) matrix, and the third is a rectangular matrix. 3.6 LINEAR FUNCTIONS OF RANDOM VECTORS We often use linear combinations of the variables y1,y2,... ,ypfrom a random vector y. Leta¼(a1,a2,... ,ap)0be a vector of constants.",
    "of the variables y1,y2,... ,ypfrom a random vector y. Leta¼(a1,a2,... ,ap)0be a vector of constants. Then, by an expression preceding (2.18), the linear combination using the aterms as coefﬁcients can be written as z¼a1y1þa2y2þ/C1/C1/C1þ apyp¼a0y: (3:35) We consider the means, variances, and covariances of such linear combinations in Sections 3.6.1 and 3.6.2.3.6 LINEAR FUNCTIONS OF RANDOM VECTORS 79 --- Page 96 --- 3.6.1 Means Since yis a random vector, the linear combination z¼a0yis a (univariate) random variable.",
    ".6.1 Means Since yis a random vector, the linear combination z¼a0yis a (univariate) random variable. The mean of a0yis given the following theorem. Theorem 3.6a. Ifais ap/C21 vector of constants and yis ap/C21 random vector with mean vector m, then mz¼E(a0y)¼a0E(y)¼a0m: (3:36) PROOF. Using (3.4), (3.5), and (3.35), we obtain E(a0y)¼E(a1y1þa2y2þ/C1/C1/C1þ apyp) ¼E(a1y1)þE(a2y2)þ/C1/C1/C1þ E(apyp) ¼a1E(y1)þa2E(y2)þ/C1/C1/C1þ apE(yp) ¼(a1,a2,... ,ap)E(y1) E(y2) ...",
    "a1y1)þE(a2y2)þ/C1/C1/C1þ E(apyp) ¼a1E(y1)þa2E(y2)þ/C1/C1/C1þ apE(yp) ¼(a1,a2,... ,ap)E(y1) E(y2) ... E(yp)0 BBBB@1 CCCCA ¼a0E(y)¼a0m:A Suppose that we have several linear combinations of ywith constant coefﬁcients: z1¼a11y1þa12y2þ...þa1pyp¼a0 1y z2¼a21y1þa22y2þ...þa2pyp¼a0 2y ...... zk¼ak1y1þak2y2þ...þakpyp¼a0 ky, where a0 i¼(ai1,ai2,... ,aip) and y¼(y1,y2,... ,yp)0. These klinear functions can be written in the form z¼Ay , (3:37)80 RANDOM VECTORS AND MATRICES --- Page 97 --- where z¼z1 z2 ...",
    "be written in the form z¼Ay , (3:37)80 RANDOM VECTORS AND MATRICES --- Page 97 --- where z¼z1 z2 ... zk0 BBB@1 CCCA,A¼a0 1 a0 2 ... a0 k0 BBB@1 CCCA¼a11a12 ... a1p a21a22 ... a2p ......... ak1ak2 ... akp0 BBB@1 CCCA: It is possible to have k.p, but we typically have k/C20pwith the rows of A linearly independent, so that Ais full-rank. Since yis a random vector, each zi¼a0 iyis a random variable and z¼(z1,z2,... ,zk)0is a random vector.",
    "nce yis a random vector, each zi¼a0 iyis a random variable and z¼(z1,z2,... ,zk)0is a random vector. The expected value of z¼Ayis given in the following theorem, as well as some extensions. Theorem 3.6b. Suppose that yis a random vector, Xis a random matrix, aandbare vectors of constants, and AandBare matrices of constants.",
    "ndom vector, Xis a random matrix, aandbare vectors of constants, and AandBare matrices of constants. Then, assuming the matrices and vectors in each product are conformal, we have the following expected values: ðiÞE(Ay)¼AE(y): (3:38) ðiiÞE(a0Xb)¼a0E(X)b: (3:39) ðiiiÞE(AXB )¼AE(X)B: (3:40) PROOF. These results follow from Theorem 3.6A (see Problem 3.14). A Corollary 1.",
    "XB )¼AE(X)B: (3:40) PROOF. These results follow from Theorem 3.6A (see Problem 3.14). A Corollary 1. IfAis ak/C2pmatrix of constants, bis ak/C21 vector of constants, and yis ap/C21 random vector, then E(Ayþb)¼AE(y)þb: (3:41) A 3.6.2 Variances and Covariances The variance of the random variable z¼a0yis given in the following theorem. Theorem 3.6c.",
    "ovariances The variance of the random variable z¼a0yis given in the following theorem. Theorem 3.6c. Ifais ap/C21 vector of constants and yis ap/C21 random vector with covariance matrix S, then the variance of z¼a0yis given by s2 z¼var(a0y)¼a0Sa: (3:42)3.6 LINEAR FUNCTIONS OF RANDOM VECTORS 81 --- Page 98 --- PROOF.",
    "given by s2 z¼var(a0y)¼a0Sa: (3:42)3.6 LINEAR FUNCTIONS OF RANDOM VECTORS 81 --- Page 98 --- PROOF. By (3.6) and Theorem 3.6a, we obtain var(a0y)¼E(a0y/C0a0m)2¼E[a0(y/C0m)]2 ¼E[a0(y/C0m)a0(y/C0m)] ¼E[a0(y/C0m)(y/C0m)0a] [by (2 :18)] ¼a0E[(y/C0m)(y/C0m)0]a [by Theorem 3 :6b(ii)] ¼a0Sa [by(3 :24)] :A We illustrate 3.42 for p¼3: var(a0y)¼var(a1y1þa2y2þa3y3)¼a0Sa ¼a2 1s21þa22s22þa23s23þ2a1a2s12þ2a1a3s13þ2a2a3s23: Thus, var( a0y)¼a0Sainvolves all the variances and covariances of y1,y2, and y3.",
    "þ2a1a3s13þ2a2a3s23: Thus, var( a0y)¼a0Sainvolves all the variances and covariances of y1,y2, and y3. The covariance of two linear combinations is given in the following corollary to Theorem 3.6c. Corollary 1. Ifaandbarep/C21 vectors of constants, then cov(a0y,b0y)¼a0Sb: (3:43) A Each variable ziin the random vector z¼(z1,z2,... ,zk)0¼Ayin (3.37) has a variance, and each pair ziandzj(i=j) has a covariance.",
    "m vector z¼(z1,z2,... ,zk)0¼Ayin (3.37) has a variance, and each pair ziandzj(i=j) has a covariance. These variances and covari- ances are found in the covariance matrix for z, which is given in the following theorem, along with cov( z,w), where w¼Byis another set of linear functions. Theorem 3.6d. Letz¼Ayandw¼By, where Ais ak/C2pmatrix of constants, B is an m/C2pmatrix of constants, and yis a p/C21 random vector with covariance matrixS.",
    "constants, B is an m/C2pmatrix of constants, and yis a p/C21 random vector with covariance matrixS. Then ðiÞcov(z)¼cov(Ay)¼ASA0, (3:44) ðiiÞcov(z,w)¼cov(Ay ,By)¼ASB0: (3:45) A Typically, k/C20p, and the k/C2pmatrix Ais full rank, in which case, by Corollary 1 to 2.6b, ASA0is positive deﬁnite (assuming Sto be positive deﬁnite). If k.p, then by Corollary 2 to Theorem 2.6b, ASA0is positive semideﬁnite.",
    "Sto be positive deﬁnite). If k.p, then by Corollary 2 to Theorem 2.6b, ASA0is positive semideﬁnite. In this case, ASA0is still a covariance matrix, but it cannot be used in either the numerator or denominator of the multivariate normal density given in (4.9) in Chapter 4.82 RANDOM VECTORS AND MATRICES --- Page 99 --- Note that cov( z,w)¼ASB0is ak/C2mrectangular matrix containing the covari- ance of each ziwith each wj, that is, cov( z,w) contains cov( zi,wj),i¼1,2,... ,k, j¼1,2,... ,m.",
    "ri- ance of each ziwith each wj, that is, cov( z,w) contains cov( zi,wj),i¼1,2,... ,k, j¼1,2,... ,m. These kmcovariances can also be found individually by (3.43). Corollary 1. Ifbis ak/C21 vector of constants, then cov(Ayþb)¼ASA0: (3:46) A The covariance matrix of linear functions of two different random vectors is given in the following theorem. Theorem 3.6e. Letybe ap/C21 random vector and xbe aq/C21 random vector such that cov( y,x)¼Syx.",
    "rem. Theorem 3.6e. Letybe ap/C21 random vector and xbe aq/C21 random vector such that cov( y,x)¼Syx. Let Abe a k/C2pmatrix of constants and Bbe an h/C2qmatrix of constants. Then cov(Ay ,Bx)¼ASyxB0: (3:47) PROOF. Let v¼y x/C18/C19 and C¼AO OB/C18/C19 : A Use Theorem 3.6d(i) to obtain cov( Cv). The result follows. PROBLEMS 3.1 Show that E(ay)¼aE(y) as in (3.4). 3.2 Show that E(y/C0m)2¼E(y2)/C0m2as in (3.8). 3.3 Show that var( ay)¼a2s2as in (3.9).",
    "as in (3.4). 3.2 Show that E(y/C0m)2¼E(y2)/C0m2as in (3.8). 3.3 Show that var( ay)¼a2s2as in (3.9). 3.4 Show that cov( yi,yj)¼E(yiyj)/C0mimjas in (3.11). 3.5 Show that if yiandyjare independent, then E(yiyj)¼E(yi)E(yj) as in (3.14). 3.6 Show that if yiandyjare independent, then sij¼0 as in (3.15). 3.7 Establish the following results in Example 3.2: (a)Show that f2(y)¼1/C0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0ypfor 0 /C20y/C201 and f2(y)¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ2/C0ypfor 1/C20y/C202. (b)Show that E(y)¼7 6andE(xy)¼76.",
    "1/C0ypfor 0 /C20y/C201 and f2(y)¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ2/C0ypfor 1/C20y/C202. (b)Show that E(y)¼7 6andE(xy)¼76. (c)Show that E(yjx)¼12(1þ4x/C02x2).PROBLEMS 83 --- Page 100 --- 3.8 Suppose the bivariate random variable ( x,y) is uniformly distributed over the region bounded below by y¼x/C01 for 1 /C20x/C202 and by y¼3/C0xfor 2/C20x/C203 and bounded above by y¼xfor 1 /C20x/C202 and by y¼4/C0xfor 2/C20x/C203. (a)Show that the area of this region is 2, so that f(x,y)¼1 2.",
    "x/C202 and by y¼4/C0xfor 2/C20x/C203. (a)Show that the area of this region is 2, so that f(x,y)¼1 2. (b)Find f1(x),f2(y),E(x),E(y),E(xy), andsxy, as was done in Example 3.2. Arexandyindependent? (c)Find f(yjx) and E(yjx). 3.9 Show that E(xþy)¼E(x)þE(y) as in (3.21). 3.10 Show that E[(y/C0m)(y/C0m)0]¼E(yy0)/C0mm0as in (3.25). 3.11 Show that the standardized distance transforms the variables so that they are uncorrelated and have means equal to 0 and variances equal to 1 as notedfollowing (3.27).",
    "t they are uncorrelated and have means equal to 0 and variances equal to 1 as notedfollowing (3.27). 3.12 Illustrate P r¼D/C01 sSD/C01 sin (3.30) for p¼3. 3.13 Using (3.24), show that cov(v)¼covy x/C18/C19 ¼SyySyx SxySxx/C18/C19 as in (3.33). 3.14 Prove Theorem 3.6b. 3.15 Prove Corollary 1 to Theorem 3.6b. 3.16 Prove Corollary 1 to Theorem 3.6c. 3.17 Prove Theorem 3.6d. 3.18 Prove Corollary 1 to Theorem 3.6d.",
    "Prove Corollary 1 to Theorem 3.6c. 3.17 Prove Theorem 3.6d. 3.18 Prove Corollary 1 to Theorem 3.6d. 3.19 Consider four k/C21 random vectors y,x,v, and w, and four h/C2kconstant matrices A,B,C, and D. Find cov( AyþBx ,CvþDw). 3.20 Lety¼(y1,y2,y3)0be a random vector with mean vector and covariance matrix m¼1 /C01 30 @1A,S¼11 0 12 3 031 00 @1A:84 RANDOM VECTORS AND MATRICES --- Page 101 --- (a)Letz¼2y1/C03y2þy3. Find E(z) and var( z). (b)Letz1¼y1þy2þy3and z2¼3y1þy2/C02y3.",
    "--- Page 101 --- (a)Letz¼2y1/C03y2þy3. Find E(z) and var( z). (b)Letz1¼y1þy2þy3and z2¼3y1þy2/C02y3. Find E(z) and cov( z), where z¼(z1,z2)0. 3.21 Letybe a random vector with mean vector and covariance matrix mandSas given in Problem 3.20, and deﬁne w¼(w1,w2,w3)0as follows: w1¼2y1/C0y2þy3 w2¼y1þ2y2/C03y3 w3¼y1þy2þ2y3: (a)Find E(w) and cov( w).",
    "eﬁne w¼(w1,w2,w3)0as follows: w1¼2y1/C0y2þy3 w2¼y1þ2y2/C03y3 w3¼y1þy2þ2y3: (a)Find E(w) and cov( w). (b)Using zas deﬁned in Problem 3.20b, ﬁnd cov( z,w).PROBLEMS 85 --- Page 102 --- 4Multivariate Normal Distribution In order to make inferences, we often assume that the random vector of interest has a multivariate normal distribution. Before developing the multivariate normal density function and its properties, we ﬁrst review the univariate normal distribution.",
    "riate normal density function and its properties, we ﬁrst review the univariate normal distribution. 4.1 UNIVARIATE NORMAL DENSITY FUNCTION We begin with a standard normal random variable zwith mean 0 and variance 1. We then transform zto a random variable ywith arbitrary mean mand variance s2, and we ﬁnd the density of yfrom that of z. In Section 4.2, we will follow an analogous procedure to obtain the density of a multivariate normal random vector.",
    "we will follow an analogous procedure to obtain the density of a multivariate normal random vector. The standard normal density is given by g(z)¼1ﬃﬃﬃﬃﬃﬃ 2pp e/C0z2=2,/C01,z,1, (4:1) for which E(z)¼0 and var( z)¼1. When zhas the density (4.1), we say that zis dis- tributed as N(0, 1), or simply that zisN(0,1). To obtain a normal random variable ywith arbitrary mean mand variance s2,w e use the transformation z¼(y/C0m)=sory¼szþm,s ot h a t E(y)¼mand var(y)¼s2.",
    "mean mand variance s2,w e use the transformation z¼(y/C0m)=sory¼szþm,s ot h a t E(y)¼mand var(y)¼s2. We now ﬁnd the density f(y)f r o m g(z) in (4.1). For a continuous increasing function (such as y¼szþm) or for a continuous decreasing function, the change-of-variable technique for a deﬁnite integral gives f(y)¼g(z)dz dy/C12/C12/C12/C12/C12/C12/C12/C12, (4:2) where jdz=dyjis the absolute value of dz/dy(Hogg and Craig 1995, p. 169).",
    "C12/C12/C12/C12/C12, (4:2) where jdz=dyjis the absolute value of dz/dy(Hogg and Craig 1995, p. 169). To use (4.2) to ﬁnd the density of y, it is clear that both zanddz/dyon the right side must be expressed in terms of y. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 87 --- Page 103 --- Let us apply (4.2) to y¼szþm. The density g(z) is given in (4.1), and for z¼(y/C0m)=s,w eh a v e jdz=dyj¼1=s.",
    "ply (4.2) to y¼szþm. The density g(z) is given in (4.1), and for z¼(y/C0m)=s,w eh a v e jdz=dyj¼1=s. Thus f(y)¼g(z)dz dy/C12/C12/C12/C12/C12/C12/C12/C12¼gy/C0m s/C16/C171 s ¼1ﬃﬃﬃﬃﬃﬃ 2pp se/C0(y/C0m)2=2s2, (4:3) which is the normal density with E(y)¼mand var( y)¼s2. When yhas the density (4.3), we say that yis distributed as N(m,s2) or simply that yisN(m,s2). In Section 4.2, we use a multivariate extension of this technique to ﬁnd the multi- variate normal density function.",
    "we use a multivariate extension of this technique to ﬁnd the multi- variate normal density function. 4.2 MULTIVARIATE NORMAL DENSITY FUNCTION We begin with independent standard normal random variables z1,z2,... ,zp, with mi¼0 ands2 i¼1 for all iandsij¼0 for i=j, and we then transform the zi0st o multivariate normal variables y1,y2,... ,yp, with arbitrary means, variances, and covariances. We thus start with a random vector z¼(z1,z2,...",
    "p, with arbitrary means, variances, and covariances. We thus start with a random vector z¼(z1,z2,... ,zp)0, where E(z)¼0,cov(z)¼I, and each zihas the N(0,1) density in (4.1). We wish to trans- form zto a multivariate normal random vector y¼(y1,y2... ,yp)0with E(y)¼m and cov( y)¼S, wheremis any p/C21 vector and Sis any p/C2ppositive deﬁnite matrix. By (4.1) and an extension of (3.12), we have g(z1,z2,...",
    "r and Sis any p/C2ppositive deﬁnite matrix. By (4.1) and an extension of (3.12), we have g(z1,z2,... ,zp)¼g(z)¼g1(z1)g2(z2)/C1/C1/C1gp(zp) ¼1ﬃﬃﬃﬃﬃﬃ 2pp e/C0z2 1=21ﬃﬃﬃﬃﬃﬃ 2pp e/C0z2 2=2/C1/C1/C11ﬃﬃﬃﬃﬃﬃ 2pp e/C0z2 p=2 ¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )pe/C0P iz2 i=2 ¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )pe/C0z0z=2[by (2 :20)] : (4:4) Ifzhas the density (4.4), we say that zhas a multivariate normal density with mean vector 0and covariance matrix Ior simply that zis distributed as Np(0,I), where pis the dimension of the distribution and corresponds to the number of variables in y.",
    "p(0,I), where pis the dimension of the distribution and corresponds to the number of variables in y. To transform ztoywith arbitrary mean vector E(y)¼mand arbitrary (positive deﬁnite) covariance matrix cov( y)¼S, we deﬁne the transformation y¼S1=2zþm, (4:5)88 MULTIVARIATE NORMAL DISTRIBUTION --- Page 104 --- whereS1=2is the (symmetric) square root matrix deﬁned in (2.109).",
    "RMAL DISTRIBUTION --- Page 104 --- whereS1=2is the (symmetric) square root matrix deﬁned in (2.109). By (3.41) and (3.46), we obtain E(y)¼E(S1=2zþm)¼S1=2E(z)þm¼S1=20þm¼m, cov(y)¼cov(S1=2zþm)¼S1=2cov(z)(S1=2)0¼S1=2IS1=2¼S: Note the analogy of (4.5) to y¼szþmin Section 4.1. Let us now ﬁnd the density of y¼S1=2zþmfrom the density of zin (4.4). By (4.2), the density of y¼szþmisf(y)¼g(z)jdz=dy¼g(z)j1=sj.",
    "y¼S1=2zþmfrom the density of zin (4.4). By (4.2), the density of y¼szþmisf(y)¼g(z)jdz=dy¼g(z)j1=sj. The analogous expression for the multivariate linear transformation y¼S1=2zþmis f(y)¼g(z)abs(jS/C01=2j), (4:6) whereS/C01=2is deﬁned as ( S1=2)/C01and abs( jS/C01=2j) represents the absolute value of the determinant of S/C01=2, which parallels the absolute value expression jdz=dyj¼j1=sjin the univariate case.",
    "inant of S/C01=2, which parallels the absolute value expression jdz=dyj¼j1=sjin the univariate case. (The determinant jS/C01=2jis the Jacobian of the transformation; see any advanced calculus text.) Since S/C01=2is positive deﬁnite, we can dispense with the absolute value and write (4.6) as f(y)¼g(z)jS/C01=2j (4:7) ¼g(z)jSj/C01=2[by (2 :67)] : (4:8) In order to express zin terms of y, we use (4.5) to obtain z¼S/C01=2(y/C0m).",
    "1=2[by (2 :67)] : (4:8) In order to express zin terms of y, we use (4.5) to obtain z¼S/C01=2(y/C0m). Then using (4.4) and (4.8), we can write the density of yas f(y)¼g(z)jSj/C01=2¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )pjSj1=2e/C0z0z=2 ¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )pjSj1=2e/C0[S/C01=2(y/C0m)]0[S/C01=2(y/C0m)]=2 ¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )pjSj1=2e/C0(y/C0m)0(S1=2S1=2)/C01(y/C0m)=2 ¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )pjSj1=2e/C0(y/C0m)0S/C01(y/C0m)=2, (4:9) which is the multivariate normal density function with mean vector mand covariance matrixS.",
    "2, (4:9) which is the multivariate normal density function with mean vector mand covariance matrixS. When yhas the density (4.9), we say that yis distributed as Np(m,S)o r4.2 MULTIVARIATE NORMAL DENSITY FUNCTION 89 --- Page 105 --- simply that yisNp(m,S). The subscript pis the dimension of the p-variate normal distribution and indicates the number of variables, that is, yisp/C21,misp/C21, andSisp/C2p.",
    "normal distribution and indicates the number of variables, that is, yisp/C21,misp/C21, andSisp/C2p. A comparison of (4.9) and (4.3) shows the standardized distance ( y/C0m)0 S/C01(y/C0m) in place of ( y/C0m)2=s2in the exponent and the square root of the gen- eralized variance jSjin place of the square root of s2in the denominator.",
    "he square root of the gen- eralized variance jSjin place of the square root of s2in the denominator. [For stan- dardized distance, see (3.27), and for generalized variance, see (3.26).] These distance and variance functions serve analogous purposes in the densities (4.9) and (4.3). In (4.9), f(y) decreases as the distance from ytomincreases, and a small value of jSjindicates that the y0s are concentrated closer to mthan is the case when jSjis large.",
    "ll value of jSjindicates that the y0s are concentrated closer to mthan is the case when jSjis large. A small value of jSjmay also indicate a high degree of multicollinearity among the variables. High multicollinearity indicates that the variables are highly intercorrelated, in which case the y0s tend to occupy a subspace of the pdimensions.",
    "bles are highly intercorrelated, in which case the y0s tend to occupy a subspace of the pdimensions. 4.3 MOMENT GENERATING FUNCTIONS We now review moment generating functions, which can be used to obtain some of the properties of multivariate normal random variables. We begin with the univariate case.",
    "n some of the properties of multivariate normal random variables. We begin with the univariate case. Themoment generating function for a univariate random variable yis deﬁned as My(t)¼E(ety), (4:10) provided E(ety) exists for every real number tin the neighborhood /C0h,t,hfor some positive number h.",
    "rovided E(ety) exists for every real number tin the neighborhood /C0h,t,hfor some positive number h. For the univariate normal N(m,s2), the moment generating function of yis given by My(t)¼etmþt2s2=2: (4:11) Moment generating functions characterize a distribution in some important ways that prove very useful (see the two properties at the end of this section). As theirname implies, moment generating functions can also be used to generate moments. We now demonstrate this.",
    "implies, moment generating functions can also be used to generate moments. We now demonstrate this. For a continuous random variable y, the moment generating function can be written as M y(t)¼E(ety)¼Ð1 /C01etyf(y)dy.",
    "random variable y, the moment generating function can be written as M y(t)¼E(ety)¼Ð1 /C01etyf(y)dy. Then, provided we can interchange the order of integration and differentiation,we have dM y(t) dt¼M0 y(t)¼ð1 /C01yetyf(y)dy: (4:12) Setting t¼0 gives the ﬁrst moment or mean: M0 y(0)¼ð1 /C01yf(y)dy¼E(y): (4:13)90 MULTIVARIATE NORMAL DISTRIBUTION --- Page 106 --- Similarly, the kth moment can be obtained using the kth derivative evaluated at 0: M(k) y(0)¼E(yk): (4:14) The second moment, E(y2), can be used to ﬁnd the variance [see (3.8)].",
    "at 0: M(k) y(0)¼E(yk): (4:14) The second moment, E(y2), can be used to ﬁnd the variance [see (3.8)]. For a random vector y, the moment generating function is deﬁned as My(t)¼E(et1y1þt2y2þ/C1/C1/C1þ tpyp)¼E(et0y): (4:15) By analogy with (4.13), we have @My(0) @t¼E(y), (4:16) where the notation @My(0)=@tindicates that @My(t)=@tis evaluated at t¼0.",
    "have @My(0) @t¼E(y), (4:16) where the notation @My(0)=@tindicates that @My(t)=@tis evaluated at t¼0. Similarly, @2My(t)=@tr@tsevaluated at tr¼ts¼0 gives E(yrys): @2My(0) @tr@ts¼E(yrys): (4:17) For a multivariate normal random vector y, the moment generating function is given in the following theorem. Theorem 4.3. Ifyis distributed as Np(m,S), its moment generating function is given by My(t)¼et0mþt0St=2: (4:18) PROOF.",
    "distributed as Np(m,S), its moment generating function is given by My(t)¼et0mþt0St=2: (4:18) PROOF. By (4.15) and (4.9), the moment generating function is My(t)¼ð1 /C01...ð1 /C01ket0y/C0(y/C0m)0S/C01(y/C0m)=2dy, (4:19) where k¼1=(ﬃﬃﬃﬃﬃﬃ 2pp )pjSj1=2anddy¼dy1dy2/C1/C1/C1dyp.",
    "/C01ket0y/C0(y/C0m)0S/C01(y/C0m)=2dy, (4:19) where k¼1=(ﬃﬃﬃﬃﬃﬃ 2pp )pjSj1=2anddy¼dy1dy2/C1/C1/C1dyp. By rewriting the exponent, we obtain My(t)¼ð1 /C01...ð1 /C01ket0mþt0St=2/C0(y/C0m/C0St)0S/C01(y/C0m/C0St)=2dy (4:20) ¼et0mþt0St=2ð1 /C01...ð1 /C01ke/C0[y/C0(mþSt)]0S/C01[y/C0(mþSt)]=2dy(4:21) ¼et0mþt0St=2:4.3 MOMENT GENERATING FUNCTIONS 91 --- Page 107 --- The multiple integral in (4.21) is equal to 1 because the multivariate normal density in (4.9) integrates to 1 for any mean vector, including mþSt.",
    "ecause the multivariate normal density in (4.9) integrates to 1 for any mean vector, including mþSt. A Corollary 1. The moment generating function for y/C0mis My/C0m(t)¼et0St=2: (4:22) A We now list two important properties of moment generating functions. 1. If two random vectors have the same moment generating function, they have the same density. 2.",
    "s. 1. If two random vectors have the same moment generating function, they have the same density. 2. Two random vectors are independent if and only if their joint moment gener- ating function factors into the product of their two separate moment generating functions; that is, if y0¼(y0 1,y02) and t0¼(t01,t02), then y1andy2are indepen- dent if and only if My(t)¼My1(t1)My2(t2): (4:23) 4.4 PROPERTIES OF THE MULTIVARIATE NORMAL DISTRIBUTION We ﬁrst consider the distribution of linear functions of multivariate normal random variables.",
    "UTION We ﬁrst consider the distribution of linear functions of multivariate normal random variables. Theorem 4.4a. Let the p/C21 random vector ybeNp(m,S), let abe any p/C21 vector of constants, and let Abe any k/C2pmatrix of constants with rank k/C20p. Then (i)z¼a0yisN(a0m,a0Sa) (ii)z¼AyisNk(Am,ASA0).",
    "Abe any k/C2pmatrix of constants with rank k/C20p. Then (i)z¼a0yisN(a0m,a0Sa) (ii)z¼AyisNk(Am,ASA0). PROOF (i) The moment generating function for z¼a0yis given by Mz(t)¼E(etz)¼E(eta0y)¼E(e(ta)0y) ¼e(ta)0mþ(ta)0S(ta)=2[by (4 :18)] ¼e(a0m)tþ(a0Sa)t2=2: (4:24)92 MULTIVARIATE NORMAL DISTRIBUTION --- Page 108 --- On comparing (4.24) with (4.11), it is clear that z¼a0yis univariate normal with mean a0mand variance a0Sa.",
    "aring (4.24) with (4.11), it is clear that z¼a0yis univariate normal with mean a0mand variance a0Sa. (ii) The moment generating function for z¼Ayis given by Mz(t)¼E(et0z)¼E(et0Ay), which becomes Mz(t)¼et0(Am)þt0(ASA0)t=2(4:25) (see Problem 4.7). By Corollary 1 to Theorem 2.6b, the covariance matrix ASA0is positive deﬁnite. Thus, by (4.18) and (4.25), the k/C21 random vector z¼Ayis distributed as the k-variate normal Nk(Am, ASA0). A Corollary 1.",
    "25), the k/C21 random vector z¼Ayis distributed as the k-variate normal Nk(Am, ASA0). A Corollary 1. Ifbis any k/C21 vector of constants, then z¼AyþbisNk(Amþb,ASA0):A The marginal distributions of multivariate normal variables are also normal, as shown in the following theorem. Theorem 4.4b. IfyisNp(m,S), then any r/C21 subvector of yhas an r-variate normal distribution with the same means, variances, and covariances as in the orig- inalp-variate normal distribution. PROOF.",
    "the same means, variances, and covariances as in the orig- inalp-variate normal distribution. PROOF. Without loss of generality, let ybe partitioned as y0¼(y0 1,y02), where y1is the r/C21 subvector of interest. Let mandSbe partitioned accordingly: y¼y1 y2/C18/C19 ,m¼m1 m2/C18/C19 ,S¼S11S12 S21S22/C18/C19 : Deﬁne A¼(Ir,O), where Iris an r/C2ridentity matrix and Ois an r/C2(p2r) matrix of 0s. Then Ay¼y1, and by Theorem 4.4a (ii), y1is distributed as Nr(m1,S11). A Corollary 1.",
    ") matrix of 0s. Then Ay¼y1, and by Theorem 4.4a (ii), y1is distributed as Nr(m1,S11). A Corollary 1. IfyisNp(m,S), then any individual variable yiinyis distributed as N(mi,sii).",
    "m1,S11). A Corollary 1. IfyisNp(m,S), then any individual variable yiinyis distributed as N(mi,sii). A For the next two theorems, we use the notation of Section 3.5, in which the random vector vis partitioned into two subvectors denoted by yandx, where yisp/C21 and x4.4 PROPERTIES OF THE MULTIVARIATE NORMAL DISTRIBUTION 93 --- Page 109 --- isq/C21, with a corresponding partitioning of mandS[see (3.32) and (3.33)]: v¼y x/C18/C19 ,m¼Eyx/C18/C19 ¼my mx/C18/C19 ,S¼covyx/C18/C19 ¼S yySyx SxySxx/C18/C19 : By (3.15), if two random variables yiandyjare independent, then sij¼0.",
    "C19 ¼S yySyx SxySxx/C18/C19 : By (3.15), if two random variables yiandyjare independent, then sij¼0. The converse of this is not true, as illustrated in Example 3.2. By extension, if two random vectors yandxare independent (i.e., each yiis independent of each xj), thenSyx¼O(the covariance of each yiwith each xjis 0). The converse is not true in general, but it is true for multivariate normal random vectors. Theorem 4.4c. Ifv¼y x/C18/C19 isNpþq(m,S), then yand xare independent if Syx¼O: PROOF.",
    "ndom vectors. Theorem 4.4c. Ifv¼y x/C18/C19 isNpþq(m,S), then yand xare independent if Syx¼O: PROOF. Suppose Syx¼O. Then S¼SyyO OSxx/C18/C19 , and the exponent of the moment generating function in (4.18) becomes t0mþ1 2t0St¼(t0 y,t0x)my mx/C18/C19 þ1 2(t0 y,t0x)SyyO OSxx/C18/C19ty tx/C18/C19 ¼t0 ymyþt0xmxþ1 2t0 ySyytyþ1 2t0 xSxxtx: The moment generating function can then be written as Mv(t)¼et0 ymyþt0ySyyty=2et0xmxþt0xSxxtx=2, which is the product of the moment generating functions of yandx.",
    "0 ymyþt0ySyyty=2et0xmxþt0xSxxtx=2, which is the product of the moment generating functions of yandx. Hence, by (4.23), yandxare independent. A Corollary 1. IfyisNp(m,S), then any two individual variables yiandyjare inde- pendent if sij¼0. Corollary 2. IfyisNp(m,S) and if cov( Ay ,By)¼ASB0¼O, then AyandByare independent.",
    "pendent if sij¼0. Corollary 2. IfyisNp(m,S) and if cov( Ay ,By)¼ASB0¼O, then AyandByare independent. A The relationship between subvectors yand xwhen they are not independent (Syx=O) is given in the following theorem.94 MULTIVARIATE NORMAL DISTRIBUTION --- Page 110 --- Theorem 4.4d.",
    "is given in the following theorem.94 MULTIVARIATE NORMAL DISTRIBUTION --- Page 110 --- Theorem 4.4d. Ifyandxare jointly multivariate normal with Syx=O, then the conditional distribution of ygiven x,f(yjx), is multivariate normal with mean vector and covariance matrix E(yjx)¼myþSyxS/C01 xx(x/C0mx), (4:26) cov(yjx)¼Syy/C0SyxS/C01 xxSxy: (4:27) PROOF.",
    "covariance matrix E(yjx)¼myþSyxS/C01 xx(x/C0mx), (4:26) cov(yjx)¼Syy/C0SyxS/C01 xxSxy: (4:27) PROOF. By an extension of (3.18), the conditional density of ygiven xis f(yjx)¼g(y,x) h(x), (4:28) where g(y,x) is the joint density of yandx, and h(x) is the marginal density of x. The proof can be carried out by directly evaluating the ratio on the right hand side of (4.28), using results (2.50) and (2.71) (see Problem 4.13).",
    "ting the ratio on the right hand side of (4.28), using results (2.50) and (2.71) (see Problem 4.13). For variety, we use an alternative approach that avoids working explicitly with g(y,x) and h(x) and the resulting partitioned matrix formulas. Consider the function w u/C18/C19 ¼Ay x/C18/C19 /C0my mx/C18/C19/C20/C21 , (4:29) where A¼A1 A2/C18/C19 ¼I/C0SyxS/C01 xx OI/C18/C19 : To be conformal, the identity matrix in A1isp/C2pwhile the identity in A2isq/C2q.",
    "01 xx OI/C18/C19 : To be conformal, the identity matrix in A1isp/C2pwhile the identity in A2isq/C2q. Simplifying and rearranging (4.29), we obtain w¼y/C0[myþSyxS/C01 xx(x/C0mx)] and u¼x/C0mx. Using the multivariate change-of-variable technique [referred to in (4.6], the joint density of ( w,u)i s p(w,u)¼g(y,x)jA/C01j¼g(y,x) [employing Theorem 2.9a (ii) and (vi)].",
    ", the joint density of ( w,u)i s p(w,u)¼g(y,x)jA/C01j¼g(y,x) [employing Theorem 2.9a (ii) and (vi)]. Similarly, the marginal density of uis q(u)¼h(x)jI/C01j¼h(x): Using (3.45), it also turns out that cov(w,u)¼A1SA2¼Syx/C0SyxS/C01 xxSxx¼O (4:30) (see Problem 4.14). Thus, by Theorem 4.4c, wis independent of u. Hence p(w,u)¼r(w)q(u),4.4 PROPERTIES OF THE MULTIVARIATE NORMAL DISTRIBUTION 95 --- Page 111 --- where r(w) is the density of w.",
    "OPERTIES OF THE MULTIVARIATE NORMAL DISTRIBUTION 95 --- Page 111 --- where r(w) is the density of w. Since p(w,u)¼g(y,x) and q(u)¼h(x), we also have g(y,x)¼r(w)h(x), and by (4.28), r(w)¼g(y,x) h(x)¼f(yjx): Hence we obtain f(yjx) simply by ﬁnding r(w).",
    "y,x)¼r(w)h(x), and by (4.28), r(w)¼g(y,x) h(x)¼f(yjx): Hence we obtain f(yjx) simply by ﬁnding r(w). By Corollary 1 to Theorem 4.4a, r(w) is the multivariate normal density with mw¼A1my mx/C18/C19 /C0my mx/C18/C19/C20/C21 ¼0, (4:31) Sww¼A1SA0 1 ¼(I,/C0SyxS/C01 xx)SyySyx SxySxx/C18/C19I /C0S/C01 xxSxy/C18/C19 ¼Syy/C0SyxS/C01 xxSxy: (4:32) Thus r(w)¼r(y/C0[myþSyxS/C01 xx(x/C0mx)]) is of the form Np(0,Syy/C0SyxS/C01 xxSxy). Equivalently, yjxisNp[myþSyxS/C01 xx(x/C0mx),Syy/C0SyxS/C01 xxSxy].",
    "form Np(0,Syy/C0SyxS/C01 xxSxy). Equivalently, yjxisNp[myþSyxS/C01 xx(x/C0mx),Syy/C0SyxS/C01 xxSxy]. A Since E(yjx)¼myþSyxS/C01 xx(x/C0mx) in (4.26) is a linear function of x, any pair of variables yiand yjin a multivariate normal vector exhibits a linear trend E(yijyj)¼miþ(sij=sjj)(yj/C0mj). Thus the covariance sijis related to the slope of the line representing the trend, and sijis a useful measure of relationship between two normal variables.",
    "ine representing the trend, and sijis a useful measure of relationship between two normal variables. In the case of nonnormal variables that exhibit a curved trend,sijmay give a very misleading indication of the relationship, as illustrated in Example 3.2. The conditional covariance matrix cov( yjx)¼Syy/C0SyxS/C01 xxSxyin (4.27) does not involve x. For some nonnormal distributions, on the other hand, cov( yjx)i sa function of x.",
    "oes not involve x. For some nonnormal distributions, on the other hand, cov( yjx)i sa function of x. If there is only one y, so that vis partitioned in the form v¼(y,x1,x2,... ,xq)¼(y,x0), thenmandShave the form m¼my mx/C18/C19 ,S¼s2 ys0yx syxSxx/C18/C19 , wheremyands2 yare the mean and variance of y,s0 yx¼(sy1,sy2,... ,syq) contains the covariances syi¼cov(y,xi), andSxxcontains the variances and covariances of96 MULTIVARIATE NORMAL DISTRIBUTION --- Page 112 --- thexvariables.",
    "the variances and covariances of96 MULTIVARIATE NORMAL DISTRIBUTION --- Page 112 --- thexvariables. The conditional distribution is given in the following corollary to Theorem 4.4d. Corollary 1. Ifv¼(y,x1,x2,... ,xq)¼(y,x0), with m¼my mx/C18/C19 ,S¼s2 ys0yx syxSxx/C18/C19 , then yjxis normal with E(yjx)¼myþs0 yxS/C01 xx(x/C0mx), (4:33) var(yjx)¼s2y/C0s0yxS/C01 xxsyx: (4:34) A In (4.34), s0 yxS/C01 xxsyx/C210 because S/C01 xxis positive deﬁnite. Therefore var(yjx)/C20var(y): (4:35) Example 4.4a.",
    "1 xxsyx/C210 because S/C01 xxis positive deﬁnite. Therefore var(yjx)/C20var(y): (4:35) Example 4.4a. To illustrate Theorems 4.4a–c, suppose that yisN3(m,S), where m¼3 120 @1A,S¼402 01 /C01 2/C0130 @1A: For z¼y 1/C02y2þy3¼(1,/C02,1)y¼a0y,w eh a v e a0m¼3 and a0Sa¼19. Hence by Theorem 4.4a(i), zisN(3, 19).",
    "z¼y 1/C02y2þy3¼(1,/C02,1)y¼a0y,w eh a v e a0m¼3 and a0Sa¼19. Hence by Theorem 4.4a(i), zisN(3, 19). The linear functions z1¼y1/C0y2þy3,z2¼3y1þy2/C02y3 can be written as z¼z1 z2/C18/C19 ¼1/C011 31 /C02/C18/C19 y1 y2 y30 @1A¼Ay: Then by Theorem 3.6b(i) and Theorem 3.6d(i), we obtain A m¼4 6/C18/C19 ,ASA0¼14 4 42 9/C18/C19 , and by Theorem 4.4a(ii), we have zisN246/C18/C19 ,14 4 42 9/C18/C19/C20/C21 :4.4 PROPERTIES OF THE MULTIVARIATE NORMAL DISTRIBUTION 97 --- Page 113 --- To illustrate the marginal distributions in Theorem 4.4b, note that y1isN(3, 4), y3is N(2,3),y1 y2/C18/C19 isN23 1/C18/C19 ,4001/C18/C19/C20/C21 ,andy 1 y3/C18/C19 isN232/C18/C19 ;4223/C18/C19/C20/C21 : To illustrate Theorem 4.4c, we note that s12¼0, and therefore y1andy2are independent.",
    "/C19/C20/C21 : To illustrate Theorem 4.4c, we note that s12¼0, and therefore y1andy2are independent. A Example 4.4b. To illustrate Theorem 4.4d, let the random vector vbeN4(m,S), where m¼2 5 /C0210 BB@1 CCA,S¼9033 01 /C012 3/C016 /C03 32 /C0370 BB@1 CCA: Ifvis partitioned as v¼(y 1,y2,x1,x2)0, thenmy¼2 5/C18/C19 ,mx¼/C021/C18/C19 ,S yy¼ 9001/C18/C19 ,S yx¼33 /C012/C18/C19 , andSxx¼6/C03 /C037/C18/C19 .",
    "/C18/C19 ,mx¼/C021/C18/C19 ,S yy¼ 9001/C18/C19 ,S yx¼33 /C012/C18/C19 , andSxx¼6/C03 /C037/C18/C19 . By (4.26), we obtain E(yjx)¼myþSyxS/C01 xx(x/C0mx) ¼2 5/C18/C19 þ33 /C012/C18/C196/C03 /C037/C18/C19 /C01x1þ2 x2/C01/C18/C19 ¼2 5/C18/C19 þ1 3330 27 /C019/C18/C19x1þ2 x2/C01/C18/C19 ¼3þ10 11x1þ9 11x2 14 3/C01 33x1þ3 11x20 BB@1 CCA: By (4.27), we have cov(yjx)¼Syy/C0SyxS/C01 xxSxy ¼90 01/C18/C19 /C033 /C012/C18/C196/C03 /C037/C18/C19 /C013/C01 32/C18/C19 ¼90 01/C18/C19 /C01 33171 24 24 19/C18/C19 ¼1 33126 /C024 /C024 14/C18/C19 :98 MULTIVARIATE NORMAL DISTRIBUTION --- Page 114 --- Thus yjxisN23þ10 11x1þ9 11x2 14 3/C01 33x1þ3 11x2 !",
    "ULTIVARIATE NORMAL DISTRIBUTION --- Page 114 --- Thus yjxisN23þ10 11x1þ9 11x2 14 3/C01 33x1þ3 11x2 ! ,1 33126 /C024 /C024 14/C18/C19\"# : A Example 4.4c. To illustrate Corollary 1 to Theorem 4.4d, let vbeN4(m,S), wherem andSare as given in Example 4.4b. If vis partitioned as v¼(y,x1,x2,x3)0, thenm andSare partitioned as follows: m¼my mx/C18/C19 ¼2 5 /C02 10 BBB@1 CCCA, S¼s2 ys0yx syxSxx !",
    ", thenm andSare partitioned as follows: m¼my mx/C18/C19 ¼2 5 /C02 10 BBB@1 CCCA, S¼s2 ys0yx syxSxx ! ¼9 033 01 /C012 3/C016 /C03 32 /C0370 BBB@1 CCCA: By (4.33), we have E(yjx1,x2,x3)¼myþs0 yxS/C01 xx(x/C0mx) ¼2þ(0,3,3)1/C012 /C016 /C03 2/C0370 B@1 CA/C01x1/C05 x2þ2 x3þ10 B@1 CA ¼95 7/C012 7x1þ6 7x2þ97x3: By (4.34), we obtain var(yjx1,x2,x3)¼s2 y/C0s0yxS/C01 xxsyx ¼9/C0(0,3,3)1/C012 /C016 /C03 2/C0370 B@1 CA/C010 3 30 B@1 CA ¼9/C045 7¼18 7: Hence yjx1,x2,x3isN(95 7/C012 7x1þ6 7x2þ97x3,18 7).",
    "2/C0370 B@1 CA/C010 3 30 B@1 CA ¼9/C045 7¼18 7: Hence yjx1,x2,x3isN(95 7/C012 7x1þ6 7x2þ97x3,18 7). Note that var( yjx1,x2,x3)¼18 7 is less than var( y)¼9, which illustrates (4.35). A4.4 PROPERTIES OF THE MULTIVARIATE NORMAL DISTRIBUTION 99 --- Page 115 --- 4.5 PARTIAL CORRELATION We now deﬁne the partial correlation of yiandyjadjusted for a subset of other yvari- ables. For convenience, we use the notation of Theorems 4.4c and 4.4d.",
    "for a subset of other yvari- ables. For convenience, we use the notation of Theorems 4.4c and 4.4d. The subset of y0s containing yiandyjis denoted by y, and the other subset of y0s is denoted by x. LetvbeNpþq(m,S) and let v,m, andSbe partitioned as in Theorem 4.4c and 4.4d: v¼yx/C18/C19 , m¼my mx/C18/C19 ,S¼SyySyx SxySxx/C18/C19 : The covariance of yiandyjin the conditional distribution of ygiven xwill be denoted bysij/C1rs...q, where yiandyjare two of the variables in yandyr,ys,...",
    "on of ygiven xwill be denoted bysij/C1rs...q, where yiandyjare two of the variables in yandyr,ys,... ,yqare all the variables in x. Thussij/C1rs...qis the ( ij)th element of cov( yjx)¼Syy/C0SyxS/C01 xxSxy.F o r example, s13/C1567represents the covariance between y1andy3in the conditional dis- tribution of y1,y2,y3,y4given y5,y6, and y7[in this case x¼(y5,y6,y7)0]. Similarly, s22/C1567represents the variance of y2in the conditional distribution of y1,y2,y3,y4 given y5,y6,y7.",
    "s22/C1567represents the variance of y2in the conditional distribution of y1,y2,y3,y4 given y5,y6,y7. We now deﬁne the partial correlation coefﬁcient rij/C1rs...qto be the correlation between yiand yjin the conditional distribution of ygiven x, where x¼(yr,ys,... ,yq)0. From the usual deﬁnition of a correlation [see (3.19)], we can obtain rij/C1rs...qfromsij/C1rs...q: rij/C1rs...q¼sij/C1rs...qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃsii/C1rs...qsjj/C1rs...qp : (4:36) This is the population partial correlation.",
    "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃsii/C1rs...qsjj/C1rs...qp : (4:36) This is the population partial correlation. The sample partial correlation rij/C1rs/C1/C1/C1qis discussed in Section 10.7, including a formulation that does not require normality. The matrix of partial correlations, Py/C1x¼(rij/C1rs...q) can be found by (3.30) and (4.27) as Py/C1x¼D/C01 y/C1xSy/C1xD/C01 y/C1x, (4:37) whereSy/C1x¼cov(yjx)¼Syy/C0SyxS/C01 xxSxyandDy/C1x¼[diag(Sy/C1x)]1=2.",
    "y/C1xSy/C1xD/C01 y/C1x, (4:37) whereSy/C1x¼cov(yjx)¼Syy/C0SyxS/C01 xxSxyandDy/C1x¼[diag(Sy/C1x)]1=2. Unless yandxare independent ( Syx¼O), the partial correlation rij/C1rs...qis differ- ent from the usual correlation rij¼sij=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃsiisjjp.I nf a c t ,rij/C1rs...qandrijcan be of oppo- site signs (for an illustration, see Problem 4.16 g, h). To show this, we express sij/C1rs...q in terms of sij. We ﬁrst write Syxin terms of its rows Syx¼cov(y,x)¼sy1x1sy1x2...sy1xq sy2x1sy2x2...sy2xq .........",
    ". We ﬁrst write Syxin terms of its rows Syx¼cov(y,x)¼sy1x1sy1x2...sy1xq sy2x1sy2x2...sy2xq ......... sypx1sypx2...sypxq0 BBB@1 CCCA¼s 0 1x s02x ... s0 px0 BBB@1 CCCA, (4:38)100 MULTIVARIATE NORMAL DISTRIBUTION --- Page 116 --- wheres0 ix¼(syix1,syix2,... ,syixq). Then sij/C1rs...q, the ( ij)th element of Syy/C0SyxS/C01 xxSxy, can be written as sij/C1rs...q¼sij/C0s0 ixS/C01 xxsjx: (4:39) Suppose that sijis positive. Then sij/C1rs...qis negative if s0 ixS/C01 xxsjx.sij.",
    "C01 xxsjx: (4:39) Suppose that sijis positive. Then sij/C1rs...qis negative if s0 ixS/C01 xxsjx.sij. Note also that since S/C01 xxis positive deﬁnite, (4.39) shows that sii/C1rs...q¼sii/C0s0 ixS/C01 xxsix/C20sii: Example 4.5. We compare r12andr12/C134usingmandSin Example 4.4b.",
    "...q¼sii/C0s0 ixS/C01 xxsix/C20sii: Example 4.5. We compare r12andr12/C134usingmandSin Example 4.4b. From S, we obtain r12¼s12ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃs11s22p ¼0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ(9)(1)p ¼0: From cov( yjx)¼1 33126 /C024 /C024 14/C18/C19 in Example 4.4b, we obtain r12/C134¼s12/C134ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃs11/C134s22/C134p ¼/C024=33ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (126 =33)(14 =33)p ¼/C024ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ(36)(49)p ¼/C04 7¼/C0 :571 : A PROBLEMS 4.1 Show that E(z)¼0a n dv a r ( z)¼1w h e n zhas the standard normal density (4.1).",
    "71 : A PROBLEMS 4.1 Show that E(z)¼0a n dv a r ( z)¼1w h e n zhas the standard normal density (4.1). 4.2 Obtain (4.8) from (4.7); that is, show that jS/C01=2j¼jSj/C01=2. 4.3 Show that @My(0)=@t¼E(y) as in (4.16). 4.4 Show that @2My(0)=@tr@ts¼E(yrys) as in (4.17). 4.5 Show that the exponent in (4.19) can be expressed as in (4.20); that is, show that t0y/C0(y/C0m)0S/C01(y/C0m)=2¼t0mþt0St=2/C0(y/C0m/C0St)0S/C01 (y/C0m/C0St)=2. 4.6 Prove Corollary 1 to Theorem 4.3.",
    "S/C01(y/C0m)=2¼t0mþt0St=2/C0(y/C0m/C0St)0S/C01 (y/C0m/C0St)=2. 4.6 Prove Corollary 1 to Theorem 4.3. 4.7 Show that E(et0Ay)¼et0(Am)þt0(ASA0)t=2as in (4.25). 4.8 Consider a random variable with moment generating function M(t). Show that the second derivative of ln[ M(t)] evaluated at t¼0 is the variance of the random variable. 4.9 Assuming that yisNp(m,s2I) and Cis an orthogonal matrix, show that Cyis Np(Cm,s2I).PROBLEMS 101 --- Page 117 --- 4.10 Prove Corollary 1 to Theorem 4.4a.",
    "rix, show that Cyis Np(Cm,s2I).PROBLEMS 101 --- Page 117 --- 4.10 Prove Corollary 1 to Theorem 4.4a. 4.11 LetA¼(Ir,O), as deﬁned in the proof of Theorem 4.4b. Show that Ay¼y1,Am¼m1, and ASA0¼S11. 4.12 Prove Corollary 2 to Theorem 4.4c. 4.13 Prove Theorem 4.4d by direct evaluation of (4.28). 4.14 Given w¼y/C0Bx, show that cov( w,x)¼Syx/C0BSxx, as in (4.30). 4.15 Show that E(y/C0SyxS/C01 xxx)¼my/C0SyxS/C01 xxmxas in (4.31) and that cov(y/C0SyxS/C01 xxx)¼Syy/C0SyxS/C01 xxSxyas in (4.32).",
    "xxx)¼my/C0SyxS/C01 xxmxas in (4.31) and that cov(y/C0SyxS/C01 xxx)¼Syy/C0SyxS/C01 xxSxyas in (4.32). 4.16 Suppose that yisN4(m,S), where m¼1 23 /C020 BB@1 CCA,S¼42 /C012 263 /C02 /C0135 /C04 2/C02/C0440 BB@1 CCA: Find the following.",
    "here m¼1 23 /C020 BB@1 CCA,S¼42 /C012 263 /C02 /C0135 /C04 2/C02/C0440 BB@1 CCA: Find the following. (a)The joint marginal distribution of y 1andy3 (b)The marginal distribution of y2 (c)The distribution of z¼y1þ2y2/C0y3þ3y4 (d)The joint distribution of z1¼y1þy2/C0y3/C0y4and z2¼/C03y1þy2þ 2y3/C02y4 (e)f(y1,y2jy3,y4) (f)f(y1,y3jy2,y4) (g)r13 (h)r13/C124 (i)f(y1jy2,y3,y4) 4.17 Letybe distributed as N3(m,S), where m¼2 /C01 30 @1A,S¼410 121 0130 @1A: Find the following.",
    "4) 4.17 Letybe distributed as N3(m,S), where m¼2 /C01 30 @1A,S¼410 121 0130 @1A: Find the following. (a)The distribution of z¼4y 1/C06y2þy3 (b)The distribution of z¼y1/C0y2þy3 2y1þy2/C0y3/C18/C19 (c)f(y2jy1,y3) (d)f(y1,y2jy3) (e)r12andr12/C13102 MULTIVARIATE NORMAL DISTRIBUTION --- Page 118 --- 4.18 IfyisN3(m,S), where S¼20 /C01 04 0 /C010 30 @1 A, which variables are independent?",
    "Page 118 --- 4.18 IfyisN3(m,S), where S¼20 /C01 04 0 /C010 30 @1 A, which variables are independent? (See Corollary 1 to Theorem 4.4a) 4.19 IfyisN4(m,S), where S¼1 000 0 200 00 3 /C04 00 /C0460 BB@1 CCA, which variables are independent? 4.20 Show that sij/C1rs/C1/C1/C1q¼sij/C0s0 ixS/C01 xxsjxas in (4.39).PROBLEMS 103 --- Page 119 --- 5Distribution of Quadratic Forms in y 5.1 SUMS OF SQUARES In Chapters 3 and 4, we discussed some properties of linear functions of the random vector y.",
    "QUARES In Chapters 3 and 4, we discussed some properties of linear functions of the random vector y. We now consider quadratic forms in y. We will ﬁnd it useful in later chapters to express a sum of squares encountered in regression or analysis of variance as a quadratic form y0Ay, where yis a random vector and Ais a symmetric matrix of con- stants [see (2.33)].",
    "adratic form y0Ay, where yis a random vector and Ais a symmetric matrix of con- stants [see (2.33)]. In this format, we will be able to show that certain sums of squares have chi-square distributions and are independent, thereby leading to Ftests. Example 5.1. We express some simple sums of squares as quadratic forms in y.",
    "reby leading to Ftests. Example 5.1. We express some simple sums of squares as quadratic forms in y. Let y1,y2,...,ynbe a random sample from a population with mean mand variance s2.I n the following identity, the total sum of squaresPn i¼1y2 iis partitioned into a sum of squares about the sample mean /C22y¼Pn i¼1yi=nand a sum of squares due to the mean: Xn i¼1y2 i¼Xn i¼1y2i/C0ny/C02 !",
    "he sample mean /C22y¼Pn i¼1yi=nand a sum of squares due to the mean: Xn i¼1y2 i¼Xn i¼1y2i/C0ny/C02 ! þn/C22y2 ¼Xn i¼1(yi/C0/C22y)2þn/C22y2: (5:1) Using (2.20), we can expressPn i¼1y2 ias a quadratic form Xn i¼1y2 i¼y0y¼y0Iy, where y0¼(y1,y2,...,yn). Using j¼(1, 1, ...,1 )0as deﬁned in (2.6), we can Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 105 --- Page 120 --- write y¯as /C22y¼1 nXn i¼1yi¼1nj 0y [see (2.24)].",
    "#2008 John Wiley & Sons, Inc. 105 --- Page 120 --- write y¯as /C22y¼1 nXn i¼1yi¼1nj 0y [see (2.24)]. Then ny¯2becomes n/C22y2¼n1nj 0y/C18/C192 ¼n1nj 0y/C18/C191nj 0y/C18/C19 ¼n1n/C18/C19 2 y0jj0y[by (2 :18)] ¼n1n/C18/C19 2 y0Jy [by (2 :23)] ¼y01nJ/C18/C19 y: We can now writeP n i¼1(yi/C0/C22y)2as Xn i¼1(yi/C0/C22y)2¼Xn i¼1y2 i/C0n/C22y2¼y0Iy/C0y01 nJ/C18/C19 y ¼y0I/C01nJ/C18/C19 y: (5:2) Hence (5.1) can be written in terms of quadratic forms as y 0Iy¼y0I/C01nJ/C18/C19 yþy 01nJ/C18/C19 y: (5:3) A The matrices of the three quadratic forms in (5.3) have the following properties: 1.I¼I/C01 nJ/C18/C19 þ1nJ: 2.I,I/C01nJ, and1nJare idempotent.",
    "n (5.3) have the following properties: 1.I¼I/C01 nJ/C18/C19 þ1nJ: 2.I,I/C01nJ, and1nJare idempotent. 3. I/C01nJ/C18/C19 1nJ/C18/C19 ¼O:106 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 121 --- Using theorems given later in this chapter (and assuming normality of the yi’s), these three properties lead to the conclusion thatPn i¼1(yi/C0/C22y)2=s2andn/C22y2=s2have chi- square distributions and are independent.",
    "onclusion thatPn i¼1(yi/C0/C22y)2=s2andn/C22y2=s2have chi- square distributions and are independent. 5.2 MEAN AND VARIANCE OF QUADRATIC FORMS We ﬁrst consider the mean of a quadratic form y0Ay. Theorem 5.2a. Ifyis a random vector with mean mand covariance matrix Sand if Ais a symmetric matrix of constants, then E(y0Ay)¼tr(AS)þm0Am: (5:4) PROOF. By (3.25), S¼E(yy)0/C0mm0, which can be written as E(yy0)¼Sþmm0: (5:5) Since y0Ayis a scalar, it is equal to its trace.",
    "/C0mm0, which can be written as E(yy0)¼Sþmm0: (5:5) Since y0Ayis a scalar, it is equal to its trace. We thus have E(y0Ay)¼E[tr(y0Ay)] ¼E[tr(Ayy0)] [by (2 :87)] ¼tr[E(Ayy0)] [by (3 :5)] ¼tr[AE(yy0)] [by (3 :40)] ¼tr[A(Sþmm0)] [by (5 :8)] ¼tr[ASþAmm0] [by (2 :15)] ¼tr(AS)þtr(m0Am) [by (2 :86)] ¼tr(AS)þm0Am: Note that since y0Ayis not a linear function of y,E(y0Ay)=E(y0)AE(y).A Example 5.2a.",
    "] ¼tr(AS)þm0Am: Note that since y0Ayis not a linear function of y,E(y0Ay)=E(y0)AE(y).A Example 5.2a. To illustrate Theorem 5.2a, consider the sample variance s2¼Pn i¼1(yi/C0/C22y)2 n/C01: (5:6) By (5.2), the numerator of (5.6) can be written as Xn i¼1(yi/C0/C22y)2¼y0I/C01 nJ/C18/C19 y,5.2 MEAN AND VARIANCE OF QUADRATIC FORMS 107 --- Page 122 --- where y¼(y1,y2,... ,yn)0. If the y’s are assumed to be independently distributed with mean mand variance s2, then E(y)¼(m,m,... ,m)0¼mjand cov( y)¼s2I.",
    "be independently distributed with mean mand variance s2, then E(y)¼(m,m,... ,m)0¼mjand cov( y)¼s2I. Thus for use in (5.4) we have A¼I/C0(1=n)J,S¼s2I,andm¼mj; hence EXn i¼1(yi¼/C22y)2\"# ¼trI/C01 nJ/C18/C19 (s2I)/C20/C21 þmj0I/C01nJ/C18/C19 mj ¼s2trI/C01nJ/C18/C19 þ m2j0j/C0j0jj0j ðÞ [by (2 :23)] ¼s2n/C0nn/C16/C17 þ m2n/C01nn 2/C18/C19 [by (2 :23)] ¼s2(n/C01)þ0: Therefore E(s2)¼EPn i¼1(yi/C0/C22y)2/C2/C3 n/C01¼(n/C01)s2 n/C01¼s2: (5:7) A Note that normality of the y’s is not assumed in Theorem 5.2a.",
    "/C3 n/C01¼(n/C01)s2 n/C01¼s2: (5:7) A Note that normality of the y’s is not assumed in Theorem 5.2a. However, normality is assumed in obtaining the moment generating function of y0Ayand var( y0Ay) in the following theorems. Theorem 5.2b. IfyisNp(m,S),then the moment generating function of y0Ayis My0Ay(t)¼jI/C02tASj/C01=2e/C0m0[I/C0(I/C02tAS)/C01]S/C01m=2(5:8) PROOF.",
    "enerating function of y0Ayis My0Ay(t)¼jI/C02tASj/C01=2e/C0m0[I/C0(I/C02tAS)/C01]S/C01m=2(5:8) PROOF. By the multivariate analog of (3.3), we obtain My0Ay(t)¼E(ety0Ay)¼ð1 /C01...ð1 /C01ety0Ayk1e/C0(y/C0m)0S/C01(y/C0m)=2dy ¼k1ð1 /C01...ð1 /C01e/C0[y0(I/C02tAS)S/C01y/C02m0S/C01yþm0S/C01m]=2dy, where k1¼1=[(ﬃﬃﬃﬃﬃﬃ 2pp )pjSj1=2] and dy¼dy1dy2...dyp:For t sufﬁciently close to 0, I/C02tASis nonsingular.",
    "1¼1=[(ﬃﬃﬃﬃﬃﬃ 2pp )pjSj1=2] and dy¼dy1dy2...dyp:For t sufﬁciently close to 0, I/C02tASis nonsingular. Letting u0¼m0(I/C02tAS)/C01andV/C01¼(I/C02tAS)S/C01, we obtain My0Ay(t)¼k1k2ð1 /C01...ð1 /C01k3e/C0(y/C0u)0V/C01(y/C0u)=2dy108 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 123 --- (Problem 5.4), where k2¼(ﬃﬃﬃﬃﬃﬃﬃﬃ(2pp)pjVj1=2e/C0½m0S/C01m/C0u0V/C01u/C138=2and k3¼ 1=½ðﬃﬃﬃﬃﬃﬃﬃﬃ(2pp)pjVj1=2/C138. The multiple integral is equal to 1 since the multivariate normal density integrates to 1.",
    "1=2/C138. The multiple integral is equal to 1 since the multivariate normal density integrates to 1. Thus My0AyðtÞ¼k1k2. Substituting and simplifying, we obtain (5.8) (see Problem 5.5). A Theorem 5.2c. IfyisNp(m,S), then var(y0Ay)¼2tr[(AS)2]þ4m0ASAm: (5:9) PROOF. The variance of a random variable can be obtained by evaluating the second derivative of the natural logarithm of its moment generating function at t¼0 (see hint to Problem 5.14). Let C¼I/C02tAS.",
    "tural logarithm of its moment generating function at t¼0 (see hint to Problem 5.14). Let C¼I/C02tAS. Then, from (5.8) k(t)¼ln[My0Ay(t)]¼/C01 2lnjCj/C012m0(I/C0C/C01)S/C01m: Using (2.117), we differentiate k(t) twice to obtain k00(t)¼121 jCj2djCj dt/C20/C212 /C0121 jCjd2jCj dt2/C012m0C/C01d2C dt2C/C01S/C01m þmC/C01dC dt/C20/C212 C/C01S/C01m (Problem 5.6). A useful expression for jCjcan be found using (2.97) and (2.107). Thus, if the eigenvalues of ASareli,i¼1,...",
    "expression for jCjcan be found using (2.97) and (2.107). Thus, if the eigenvalues of ASareli,i¼1,... ,p,we obtain jCj¼Yp i¼1(1/C02tli) ¼1/C02tX iliþ4t2X i=jlilj/C0/C1/C1/C1þ (/C01)p2ptpl1l2/C1/C1/C1lp: Then ( djCj=dt)¼/C02Siliþ8tSi=jliljþhigher-order terms in t, and (d2jCj=dt2)¼8Si=jliljþhigher-order terms in t.",
    "=dt)¼/C02Siliþ8tSi=jliljþhigher-order terms in t, and (d2jCj=dt2)¼8Si=jliljþhigher-order terms in t. Evaluating these expressions at t¼0, we obtain jCj¼1,(djCj=dt)jt¼0¼/C02Sili¼/C02t r (AS) and (d2jCj=dt2)jt¼0¼8Si=jlilj:For t¼0 it is also true that C¼I,C/C01¼I, (dC=dt)jt¼0¼2ASand ( d2C=dt)jt¼0¼O:Thus k00(0)¼2[tr(AS)]2/C04X i=jliljþ0þ4m0ASAm ¼2 [tr( AS)]2/C02X i=jlilj() þ4m0ASAm:5.2 MEAN AND VARIANCE OF QUADRATIC FORMS 109 --- Page 124 --- By Problem 2.81, this can be written as 2t r [ (AS)2]þ4m0ASAm: A We now consider cov( y,y0Ay).",
    "4 --- By Problem 2.81, this can be written as 2t r [ (AS)2]þ4m0ASAm: A We now consider cov( y,y0Ay). To clarify the meaning of the expression cov( y, y0Ay), we denote y0Ayby the scalar random variable v. Then cov( y,v) is a column vector containing the covariance of each yiand v: cov(y,v)¼E{[y/C0E(y)][v/C0E(v)]}¼sy1v sy2v ... sypv0 BBB@1 CCCA: (5:10) [On the other hand, cov( v,y) would be a row vector.] An expression for cov( y,y 0Ay) is given in the next theorem. Theorem 5.2d.",
    "would be a row vector.] An expression for cov( y,y 0Ay) is given in the next theorem. Theorem 5.2d. IfyisNp(m,S),then cov(y,y0Ay)¼2SAm: (5:11) PROOF.",
    "y 0Ay) is given in the next theorem. Theorem 5.2d. IfyisNp(m,S),then cov(y,y0Ay)¼2SAm: (5:11) PROOF. By the deﬁnition in (5.10), we have cov(y,y0Ay)¼E{[y/C0E(y)][y0Ay/C0E(y0Ay)]}: By Theorem 5.2a, this becomes cov(y,y0Ay)¼E{(y/C0m)[y0Ay/C0tr(AS)/C0m0Am]}: Rewriting y0Ay/C0m0Amin terms of y/C0m(see Problem 5.7), we obtain cov(y,y0Ay)¼E{(y/C0m)[(y/C0m)0A(y/C0m)þ2(y/C0m)0Am/C0tr(AS)]} (5 :12) ¼E[(y/C0m)(y/C0m)0A(y/C0m)]þ2E[(y/C0m)(y/C0m)0Am] /C0E[(y/C0m)tr(AS)] ¼0þ2SAm/C00: The ﬁrst term on the right side is 0because all third central moments of the multivariate normal are zero.",
    "st term on the right side is 0because all third central moments of the multivariate normal are zero. The results for the other two terms do not depend on normality (see Problem 5.7). A110 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 125 --- Corollary 1. LetBbe a k/C2pmatrix of constants. Then cov(By,y0Ay)¼2BSAm: (5:13) A For the partitioned random vector v¼y x/C18/C19 , the bilinear form x0Aywas intro- duced in (2.34). The expected value of x0Ayis given in the following theorem. Theorem 5.2e.",
    "s intro- duced in (2.34). The expected value of x0Ayis given in the following theorem. Theorem 5.2e. Letv¼yx/C18/C19 be a partitioned random vector with mean vector and covariance matrix given by (3.32) and (3.33) Ey x/C18/C19 ¼my mx/C18/C19 and covy x/C18/C19 ¼SyySyx SxySxx/C18/C19 , where yisp/C21,xisq/C21, andSyxisp/C2q. Let Abe a q/C2pmatrix of constants. Then E(x0Ay)¼tr(ASyx)þm0 xAmy: (5:14) PROOF. The proof is similar to that of Theorem 5.2a; see Problem 5.10. A Example 5.2b.",
    "xAmy: (5:14) PROOF. The proof is similar to that of Theorem 5.2a; see Problem 5.10. A Example 5.2b. To estimate the population covariance sxy¼E[(x/C0mx)(y/C0my)] in (3.10), we use the sample covariance sxy¼Pn i¼1(xi/C0/C22x)(yi/C0/C22y) n/C01, (5:15) where ( x1,y1),(x2,y2),... ,(xn,yn) is a bivariate random sample from a population with means mxandmy, variances sx2andsy2, and covariance sxy.",
    "variate random sample from a population with means mxandmy, variances sx2andsy2, and covariance sxy. We can write (5.15) in the form sxy¼Pn i¼1xiyi/C0n/C22x/C22y n/C01¼x0[I/C0(1=n)J]y n/C01, (5:16) where x¼(x1,x2,... ,xn)0andy¼(y1,y2,... ,yn)0. Since ( xi,yi) is independent of (xj,yj) for i=j, the random vector v¼y x/C18/C19 has mean vector and covariance matrix Ey x/C18/C19 ¼my mx/C18/C19 ¼myj mxj/C18/C19 , covyx/C18/C19 ¼S yySyx SxySxx/C18/C19 ¼s2 yIsxyI sxyIs2xI !",
    "18/C19 ¼my mx/C18/C19 ¼myj mxj/C18/C19 , covyx/C18/C19 ¼S yySyx SxySxx/C18/C19 ¼s2 yIsxyI sxyIs2xI ! ,5.2 MEAN AND VARIANCE OF QUADRATIC FORMS 111 --- Page 126 --- where each Iisn/C2n. Thus for use in (5.14), we have A¼I/C0(1=n)J, Syx¼sxyI,mx¼mxj,andmy¼myj.",
    "26 --- where each Iisn/C2n. Thus for use in (5.14), we have A¼I/C0(1=n)J, Syx¼sxyI,mx¼mxj,andmy¼myj. Hence Ex0I/C01 nJ/C18/C19 y/C20/C21 ¼trI/C01nJ/C18/C19 sxyI/C20/C21 þmxj0I/C01nJ/C18/C19 myj ¼sxytrI/C01nJ/C18/C19 þ mxmyj0j/C01nj 0jj0j/C18/C19 ¼sxy(n/C01)þ0: Therefore E(sxy)¼EPn i¼1(xi/C0/C22x)(yi/C0/C22y)/C2/C3 n/C01¼(n/C01)sxy n/C01¼sxy: (5:17) A 5.3 NONCENTRAL CHI-SQUARE DISTRIBUTION Before discussing the noncentral chi-square distribution, we ﬁrst review the central chi-square distribution.",
    "scussing the noncentral chi-square distribution, we ﬁrst review the central chi-square distribution. Let z1,z2,... ,znbe a random sample from the standard normal distribution N(0, 1). Since the z’s are independent (by deﬁnition of random sample) and each ziisN(0, 1), the random vector z¼(z1,z2,...",
    "are independent (by deﬁnition of random sample) and each ziisN(0, 1), the random vector z¼(z1,z2,... ,zn)0is distributed asNn(0,I):By deﬁnition Xn i¼1z2 i¼z0zisx2(n); (5:18) that is, the sum of squares of nindependent standard normal random variables is dis- tributed as a (central) chi-square random variable with ndegrees of freedom. The mean, variance, and moment generating function of a chi-square random vari- able are given in the following theorem. Theorem 5.3a.",
    "erating function of a chi-square random vari- able are given in the following theorem. Theorem 5.3a. Ifuis distributed as x2(n), then E(u)¼n, (5:19) var(u)¼2n, (5:20) Mu(t)¼1 (1/C02t)n=2: (5:21) PROOF. Since uis the quadratic form z0Iz,E(u),var(u),andMu(t) can be obtained by applying Theorems 5.2a, 5.2c, and 5.2b, respectively. A112 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 127 --- Now suppose that y1,y2,... ,ynare independently distributed as N(mi, 1), so that y isNn(m,I), wherem¼(m1,m2,...",
    "that y1,y2,... ,ynare independently distributed as N(mi, 1), so that y isNn(m,I), wherem¼(m1,m2,... ,mn)0. In this case, Sn i¼1y2 i¼y0ydoes not have a chi-square distribution, but S(yi/C0mi)2¼(y/C0m)0(y/C0m)i sx2(n) since yi/C0miis distributed as N(0,1). The density of v¼Sn i¼1y2 i¼y0y,where the y’s are independently distributed as N(mi,1),is called the noncentral chi-square distribution and is denoted by x2(n,l).",
    "y distributed as N(mi,1),is called the noncentral chi-square distribution and is denoted by x2(n,l). The noncentrality parameter lis deﬁned as l¼1 2Xn i¼1m2 i¼1 2m0m: (5:22) Note that lis not an eigenvalue here and that the mean of v¼Sn i¼1y2 iis greater than the mean of u¼Sn i¼1(yi/C0mi)2: EXn i¼1(yi/C0mi)2\"# ¼Xn i¼1E(yi/C0mi)2¼Xn i¼1var(yi)¼Xn i¼11¼n, EXn i¼1y2 i ! ¼Xn i¼1E(y2i)¼Xn i¼1(s2iþm2i)¼Xn i¼1(1þm2i) ¼nþXn i¼1m2 i¼nþ2l, wherelis as deﬁned in (5.22).",
    "y2 i ! ¼Xn i¼1E(y2i)¼Xn i¼1(s2iþm2i)¼Xn i¼1(1þm2i) ¼nþXn i¼1m2 i¼nþ2l, wherelis as deﬁned in (5.22). The densities of uandvare illustrated in Figure 5.1. Figure 5.1 Central and noncentral chi-square densities.5.3 NONCENTRAL CHI-SQUARE DISTRIBUTION 113 --- Page 128 --- The mean, variance, and moment generating function of a noncentral chi-square random variable are given in the following theorem. Theorem 5.3b.",
    "unction of a noncentral chi-square random variable are given in the following theorem. Theorem 5.3b. Ifvis distributed as x2(n,l), then E(v)¼nþ2l, (5:23) var(v)¼2nþ8l, (5:24) Mv(t)¼1 (1/C02t)n=2e/C0l[1/C01=(1/C02t)]: (5:25) PROOF.F o r E(v) and var( v), see Problems 5.13 and 5.14. For Mv(t), use Theorem 5.2b. A Corollary 1.",
    "ROOF.F o r E(v) and var( v), see Problems 5.13 and 5.14. For Mv(t), use Theorem 5.2b. A Corollary 1. Ifl¼0 (which corresponds to mi¼0 for all i), then E(v), var( v), and Mv(t) in Theorem 5.3b reduce to E(u), var( u),Mu(t) for the central chi-square distri- bution in Theorem 5.3a. Thus x2(n,0)¼x2(n): (5:26) A The chi-square distribution has an additive property, as shown in the following theorem. Theorem 5.3c. Ifv1,v2,...",
    "distribution has an additive property, as shown in the following theorem. Theorem 5.3c. Ifv1,v2,... ,vkare independently distributed as x2(ni,li),then Xk i¼1viis distributed as x2Xk i¼1ni,Xk i¼1li ! : (5:27) A Corollary 1. Ifu1,u2,... ,ukare independently distributed as x2(ni), then Xk i¼1uiis distributed as x2Xk i¼1ni !",
    "Ifu1,u2,... ,ukare independently distributed as x2(ni), then Xk i¼1uiis distributed as x2Xk i¼1ni ! : A 5.4 NONCENTRAL FAND tDISTRIBUTIONS 5.4.1 Noncentral FDistribution Before deﬁning the noncentral Fdistribution, we ﬁrst review the central F.I fuis x2(p),visx2(q), and uandvare independent, then by deﬁnition w¼u=p v=qis distributed as F(p,q), (5:28)114 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 129 --- the (central) Fdistribution with pandqdegress of freedom.",
    "N OF QUADRATIC FORMS IN y --- Page 129 --- the (central) Fdistribution with pandqdegress of freedom. The mean and variance ofware given by E(w)¼q q/C02,var(w)¼2q2(pþq/C02) p(q/C01)2(q/C04): (5:29) Now suppose that uis distributed as a noncentral chi-square random variable, x2(p,l),while vremains central chi-square random variable, x2(q), with uandv independent.",
    "variable, x2(p,l),while vremains central chi-square random variable, x2(q), with uandv independent. Then z¼u=p v=qis distributed as F(p,q,l), (5:30) thenoncentral F distribution with noncentrality parameter l, wherelis the same noncentrality parameter as in the distribution of u(noncentral chi-square distribution). The mean of zis E(z)¼q q/C021þ2l p/C18/C19 , (5:31) which is, course, greater than E(w) in (5.29).",
    ". The mean of zis E(z)¼q q/C021þ2l p/C18/C19 , (5:31) which is, course, greater than E(w) in (5.29). When an Fstatistic is used to test a hypothesis H0, the distribution will typically be central if the (null) hypothesis is true and noncentral if the hypothesis is false. Thus the noncentral Fdistribution can often be used to evaluate the power of an Ftest.",
    "sis is false. Thus the noncentral Fdistribution can often be used to evaluate the power of an Ftest. The power of a test is the probability of rejecting H0for a given value of l.I fFais the upperapercentage point of the central Fdistribution, then the power, P(p,q,a, l), can be deﬁned as P(p,q,a,l)¼Prob ( z/C21Fa), (5:32) where zis the noncentral Frandom variable deﬁned in (5.30). Ghosh (1973) showed thatP(p,q,a,l) increases if qoraorlincreases, and P(p,q,a,l) decreases if p increases.",
    "(1973) showed thatP(p,q,a,l) increases if qoraorlincreases, and P(p,q,a,l) decreases if p increases. The power is illustrated in Figure 5.2. The power as deﬁned in (5.32) can be evaluated from tables (Tiku 1967) or directly from distribution functions available in many software packages.",
    "from tables (Tiku 1967) or directly from distribution functions available in many software packages. For example, in SAS, the noncentral F-distribution function PROBF can be used to ﬁnd the power in (5.32) as follows: P(p,q,a,l)¼1/C0PROBF( Fa,p,q,l): A probability calculator for the Fand other distributions is available free of charge from NCSS (download at www.ncss.com).5.4 NONCENTRAL FAND tDISTRIBUTIONS 115 --- Page 130 --- 5.4.2 Noncentral tDistribution We ﬁrst review the central tdistribution.",
    "UTIONS 115 --- Page 130 --- 5.4.2 Noncentral tDistribution We ﬁrst review the central tdistribution. If zisN(0,1),uisx2(p), and zanduare independent, then by deﬁnition t¼zﬃﬃﬃﬃﬃﬃﬃﬃ u=pp is distributed as t(p), (5:33) the (central) tdistribution with pdegrees of freedom. Now suppose that yisN(m,1),uisx2(p),andyanduare independent. Then t¼yﬃﬃﬃﬃﬃﬃﬃﬃu=pp is distributed as t(p,m), (5:34) the noncentral tdistribution with pdegrees of freedom and noncentrality parameter m.",
    "t(p,m), (5:34) the noncentral tdistribution with pdegrees of freedom and noncentrality parameter m. IfyisN(m,s2),then t¼y=sﬃﬃﬃﬃﬃﬃﬃﬃu=pp is distributed as t(p,m=s), since by (3.4), (3.9), and Theorem 4.4a(i), y/sis distributed as N(m=s,1):Figure 5.2 Central F, noncentral F, and power of the Ftest (shaded area).116 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 131 --- 5.5 DISTRIBUTION OF QUADRATIC FORMS It was noted following Theorem 5.3a that if yisNn(m,I), then ( y/C0m)0(y/C0m)i s x2(n).",
    "DRATIC FORMS It was noted following Theorem 5.3a that if yisNn(m,I), then ( y/C0m)0(y/C0m)i s x2(n). IfyisNn(m,S), we can extend this to (y/C0m)0S/C01(y/C0m)i sx2(n): (5:35) To show this, we write ( y/C0m)0S/C01(y/C0m) in the form (y/C0m)0S/C01(y/C0m)¼(y/C0m)0S/C01=2S/C01=2(y/C0m) ¼S/C01=2(y/C0m)hi0 S/C01=2(y/C0m)hi ¼z0z, where z¼S/C01=2(y/C0m) andS/C01=2¼(S1=2)/C01, withS1=2given by (2.109).",
    "m)hi0 S/C01=2(y/C0m)hi ¼z0z, where z¼S/C01=2(y/C0m) andS/C01=2¼(S1=2)/C01, withS1=2given by (2.109). The vector zis distributed as Nn(0,I) (see Problem 5.17); therefore, z0zisx2(n)b y deﬁnition [see (5.18)]. Note the analogy of ( y/C0m)0S/C01(y/C0m) to the univariate random variable ( y/C0m)2=s2, which is distributed as x2(1) if yisN(m,s2). In the following theorem, we consider the distribution of quadratic forms in general. In the proof we follow Searle (1971, p. 57).",
    "onsider the distribution of quadratic forms in general. In the proof we follow Searle (1971, p. 57). For alternative proofs, see Graybill (1976, pp. 134–136) and Hocking (1996, p. 51). Theorem 5.5. Letybe distributed as Np(m,S), let Abe a symmetric matrix of con- stants of rank r, and let l¼1 2m0Am. Then y0Ayisx2(r,l), if and only if ASis idempotent. PROOF.",
    "con- stants of rank r, and let l¼1 2m0Am. Then y0Ayisx2(r,l), if and only if ASis idempotent. PROOF. By Theorem 5.2b the moment generating function of y0Ayis My0Ay(t)¼jI/C02tASj/C01=2e/C0(1=2)m0[I/C0(I/C02tAS)/C01]S/C01m: By (2.98), the eigenvalues of I/C02tASare 1 /C02tli,i¼1,2,... ,p, whereliis an eigenvalue of AS. By (2.107), jI/C02tASj¼Qp i¼1(1/C02tli). By (2.102), (I/C02tAS)/C01¼IþP1k¼1ð2tÞkðASÞk, provided /C01,2tli,1 for all i.",
    "tASj¼Qp i¼1(1/C02tli). By (2.102), (I/C02tAS)/C01¼IþP1k¼1ð2tÞkðASÞk, provided /C01,2tli,1 for all i. Thus My0AyðtÞcan be written as My0Ay(t)¼Yp i¼1(1/C02tli)/C01=2 ! e/C0(1=2)m0/C0P1 k¼1(2t)k(AS)k½/C138 S/C01m:5.5 DISTRIBUTION OF QUADRATIC FORMS 117 --- Page 132 --- Suppose that ASis idempotent of rank r(the rank of A); then r of the li’s are equal to 1, p2rof theli’s are equal to 0, and ( AS)k¼AS. Therefore, My0Ay(t)¼Yr i¼1(1/C02t)/C01=2 !",
    "equal to 1, p2rof theli’s are equal to 0, and ( AS)k¼AS. Therefore, My0Ay(t)¼Yr i¼1(1/C02t)/C01=2 ! e/C0(1=2)m0/C0P1 k¼1(2t)k½/C138 ASS/C01m ¼(1/C02t)/C0r=2e/C01=2m01/C0(1/C02t)/C01 ½/C138 Am, provided /C01,2t,1o r/C01 2,t,12, which is compatible with the requirement that the moment generating function exists for t in a neighborhood of 0.",
    "atible with the requirement that the moment generating function exists for t in a neighborhood of 0. Thus My0Ay(t)¼1 (1/C02t)r=2e/C0ð1=2Þm0Am1/C01=(1/C02t) ½/C138, which by (5.25) is the moment generating function of a noncentral chi-square random variable with degrees of freedom r¼rank( A) and noncentrality parameter l¼1 2m0Am. For a proof of the converse, namely, if y0Ayisx2(r,l), then ASis idempotent; see Driscoll (1999).",
    "m. For a proof of the converse, namely, if y0Ayisx2(r,l), then ASis idempotent; see Driscoll (1999). A Some corollaries of interest are the following (for additional corollaries, see Problem 5.20). Corollary 1. IfyisNp(0,I), then y0Ay isx2(r) if and only if Ais idempotent of rank r. A Corollary 2. IfyisNp(m,s2I), then y0Ay=s2isx2(r,m0Am=2s2) if and only if A is idempotent of rank r. A Example 5.",
    "IfyisNp(m,s2I), then y0Ay=s2isx2(r,m0Am=2s2) if and only if A is idempotent of rank r. A Example 5. To illustrate Corollary 2 to Theorem 5.5, consider the distribution of (n/C01)s2=s2¼Pn i¼1(yi/C0/C22y)2=s2, where y¼(y1,y2,... ,yn)0is distributed as Nn(mj,s2I) as in Examples 5.1 and 5.2 In (5.2) we havePn i¼1(yi/C0/C22y)2¼ y0I/C0(1=n)J ½/C138 y. The matrix I/C0(1=n)Jis shown to be idempotent in Problem 5.2. Then by Theorem 2.13d, rank I/C0(1=n)J ½/C138 ¼ tr[I/C0(1=n)J]¼n/C01.",
    "be idempotent in Problem 5.2. Then by Theorem 2.13d, rank I/C0(1=n)J ½/C138 ¼ tr[I/C0(1=n)J]¼n/C01. We next ﬁnd l, which is given by l¼m0Am 2s2¼mj0(I/C01 nJ)mj 2s2¼m2(j0j/C01nj0Jj) 2s2 ¼m2(n/C01 nj0jj0j) 2s2¼m2[n/C01n(n)(n)] 2s2¼0: Therefore, y0I/C0(1=n)J ½/C138 y=s2isx2(n/C01).",
    ") 2s2 ¼m2(n/C01 nj0jj0j) 2s2¼m2[n/C01n(n)(n)] 2s2¼0: Therefore, y0I/C0(1=n)J ½/C138 y=s2isx2(n/C01). A118 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 133 --- 5.6 INDEPENDENCE OF LINEAR FORMS AND QUADRATIC FORMS In this section, we discuss the independence of (1) a linear form and a quadratic form, (2) two quadratic forms, and (3) several quadratic forms. For an example of (1), consider y¯ands2in a simple random sample or bˆands2in a regression setting.",
    "For an example of (1), consider y¯ands2in a simple random sample or bˆands2in a regression setting. To illustrate (2), consider the sum of squares due to regression and the sum of squares due to error. An example of (3) is given by the sums of squares due to main effects and interaction in a balanced two-way analysis of variance. We begin with the independence of a linear form and a quadratic form. Theorem 5.6a.",
    "sis of variance. We begin with the independence of a linear form and a quadratic form. Theorem 5.6a. Suppose that Bis ak/C2pmatrix of constants, Ais ap/C2psym- metric matrix of constants, and yis distributed as Np(m,S). Then Byandy0Ayare independent if and only if BSA¼O. PROOF. Suppose BSA¼O. We prove that Byandy0Ayare independent for the special case in which Ais symmetric and idempotent. For a general proof, see Searle (1971, p. 59).",
    "e special case in which Ais symmetric and idempotent. For a general proof, see Searle (1971, p. 59). Assuming that Ais symmetric and idempotent, y0Aycan be written as y0Ay¼y0A0Ay¼(Ay)0Ay: IfBSA¼O, we have by (3.45) BSA¼cov(By,Ay)¼O: Hence, by Corollary 2 to Theorem 4.4c, ByandAyare independent, and therefore Byand the function ( Ay)0Ayare also independent (Seber 1977, pp. 17, 33–34). We now establish the converse, namely, if Byandy0Ayare independent, then BSA¼O.",
    "977, pp. 17, 33–34). We now establish the converse, namely, if Byandy0Ayare independent, then BSA¼O. By Corollary 1 to Theorem 5.2d, cov( By,y0Ay)¼0becomes 2BSAm¼0: Since this holds for all possible m,w eh a v e BSA¼O[see (2.44)]. A Note that BSA¼Odoes not imply ASB¼O. In fact, the product ASBwill not be deﬁned unless Bhasprows. Corollary 1. IfyisNp(m,s2I), then Byandy0Ayare independent if and only if BA¼O. A5.6 INDEPENDENCE OF LINEAR FORMS AND QUADRATIC FORMS 119 --- Page 134 --- Example 5.6a.",
    "ly if BA¼O. A5.6 INDEPENDENCE OF LINEAR FORMS AND QUADRATIC FORMS 119 --- Page 134 --- Example 5.6a. To illustrate Corollary 1, consider s2¼Pn i¼1(yi/C0/C22y)2=(n/C01) and /C22y¼Pni¼1yi=n, where y¼(y1,y2,... ,yn)0isNn(mj,s2I). As in Example 5.1, y¯ and s2can be written as /C22y¼(1=n)j0yand s2¼y0I/C0(1=n)J ½/C138 y=(n/C01). By Corollary 1, y¯is independent of s2since (1 =n)j0I/C0(1=n)J ½/C138 ¼ 00: A We now consider the independence of two quadratic forms. Theorem 5.6b.",
    "n)j0I/C0(1=n)J ½/C138 ¼ 00: A We now consider the independence of two quadratic forms. Theorem 5.6b. LetAandBbe symmetric matrices of constants. If yisNp(m,S), theny0Ayandy0Byare independent if and only if ASB¼O. PROOF. Suppose ASB¼O. We prove that y0Ayandy0Byare independent for the special case in which AandBare symmetric and idempotent. For a general proof, see Searle (1971, pp. 59–60) or Hocking (1996, p. 52).",
    "ymmetric and idempotent. For a general proof, see Searle (1971, pp. 59–60) or Hocking (1996, p. 52). Assuming that AandBare symmetric and idempotent, y0Ayandy0Bycan be written as y0Ay¼y0A0Ay¼(Ay)0Ay andy0By¼y0B0By¼(By)0By.I fASB¼O, we have [see (3.45)] ASB¼cov(Ay,By)¼O: Hence, by Corollary 2 to Theorem 4.4c, AyandByare independent. It follows that the functions ( Ay)0(Ay)¼y0Ayand (By)0(By)¼y0Byare independent (Seber 1977, pp. 17, 33–34).",
    "ows that the functions ( Ay)0(Ay)¼y0Ayand (By)0(By)¼y0Byare independent (Seber 1977, pp. 17, 33–34). A Note that ASB¼Ois equivalent to BSA¼Osince transposing both sides of ASB¼Ogives BSA¼O(AandBare symmetric). Corollary 1. IfyisNp(m,s2I), then y0Ayandy0Byare independent if and only if AB¼O(or, equivalently, BA¼O). A Example 5.6b.",
    "m,s2I), then y0Ayandy0Byare independent if and only if AB¼O(or, equivalently, BA¼O). A Example 5.6b. To illustrate Corollary 1, consider the partitioning in (5.1),Pni¼1y2 i¼Pn i¼1(yi/C0/C22y)2þn/C22y2, which was expressed in (5.3) as y0y¼y0(I/C0(1=n)J)yþy0((1=n)J)y: IfyisNn(mj,s2I), then by Corollary 1, y0[I/C0(1=n)J]yandy0[(1=n)J]yare indepen- dent if and only if [ I/C0(1=n)J][(1 =n)J]¼O, which is shown in Problem 5.2.",
    "y0[(1=n)J]yare indepen- dent if and only if [ I/C0(1=n)J][(1 =n)J]¼O, which is shown in Problem 5.2. A The distribution and independence of several quadratic forms are considered in the following theorem.120 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 135 --- Theorem 5.6c. Let ybeNn(m,s2I), let Aibe symmetric of rank rifor i¼1,2,... ,k, and let y0Ay¼Pk i¼1y0Aiy, where A¼Pki¼1Aiis symmetric of rank r. Then (i)y0Aiy=s2isx2(ri,m0Aim=2s2),i¼1,2,... ,k. (ii)y0Aiyandy0Ajyare independent for all i=j.",
    "nk r. Then (i)y0Aiy=s2isx2(ri,m0Aim=2s2),i¼1,2,... ,k. (ii)y0Aiyandy0Ajyare independent for all i=j. (iii) y0Ay=s2isx2(r,m0Am=2s2). These results are obtained if and only if any two of the following three statements are true: (a) Each Aiis idempotent. (b)AiAj¼Ofor all i=j: (c)A¼Pk i¼1Aiis idempotent. Or if and only if (c) and (d) are true, where (d) is the following statement: (d)r¼Pki¼1ri: PROOF. See Searle (1971, pp. 61–64).",
    ") are true, where (d) is the following statement: (d)r¼Pki¼1ri: PROOF. See Searle (1971, pp. 61–64). A Note that by Theorem 2.13g, any two of (a), (b), or (c) implies the third. Theorem 5.6c pertains to partitioning a sum of squares into several component sums of squares. The following corollary treats the special case where A¼I; that is, the case of partitioning the total sum of squares y0yinto several sums of squares. Corollary 1. LetybeNn(m,s2I), let Aibe symmetric of rank rifori¼1,2,...",
    "nto several sums of squares. Corollary 1. LetybeNn(m,s2I), let Aibe symmetric of rank rifori¼1,2,... ,k, and let y0y¼Pk i¼1y0Aiy. Then (i) each y0Aiy=s2isx2(ri,m0Aim=2s2) and (ii) the y0Aiyterms are mutually independent if and only if any oneof the following state- ments holds: (a) Each Aiis idempotent. (b)AiAj¼Ofor all i=j. (c)n¼Pk i¼1ri. A Note that by Theorem 2.13h, condition (c) implies the other two conditions. Cochran (1934) ﬁrst proved a version of Corollary 1 to Theorem 5.6c.",
    "plies the other two conditions. Cochran (1934) ﬁrst proved a version of Corollary 1 to Theorem 5.6c. PROBLEMS 5.1 Show thatPn i¼1(yi/C0/C22y)2¼Pni¼1y2 i/C0n/C22y2as in (5.1).PROBLEMS 121 --- Page 136 --- 5.2 Show that (1 /n)Jis idempotent, I/C0(1=n)Jis idempotent, and [I/C0(1=n)J][(1 =n)J]¼O, as noted in Section 5.1. 5.3 Obtain var( s2) in the following two ways, where s2is deﬁned in (5.6) as s2¼Pn i¼1(yi/C0/C22y)2=(n/C01) and we assume that y¼(y1,y2,... ,yn)0is Nn(mj,s2I).",
    "eﬁned in (5.6) as s2¼Pn i¼1(yi/C0/C22y)2=(n/C01) and we assume that y¼(y1,y2,... ,yn)0is Nn(mj,s2I). (a)Write s2ass2¼y0[I/C0(1=n)J]y=(n/C01) and use Theorem 5.2b. (b)The function u¼(n/C01)s2=s2is distributed as x2(n/C01), and therefore var(u)¼2(n/C01). Then var( s2)¼vars2u=(n/C01) ½/C138 .",
    "=s2is distributed as x2(n/C01), and therefore var(u)¼2(n/C01). Then var( s2)¼vars2u=(n/C01) ½/C138 . 5.4 Show that jSj/C0(1=2)jVj(1=2)e/C0(m0S/C01m/C0u0V/C01u)=2 ¼jI/C02tASje/C0(1=2)m0[I/C0(I/C02tAS)/C01]S/C01m=2 as in the proof of Theorem 5.2b, where u0¼m0(I/C02tAS)/C01and V/C01¼(I/C02tAS)S/C01: 5.5 Show that e/C0[y0(I/C02tAS)S/C01y/C02m0S/C01yþm0S/C01m]=2¼e/C0[m0S/C01m/C0u0V/C01u]=2e/C0(y/C0u)0V/C01(y/C0u)=2 as in the proof of Theorem 5.2b, where u0¼m0(I/C02tAS)/C01and V/C01¼(I/C02tAS)S/C01.",
    "0V/C01(y/C0u)=2 as in the proof of Theorem 5.2b, where u0¼m0(I/C02tAS)/C01and V/C01¼(I/C02tAS)S/C01. 5.6 Letk(t)¼/C01 2lnjCj/C012m0ðI/C0C/C01ÞS/C01mas in the proof of Theorem 5.2c, where Cis a nonsingular matrix. Derive k00ðtÞ. 5.7 Show that y0Ay/C0m0Am¼ðy/C0mÞ0Aðy/C0mÞþ2ðy/C0mÞ0Amas in (5.12). 5.8 Obtain the three terms 0,2SAm, and 0in the proof of Theorem 5.2d. 5.9 Prove Corollary 1 to Theorem 5.2d. 5.10 Prove Theorem 5.2e.",
    ", and 0in the proof of Theorem 5.2d. 5.9 Prove Corollary 1 to Theorem 5.2d. 5.10 Prove Theorem 5.2e. 5.11 (a) Show thatPn i¼1ðxi/C0/C22xÞðyi/C0/C22yÞin (5.15) is equal toPni¼1xiyi/C0n/C22x/C22y in (5.16). (b)Show thatPni¼1xiyi/C0n/C22x/C22y¼x0[I/C0ð1=nÞJ]y, as in (5.16) in Example 5.2. 5.12 Prove Theorem 5.3a. 5.13 Ifv¼x2ðn,lÞ, use Theorem 5.2c to show that var ðvÞ¼2nþ8las in (5.24). 5.14 Ifvisx2ðn;lÞ, use the moment generating function in (5.25) to ﬁnd E(v) and var( v).",
    "in (5.24). 5.14 Ifvisx2ðn;lÞ, use the moment generating function in (5.25) to ﬁnd E(v) and var( v). [Hint: Use ln[ MvðtÞ]; then dln[Mvð0Þ]=dt¼EðvÞand d2ln[Mvð0Þ]=dt¼varðvÞ(see Problem 4.8). The notation dln[Mvð0Þ]=dt122 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 137 --- indicates that dln[MvðtÞ]=dtis evaluated at t¼0; the notation d2ln[Mvð0Þ]=dt2is deﬁned similarly.] 5.15 Prove Theorem 5.3c. 5.16 (a) Show that if t¼z=ﬃﬃﬃﬃﬃﬃﬃﬃ u=pp ist(p) as in (5.33), then t2isF(1,p).",
    "15 Prove Theorem 5.3c. 5.16 (a) Show that if t¼z=ﬃﬃﬃﬃﬃﬃﬃﬃ u=pp ist(p) as in (5.33), then t2isF(1,p). (b)Show that if t¼y=ﬃﬃﬃﬃﬃﬃﬃﬃu=pp ist(p,m) as in (5.34), then t2isFð1,p,1 2m2Þ. 5.17 Show that S/C01=2ðy/C0mÞisNnð0,IÞ, as used in the illustration at the beginning of Section 5.5. 5.18 (a) Prove Corollary 1 of Theorem 5.5a. (b)Prove Corollary 2 of Theorem 5.5a. 5.19 IfyisNnðm,SÞ, verify that ðy/C0mÞ0S/C01ðy/C0mÞisx2(n), as in (5.25), by using Theorem 5.5a. What is the distribution of y0S/C01y?",
    "C0mÞ0S/C01ðy/C0mÞisx2(n), as in (5.25), by using Theorem 5.5a. What is the distribution of y0S/C01y? 5.20 Prove the following additional corollaries to Theorem 5.5a: (a)IfyisNpð0,SÞ, then y0Ayisx2(r) if and only if ASis idempotent of rank r. (b)IfyisNpðm,s2IÞ, then y0y=s2isx2ðp,m0m=2s2Þ. (c)IfyisNpðm,IÞ, then y0Ayisx2ðr,12m0AmÞif and only if Ais idempotent of rank r. (d)IfyisNpðm,s2SÞ, then y0Ay=s2isx2ðr,m0Am=2s2Þif and only if AS is idempotent of rank r.",
    "of rank r. (d)IfyisNpðm,s2SÞ, then y0Ay=s2isx2ðr,m0Am=2s2Þif and only if AS is idempotent of rank r. (e)IfyisNpðm,s2SÞ, then y0S/C01y=s2isx2ðp,m0S/C01m=2s2Þ. 5.21 Prove Corollary 1 of Theorem 5.6a. 5.22 Show that j0½I/C0ð1=nÞJ/C138¼00, as in Example 5.6a. 5.23 Prove Corollary 1 of Theorem 5.6b. 5.24 Suppose that y1,y2,...,ynis a random sample from Nðm,s2Þso that y¼ðy1,y2,... ,ynÞ0isNnðmj,s2IÞ. It was shown in Example 5.5 that ðn/C01Þs2=s2¼Pn i¼1ðyi/C0/C22yÞ2=s2isx2ðn/C01Þ.",
    ",ynÞ0isNnðmj,s2IÞ. It was shown in Example 5.5 that ðn/C01Þs2=s2¼Pn i¼1ðyi/C0/C22yÞ2=s2isx2ðn/C01Þ. In Example 5.6a, it was demonstrated that y¯ands2¼Pn i¼1ðyi/C0/C22yÞ2=ðn/C01Þare independent. (a)Show that y¯isNðm,s2=nÞ. (b)Show that t¼ð/C22y/C0mÞ=ðs=ﬃﬃﬃnpÞis distributed as t(n21). (c)Givenm0=m, show that t¼ð/C22y/C0m0Þ=ðs=ﬃﬃﬃnpÞis distributed as t(n21,d). Findd. 5.25 Suppose that yisNnðmj,s2IÞ.",
    "how that t¼ð/C22y/C0m0Þ=ðs=ﬃﬃﬃnpÞis distributed as t(n21,d). Findd. 5.25 Suppose that yisNnðmj,s2IÞ. Find the distribution of u¼n/C22y2 Pn i¼1(yi/C0/C22y)2=(n/C01): (This statistic could be used to test H0:m¼0.)PROBLEMS 123 --- Page 138 --- 5.26 Suppose that yisNnðm,SÞ, wherem¼mjand S¼s21r ...r r1 ...r ......... rr ... 10 BBB@1 CCCA: Thus EðyiÞ¼mfor all i, varðyiÞ¼s2for all i, and cov ðyi,yjÞ¼s2rfor all i=j; that is, the y’s are equicorrelated.",
    "for all i, varðyiÞ¼s2for all i, and cov ðyi,yjÞ¼s2rfor all i=j; that is, the y’s are equicorrelated. (a)Show that Scan be written in the form S¼s2½ð1/C0rÞIþrJ/C138. (b)Show thatPn i¼1ðyi/C0/C22yÞ2=½s2ð1/C0rÞ/C138isx2ðn/C01Þ. 5.27 Suppose that yisN3ðm,SÞ, where m¼2 /C01 30 @1A,S¼410 121 0130 @1A: Let A¼1/C03/C08 /C032 /C06 /C08/C0630@1A: (a)Find E(y 0Ay). (b)Find var ( y0Ay). (c)Does y0Ayhave a chi-square distribution? (d)IfS¼s2I, does y0Ay=s2have a chi-square distribution?",
    "(c)Does y0Ayhave a chi-square distribution? (d)IfS¼s2I, does y0Ay=s2have a chi-square distribution? 5.28 Assuming that yisN3ðm,SÞ, where m¼3 /C02 10 @1A,S¼200 0400030 @1A, ﬁnd a symmetric matrix Asuch that y 0Ayisx2ð3,1 2m0AmÞ. What is l¼12m0Am?124 DISTRIBUTION OF QUADRATIC FORMS IN y --- Page 139 --- 5.29 Assuming that yisN4ðm,SÞ, where m¼3 /C02 140 BB@1 CCA,S¼10 0 0 02 0 0 00 3 /C04 00 /C0460 BB@1 CCA, ﬁnd a matrix Asuch that y 0Ayisx2ð4,1 2m0AmÞ. What is l¼12m0Am?",
    "02 0 0 00 3 /C04 00 /C0460 BB@1 CCA, ﬁnd a matrix Asuch that y 0Ayisx2ð4,1 2m0AmÞ. What is l¼12m0Am? 5.30 Suppose that yisN3ðm,s2IÞand let m¼3 /C02 10 @1A,A¼ 1 32/C01/C01 /C012 /C01 /C01/C0120 @1A,B¼11 1 10 /C01/C18/C19 : (a)What is the distribution of y 0Ay=s2? (b)Arey0AyandByindependent? (c)Arey0Ayandy1þy2þy3independent? 5.31 Suppose that yisN3ðm,s2IÞ, wherem¼ð1,2,3Þ0, and let B¼1 3111 111 1110 @1A: (a)What is the distribution of y 0By=s2?",
    "sN3ðm,s2IÞ, wherem¼ð1,2,3Þ0, and let B¼1 3111 111 1110 @1A: (a)What is the distribution of y 0By=s2? (b)Isy0Byindependent of y0Ay, where Ais as deﬁned in Problem 5.30? 5.32 Suppose that yisNnðm,s2IÞand that Xis an n/C2pmatrix of constants with rank p,n. (a)Show that H¼XðX0XÞ/C01X0andI/C0H¼I/C0XðX0XÞ/C01X0are idempo- tent, and ﬁnd the rank of each.",
    "p,n. (a)Show that H¼XðX0XÞ/C01X0andI/C0H¼I/C0XðX0XÞ/C01X0are idempo- tent, and ﬁnd the rank of each. (b)Assuming mis a linear combination of the columns of X, that ism¼Xbfor some b[see (2.37)], ﬁnd Eðy0HyÞandE½y0ðI/C0HÞy/C138,w h e r e His as deﬁned in part (a) . (c)Find the distributions of y0Hy=s2andy0ðI/C0HÞy=s2. (d)Show that y0Hyandy0ðI/C0HÞyare independent.",
    "(c)Find the distributions of y0Hy=s2andy0ðI/C0HÞy=s2. (d)Show that y0Hyandy0ðI/C0HÞyare independent. (e)Find the distribution of y0Hy=p y0(I/C0H)y=(n/C0p):PROBLEMS 125 --- Page 140 --- 6Simple Linear Regression 6.1 THE MODEL By (1.1), the simple linear regression model for nobservations can be written as yi¼b0þb1xiþ1i,i¼1,2,...,n: (6:1) The designation simple indicates that there is only one xto predict the response y, and linear means that the model (6.1) is linear in b0andb1.",
    "is only one xto predict the response y, and linear means that the model (6.1) is linear in b0andb1. [Actually, it is the assumption E(yi)¼b0þb1xithat is linear; see assumption 1 below.] For example, a model such asyi¼b0þb1x2 iþ1iis linear in b0andb1, whereas the model yi¼b0þeb1xiþ1i is not linear. In this chapter, we assume that yiand1iare random variables and that the values of xiare known constants, which means that the same values of x1,x2,...,xnwould be used in repeated sampling.",
    "known constants, which means that the same values of x1,x2,...,xnwould be used in repeated sampling. The case in which the xvariables are random variables is treated in Chapter 10. To complete the model in (6.1), we make the following additional assumptions: 1.E(1i)¼0 for all i¼1, 2, ...,n, or, equivalently, E(yi)¼b0þb1xi. 2. var(1i)¼s2for all i¼1, 2, ...,n, or, equivalently, var( yi)¼s2. 3. cov(1i,1j)¼0 for all i=j, or, equivalently, cov( yi,yj)¼0.",
    "...,n, or, equivalently, var( yi)¼s2. 3. cov(1i,1j)¼0 for all i=j, or, equivalently, cov( yi,yj)¼0. Assumption 1 states that the model (6.1) is correct, implying that yidepends only on xi and that all other variation in yiis random. Assumption 2 asserts that the variance of 1 orydoes not depend on the values of xi.",
    "in yiis random. Assumption 2 asserts that the variance of 1 orydoes not depend on the values of xi. (Assumption 2 is also known as the assump- tion of homoscedasticity ,homogeneous variance orconstant variance .) Under assumption 3, the 1variables (or the yvariables) are uncorrelated with each other. In Section 6.3, we will add a normality assumption, and the y(or the1) variables will thereby be independent as well as uncorrelated. Each assumption has been stated in terms of the 1’s or the y’s.",
    "independent as well as uncorrelated. Each assumption has been stated in terms of the 1’s or the y’s. For example, if var( 1i)¼s2, then var(yi)¼E[yi/C0E(yi)]2¼E(yi/C0b0/C0b1xi)2¼E(12 iÞ¼s2. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 127 --- Page 141 --- Any of these assumptions may fail to hold with real data.",
    "hn Wiley & Sons, Inc. 127 --- Page 141 --- Any of these assumptions may fail to hold with real data. A plot of the data will often reveal departures from assumptions 1 and 2 (and to a lesser extent assump- tion 3). Techniques for checking on the assumptions are discussed in Chapter 9. 6.2 ESTIMATION OF b0,b1, ANDs2 Using a random sample of nobservations y1,y2,...,ynand the accompanying ﬁxed values x1,x2,...,xn, we can estimate the parameters b0,b1, ands2.",
    "y2,...,ynand the accompanying ﬁxed values x1,x2,...,xn, we can estimate the parameters b0,b1, ands2. To obtain the estimates ^b0and ^b1, we use the method of least squares, which does not require any distributional assumptions (for maximum likelihood estimators based on normal- ity, see Section 7.6.2).",
    "tributional assumptions (for maximum likelihood estimators based on normal- ity, see Section 7.6.2). In the least-squares approach, we seek estimators ^b0and ^b1that minimize the sum of squares of the deviations yi/C0^yiof the nobserved yi’s from their predicted values ^yi¼^b0þ^b1xi: ^10^1¼Xn i¼1^12 i¼Xn i¼1(yi/C0^yi)2¼Xn i¼1(yi/C0^b0/C0^b1xi)2: (6:2) Note that the predicted value ^yiestimates E(yi), not yi;t h a ti s , ^b0þ^b1xiestimates b0þb1xi, notb0þb1xiþ1i.",
    "e predicted value ^yiestimates E(yi), not yi;t h a ti s , ^b0þ^b1xiestimates b0þb1xi, notb0þb1xiþ1i. A better notation would be dE(yi), but ^yiis commonly used.",
    "0þ^b1xiestimates b0þb1xi, notb0þb1xiþ1i. A better notation would be dE(yi), but ^yiis commonly used. To ﬁnd the values of ^b0and ^b1that minimize ^10^1in (6.2), we differentiate with respect to ^b0and ^b1and set the results equal to 0: @^10^1 @^b0¼/C02Xn i¼1(yi/C0^b0/C0^b1xi)¼0, (6:3) @^10^1 @^b1¼/C02Xn i¼1(yi/C0^b0/C0^b1xi)xi¼0: (6:4) The solution to (6.3) and (6.4) is given by ^b1¼Pn i¼1xiyi/C0n/C22x/C22yPni¼1x2 i/C0n/C22x2¼Pn i¼1(xi/C0/C22x)(yi/C0/C22y)Pni¼1(xi/C0/C22x)2, (6:5) ^b0¼/C22y/C0^b1/C22x: (6:6) To verify that ^b0and ^b1in (6.5) and (6.6) minimize ^10^1in (6.2), we can examine the second derivatives or simply observe that ^10^1has no maximum and therefore the ﬁrst128 SIMPLE LINEAR REGRESSION --- Page 142 --- derivatives yield a minimum.",
    "mum and therefore the ﬁrst128 SIMPLE LINEAR REGRESSION --- Page 142 --- derivatives yield a minimum. For an algebraic proof that ^b0and ^b1minimize (6.2), see (7.10) in Section 7.3.1. Example 6.2. Students in a statistics class (taught by one of the authors) claimed that doing the homework had not helped prepare them for the midterm exam.",
    "ne of the authors) claimed that doing the homework had not helped prepare them for the midterm exam. The exam score yand homework score x(averaged up to the time of the midterm) for the 18 students in the class were as follows: yxyxyx 95 96 72 89 35 0 80 77 66 47 50 30 0 0 98 90 72 59 0 0 90 93 55 77 79 78 0 18 75 7477 64 95 86 66 67 Using (6.5) and (6.6), we obtain ^b1¼Pn i¼1xiyi/C0n/C22x/C22yPni¼1x2 i/C0n/C22x2 ¼81,195/C018(58 :056)(61 :389) 80,199/C018(58 :056)2¼:8726 , ^b0¼/C22y/C0^b1/C22x¼61:389/C0:8726(58 :056)¼10:73: The prediction equation is thus given by ^y¼10:73þ:8726 x: This equation and the 18 points are plotted in Figure 6.1.",
    "uation is thus given by ^y¼10:73þ:8726 x: This equation and the 18 points are plotted in Figure 6.1. It is readily apparent in the plot that the slope ^b1is the rate of change of ^yasxvaries and that the intercept ^b0is the value of ^yatx¼0. The apparent linear trend in Figure 6.1 does not establish cause and effect between homework and test results (for inferences that can be drawn, see Section 6.3). The assumption var( 1i)¼s2(constant variance) for all i¼1,2,...,18 appears to be reasonable.",
    "n 6.3). The assumption var( 1i)¼s2(constant variance) for all i¼1,2,...,18 appears to be reasonable. A Note that the three assumptions in Section 6.1 were not used in deriving the least- squares estimators ^b0and ^b1in (6.5) and (6.6). It is not necessary that ^yi¼^b0þ^b1xi be based on E(yi)¼b0þb1xi; that is, ^yi¼^b0þ^b1xican be ﬁt to a set of data for which E(yi)=b0þb1xi.",
    "be based on E(yi)¼b0þb1xi; that is, ^yi¼^b0þ^b1xican be ﬁt to a set of data for which E(yi)=b0þb1xi. This is illustrated in Figure 6.2, where a straight line has been ﬁtted to curved data.6.2 ESTIMATION OF b0,b1, ANDs2129 --- Page 143 --- However, if the three assumptions in Section 6.1 hold, then the least-squares esti- mators ^b0and ^b1are unbiased and have minimum variance among all linear unbiased estimators (for the minimum variance property, see Theorem 7.3d in Section 7.3.2; note that ^b0and ^b1are linear functions of y1,y2,...,yn).",
    "perty, see Theorem 7.3d in Section 7.3.2; note that ^b0and ^b1are linear functions of y1,y2,...,yn). Using the threeFigure 6.1 Regression line and data for homework and test scores.",
    "s of y1,y2,...,yn). Using the threeFigure 6.1 Regression line and data for homework and test scores. Figure 6.2 A straight line ﬁtted to data with a curved trend.130 SIMPLE LINEAR REGRESSION --- Page 144 --- assumptions, we obtain the following means and variances of ^b0and ^b1: E(^b1)¼b1 (6:7) E(^b0)¼b0 (6:8) var( ^b1)¼s2 Pn i¼1(xi/C0/C22x)2(6:9) var( ^b0)¼s21 nþ/C22x2 Pn i¼1(xi/C0/C22x)2/C20/C21 : (6:10) Note that in discussing E(^b1) and var( ^b1), for example, we are considering random variation of ^b1from sample to sample.",
    "E(^b1) and var( ^b1), for example, we are considering random variation of ^b1from sample to sample. It is assumed that the nvalues x1, x2,...,xnwould remain the same in future samples so that var( ^b1) and var( ^b0) are constant. In (6.9), we see that var( ^b1) is minimized whenPn i¼1(xi/C0/C22x)2is maximized. If thexivalues have the range a/C20xi/C20b, thenPn i¼1(xi/C0/C22x)2is maximized if half thex’s are selected equal to aand half equal to b(assuming that nis even; see Problem 6.4).",
    "if half thex’s are selected equal to aand half equal to b(assuming that nis even; see Problem 6.4). In (6.10), it is clear that var( ^b0) is minimized when /C22x¼0. The method of least squares does not yield an estimator of var( yi)¼s2; minimiz- ation of ^10^1yields only ^b0and ^b1. To estimate s2, we use the deﬁnition in (3.6), s2¼E[yi/C0E(yi)]2. By assumption 2 in Section 6.1, s2is the same for each yi,i¼1,2,...,n.",
    "in (3.6), s2¼E[yi/C0E(yi)]2. By assumption 2 in Section 6.1, s2is the same for each yi,i¼1,2,...,n. Using ^yias an estimator of E(yi), we estimate s2by an average from the sample, that is s2¼Pni¼1(yi/C0^yi)2 n/C02¼P i(yi/C0^b0/C0^b1xi)2 n/C02¼SSE n/C02, (6:11) where ^b0and ^b1are given by (6.5) and (6.6) and SSE ¼P i(yi/C0^yi)2. The deviation ^1i¼yi/C0^yiis often called the residual ofyi, and SSE is called the residual sum of squares orerror sum of squares .",
    "ten called the residual ofyi, and SSE is called the residual sum of squares orerror sum of squares . With n22 in the denominator, s2is an unbiased estimator of s2: E(s2)¼E(SSE) n/C02¼(n/C02)s2 n/C02¼s2: (6:12) Intuitively, we divide by n22 in (6.11) instead of n21a si n s2¼P i(yi/C0/C22y)2=(n/C01) in (5.6), because ^yi¼^b0þ^b1xihas two estimated para- meters and should thereby be a better estimator of E(yi) than y¯.",
    "i¼^b0þ^b1xihas two estimated para- meters and should thereby be a better estimator of E(yi) than y¯. Thus we6.2 ESTIMATION OF b0,b1, ANDs2131 --- Page 145 --- expect SSE ¼P i(yi/C0^yi)2to be less thanP i(yi/C0/C22y)2. In fact, using (6.5) and (6.6), we can write the numerator of (6.11) in the form SSE¼Xn i¼1(yi/C0^yi)2¼Xn i¼1(yi/C0/C22y)2/C0Pn i¼1(xi/C0/C22x)(yi/C0/C22y)/C2/C32 Pni¼1(xi/C0/C22x)2, (6:13) which shows thatP i(yi/C0^yi)2is indeed smaller thanP i(yi/C0/C22y)2.",
    "C32 Pni¼1(xi/C0/C22x)2, (6:13) which shows thatP i(yi/C0^yi)2is indeed smaller thanP i(yi/C0/C22y)2. 6.3 HYPOTHESIS TEST AND CONFIDENCE INTERVAL FOR b1 Typically, hypotheses about b1are of more interest than hypotheses about b0, since our ﬁrst priority is to determine whether there is a linear relationship between yandx.",
    "ut b0, since our ﬁrst priority is to determine whether there is a linear relationship between yandx. (See Problem 6.9 for a test and conﬁdence interval for b0.) In this section, we con- sider the hypothesis H0:b1¼0, which states that there is no linear relationship between yandxin the model yi¼b0þb1xiþ1i. The hypothesis H0:b1¼c(for c=0) is of less interest. In order to obtain a test for H0:b1¼0, we assume that yiisN(b0þb1xi,s2).",
    "r c=0) is of less interest. In order to obtain a test for H0:b1¼0, we assume that yiisN(b0þb1xi,s2). Then ^b1ands2have the following properties (these are special cases of results estab- lished in Theorem 7.6b in Section 7.6.3): 1.^b1isNb1,s2=P i(xi/C0/C22x)2/C2/C3 . 2. (n/C02)s2=s2isx2(n/C02). 3.^b1ands2are independent.",
    ".6.3): 1.^b1isNb1,s2=P i(xi/C0/C22x)2/C2/C3 . 2. (n/C02)s2=s2isx2(n/C02). 3.^b1ands2are independent. From these three properties it follows by (5.29) that t¼^b1 s.ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q (6:14) is distributed as t(n22,d), the noncentral twith noncentrality parameter d. By a comment following (5.29), dis given by d¼E(^b1)=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ var( ^b1)q ¼b1=[s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q ]. Ifb1¼0, then by (5.28), tis distributed as t(n22).",
    "1=[s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q ]. Ifb1¼0, then by (5.28), tis distributed as t(n22). For a two-sided alternative hypothesis H1:b1=0, we reject H0:b1¼0i f jtj/C21ta=2,n/C02,w h e r e ta=2,n/C02is the upper a/2 percentage point of the central tdistri- bution and ais the desired signiﬁcance level of the test (probability of rejecting H0 when it is true). Alternatively, we reject H0ifp/C20a, where pis the pvalue.",
    "ability of rejecting H0 when it is true). Alternatively, we reject H0ifp/C20a, where pis the pvalue. For a two- sided test, the pvalue is deﬁned as twice the probability that t(n22) exceeds the absolute value of the observed t.132 SIMPLE LINEAR REGRESSION --- Page 146 --- A 100(1 2a)% conﬁdence interval for b1is given by ^b1+ta=2,n/C02sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(xi/C0/C22x)2q : (6:15) Conﬁdence intervals are deﬁned and discussed further in Section 8.6.",
    "ﬃﬃPn i¼1(xi/C0/C22x)2q : (6:15) Conﬁdence intervals are deﬁned and discussed further in Section 8.6. A conﬁdence interval for E(y) and a prediction interval for yare also given in Section 8.6. Example 6.3. We test the hypothesis H0:b1¼0 for the grades data in Example 6.2. By (6.14), the tstatistic is t¼^b1 s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(xi/C0/C22x)2q ¼:8726 (13:8547) =(139 :753)¼8:8025 : Since t¼8.8025.t.025, 16¼2.120, we reject H0:b1¼0 at thea¼.05 level of sig- niﬁcance.",
    ":753)¼8:8025 : Since t¼8.8025.t.025, 16¼2.120, we reject H0:b1¼0 at thea¼.05 level of sig- niﬁcance. Alternatively, the pvalue is 1.571 /C21027, which is less than .05.",
    "thea¼.05 level of sig- niﬁcance. Alternatively, the pvalue is 1.571 /C21027, which is less than .05. A 95% conﬁdence interval for b1is given by (6.15) as ^b1+t:025,16sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(xi/C0/C22x)2q :8726+2:120( :09914) :8726+:2102 (:6624 ,1:0828) : 6.4 COEFFICIENT OF DETERMINATION Thecoefﬁcient of determination r2is deﬁned as r2¼SSR SST¼Pn i¼1(^yi/C0/C22y)2 Pn i¼1(yi/C0/C22y)2, (6:16) where SSR ¼P i(^yi/C0/C22y)2is the regression sum of squares and SST ¼P i(yi/C0/C22y)2 is the total sum of squares.",
    "^yi/C0/C22y)2is the regression sum of squares and SST ¼P i(yi/C0/C22y)2 is the total sum of squares. The total sum of squares can be partitioned into SST ¼ SSRþSSE, that is, Xn i¼1(yi/C0/C22y)2¼Xn i¼1(^yi/C0/C22y)2þXn i¼1(yi/C0^yi)2: (6:17)6.4 COEFFICIENT OF DETERMINATION 133 --- Page 147 --- Thus r2in (6.16) gives the proportion of variation in ythat is explained by the model or, equivalently, accounted for by regression on x.",
    "of variation in ythat is explained by the model or, equivalently, accounted for by regression on x. We have labeled (6.16) as r2because it is the same as the square of the sample correlation coefﬁcient r between yandx r¼sxyﬃﬃﬃﬃﬃﬃﬃﬃ s2 xs2yq ¼Pn i¼1(xi/C0/C22x)(yi/C0/C22y)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(xi/C0/C22x)2/C2/C3 Pni¼1(yi/C0/C22y)2/C2/C3q , (6:18) where sxyis given by 5.15 (see Problem 6.11).",
    "(xi/C0/C22x)2/C2/C3 Pni¼1(yi/C0/C22y)2/C2/C3q , (6:18) where sxyis given by 5.15 (see Problem 6.11). When xis a random variable, r estimates the population correlation in (3.19). The coefﬁcient of determination r2is discussed further in Sections 7.7, 10.4, and 10.5. Example 6.4. For the grades data of Example 6.2, we have r2¼SSR SST¼14,873:0 17,944:3¼:8288 : The correlation between homework score and exam score is r¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :8288p ¼:910.",
    "7,944:3¼:8288 : The correlation between homework score and exam score is r¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :8288p ¼:910. Thetstatistic in (6.14) can be expressed in terms of ras follows: t¼^b1 s.ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q (6:19) ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C02p rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1/C0r2p : (6:20) IfH0:b1¼0 is true, then, as noted following (6.14), the statistic in (6.19) is dis- tributed as t(n22) under the assumption that the xi’s are ﬁxed and the yi’s are inde- pendently distributed as N(b0þb1xi,s2).",
    "the assumption that the xi’s are ﬁxed and the yi’s are inde- pendently distributed as N(b0þb1xi,s2). Ifxis a random variable such that xandy have a bivariate normal distribution, then t¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C02p r=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1/C0r2p in (6.20) also has thet(n22) distribution provided that H0:r¼0 is true, where ris the population cor- relation coefﬁcient deﬁned in (3.19) (see Theorem 10.5). However, (6.19) and (6.20) have different distributions if H0:b1¼0 and H0:r¼0 are false (see Section 10.4).",
    ", (6.19) and (6.20) have different distributions if H0:b1¼0 and H0:r¼0 are false (see Section 10.4). Ifb1=0, then (6.19) has a noncentral tdistribution, but if r=0, (6.20) does not have a noncentral tdistribution. PROBLEMS 6.1 Obtain the least-squares solutions (6.5) and (6.6) from (6.3) and (6.4). 6.2 (a) Show that E(^b1)¼b1as in (6.7). (b)Show that E(^b0)¼b0as in (6.8).134 SIMPLE LINEAR REGRESSION --- Page 148 --- 6.3 (a) Show that var( ^b1)¼s2=Pn i¼1(xi/C0/C22x)2as in (6.9).",
    "LE LINEAR REGRESSION --- Page 148 --- 6.3 (a) Show that var( ^b1)¼s2=Pn i¼1(xi/C0/C22x)2as in (6.9). (b)Show that var( ^b0)¼s21=nþ/C22x2=Pn i¼1(xi/C0/C22x)2/C2/C3 as in (6.10). 6.4 Suppose that nis even and the nvalues of xican be selected anywhere in the interval from atob. Show that var( ^b1) is a minimum if n/2 values of xiare equal to aandn/2 values are equal to b. 6.5 Show that SSE ¼Pni¼1(yi/C0^yi)2in (6.11) can be expressed in the form given in (6.13). 6.6 Show that E(s2)¼s2as in (6.12).",
    "i/C0^yi)2in (6.11) can be expressed in the form given in (6.13). 6.6 Show that E(s2)¼s2as in (6.12). 6.7 Show that t¼^b1=[s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q ] in (6.14) is distributed as t(n22,d), whered¼b1=[s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q ]. 6.8 Obtain a test for H0:b1¼cversus H1:b1=c. 6.9 (a)Obtain a test for H0:b0¼aversus H1:b0=a. (b)Obtain a conﬁdence interval for b0. 6.10 Show thatPn i¼1(yi/C0/C22y)2¼Pni¼1(^yi/C0/C22y)2þPni¼1(yi/C0^yi)2as in (6.17).",
    "interval for b0. 6.10 Show thatPn i¼1(yi/C0/C22y)2¼Pni¼1(^yi/C0/C22y)2þPni¼1(yi/C0^yi)2as in (6.17). 6.11 Show that r2in (6.16) is the square of the correlation r¼Pni¼1(xi/C0/C22x)(yi/C0/C22y)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPni¼1(xi/C0/C22x)2/C2/C3 Pni¼1(yi/C0/C22y)2/C2/C3q as given by (6.18).",
    "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPni¼1(xi/C0/C22x)2/C2/C3 Pni¼1(yi/C0/C22y)2/C2/C3q as given by (6.18). TABLE 6.1 Eruptions of Old Faithful Geyser, August 1–4, 1978a y xyxyxyx 78 4.4 80 4.3 76 4.5 75 4.0 74 3.9 56 1.7 82 3.9 73 3.7 68 4.0 80 3.9 84 4.3 67 3.776 4.0 69 3.7 53 2.3 68 4.380 3.5 57 3.1 86 3.8 86 3.684 4.1 90 4.0 51 1.9 72 3.8 50 2.3 42 1.8 85 4.6 75 3.8 93 4.7 91 4.1 45 1.8 75 3.855 1.7 51 1.8 88 4.7 66 2.576 4.9 79 3.2 51 1.8 84 4.558 1.7 53 1.9 80 4.6 70 4.174 4.6 82 4.6 49 1.9 79 3.775 3.4 51 2.0 82 3.5 60 3.8 — —————8 6 3 .",
    "4.558 1.7 53 1.9 80 4.6 70 4.174 4.6 82 4.6 49 1.9 79 3.775 3.4 51 2.0 82 3.5 60 3.8 — —————8 6 3 . 4 aWhere x¼duration, y¼interval (both in minutes).PROBLEMS 135 --- Page 149 --- 6.12 Show that r¼cosu, whereuis the angle between the vectors x/C0/C22xjand y/C0/C22yj,w h e r e x/C0/C22xj¼(x1/C0/C22x,x2/C0/C22x,...,xn/C0/C22x)0and y/C0/C22yj¼(y1/C0/C22y, y2/C0/C22y,...,yn/C0/C22y)0.",
    "22xj¼(x1/C0/C22x,x2/C0/C22x,...,xn/C0/C22x)0and y/C0/C22yj¼(y1/C0/C22y, y2/C0/C22y,...,yn/C0/C22y)0. 6.13 Show that t¼^b1=[s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(xi/C0/C22x)2q ] in (6.19) is equal to t¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C02p r=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1/C0r2p in (6.20). 6.14 Table 6.1 (Weisberg 1985, p. 231) gives the data on daytime eruptions of Old Faithful Geyser in Yellowstone National Park during August 1–4, 1978. The variables are x¼duration of an eruption and y¼interval to the next eruption.",
    "g August 1–4, 1978. The variables are x¼duration of an eruption and y¼interval to the next eruption. Can xbe used to successfully predict yusing a simple linear model yi¼b0þb1xiþ1i? (a)Find ^b0and ^b1. (b)Test H0:b1¼0 using (6.14). (c)Find a conﬁdence interval for b1.",
    "i¼b0þb1xiþ1i? (a)Find ^b0and ^b1. (b)Test H0:b1¼0 using (6.14). (c)Find a conﬁdence interval for b1. (d)Find r2using (6.16).136 SIMPLE LINEAR REGRESSION --- Page 150 --- 7Multiple Regression: Estimation 7.1 INTRODUCTION Inmultiple regression , we attempt to predict a dependent orresponse variable yon the basis of an assumed linear relationship with several independent orpredictor vari- ables x1,x1,...,xk.",
    "sis of an assumed linear relationship with several independent orpredictor vari- ables x1,x1,...,xk. In addition to constructing a model for prediction, we may wish to assess the extent of the relationship between yand the xvariables. For this purpose, we use the multiple correlation coefﬁcient R(Section 7.7). In this chapter, yis a continuous random variable and the xvariables are ﬁxed con- stants (either discrete or continuous) that are controlled by the experimenter.",
    "iables are ﬁxed con- stants (either discrete or continuous) that are controlled by the experimenter. The case in which the xvariables are random variables is covered in Chapter 10. In analysis-of- variance (Chapters 12–15), the xvariables are ﬁxed and discrete.",
    "ered in Chapter 10. In analysis-of- variance (Chapters 12–15), the xvariables are ﬁxed and discrete. Useful applied expositions of multiple regression for the ﬁxed- xcase can be found in Morrison (1983), Myers (1990), Montgomery and Peck (1992), Graybill and Iyer (1994), Mendenhall and Sincich (1996), Ryan (1997), Draper and Smith (1998), and Kutner et al. (2005).",
    "994), Mendenhall and Sincich (1996), Ryan (1997), Draper and Smith (1998), and Kutner et al. (2005). Theoretical treatments are given by Searle (1971), Graybill (1976), Guttman (1982), Kshirsagar (1983), Myers and Milton (1991), Jørgensen(1993), Wang and Chow (1994), Christensen (1996), Seber and Lee (2003), and Hocking (1976, 1985, 2003).",
    "93), Wang and Chow (1994), Christensen (1996), Seber and Lee (2003), and Hocking (1976, 1985, 2003). 7.2 THE MODEL The multiple linear regression model, as introduced in Section 1.2, can be expressed as y¼ b0þb1x1þb2x2þ/C1/C1/C1þbkxkþ1: (7:1) We discuss estimation of the bparameters when the model is linear in the b’s. An example of a model that is linear in the b’s but not the x’s is the second-order Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G.",
    "the x’s is the second-order Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 137 --- Page 151 --- response surface model y¼b0þb1x1þb2x2þb3x2 1þb4x22þb5x1x2þ1: (7:2) To estimate the b’s in (7.1), we will use a sample of nobservations on yand the associated xvariables.",
    "estimate the b’s in (7.1), we will use a sample of nobservations on yand the associated xvariables. The model for the ith observation is yi¼b0þb1xi1þb2xi2þ/C1/C1/C1þbkxikþ1i,i¼1,2,...,n: (7:3) The assumptions for 1ioryiare essentially the same as those for simple linear regression in Section 6.1: 1.E(1i)¼0f o r i¼1, 2, ...,n, or, equivalently, E(yi)¼b0þb1xi1þb2xi2þ /C1/C1/C1þbkxik. 2. var(1i)¼s2fori¼1, 2, ...,n, or, equivalently, var( yi)¼s2. 3.",
    "i)¼b0þb1xi1þb2xi2þ /C1/C1/C1þbkxik. 2. var(1i)¼s2fori¼1, 2, ...,n, or, equivalently, var( yi)¼s2. 3. cov(1i,1j)¼0 for all i=j, or, equivalently, cov( yi,yj)¼0. Assumption 1 states that the model is correct, in other words that all relevant x’s are included and the model is indeed linear. Assumption 2 asserts that the variance of y is constant and therefore does not depend on the x’s.",
    "r. Assumption 2 asserts that the variance of y is constant and therefore does not depend on the x’s. Assumption 3 states that the y’s are uncorrelated with each other, which usually holds in a random sample (the observations would typically be correlated in a time series or when repeated measurements are made on a single plant or animal). Later we will add a normality assumption (Section 7.6), under which the yvariable will be independent as well as uncorrelated.",
    "ity assumption (Section 7.6), under which the yvariable will be independent as well as uncorrelated. When all three assumptions hold, the least-squares estimators of the b’s have some good properties (Section 7.3.2). If one or more assumptions do not hold, the estima- tors may be poor. Under the normality assumption (Section 7.6), the maximum like-lihood estimators have excellent properties. Any of the three assumptions may fail to hold with real data.",
    "estimators have excellent properties. Any of the three assumptions may fail to hold with real data. Several procedures have been devised for checking the assumptions. These diagnostic techniques are discussed in Chapter 9. Writing (7.3) for each of the nobservations, we have y 1¼b0þb1x11þb2x12þ/C1/C1/C1þbkx1kþ11 y2¼b0þb1x21þb2x22þ/C1/C1/C1þbkx2kþ12 ...",
    "observations, we have y 1¼b0þb1x11þb2x12þ/C1/C1/C1þbkx1kþ11 y2¼b0þb1x21þb2x22þ/C1/C1/C1þbkx2kþ12 ... yn¼b0þb1xn1þb2xn2þ/C1/C1/C1þbkxnkþ1n:138 MULTIPLE REGRESSION: ESTIMATION --- Page 152 --- These nequations can be written in matrix form as y1 y2 ... yn0 BBB@1 CCCA¼1x11x12 ... x1k 1x21x22 ... x2k ............ 1xn1xn2... xnk0 BBB@1 CCCAb0 b1 ... bk0 BBB@1 CCCAþ11 12 ...",
    "12 ... x1k 1x21x22 ... x2k ............ 1xn1xn2... xnk0 BBB@1 CCCAb0 b1 ... bk0 BBB@1 CCCAþ11 12 ... 1n0 BBB@1 CCCA or y¼Xbþ1: (7:4) The preceding three assumptions on 1ioryican be expressed in terms of the model in (7.4): 1.E(1)¼0orE(y)¼Xb. 2. cov(1)¼s2Ior cov( y)¼s2I. Note that the assumption cov( 1)¼s2Iincludes both the previous assumptions var(1i)¼s2and cov(1i,1j)¼0. The matrix Xin (7.4) is n/C2(kþ1). In this chapter we assume that n.kþ1 and rank ( X)¼kþ1.",
    "1i,1j)¼0. The matrix Xin (7.4) is n/C2(kþ1). In this chapter we assume that n.kþ1 and rank ( X)¼kþ1. If n,kþ1 or if there is a linear relationship among the x’s, for example, x5¼P4 j¼1xj=4, then Xwill not have full column rank. If the values of the xij’s are planned (chosen by the researcher), then the Xmatrix essentially contains the experimental design and is sometimes called the design matrix . Thebparameters in (7.1) or (7.4) are called regression coefﬁcients .",
    "imes called the design matrix . Thebparameters in (7.1) or (7.4) are called regression coefﬁcients . To emphasize their collective effect, they are sometimes referred to as partial regression coefﬁ- cients . The word partial carries both a mathematical and a statistical meaning. Mathematically, the partial derivative of E(y)¼b0þb1x1þb2x2þ/C1/C1/C1þbkxk with respect to x1, for example, is b1. Thusb1indicates the change in E(y) with a unit increase in x1when x2,x3,...,xkare held constant.",
    "b1. Thusb1indicates the change in E(y) with a unit increase in x1when x2,x3,...,xkare held constant. Statistically, b1shows the effect of x1onE(y) in the presence of the other x’s. This effect would typically be different from the effect of x1onE(y) if the other x’s were not present in the model.",
    "d typically be different from the effect of x1onE(y) if the other x’s were not present in the model. Thus, for example, b0andb1in y¼b0þb1x1þb2x2þ1 will usually be different from b0/C3andb1/C3in y¼b/C3 0þb/C31x1þ1/C3: [Ifx1andx2are orthogonal, that is, if x01x2¼0o ri f( x1/C0/C22x1j)0(x2/C0/C22x2j)¼0, where x1andx2are columns in the Xmatrix, then b0¼b/C3 0andb1¼b/C31; see Corollary 1 to Theorem 7.9a and Theorem 7.10].",
    "olumns in the Xmatrix, then b0¼b/C3 0andb1¼b/C31; see Corollary 1 to Theorem 7.9a and Theorem 7.10]. The change in parameters when an xis deleted from the model is illustrated (with estimates) in the following example.7.2 THE MODEL 139 --- Page 153 --- Example 7.2. [See Freund and Minton (1979, pp. 36–39)]. Consider the (contrived) data in Table 7.1.",
    "Example 7.2. [See Freund and Minton (1979, pp. 36–39)]. Consider the (contrived) data in Table 7.1. Using (6.5) and (6.6) from Section 6.2 and (7.6) in Section 7.3 (see Example 7.3.1), we obtain prediction equations for yregressed on x1alone, on x2alone, and on both x1andx2: ^y¼1:86þ1:30x1, ^y¼:86þ:78x2, ^y¼5:37þ3:01x1/C01:29x2:TABLE 7.1 Data for Example 7.2 Observation Number yx 1 x2 12 0 2 23 2 6 32 2 747 2 5 56 4 968 4 871 0 4 787 6 1 098 6 1 110 12 6 911 11 8 1512 14 8 13 Figure 7.1 Regression of yonx2ignoring x1.140 MULTIPLE REGRESSION: ESTIMATION --- Page 154 --- As expected, the coefﬁcients change from either of the reduced models to the full model.",
    "age 154 --- As expected, the coefﬁcients change from either of the reduced models to the full model. Note the sign change as the coefﬁcient of x2changes from .78 to 21.29. The values of yandx2are plotted in Figure 7.1 along with the prediction equation ^y¼:86þ:78x2. The linear trend is clearly evident. In Figure 7.2 we have the same plot as in Figure 7.1, except that each point is labeled with the value of x1.",
    "7.2 we have the same plot as in Figure 7.1, except that each point is labeled with the value of x1. Examining values of yandx2for a ﬁxed value of x1(2, 4, 6, or 8) shows a negative slope for the relationship. These negative relation- ships are shown as partial regressions of yonx2for each value of x1. The partial regression coefﬁcient ^b2¼/C01:29 reﬂects the negative slopes of these four partial regressions.",
    "ial regression coefﬁcient ^b2¼/C01:29 reﬂects the negative slopes of these four partial regressions. Further insight into the meaning of the partial regression coefﬁcients is given in Section 7.10. A 7.3 ESTIMATION OF bANDs2 7.3.1 Least-Squares Estimator for b In this section, we discuss the least-squares approach to estimation of the b’s in the ﬁxed- xmodel (7.1) or (7.4). No distributional assumptions on yare required to obtain the estimators.",
    "xed- xmodel (7.1) or (7.4). No distributional assumptions on yare required to obtain the estimators. For the parameters b0,b1,...,bk, we seek estimators that minimize the sum of squares of deviations of the nobserved y’s from their predicted values ^y.",
    "that minimize the sum of squares of deviations of the nobserved y’s from their predicted values ^y. By extensionFigure 7.2 Regression of yonx2showing the value of x1at each point and partial regressions ofyonx2.7.3 ESTIMATION OF bANDs2141 --- Page 155 --- of (6.2), we seek ^b0,^b1,...,^bkthat minimize Xn i¼1^12 i¼Xn i¼1(yi/C0^yi)2 ¼Xn i¼1(yi/C0^b0/C0^b1xi1/C0^b2xi2/C0/C1/C1/C1/C0 ^bkxik)2: (7:5) Note that the predicted value ^yi¼^b0þ^b1xi1þ/C1/C1/C1þ ^bkxikestimates E(yi), not yi.A better notation would be dE(yi), but ^yiis commonly used.",
    "/C1/C1/C1þ ^bkxikestimates E(yi), not yi.A better notation would be dE(yi), but ^yiis commonly used. To obtain the least-squares estimators, it is not necessary that the prediction equation ^yi¼^b0þ^b1xi1þ/C1/C1/C1þ ^bkxikbe based on E(yi). It is only necessary to pos- tulate an empirical model that is linear in the ^b’s, and the least-squares method will ﬁnd the “best” ﬁt to this model. This was illustrated in Figure 6.2.",
    "d the least-squares method will ﬁnd the “best” ﬁt to this model. This was illustrated in Figure 6.2. To ﬁnd the values of ^b0,^b1,...,^bkthat minimize (7.5), we could differentiateP i^12 i with respect to each ^bjand set the results equal to zero to yield kþ1 equations that can be solved simultaneously for the ^bj’s. However, the procedure can be carried out in more compact form with matrix notation. The result is given in the following theorem. Theorem 7.3a.",
    "more compact form with matrix notation. The result is given in the following theorem. Theorem 7.3a. Ify¼Xbþ1, where Xisn/C2(kþ1) of rank kþ1,n, then the value of ^b¼(^b0,^b1,...,^bk)0that minimizes (7.5) is ^b¼(X0X)/C01X0y: (7:6) PROOF. Using (2.20) and (2.27), we can write (7.5) as ^10^1¼Xn i¼1(yi/C0x0 i^b)2¼(y/C0X^b)0(y/C0X^b), (7:7) where x0 i¼(1,xi1,...,xik) is the ith row of X.",
    "10^1¼Xn i¼1(yi/C0x0 i^b)2¼(y/C0X^b)0(y/C0X^b), (7:7) where x0 i¼(1,xi1,...,xik) is the ith row of X. When the product ( y/C0X^b)0(y/C0X^b) in (7.7) is expanded as in (2.17), two of the resulting four terms can be combined to yield ^10^1¼y0y/C02y0X^bþ^b0X0X^b: We can ﬁnd the value of ^bthat minimizes ^10^1by differentiating ^10^1with respect to ^b [using (2.112) and (2.113)] and setting the result equal to zero: @^10^1 @^b¼0/C02X0yþ2X0X^b¼0, This gives the normal equations X0X^b¼X0y: (7:8)142 MULTIPLE REGRESSION: ESTIMATION --- Page 156 --- By Theorems 2.4(iii) and 2.6d(i) and Corollary 1 of Theorem 2.6c, if Xis full-rank, X0Xis nonsingular, and the solution to (7.8) is given by (7.6).",
    "1 of Theorem 2.6c, if Xis full-rank, X0Xis nonsingular, and the solution to (7.8) is given by (7.6). A Since ^bin (7.6) minimizes the sum of squares in (7.5), ^bis called the least- squares estimator . Note that each ^bjin^bis a linear function of y; that is, ^bj¼a0 jy,w h e r e a0jis the jth row of ( X0X)/C01X0. This usage of the word linear in linear estimator is different from that in linear model , which indicates that the model is linear in the b’s.",
    "imator is different from that in linear model , which indicates that the model is linear in the b’s. We now show that ^b¼(X0X)/C01X0yminimizes ^10^1.",
    "hich indicates that the model is linear in the b’s. We now show that ^b¼(X0X)/C01X0yminimizes ^10^1. Let bbe an alternative estima- tor that may do better than ^bso that ^10^1is ^10^1¼(y/C0Xb)0(y/C0Xb): Now adding and subtracting X^b, we obtain ¼(y/C0X^bþX^b/C0Xb)0(y/C0X^bþX^b/C0Xb)( 7 :9) ¼(y/C0X^b)0(y/C0X^b)þ(^b/C0b)0X0X(^b/C0b) þ2(^b/C0b)0(X0y/C0X0X^b): (7:10) The third term on the right side of (7.10) vanishes because of the normal equations X0y¼X0X^bin (7.8).",
    "e third term on the right side of (7.10) vanishes because of the normal equations X0y¼X0X^bin (7.8). The second term is a positive deﬁnite quadratic form (assuming thatXis full-rank; see Theorem 2.6d), and ^10^1is therefore minimized when b¼^b.",
    "atic form (assuming thatXis full-rank; see Theorem 2.6d), and ^10^1is therefore minimized when b¼^b. To examine the structure of X0Xand X0y, note that by Theorem 2.2c(i), the (kþ1)/C2(kþ1) matrix X0Xcan be obtained as products of columns of X; similarly, X0ycontains products of columns of Xandy: X0X¼nP ixi1P ixi2 ...P ixikP ixi1P ix2 i1P ixi1xi2...P ixi1xik ............ PxikP ixi1xikP ixi2xik...P ix2 ik0 BBBB@1 CCCCA, X0y¼P iyiP ixi1yi ...",
    "1xi2...P ixi1xik ............ PxikP ixi1xikP ixi2xik...P ix2 ik0 BBBB@1 CCCCA, X0y¼P iyiP ixi1yi ... P ixikyi0 BBBB@1 CCCCA: If^b¼(X0X)/C01X0yas in (7.6), then ^1¼y/C0X^b¼y/C0^y (7:11)7.3 ESTIMATION OF bANDs2143 --- Page 157 --- is the vector of residuals ,^11¼y1/C0^y1,^12¼y2/C0^y2,...,^1n¼yn/C0^yn. The residual vector ^1estimates 1in the model y¼Xbþ1and can be used to check the validity of the model and attendant assumptions; see Chapter 9. Example 7.3.1a.",
    "be used to check the validity of the model and attendant assumptions; see Chapter 9. Example 7.3.1a. We use the data in Table 7.1 to illustrate computation of ^busing (7.6).",
    "Chapter 9. Example 7.3.1a. We use the data in Table 7.1 to illustrate computation of ^busing (7.6). y¼2 3 2 7 6 8 10 78 12 11 140 BBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCA,X¼10 2 12 6 12 7 12 5 14 9 14 8 14 7161 0 161 1 16 9 181 5 181 30 BBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCC CCCCCCCCCCCCCCA,X 0X¼12 52 102 52 395 536 102 536 10040 B@1 CA, X0y¼90 482 8720 B@1 CA,(X0X)/C01¼:97476 :24290 /C0:22871 :24290 :16207 /C0:11120 /C0:22871 /C0:11120 :083600 B@1 CA, ^b¼(X0X)/C01X0y¼5:3754 3:0118 /C01:28550 B@1 CA: A Example 7.3.1b.",
    "0:22871 /C0:11120 :083600 B@1 CA, ^b¼(X0X)/C01X0y¼5:3754 3:0118 /C01:28550 B@1 CA: A Example 7.3.1b. Simple linear regression from Chapter 6 can also be expressed in matrix terms: y¼y1 y2 ... yn0 BBBB@1 CCCCA,X¼1x1 1x2 ...... 1xn0 BBBB@1 CCCCA,b¼b0 b1/C18/C19 , X0X¼nP ixiP ixiP ix2 i/C18/C19 ,X0y¼P iyiP ixiyi/C18/C19 , (X0X)/C01¼1 nP ix2 i/C0(P ixi)2P ix2 i/C0P ixi /C0P ixi n !",
    "x2 i/C18/C19 ,X0y¼P iyiP ixiyi/C18/C19 , (X0X)/C01¼1 nP ix2 i/C0(P ixi)2P ix2 i/C0P ixi /C0P ixi n ! :144 MULTIPLE REGRESSION: ESTIMATION --- Page 158 --- Then ^b0and ^b1can be obtained using (7.6), ^b¼(X0X)/C01X0y: ^b¼^b0^b1/C18/C19 ¼1 nP ix2 i/C0(P ixi)2/C0P ix2 i/C1/C0Pyi/C1 /C0/C0P ixi/C1/C0P ixiyi/C1 /C0/C0P ixi/C1/C0Pyi/C1 þnP ixiyi0 @1A:(7:12) The estimators ^ b0and ^b1in (7.11) are the same as those in (6.5) and (6.6).",
    "þnP ixiyi0 @1A:(7:12) The estimators ^ b0and ^b1in (7.11) are the same as those in (6.5) and (6.6). A 7.3.2 Properties of the Least-Squares Estimator ^b The least-squares estimator ^b¼(X0X)/C01X0yin Theorem 7.3a was obtained without using the assumptions E(y)¼Xband cov( y)¼s2Igiven in Section 7.2. We merely postulated a model y¼Xbþ1as in (7.4) and ﬁtted it. If E(y)=Xb, the model y¼Xbþ1could still be ﬁtted to the data, in which case, ^bmay have poor proper- ties.",
    ")=Xb, the model y¼Xbþ1could still be ﬁtted to the data, in which case, ^bmay have poor proper- ties. If cov( y)=s2I, there may be additional adverse effects on the estimator ^b. However, if E(y)¼Xband cov( y)¼s2Ihold, ^bhas some good properties, as noted in the four theorems in this section. Note that ^bis a random vector (from sample to sample).",
    "noted in the four theorems in this section. Note that ^bis a random vector (from sample to sample). We discuss its mean vector and covariance matrix in this section (with no distributional assumptions on y) and its distribution (assuming that the yvariables are normal) in Section 7.6.3. In the following theorems, we assume that Xis ﬁxed (remains constant in repeated sampling) and full rank. Theorem 7.3b. IfE(y)¼Xb, then ^bis an unbiased estimator for b.",
    "in repeated sampling) and full rank. Theorem 7.3b. IfE(y)¼Xb, then ^bis an unbiased estimator for b. PROOF E(^b)¼E[(X0X)/C01X0y] ¼(X0X)/C01X0E(y) [by (3 :38)] ¼(X0X)/C01X0Xb ¼b: (7:13) A Theorem 7.3c. If cov( y)¼s2I, the covariance matrix for ^bis given by s2(X0X)/C01. PROOF cov( ^b)¼cov[(X0X)/C01X0y] ¼(X0X)/C01X0cov(y)[(X0X)/C01X0]0[by (3 :44)] ¼(X0X)/C01X0(s2I)X(X0X)/C01 ¼s2(X0X)/C01X0X(X0X)/C01 ¼s2(X0X)/C01: (7:14) A7.3 ESTIMATION OF bANDs2145 --- Page 159 --- Example 7.3.2a.",
    ")/C01X0X(X0X)/C01 ¼s2(X0X)/C01: (7:14) A7.3 ESTIMATION OF bANDs2145 --- Page 159 --- Example 7.3.2a. Using the matrix ( X0X)/C01for simple linear regression given in Example 7.3.1, we obtain cov( ^b)¼cov^b0 ^b1 ! ¼var( ^b0) cov( ^b0,^b1) cov( ^b0,^b1) var( ^b1) ! ¼s2(X0X)/C01 ¼s2 nP ix2 i/C0(P ixi)2P ix2 i/C0P ixi /C0P ixi n !",
    "^b0,^b1) cov( ^b0,^b1) var( ^b1) ! ¼s2(X0X)/C01 ¼s2 nP ix2 i/C0(P ixi)2P ix2 i/C0P ixi /C0P ixi n ! (7:15) ¼s2 P i(xi/C0/C22x)2P ix2 i=n/C0/C22x /C0/C22x 1/C18/C19 : (7:16) Thus var( ^b0)¼s2P ix2i=nP i(xi/C0/C22x)2,var( ^b1)¼s2 P i(xi/C0/C22x)2, cov( ^b0,^b1)¼/C0s2/C22xP i(xi/C0/C22x)2: We found var( ^b0) and var( ^b1) in Section 6.2 but did not obtain cov( ^b0,^b1). Note that if /C22x.0, then cov( ^b0,^b1) is negative and the estimated slope and intercept are negatively correlated.",
    "x.0, then cov( ^b0,^b1) is negative and the estimated slope and intercept are negatively correlated. In this case, if the estimate of the slope increases from one sample to another, the estimate of the intercept tends to decrease (assuming the x’s stay the same). A Example 7.3.2b. For the data in Table 7.1, ( X0X)21is as given in Example 7.3.1.",
    "s stay the same). A Example 7.3.2b. For the data in Table 7.1, ( X0X)21is as given in Example 7.3.1. Thus, cov( ^b) is given by cov( ^b)¼s2(X0X)/C01¼s2:975 :243 /C0:229 :243 :162 /C0:111 /C0:229 /C0:111 :0840 B@1 CA: The negative value of cov( ^b1,^b2)¼/C0 :111 indicates that in repeated sampling (using the same 12 values of x1andx2),^b1and ^b2would tend to move in opposite directions; that is, an increase in one would be accompanied by a decrease in the other.",
    "in opposite directions; that is, an increase in one would be accompanied by a decrease in the other. A In addition to E(^b)¼band cov( ^b)¼s2(X0X)/C01, a third important property of ^b is that under the standard assumptions, the variance of each ^bjis minimum (see the following theorem). Theorem 7.3d (Gauss–Markov Theorem).",
    "the variance of each ^bjis minimum (see the following theorem). Theorem 7.3d (Gauss–Markov Theorem). IfE(y)¼Xband cov( y)¼s2I, the least-squares estimators ^bj,j¼0,1,...,k, have minimum variance among all linear unbiased estimators.146 MULTIPLE REGRESSION: ESTIMATION --- Page 160 --- PROOF. We consider a linear estimator Ayofband seek the matrix Afor which Ayis a minimum variance unbiased estimator of b. In order for Ayto be an unbiased esti- mator ofb, we must have E(Ay)¼b.",
    "nce unbiased estimator of b. In order for Ayto be an unbiased esti- mator ofb, we must have E(Ay)¼b. Using the assumption E(y)¼Xb, this can be expressed as E(Ay)¼AE(y)¼AXb¼b, which gives the unbiasedness condition AX¼I since the relationship AXb¼bmust hold for any possible value of b[see (2.44)].",
    "edness condition AX¼I since the relationship AXb¼bmust hold for any possible value of b[see (2.44)]. The covariance matrix for the estimator Ayis given by cov(Ay)¼A(s2I)A0¼s2AA0: The variances of the ^bj’s are on the diagonal of s2AA0, and we therefore need to choose A(subject to AX¼I) so that the diagonal elements of AA0are minimized.",
    "nd we therefore need to choose A(subject to AX¼I) so that the diagonal elements of AA0are minimized. To relate Ayto^b¼(X0X)/C01X0y, we add and subtract ( X0X)/C01X0to obtain AA0¼[A/C0(X0X)/C01X0þ(X0X)/C01X0][A/C0(X0X)/C01X0þ(X0X)/C01X0]0: Expanding this in terms of A2(X0X)21X0and ( X0X)21X0, we obtain four terms, two of which vanish because of the restriction AX¼I.",
    "2(X0X)21X0and ( X0X)21X0, we obtain four terms, two of which vanish because of the restriction AX¼I. The result is AA0¼[A/C0(X0X)/C01X0][A/C0(X0X)/C01X0]0þ(X0X)/C01: (7:17) The matrix [ A/C0(X0X)/C01X0][A/C0(X0X)/C01X0]0on the right side of (7.17) is positive semideﬁnite (see Theorem 2.6d), and, by Theorem 2.6a (ii), the diagonal elements are greater than or equal to zero. These diagonal elements can be made equal to zero by choosing A¼(X0X)/C01X0.",
    "than or equal to zero. These diagonal elements can be made equal to zero by choosing A¼(X0X)/C01X0. (This value of Aalso satisﬁes the unbiasedness condition AX¼I.) The resulting minimum variance estimator of bis Ay¼(X0X)/C01X0y, which is equal to the least–squares estimator ^b. A The Gauss–Markov theorem is sometimes stated as follows. If E(y)¼Xband cov(y)¼s2I, the least-squares estimators ^b0,^b1,...,^bkarebest linear unbiased estimators (BLUE).",
    "d cov(y)¼s2I, the least-squares estimators ^b0,^b1,...,^bkarebest linear unbiased estimators (BLUE). In this expression, bestmeans minimum variance and linear indi- cates that the estimators are linear functions of y. The remarkable feature of the Gauss–Markov theorem is its distributional general- ity. The result holds for any distribution of y; normality is not required. The only assumptions used in the proof are E(y)¼Xband cov( y)¼s2I.",
    "of y; normality is not required. The only assumptions used in the proof are E(y)¼Xband cov( y)¼s2I. If these assumptions do not hold, ^bmay be biased or each ^bjmay have a larger variance than that of some other estimator.7.3 ESTIMATION OF bANDs2147 --- Page 161 --- The Gauss–Markov theorem is easily extended to a linear combination of the ^b’s, as follows. Corollary 1.",
    "auss–Markov theorem is easily extended to a linear combination of the ^b’s, as follows. Corollary 1. IfE(y)¼Xband cov( y)¼s2I, the best linear unbiased estimator of a0bisa0^b, where ^bis the least–squares estimator ^b¼(X0X)/C01X0y. PROOF. See Problem 7.7. A Note that Theorem 7.3d is concerned with the form of the estimator ^bfor a given X matrix. Once Xis chosen, the variances of the ^bj’s are minimized by ^b¼(X0X)/C01X0y.",
    "bfor a given X matrix. Once Xis chosen, the variances of the ^bj’s are minimized by ^b¼(X0X)/C01X0y. However, in Theorem 7.3c, we have cov( ^b)¼s2(X0X)/C01and therefore var( ^bj) and cov( ^bi,^bj) depend on the values of the xj’s. Thus the conﬁguration of X0Xis important in estimation of the bj’s (this was illustrated in Problem 6.4). In both estimation and testing, there are advantages to choosing the x’s (or the centered x’s) to be orthogonal so that X0Xis diagonal.",
    "re are advantages to choosing the x’s (or the centered x’s) to be orthogonal so that X0Xis diagonal. These advantages include mini- mizing the variances of the ^bj’s and maximizing the power of tests about the bj’s (Chapter 8). For clariﬁcation, we note that orthogonality is necessary but not sufﬁcient for minimizing variances and maximizing power.",
    "ote that orthogonality is necessary but not sufﬁcient for minimizing variances and maximizing power. For example, if there are two x’s, with values to be selected in a rectangular space, the points could be evenly placed on a grid, which would be an orthogonal pattern. However, the optimal orthogonal pattern would be to place one-fourth of the points at each corner of the rectangle. A fourth property of ^bis as follows.",
    "lace one-fourth of the points at each corner of the rectangle. A fourth property of ^bis as follows. The predicted value ^y¼^b0þ ^b1x1þ/C1/C1/C1þ ^bkxk¼^b0xis invariant to simple linear changes of scale on the x’s, where x¼(1,x1,x2,...,xk)0. Let the rescaled variables be denoted by zj¼cjxj, j¼1,2,...,k, where the cjterms are constants. Thus xis transformed to z¼(1,c1x1,...,ckxk)0. The following theorem shows that ^ybased on zis the same as^ybased on x. Theorem 7.3e.",
    "1,...,ckxk)0. The following theorem shows that ^ybased on zis the same as^ybased on x. Theorem 7.3e. Ifx¼(1,x1,...,xk)0andz¼(1,c1x1,...,ckxk)0, then ^y¼ ^b0x¼^b0 zz, where ^bzis the least squares estimator from the regression of yonz. PROOF. From (2.29), we can rewrite zasz¼Dx, where D¼diag(1 ,c1,c2,...,ck). Then, the Xmatrix is transformed to Z¼XD[see (2.28)].",
    "ewrite zasz¼Dx, where D¼diag(1 ,c1,c2,...,ck). Then, the Xmatrix is transformed to Z¼XD[see (2.28)]. We substitute Z¼XDin the least-squares estimator ^bz¼(Z0Z)/C01Z0yto obtain ^bz¼(Z0Z)/C01Z0y¼[(XD)0(XD)]/C01(XD)0y ¼D/C01(X0X)/C01X0y [by (2 :49)] ¼D/C01^b, (7:18) where ^bis the usual estimator for yregressed on the x’s.",
    "(X0X)/C01X0y [by (2 :49)] ¼D/C01^b, (7:18) where ^bis the usual estimator for yregressed on the x’s. Then ^b0 zz¼(D/C01^b)0Dx¼^b0x: A148 MULTIPLE REGRESSION: ESTIMATION --- Page 162 --- In the following corollary to Theorem 7.3e, the invariance of ^yis extended to any full-rank linear transformation of the xvariables. Corollary 1. The predicted value ^yis invariant to a full-rank linear transformation on thex’s. PROOF.",
    "rollary 1. The predicted value ^yis invariant to a full-rank linear transformation on thex’s. PROOF. We can express a full-rank linear transformation of the x’s as Z¼XK¼(j,X1)100 0K 1/C18/C19 ¼(jþX10,j00þX1K1)¼(j,X1K1), where K1is nonsingular and X1¼x11x12 ... x1k x21x22 ... x2k ......... xn1xn2... xnk0 BBB@1 CCCA: (7:19) We partition XandKin this way so as to transform only the x’s in X1, leaving the ﬁrst column of Xunaffected.",
    "tion XandKin this way so as to transform only the x’s in X1, leaving the ﬁrst column of Xunaffected. Now ^bzbecomes ^bz¼(Z0Z)/C01Z0y¼K/C01^b, (7:20) and we have ^y¼^b0 zz¼^b0x, (7:21) where z¼K0x. A In addition to ^y, the sample variance s2(Section 7.3.3) is also invariant to changes of scale on the xvariable (see Problem 7.10).",
    "ariance s2(Section 7.3.3) is also invariant to changes of scale on the xvariable (see Problem 7.10). The following are invariant to changes of scale on yas well as on the x’s (but not to a joint linear transformation on yand the x’s):tstatistics (Section 8.5), Fstatistics (Chapter 8), and R2(Sections 7.7 and 10.3). 7.3.3 An Estimator for s2 The method of least squares does not yield a function of the yandxvalues in the sample that we can minimize to obtain an estimator of s2.",
    "yield a function of the yandxvalues in the sample that we can minimize to obtain an estimator of s2. However, we can devise an unbiased estimator for s2based on the least-squares estimator ^b. By assumption 2 following (7.3), s2is the same for each yi,i¼1,2,...,n. By (3.6), s2is deﬁned by s2¼E[yi/C0E(yi)]2, and by assumption 1, we obtain E(yi)¼b0þbixi1þb2xi2þ/C1/C1/C1þbkxik¼x0 ib, where xi0is the ith row of X.",
    "by assumption 1, we obtain E(yi)¼b0þbixi1þb2xi2þ/C1/C1/C1þbkxik¼x0 ib, where xi0is the ith row of X. Thuss2becomes s2¼E[yi/C0x0ib]2:7.3 ESTIMATION OF bANDs2149 --- Page 163 --- We estimate s2by a corresponding average from the sample s2¼1 n/C0k/C01Xn i¼1(yi/C0x0 i^b)2, (7:22) where nis the sample size and kis the number of x’s. Note that, by the corollary to Theorem 7.3d, x0 i^bis the BLUE of x0ib.",
    "e and kis the number of x’s. Note that, by the corollary to Theorem 7.3d, x0 i^bis the BLUE of x0ib. Using (7.7), we can write (7.22) as s2¼1 n/C0k/C01(y/C0X^b)0(y/C0X^b)( 7 :23) ¼y0y/C0^b0X0y n/C0k/C01¼SSE n/C0k/C01, (7:24) where SSE ¼(y/C0X^b)0(y/C0X^b)¼y0y/C0^b0X0y. With the denominator n/C0k/C01,s2is an unbiased estimator of s2, as shown below. Theorem 7.3f. Ifs2is deﬁned by (7.22), (7.23), or (7.24) and if E(y)¼Xband cov(y)¼s2I, then E(s2)¼s2: (7:25) PROOF.",
    "fs2is deﬁned by (7.22), (7.23), or (7.24) and if E(y)¼Xband cov(y)¼s2I, then E(s2)¼s2: (7:25) PROOF. Using (7.24) and (7.6), we write SSE as a quadratic form: SSE¼y0y/C0^b0X0y¼y0y/C0y0X(X0X)/C01X0y ¼y0I/C0X(X0X)/C01X0/C2/C3 y: (7:26) By Theorem 5.2a, we have E(SSE) ¼trI/C0X(X0X)/C01X0/C2/C3 s2I/C8/C9 þE(y0)I/C0X(X0X)/C01X0/C2/C3 E(y) ¼s2trI/C0X(X0X)/C01X0/C2/C3 þb0X0I/C0X(X0X)/C01X0/C2/C3 Xb ¼s2n/C0trX(X0X)/C01X0/C2/C3/C8/C9 þb0X0Xb/C0b0X0X(X0X)/C01X0Xb ¼s2n/C0tr[X0X(X0X)/C01]/C8/C9 þb0X0Xb/C0b0X0Xb [by (2 :87)] :150 MULTIPLE REGRESSION: ESTIMATION --- Page 164 --- Since X0Xis (kþ1)/C2(kþ1), this becomes E(SSE) ¼s2[n/C0tr(Ikþ1)]¼s2(n/C0k/C01): A Corollary 1.",
    "4 --- Since X0Xis (kþ1)/C2(kþ1), this becomes E(SSE) ¼s2[n/C0tr(Ikþ1)]¼s2(n/C0k/C01): A Corollary 1. An unbiased estimator of cov( ^b) in (7.14) is given by dcov( ^b)¼s2(X0X)/C01: (7:27) A Note the correspondence between n2(kþ1) and y0y/C0^b0X0y; there are nterms in y0yandkþ1 terms in ^b0X0y¼^b0X0X^b[see (7.8)]. A corresponding property of the sample is that each additional x(and ^b) in the model reduces SSE (see Problem 7.13).",
    "roperty of the sample is that each additional x(and ^b) in the model reduces SSE (see Problem 7.13). Since SSE is a quadratic function of y, it is not a best linear unbiased estimator. The optimality property of s2is given in the following theorem. Theorem 7.3g. IfE(1)¼0,cov(1)¼s2I, and E(14 i)¼3s4for the linear model y¼Xbþ1, then s2in (7.23) or (7.24) is the best (minimum variance) quadratic unbiased estimator of s2. PROOF.",
    "then s2in (7.23) or (7.24) is the best (minimum variance) quadratic unbiased estimator of s2. PROOF. See Graybill (1954), Graybill and Wortham (1956), or Wang and Chow (1994, pp. 161–163). A Example 7.3.3.",
    "Graybill (1954), Graybill and Wortham (1956), or Wang and Chow (1994, pp. 161–163). A Example 7.3.3. For the data in Table 7.1, we have SSE¼y0y/C0^b0Xy ¼840/C0(5:3754 ,3:0118 ,/C01:2855)90 482 8720 B@1 CA ¼840/C0814:541¼25:459 , s2¼SSE n/C0k/C01¼25:459 12/C02/C01¼2:829: A 7.4 GEOMETRY OF LEAST SQUARES In Sections 7.1–7.3 we presented the multiple linear regression model as the matrix equation y¼Xbþ1in (7.4).",
    "ons 7.1–7.3 we presented the multiple linear regression model as the matrix equation y¼Xbþ1in (7.4). We deﬁned the principle of least-squares estimation in terms of deviations from the model [see (7.7)], and then used matrix calculus and matrix algebra to derive the estimators of bin (7.6) and of s2in (7.23) and (7.24).",
    "rix calculus and matrix algebra to derive the estimators of bin (7.6) and of s2in (7.23) and (7.24). We now present an alternate but equivalent derivation of these estimators based com- pletely on geometric ideas.7.4 GEOMETRY OF LEAST SQUARES 151 --- Page 165 --- It is important to clarify ﬁrst what the geometric approach to least squares is not.I n two dimensions, we illustrated the principle of least squares by creating a two- dimensional scatter plot (Fig.",
    "ons, we illustrated the principle of least squares by creating a two- dimensional scatter plot (Fig. 6.1) of the npoints ( x1,y1),(x2,y2),...,(xn,yn). We then visualized the least-squares regression line as the best-ﬁtting straight line to the data.",
    ". We then visualized the least-squares regression line as the best-ﬁtting straight line to the data. This approach can be generalized to present the least-squares estimate in multiple linear regression on the basis of the best-ﬁtting hyperplane in ( kþ1)- dimensional space to the npoints ( x11,x12,...,x1k,y1),(x21,x22,...,x2k,y2),..., (xn1,xn2,...,xnk,yn).",
    "mensional space to the npoints ( x11,x12,...,x1k,y1),(x21,x22,...,x2k,y2),..., (xn1,xn2,...,xnk,yn). Although this approach is somewhat useful in visualizing multiple linear regression, the geometric approach to least-squares estimation inmultiple linear regression does notinvolve this high-dimensional generalization. The geometric approach to be discussed below is appealing because of its math- ematical elegance. For example, the estimator is derived without the use of matrix cal- culus.",
    "math- ematical elegance. For example, the estimator is derived without the use of matrix cal- culus. Also, the geometric approach provides deeper insight into statistical inference. Several advanced statistical methods including kernel smoothing (Eubank and Eubank 1999), Fourier analysis (Bloomﬁeld 2000), and wavelet analysis (Ogden 1997) can be understood as generalizations of this geometric approach. The geo- metric approach to linear models was ﬁrst proposed by Fisher (Mahalanobis 1964).",
    "approach. The geo- metric approach to linear models was ﬁrst proposed by Fisher (Mahalanobis 1964). Christensen (1996) and Jammalamadaka and Sengupta (2003) discuss the linear stat-istical model almost completely from the geometric perspective. 7.4.1 Parameter Space, Data Space, and Prediction Space The geometric approach to least squares begins with two high-dimensional spaces, a (kþ1)-dimensional space and an n-dimensional space.",
    "uares begins with two high-dimensional spaces, a (kþ1)-dimensional space and an n-dimensional space. The unknown parameter vector bcan be viewed as a single point in ( kþ1)-dimensional space, with axes cor- responding to the kþ1 regression coefﬁcients b0,b1,b0,...,bk. Hence we call this space the parameter space (Fig. 7.3).",
    "kþ1 regression coefﬁcients b0,b1,b0,...,bk. Hence we call this space the parameter space (Fig. 7.3). Similarly, the data vector ycan be viewed as a Figure 7.3 Parameter space, data space, and prediction space with representative elements.152 MULTIPLE REGRESSION: ESTIMATION --- Page 166 --- single point in n-dimensional space with axes corresponding to the nobservations. We call this space the data space .",
    "-dimensional space with axes corresponding to the nobservations. We call this space the data space . TheXmatrix of the multiple regression model (7.4) can be written as a partitioned matrix in terms of its kþ1 columns as X¼(j,x1,x2,x3,...,xk): The columns of X, including j, are all n-dimensional vectors and are therefore points in the data space. Note that because we assumed that Xis of rank kþ1, these vectors are linearly independent.",
    "ta space. Note that because we assumed that Xis of rank kþ1, these vectors are linearly independent. The set of all possible linear combinations of the columns of X(Section 2.3) constitutes a subset of the data space. Elements of this subset can be written as Xb¼b0jþb1x1þb2x2þ/C1/C1/C1þ bkxk, (7:28) where bis any kþ1 vector, that is, any vector in the parameter space.",
    "1þb2x2þ/C1/C1/C1þ bkxk, (7:28) where bis any kþ1 vector, that is, any vector in the parameter space. This subset actually has the status of a subspace because it is closed under addition and scalar multiplication (Harville 1997, pp. 28–29). This subset is said to be the subspace gen- erated or spanned by the columns of X, and we will call this subspace the prediction space . The columns of Xconstitute a basis set for the prediction space.",
    "his subspace the prediction space . The columns of Xconstitute a basis set for the prediction space. 7.4.2 Geometric Interpretation of the Multiple Linear Regression Model The multiple linear regression model (7.4) states that yis equal to a vector in the prediction space, E(y)¼Xb, plus a vector of random errors, 1(Fig. 7.4).",
    "yis equal to a vector in the prediction space, E(y)¼Xb, plus a vector of random errors, 1(Fig. 7.4). The Figure 7.4 Geometric relationships of vectors associated with the multiple linear regression model.7.4 GEOMETRY OF LEAST SQUARES 153 --- Page 167 --- problem is that neither bnor1is known. However, the data vector y, which is not in the prediction space, is known. And it is known that E(y) is in the prediction space.",
    "hich is not in the prediction space, is known. And it is known that E(y) is in the prediction space. Multiple linear regression can be understood geometrically as the process of ﬁnding a sensible estimate of E(y) in the prediction space and then determining the vector in the parameter space that is associated with this estimate (Fig. 7.4). The estimate of E(y) is denoted as ^y, and the associated vector in the parameter space is denoted as ^b.",
    "stimate of E(y) is denoted as ^y, and the associated vector in the parameter space is denoted as ^b. A reasonable geometric idea is to estimate E(y) using the point in the prediction space that is closest to y. It turns out that ^y, the closest point in the prediction space to y, can be found by noting that the difference vector ^1¼y/C0^ymust be orthogonal (perpendicular) to the prediction space (Harville 1997, p. 170).",
    "vector ^1¼y/C0^ymust be orthogonal (perpendicular) to the prediction space (Harville 1997, p. 170). Furthermore, because the prediction space is spanned by the columns of X, the point ^ymust be such that ^1is orthogonal to the columns of X.",
    "e is spanned by the columns of X, the point ^ymust be such that ^1is orthogonal to the columns of X. Using an extension of (2.80), we therefore seek ^ysuch that X0^1¼0 or X0(y/C0^y)¼X0(y/C0X^b)¼X0y/C0X0X^b¼0, (7:29) which implies that X0X^b¼X0y: Thus, using purely geometric ideas, we obtain the normal equations (7.8) and conse- quently the usual least-squares estimator ^bin (7.6). We can then calculate ^yas X^b¼X(X0X)/C01X0y¼Hy. Also, ^1¼y/C0X^b¼(I/C0H)ycan be taken as an esti- mate of1.",
    "hen calculate ^yas X^b¼X(X0X)/C01X0y¼Hy. Also, ^1¼y/C0X^b¼(I/C0H)ycan be taken as an esti- mate of1. Since ^1is a vector in ( n2k21)-dimensional space, it seems reasonable to estimate s2as the squared length (2.22) of ^1divided by n2k21. In other words, a sensible estimator of s2iss2¼y0(I/C0H)y=(n/C0k/C01), which is equal to (7.25).",
    "21. In other words, a sensible estimator of s2iss2¼y0(I/C0H)y=(n/C0k/C01), which is equal to (7.25). 7.5 THE MODEL IN CENTERED FORM The model in (7.3) for each yican be written in terms of centered xvariables as yi¼b0þb1xi1þb2xi2þ/C1/C1/C1þbkxikþ1i ¼aþb1(xi1/C0/C22x1)þb2(xi2/C0/C22x2)þ/C1/C1/C1þbk(xik/C0/C22xk)þ1i, (7:30) i¼1,2,...,n, where a¼b0þb1/C22x1þb2/C22x2þ...þbk/C22xk (7:31) and /C22xj¼Pn i¼1xij=n,j¼1,2,...,k.",
    "0) i¼1,2,...,n, where a¼b0þb1/C22x1þb2/C22x2þ...þbk/C22xk (7:31) and /C22xj¼Pn i¼1xij=n,j¼1,2,...,k. The centered form of the model is useful in expressing certain hypothesis tests (Section 8.1), in a search for inﬂuential obser- vations (Section 9.2), and in providing other insights.154 MULTIPLE REGRESSION: ESTIMATION --- Page 168 --- In matrix form, the centered model (7.30) for y1,y2,...,ynbecomes y¼(j,Xc)a b1/C18/C19 þ1, (7:32) whereb1¼(b1,b2,...,bk)0, Xc¼I/C01 nJ/C18/C19 X1¼x11/C0/C22x1x12/C0/C22x2...",
    ")a b1/C18/C19 þ1, (7:32) whereb1¼(b1,b2,...,bk)0, Xc¼I/C01 nJ/C18/C19 X1¼x11/C0/C22x1x12/C0/C22x2... x1k/C0/C22xk x21/C0/C22x1x22/C0/C22x2... x2k/C0/C22xk ......... xn1/C0/C22x1xn2/C0/C22x2... xnk/C0/C22xk0 BBB@1 CCCA, (7:33) andX1is as given in (7.19). The matrix I2(1/n)Jis sometimes called the centering matrix .",
    "CA, (7:33) andX1is as given in (7.19). The matrix I2(1/n)Jis sometimes called the centering matrix . As in (7.8), the normal equations for the model in (7.32) are (j,Xc)0(j,Xc)^a ^b1/C18/C19 ¼(j,Xc)0y: (7:34) By (2.35) and (2.39), the product ( j,Xc)0(j,Xc) on the left side of (7.34) becomes (j,Xc)0(j,Xc)¼j0 X0 c/C18/C19 (j,Xc)¼j0jj0Xc X0cjX0cXc/C18/C19 ¼n00 0X0cXc/C18/C19 , (7:35) where j0Xc¼00because the columns of Xcsum to zero (Problem 7.16).",
    "8/C19 ¼n00 0X0cXc/C18/C19 , (7:35) where j0Xc¼00because the columns of Xcsum to zero (Problem 7.16). The right side of (7.34) can be written as (j,Xc)0y¼j0 X0c/C18/C19 y¼n/C22y X0cy/C18/C19 : The least-squares estimators are then given by ^a ^b1/C18/C19 ¼[(j,Xc)0(j,Xc)]/C01(j,Xc)0y¼n00 0X0cXc/C18/C19 /C01n/C22y X0cy/C18/C19 ¼1=n 00 0 (X0cXc)/C01/C18/C19n/C22y X0cy/C18/C19 ¼/C22y (X0cXc)/C01X0cy/C18/C19 , or ^a¼/C22y, (7:36) ^b1¼(X0 cXc)/C01X0cy: (7:37)7.5 THE MODEL IN CENTERED FORM 155 --- Page 169 --- These estimators are the same as the usual least-squares estimators ^b¼(X0X)/C01X0y in (7.6), with the adjustment ^b0¼^a/C0^b1/C22x1/C0^b2/C22x/C0/C1/C1/C1/C0 ^bk/C22xk¼/C22y/C0^b0 1/C22x (7:38) obtained from an estimator of ain (7.31) (see Problem 7.17).",
    "/C0 ^bk/C22xk¼/C22y/C0^b0 1/C22x (7:38) obtained from an estimator of ain (7.31) (see Problem 7.17). When we express ^yin centered form ^y¼^aþ^b1(x1/C0/C22x1)þ/C1/C1/C1þ ^bk(xk/C0/C22xk), it is clear that the ﬁtted regression plane passes through the point ( /C22x1,/C22x2,...,/C22xk,/C22y). Adapting the expression for SSE (7.24) to the centered model with centered ^y’s, we obtain SSE¼Xn i¼1(yi/C0/C22y)2/C0^b0 1X0cy, (7:39) which turns out to be equal to SSE ¼y0y/C0^b0X0y(see Problem 7.19).",
    "yi/C0/C22y)2/C0^b0 1X0cy, (7:39) which turns out to be equal to SSE ¼y0y/C0^b0X0y(see Problem 7.19). We can use (7.36)–(7.38) to express ^b1and ^b0in terms of sample variances and covariances, which will be useful in comparing these estimators with those for the random- xcase in Chapter 10. We ﬁrst deﬁne a sample covariance matrix for the x variables and a vector of sample covariances between yand the x’s Sxx¼s2 1s12 ... s1k s21 s22... s2k ......... sk1sk2... s2 k0 BBB@1 CCCA,syx¼sy1 sy2 ...",
    "yand the x’s Sxx¼s2 1s12 ... s1k s21 s22... s2k ......... sk1sk2... s2 k0 BBB@1 CCCA,syx¼sy1 sy2 ... syk0 BBB@1 CCCA, (7:40) where, s2 i,sij, and syiare analogous to s2andsxydeﬁned in (5.6) and (5.15); for example s2 2¼Pn i¼1(xi2/C0/C22x2)2 n/C01, (7:41) s12¼Pni¼1(xi1/C0/C22x1)(xi2/C0/C22x2) n/C01, (7:42) sy2¼Pn i¼1(xi2/C0/C22x2)(yi/C0/C22y) n/C01, (7:43) with /C22x2¼Pn i¼1xi2=n.",
    "/C0/C22x2) n/C01, (7:42) sy2¼Pn i¼1(xi2/C0/C22x2)(yi/C0/C22y) n/C01, (7:43) with /C22x2¼Pn i¼1xi2=n. However, since the x’s are ﬁxed, these sample variances and covariances do not estimate population variances and covariances.",
    "e ﬁxed, these sample variances and covariances do not estimate population variances and covariances. If the x’s were random variables, as in Chapter 10, the s2 i,sij, and syivalues would estimate popu- lation parameters.156 MULTIPLE REGRESSION: ESTIMATION --- Page 170 --- To express ^b1and ^b0in terms of Sxxandsyx, we ﬁrst write Sxxandsyxin terms of the centered matrix Xc: Sxx¼X0 cXc n/C01, (7:44) syx¼X0cy n/C01: (7:45) Note that X0cyin (7.45) contains terms of the formPn i¼1(xij/C0/C22xj)yirather thanPn i¼1(xij/C0/C22xj)(yi/C0/C22y) as in (7.43).",
    "ins terms of the formPn i¼1(xij/C0/C22xj)yirather thanPn i¼1(xij/C0/C22xj)(yi/C0/C22y) as in (7.43). It can readily be shown thatP i(xij/C0/C22xj)(yi/C0/C22y)¼P i(xij/C0/C22xj)yi(see Problem 6.2). From (7.37), (7.44), and (7.45), we have ^b1¼(n/C01)(X0 cXc)/C01X0 cy n/C01¼X0cXc n/C01/C18/C19/C01X0cy n/C01¼S/C01 xxsyx, (7:46) and from (7.38) and (7.46), we obtain ^b0¼^a/C0^b0 1/C22x¼/C22y/C0s0 yxS/C01 xx/C22x: (7:47) Example 7.5.",
    "rom (7.38) and (7.46), we obtain ^b0¼^a/C0^b0 1/C22x¼/C22y/C0s0 yxS/C01 xx/C22x: (7:47) Example 7.5. For the data in Table 7.1, we calculate ^b1and ^b0using (7.46) and (7.47). ^b1¼S/C01 xxsyx¼6:4242 8 :5455 8:5455 12 :4545/C18/C19 /C018:3636 9:7273/C18/C19 ¼3:0118 /C01:2855/C18/C19 , ^b0¼/C22y/C0s0 yxS/C01 xx/C22x ¼7:5000 /C0(3:0118 ,/C01:2855)4:3333 8:5000/C18/C19 ¼7:500/C02:1246 ¼5:3754 : These values are the same as those obtained in Example 7.3.1a.",
    "000/C18/C19 ¼7:500/C02:1246 ¼5:3754 : These values are the same as those obtained in Example 7.3.1a. A 7.6 NORMAL MODEL 7.6.1 Assumptions Thus far we have made no normality assumptions about the random variables y1,y2,...,yn. To the assumptions in Section 7.2, we now add that yisNn(Xb,s2I)o r1isNn(0,s2I):7.6 NORMAL MODEL 157 --- Page 171 --- Under normality, sij¼0 implies that the y(or1) variables are independent, as well as uncorrelated.",
    "- Under normality, sij¼0 implies that the y(or1) variables are independent, as well as uncorrelated. 7.6.2 Maximum Likelihood Estimators for bands2 With the normality assumption, we can obtain maximum likelihood estimators. The likelihood function is the joint density of the y’s, which we denote by L(b,s2). We seek values of the unknown bands2that maximize L(b,s2) for the given yandx values in the sample.",
    "We seek values of the unknown bands2that maximize L(b,s2) for the given yandx values in the sample. In the case of the normal density function, it is possible to ﬁnd maximum likeli- hood estimators ^band ^s2by differentiation. Because the normal density involves a product and an exponential, it is simpler to work with ln L(b,s2), which achieves its maximum for the same values of bands2as does L(b,s2). The maximum likelihood estimators for bands2are given in the following theorem. Theorem 7.6a.",
    "b,s2). The maximum likelihood estimators for bands2are given in the following theorem. Theorem 7.6a. IfyisNn(Xb,s2I), where Xisn/C2(kþ1) of rank kþ1,n, the maximum likelihood estimators of bands2are ^b¼(X0X)/C01X0y, (7:48) ^s2¼1 n(y/C0X^b)0(y/C0X^b): (7:49) PROOF. We sketch the proof. For the remaining steps, see Problem 7.21.",
    "n(y/C0X^b)0(y/C0X^b): (7:49) PROOF. We sketch the proof. For the remaining steps, see Problem 7.21. The likeli- hood function ( joint density of y1,y2,...,yn) is given by the multivariate normal density (4.9) L(b,s2)¼f(y;b,s2)¼1 (2p)n=2js2Ij1=2e/C0(y/C0Xb)0(s2I)/C01(y/C0Xb)=2 ¼1 (2ps2)n=2e/C0(y/C0Xb)0(y/C0Xb)=2s2: (7:50) [Since the yi’s are independent, L(b,s2) can also be obtained asQn i¼1f(yi;x0 ib,s2).] Then lnL(b,s2) becomes lnL(b,s2)¼/C0n 2ln(2p)/C0n2ln s2/C01 2s2(y/C0Xb)0(y/C0Xb): (7:51) Taking the partial derivatives of lnL(b,s2) with respect to bands2and setting the results equal to zero will produce (7.48) and (7.49).",
    "nL(b,s2) with respect to bands2and setting the results equal to zero will produce (7.48) and (7.49). To verify that ^bmaximizes (7.50) or (7.51), see (7.10). A158 MULTIPLE REGRESSION: ESTIMATION --- Page 172 --- The maximum likelihood estimator ^bin (7.48) is the same as the least-squares estima- tor^bin Theorem 7.3a. The estimator ^s2in (7.49) is biased since the denominator is n rather than n2k21. We often use the unbiased estimator s2given in (7.23) or (7.24).",
    "denominator is n rather than n2k21. We often use the unbiased estimator s2given in (7.23) or (7.24). 7.6.3 Properties of ^band ^s2 We now consider some properties of ^band ^s2(ors2) under the normal model. The distributions of ^band ^s2are given in the following theorem. Theorem 7.6b. Suppose that yisNn(Xb,s2I), where Xisn/C2(kþ1) of rank kþ 1,nandb¼(b0,b1,...,bk)0.",
    "em. Theorem 7.6b. Suppose that yisNn(Xb,s2I), where Xisn/C2(kþ1) of rank kþ 1,nandb¼(b0,b1,...,bk)0. Then the maximum likelihood estimators ^band ^s2 given in Theorem 7.6a have the following distributional properties: (i)^bisNkþ1[b,s2(X0X)/C01]. (ii)n^s2=s2isx2(n/C0k/C01), or equivalently, ( n/C0k/C01)s2=s2isx2(n/C0k/C01). (iii) ^band ^s2(ors2) are independent.",
    "/C0k/C01), or equivalently, ( n/C0k/C01)s2=s2isx2(n/C0k/C01). (iii) ^band ^s2(ors2) are independent. PROOF (i) Since ^b¼(X0X)/C01X0yis a linear function of yof the form ^b¼Ay, where A¼(X0X)/C01X0is a constant matrix, then by Theorem 4.4a(ii), ^bis Nkþ1[b,s2(X0X)/C01]. (ii) The result follows from Corollary 2 to Theorem 5.5. (iii) The result follows from Corollary 1 to Theorem 5.6a. A Another property of ^band ^s2under normality is that they are sufﬁcient statistics.",
    "Theorem 5.6a. A Another property of ^band ^s2under normality is that they are sufﬁcient statistics. Intuitively, a statistic is sufﬁcient for a parameter if the statistic summarizes all the information in the sample about the parameter. Sufﬁciency of ^band ^s2can be estab- lished by the Neyman factorization theorem [see Hogg and Craig (1995, p. 318) orGraybill (1976, pp.",
    "lished by the Neyman factorization theorem [see Hogg and Craig (1995, p. 318) orGraybill (1976, pp. 69–70)], which states that ^ band ^s2are jointly sufﬁcient for b ands2if the density f(y;b,s2) can be factored as f(y;b,s2)¼ g(^b,^s2,b,s2)h(y), where h(y) does not depend on bors2. The following theorem shows that ^band ^s2satisfy this criterion. Theorem 7.6c. IfyisNn(Xb,s2I), then ^band ^s2are jointly sufﬁcient for bands2. PROOF. The density f(y;b,s2) is given in (7.50).",
    "I), then ^band ^s2are jointly sufﬁcient for bands2. PROOF. The density f(y;b,s2) is given in (7.50). In the exponent, we add and subtract X^bto obtain (y/C0Xb)0(y/C0Xb)¼(y/C0X^bþX^b/C0Xb)0(y/C0X^bþX^b/C0Xb) ¼[(y/C0X^b)þX(^b/C0b)]0[(y/C0X^b)þX(^b/C0b)]:7.6 NORMAL MODEL 159 --- Page 173 --- Expanding this in terms of y/C0X^bandX(^b/C0b), we obtain four terms, two of which vanish because of the normal equations X0X^b¼X0y.",
    "^bandX(^b/C0b), we obtain four terms, two of which vanish because of the normal equations X0X^b¼X0y. The result is (y/C0Xb)0(y/C0Xb)¼(y/C0X^b)0(y/C0X^b)þ(^b/C0b)0X0X(^b/C0b)( 7 :52) ¼n^s2þ(^b/C0b)0X0X(^b/C0b): We can now write the density (7.50) as f(y;b,s2)¼1 (2ps2)n=2e/C0[n^s2þ(^b/C0b)0X0X(^b/C0b)]=2s2, which is of the form f(y;b,s2)¼g(^b,^s2,b,s2)h(y), where h(y)¼1. Therefore, by the Neyman factorization theorem, ^band ^s2are jointly sufﬁcient for bands2.",
    "e h(y)¼1. Therefore, by the Neyman factorization theorem, ^band ^s2are jointly sufﬁcient for bands2. A Note that ^band ^s2are jointly sufﬁcient for bands2, not independently sufﬁcient; that is, f(y;b,s2) does not factor into the form g1(^b,b)g2(^s2,s2)h(y). Also note that because s2¼n^s2=(n/C0k/C01), the proof to Theorem 7.6c can be easily modiﬁed to show that ^bands2are also jointly sufﬁcient for bands2.",
    "oof to Theorem 7.6c can be easily modiﬁed to show that ^bands2are also jointly sufﬁcient for bands2. Since ^bands2are sufﬁcient, no other estimators can improve on the information they extract from the sample to estimate bands2. Thus, it is not surprising that ^band s2are minimum variance unbiased estimators (each ^bjin^bhas minimum variance). This result is given in the following theorem. Theorem 7.6d. IfyisNn(Xb,s2I), then ^bands2have minimum variance among all unbiased estimators.P ROOF.",
    "eorem 7.6d. IfyisNn(Xb,s2I), then ^bands2have minimum variance among all unbiased estimators.P ROOF. See Graybill (1976, p. 176) or Christensen (1996, pp. 25–27). A In Theorem 7.3d, the elements of ^bwere shown to have minimum variance among alllinear unbiased estimators. With the normality assumption added in Theorem 7.6d, the elements of ^bhave minimum variance among all unbiased estimators. Similarly, by Theorem 7.3g, s2has minimum variance among all quadratic unbiased estimators.",
    "imators. Similarly, by Theorem 7.3g, s2has minimum variance among all quadratic unbiased estimators. With the added normality assumption in Theorem 7.6d, s2has minimum variance among all unbiased estimators. The following corollary to Theorem 7.6d is analogous to Corollary 1 of Theorem 7.3d. Corollary 1. IfyisNn(Xb,s2I), then the minimum variance unbiased estimator of a0bisa0^b, where ^bis the maximum likelihood estimator given in (7.48).",
    "riance unbiased estimator of a0bisa0^b, where ^bis the maximum likelihood estimator given in (7.48). A160 MULTIPLE REGRESSION: ESTIMATION --- Page 174 --- 7.7 R2IN FIXED- xREGRESSION In (7.39), we have SSE ¼Pn i¼1(yi/C0/C22y)2/C0^b0 1X0 cy. Thus the corrected total sum of squares SST ¼P i(yi/C0/C22y)2can be partitioned as Xn i¼1(yi/C0/C22y)2¼^b0 1X0 cyþSSE , (7:53) SST¼SSRþSSE , where SSR ¼^b01X0 cyis the regression sum of squares.",
    "0/C22y)2¼^b0 1X0 cyþSSE , (7:53) SST¼SSRþSSE , where SSR ¼^b01X0 cyis the regression sum of squares. From (7.37), we obtain X0cy¼X0cXc^b1, and multiplying this by ^b0 1gives ^b01X0 cy¼^b0 1X0 cXc^b1. Then SSR¼^b0 1X0 cycan be written as SSR¼^b0 1X0cXc^b1¼(Xc^b1)0(Xc^b1): (7:54) In this form, it is clear that SSR is due to b1¼(b1,b2,...,bk)0.",
    "0 1X0cXc^b1¼(Xc^b1)0(Xc^b1): (7:54) In this form, it is clear that SSR is due to b1¼(b1,b2,...,bk)0. The proportion of the total sum of squares due to regression is R2¼^b0 1X0 cXc^b1Pn i¼1(yi/C0/C22y)2¼SSR SST, (7:55) which is known as the coefﬁcient of determination or the squared multiple corre- lation. The ratio in (7.55) is a measure of model ﬁt and provides an indication of how well the x’s predict y.",
    "e ratio in (7.55) is a measure of model ﬁt and provides an indication of how well the x’s predict y. The partitioning in (7.53) can be rewritten as the identity Xn i¼1(yi/C0/C22y)2¼y0y/C0n/C22y2¼(^b0X0y/C0n/C22y2)þ(y0y/C0^b0X0y) ¼SSRþSSE , which leads to an alternative expression for R2: R2¼^b0X0y/C0n/C22y2 y0y/C0n/C22y2: (7:56) The positive square root Robtained from (7.55) or (7.56) is called the multiple cor- relation coefﬁcient.",
    "ositive square root Robtained from (7.55) or (7.56) is called the multiple cor- relation coefﬁcient. If the xvariables were random, Rwould estimate a population multiple correlation (see Section (10.4)). We list some properties of R2andR: 1. The range of R2is 0/C20R2/C201. If all the ^bj’s were zero, except for ^b0,R2 would be 0.",
    "R2andR: 1. The range of R2is 0/C20R2/C201. If all the ^bj’s were zero, except for ^b0,R2 would be 0. (This event has probability 0 for continuous data.) If all the yvalues fell on the ﬁtted surface, that is, if yi¼^yi,i¼1,2,...,n, then R2 would be 1.7.7 R2IN FIXED- xREGRESSION 161 --- Page 175 --- 2.R¼ry^y; that is, the multiple correlation is equal to the simple correlation [see (6.18)] between the observed yi’s and the ﬁtted ^yi’s. 3.",
    "on is equal to the simple correlation [see (6.18)] between the observed yi’s and the ﬁtted ^yi’s. 3. Adding a variable xto the model increases (cannot decrease) the value of R2. 4. Ifb1¼b2¼/C1/C1/C1¼bk¼0, then E(R2)¼k n/C01: (7:57) Note that the ^bj’s will not be 0 when the bj’s are 0. 5.R2cannot be partitioned into kcomponents, each of which is uniquely attribu- table to an xj, unless the x’s are mutually orthogonal, that is,Pn i¼1(xij/C0/C22xj)(xim/C0/C22xm)¼0 for j=m.",
    "an xj, unless the x’s are mutually orthogonal, that is,Pn i¼1(xij/C0/C22xj)(xim/C0/C22xm)¼0 for j=m. 6.R2is invariant to full-rank linear transformations on the x’s and to a scale change ony(but not invariant to a joint linear transformation including yand the x’s). In properties 3 and 4 we see that if kis a relatively large fraction of n, it is possible to have a large value of R2that is not meaningful.",
    "a relatively large fraction of n, it is possible to have a large value of R2that is not meaningful. In this case, x’s that do not contribute to predicting ymay appear to do so in a particular example, and the estimated regression equation may not be a useful estimator of the population model. To correct for this tendency, an adjusted R2, denoted by R2 a, was proposed by Ezekiel (1930). To obtain R2a, we ﬁrst subtract k/(n21) in (7.57) from R2in order to correct for the bias when b1¼b2¼...¼bk¼0.",
    "R2a, we ﬁrst subtract k/(n21) in (7.57) from R2in order to correct for the bias when b1¼b2¼...¼bk¼0. This correction, however, would make R2atoo small when the b’s are large, so a further modiﬁcation is made so that R2a¼1 when R2¼1. Thus R2ais deﬁned as R2 a¼(R2/C0k n/C01)(n/C01) n/C0k/C01¼(n/C01)R2/C0k n/C0k/C01: (7:58) Example 7.7. For the data in Table 7.1 in Example 7.2, we obtain R2by (7.56) and R2 aby (7.58). The values of ^b0X0yandy0yare given in Example 7.3.3.",
    "7.2, we obtain R2by (7.56) and R2 aby (7.58). The values of ^b0X0yandy0yare given in Example 7.3.3. R2¼^b0X0y/C0n/C22y2 y0y/C0n/C22y2¼814:5410 /C012(7 :5)2 840/C012(7 :5)2 ¼139:5410 165:0000¼:8457 , R2 a¼(n/C01)R2/C0k n/C0k/C01¼(11)( :8457) /C02 9¼:8114 : A Using (7.44) and (7.46), we can express R2in (7.55) in terms of sample variances and covariances: R2¼^b0 1X0 cXc^b1Pn i¼1(yi/C0/C22y)2¼s0 yxS/C01 xx(n/C01)SxxS/C01 xxsyxPn i¼1(yi/C0/C22y)2¼s0 yxS/C01 xxsyx s2 y: (7:59)162 MULTIPLE REGRESSION: ESTIMATION --- Page 176 --- This form of R2will facilitate a comparison with R2for the random- xcase in Section (10.4) [see (10.34)].",
    "form of R2will facilitate a comparison with R2for the random- xcase in Section (10.4) [see (10.34)]. Geometrically, Ris the cosine of the angle ubetween yand ^ycorrected for their means. The mean of ^y1,^y2,...,^ynis/C22y, the same as the mean of y1,y2,...,yn(see Problem 7.30). Thus the centered forms of yand ^yarey/C0/C22yjand ^y/C0/C22yj. The angle between them is illustrated in Figure 7.5.",
    "d forms of yand ^yarey/C0/C22yjand ^y/C0/C22yj. The angle between them is illustrated in Figure 7.5. (Note that /C22yjis in the estimation space since it is a multiple of the ﬁrst column of X.) To show that cos uis equal to the square root of R2as given by (7.56), we use (2.81) for the cosine of the angle between two vectors: cosu¼(y/C0/C22yj)0(^y/C0/C22yj)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ [(y/C0/C22yj)0(y/C0/C22yj)][(^y/C0/C22yj)0(^y/C0/C22yj)]p : (7:60) To simplify (7.60), we use the identity y/C0/C22yj¼(^y/C0/C22yj)þ(y/C0^y), which can also be seen geometrically in Figure 7.5.",
    "the identity y/C0/C22yj¼(^y/C0/C22yj)þ(y/C0^y), which can also be seen geometrically in Figure 7.5. The vectors ^y/C0/C22yjandy/C0^yon the right side of this identity are orthogonal since ^y/C0/C22yjis in the prediction space.",
    "y/C0^yon the right side of this identity are orthogonal since ^y/C0/C22yjis in the prediction space. Thus the numer- ator of (7.60) can be written as (y/C0/C22yj)0(^y/C0/C22yj)¼[(^y/C0/C22yj)þ(y/C0^y)]0(^y/C0/C22yj) ¼(^y/C0/C22yj)0(^y/C0/C22yj)þ(y/C0^y)0(^y/C0/C22yj) ¼(^y/C0/C22yj)0(^y/C0/C22yj)þ0: Then (7.60) becomes cosu¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (^y/C0/C22yj)0(^y/C0/C22yj)p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (y/C0/C22yj)0(y/C0/C22yj)p ¼R, ð7:61ÞFigure 7.5 Multiple correlation Ras cosine of u, the angle between y/C0/C22yjand ^y/C0/C22yj.7.7 R2IN FIXED- xREGRESSION 163 --- Page 177 --- which is easily shown to be the square root of R2as given by (7.56).",
    "REGRESSION 163 --- Page 177 --- which is easily shown to be the square root of R2as given by (7.56). This is equivalent to property 2 following (7.56): R¼ry^y. We can write (7.61) in the form R2¼Pn i¼1(^yi/C0/C22y)2 Pn i¼1(yi/C0/C22y)2¼SSR SST, in which SSR ¼Pn i¼1(^yi/C0/C22y)2is a sum of squares for the ^yi’s. Then the partitioning SST¼SSRþSSE below (7.53) can be written as Xn i¼1(yi/C0/C22y)2¼Xn i¼1(^yi/C0/C22y)2þXn i¼1(yi/C0^yi)2, which is analogous to (6.17) for simple linear regression.",
    "2¼Xn i¼1(^yi/C0/C22y)2þXn i¼1(yi/C0^yi)2, which is analogous to (6.17) for simple linear regression. 7.8 GENERALIZED LEAST SQUARES: cov(Y) 5s2V We now consider models in which the yvariables are correlated or have differing var- iances, so that cov( y)=s2I. In simple linear regression, larger values of ximay lead to larger values of var( yi). In either simple or multiple regression, if y1,y2,...,ynoccur at sequential points in time, they are typically correlated.",
    "ltiple regression, if y1,y2,...,ynoccur at sequential points in time, they are typically correlated. For cases such as these, in which the assumption cov( y)¼s2Iis no longer appropriate, we use the model y¼Xbþ1,E(y)¼Xb,cov(y)¼S¼s2V, (7:62) where Xis full-rank and Vis a known positive deﬁnite matrix. The usage S¼s2V permits estimation of s2in some convenient contexts (see Examples 7.8.1 and 7.8.2). Then/C2nmatrix Vhasndiagonal elements andn 2/C18/C19 elements above (or below) the diagonal.",
    "7.8.2). Then/C2nmatrix Vhasndiagonal elements andn 2/C18/C19 elements above (or below) the diagonal. If Vwere unknown, thesen 2/C18/C19 þndistinct elements could not be esti- mated from a sample of nobservations. In certain applications, a simpler structure forVis assumed that permits estimation. Such structures are illustrated in Examples 7.8.1 and 7.8.2.",
    "forVis assumed that permits estimation. Such structures are illustrated in Examples 7.8.1 and 7.8.2. 7.8.1 Estimation of bands2when cov(y) 5s2V In the following theorem we give estimators of bands2for the model in (7.62).164 MULTIPLE REGRESSION: ESTIMATION --- Page 178 --- Theorem 7.8a. Lety¼Xbþ1, let E(y)¼Xb, and let cov( y)¼cov(1)¼s2V, where Xis a full-rank matrix and Vis a known positive deﬁnite matrix.",
    "b, and let cov( y)¼cov(1)¼s2V, where Xis a full-rank matrix and Vis a known positive deﬁnite matrix. For this model, we obtain the following results: (i) The best linear unbiased estimator (BLUE) of bis ^b¼(X0V/C01X)/C01X0V/C01y: (7:63) (ii) The covariance matrix for ^bis cov( ^b)¼s2(X0V/C01X)/C01: (7:64) (iii) An unbiased estimator of s2is s2¼(y/C0X^b)0V/C01(y/C0X^b) n/C0k/C01(7:65) ¼y0[V/C01/C0V/C01X(X0V/C01X)/C01X0V/C01]y n/C0k/C01, (7:66) where ^bis as given by (7.63). PROOF.",
    "5) ¼y0[V/C01/C0V/C01X(X0V/C01X)/C01X0V/C01]y n/C0k/C01, (7:66) where ^bis as given by (7.63). PROOF. We prove part (i). For parts (ii) and (iii), see Problems (7.32) and (7.33). 1. Since Vis positive deﬁnite, there exists an n/C2nnonsingular matrix Psuch that V¼PP0(see Theorem 2.6c).",
    "ce Vis positive deﬁnite, there exists an n/C2nnonsingular matrix Psuch that V¼PP0(see Theorem 2.6c). Multiplying y¼Xbþ1byP/C01, we obtain P/C01y¼P/C01XbþP/C011, for which E(P/C011)¼P/C01E(1)¼P/C010¼0and cov(P/C011)¼P/C01cov(1)(P/C01)0[by (3 :44)] ¼P/C01s2V(P/C01)0¼s2P/C01PP0(P0)/C01¼s2I: Thus the assumptions for Theorem 7.3d are satisﬁed for the model P/C01y¼ P/C01XbþP/C011, and the least-squares estimator ^b¼[(P/C01X)0(P/C01X)]/C01 (P/C01X)0P/C01yis BLUE.",
    "¼ P/C01XbþP/C011, and the least-squares estimator ^b¼[(P/C01X)0(P/C01X)]/C01 (P/C01X)0P/C01yis BLUE. Using Theorems 2.2b and 2.5b, this can be written as ^b¼[X0(P/C01)0P/C01X]/C01X0(P/C01)0P/C01y ¼[X0(P0)/C01P/C01X]/C01X0(P0)/C01P/C01y [by (2 :48)] ¼[X0(PP0)/C01X]/C01X0(PP0)/C01y [by (2 :49)] ¼(X0V/C01X)/C01X0V/C01y: A7.8 GENERALIZED LEAST SQUARES: cov(Y) ¼s2V 165 --- Page 179 --- Note that since Xis full-rank, X0V/C01Xis positive deﬁnite (see Theorem 2.6b).",
    "165 --- Page 179 --- Note that since Xis full-rank, X0V/C01Xis positive deﬁnite (see Theorem 2.6b). The estimator ^b¼(X0V/C01X)/C01X0V/C01yis usually called the generalized least-squares estimator. The same estimator is obtained under a normality assumption. Theorem 7.8b.",
    "least-squares estimator. The same estimator is obtained under a normality assumption. Theorem 7.8b. IfyisNn(Xb,s2V), where Xis full-rank and Vis a known positive deﬁnite matrix, where Xisn/C2(kþ1) of rank kþ1, then the maximum likelihood estimators for bands2are ^b¼(X0V/C01X)/C01X0V/C01y, ^s2¼1 n(y/C0X^b)0V/C01(y/C0X^b): PROOF. The likelihood function is L(b,s2)¼1 (2p)n=2js2Vj1=2e/C0(y/C0Xb)0(s2V)/C01(y/C0Xb)=2: By (2.69), js2Vj¼(s2)njVj.",
    "od function is L(b,s2)¼1 (2p)n=2js2Vj1=2e/C0(y/C0Xb)0(s2V)/C01(y/C0Xb)=2: By (2.69), js2Vj¼(s2)njVj. Hence L(b,s2)¼1 (2ps2)n=2jVj1=2e/C0(y/C0Xb)0V/C01(y/C0Xb)=2s2: The results can be obtained by differentiation of lnL(b,s2) with respect to band with respect to s2. A We illustrate an application of generalized least squares. Example 7.8.1. Consider the centered model in (7.32) y¼(j,Xc)a b1/C18/C19 þ1, with covariance pattern S¼s2[(1/C0r)IþrJ]¼s2V (7:67) ¼s21r...r r1...r ......... rr ...",
    "1/C18/C19 þ1, with covariance pattern S¼s2[(1/C0r)IþrJ]¼s2V (7:67) ¼s21r...r r1...r ......... rr ... 10 BBBBB@1 CCCCCA, in which all variables have the same variance s2and all pairs of variables have the same correlation r. This covariance pattern was introduced in Problem 5.26 and is assumed for certain repeated measures and intraclass correlation designs.",
    "ced in Problem 5.26 and is assumed for certain repeated measures and intraclass correlation designs. See (3.19) for a deﬁnition of r.166 MULTIPLE REGRESSION: ESTIMATION --- Page 180 --- By (7.63), we have ^b¼^a ^b1/C18/C19 ¼(X0V/C01X)/C01X0V/C01y: For the centered model with X¼(j,Xc), the matrix X0V/C01Xbecomes X0V/C01X¼j0 X0 c/C18/C19 V/C01(j,Xc) ¼j0V/C01jj0V/C01Xc X0cV/C01jX0cV/C01Xc !",
    "matrix X0V/C01Xbecomes X0V/C01X¼j0 X0 c/C18/C19 V/C01(j,Xc) ¼j0V/C01jj0V/C01Xc X0cV/C01jX0cV/C01Xc ! : The inverse of the n/C2nmatrix V¼(1/C0r)IþrJin (7.67) is given by V/C01¼a(I/C0brJ), (7:68) where a¼1=(1/C0r) and b¼1=[1þ(n/C01)r]. Using V/C01in (7.68), X0V/C01X becomes X0V/C01X¼bn 00 0aX0cXc/C18/C19 : (7:69) Similarly X0V/C01y¼bn/C22y aX0cy/C18/C19 : (7:70) We therefore have ^a ^b1/C18/C19 ¼(X0V/C01X)/C01X0V/C01y¼/C22y (X0cXc)/C01X0cy/C18/C19 , which is the same as (7.36) and (7.37).",
    "/C19 ¼(X0V/C01X)/C01X0V/C01y¼/C22y (X0cXc)/C01X0cy/C18/C19 , which is the same as (7.36) and (7.37). Thus the usual least-squares estimators are BLUE for a covariance structure with equal variances and equal correlations.",
    "-squares estimators are BLUE for a covariance structure with equal variances and equal correlations. A 7.8.2 Misspeciﬁcation of the Error Structure Suppose that the model is y¼Xbþ1with cov( y)¼s2V, as in (7.62), and we mis- takenly (or deliberately) use the ordinary least-squares estimator ^b/C3¼(X0X)/C01X0yin (7.6), which we denote here by ^b/C3to distinguish it from the BLUE estimator ^b¼(X0V/C01X)/C01X0V/C01yin (7.63).",
    "we denote here by ^b/C3to distinguish it from the BLUE estimator ^b¼(X0V/C01X)/C01X0V/C01yin (7.63). Then the mean vector and covariance matrix7.8 GENERALIZED LEAST SQUARES: cov(Y) ¼s2V 167 --- Page 181 --- for ^b/C3are E(^b/C3)¼b, (7:71) cov( ^b/C3)¼s2(X0X)/C01X0VX(X0X)/C01: (7:72) Thus the ordinary least-squares estimators are unbiased, but the covariance matrix differs from (7.64).",
    "s the ordinary least-squares estimators are unbiased, but the covariance matrix differs from (7.64). Because of Theorem 7.8a(i), the variances of the ^b/C3 j’s in (7.72) cannot be smaller than the variances in cov( ^b)¼s2(X0V/C01X)/C01in (7.64). This is illustrated in the following example. Example 7.8.2. Suppose that we have a simple linear regression model yi¼b0þb1xiþ1i, where var( yi)¼s2xiand cov( yi,yj)¼0 for i=j. Thus cov(y)¼s2V¼s2x10 ... 0 0x2... 0 ......... 00 ...",
    "where var( yi)¼s2xiand cov( yi,yj)¼0 for i=j. Thus cov(y)¼s2V¼s2x10 ... 0 0x2... 0 ......... 00 ... xn0 BBB@1 CCCA: This is an example of weighted least squares , which typically refers to the case where Vis diagonal with functions of the x’s on the diagonal. In this case X¼1x1 1x2 ...... 1xn0 BBB@1 CCCA, and by (7.63), we have ^b¼^b0 ^b1 !",
    "on the diagonal. In this case X¼1x1 1x2 ...... 1xn0 BBB@1 CCCA, and by (7.63), we have ^b¼^b0 ^b1 ! ¼(X0V/C01X)/C01X0V/C01y ¼1 Pn i¼1xi/C0/C1Pni¼11 xi/C16/C17 /C0n2Pni¼1xi/C0/C1Pni¼1yi xi/C16/C17 /C0nPni¼1yi Pni¼1yi/C0/C1Pni¼11 xi/C16/C17 /C0nPni¼1yi xi0 B@1 CA: (7:73) The covariance matrix for ^bis given by (7.64): cov( ^b)¼s2(X0V/C01X)/C01 ¼s2 P ixiP i1 xi/C0n2P ixi/C0n /C0nP i1 xi0 B@1 CA: (7:74)168 MULTIPLE REGRESSION: ESTIMATION --- Page 182 --- If we use the ordinary least-squares estimator ^b/C3¼(X0X)/C01X0yas given in (6.5) and (6.6) or in (7.12) in Example 7.3.1b, then cov( ^b/C3) is given by (7.72); that is, cov( ^b/C3)¼s2(X0X)/C01X0VX(X0X)/C01 ¼s2nP ixi P ixiP ix2 i0 B@1 CA/C01P ixiP ix2 i P ix2iP ix3i0 B@1 CAnP ixi P ixiP ix2 i0 B@1 CA/C01 ¼s2cP ix3 i(P ixi)2/C0P ixi(P ix2i)2n(P ix2i)2/C0nP ixiP ix3i n(P ix2i)2/C0nP ixiP ix3i n2P ix3i/C02nP ixiP ix2iþ(P ixi)30 B@1 CA, (7:75) where c¼1=nP ix2 i/C0P ixi/C0/C12hi2 .",
    "P ixiP ix3i n2P ix3i/C02nP ixiP ix2iþ(P ixi)30 B@1 CA, (7:75) where c¼1=nP ix2 i/C0P ixi/C0/C12hi2 . The variance of the estimator ^b/C3 1is given by the lower right diagonal element of (7.75): var( ^b/C3 1)¼s2n2P ix3 i/C02nP ixiP ix2iþ(P ixi)3 nP ix2 i/C0P ixi/C0/C12hi2, (7:76) and the variance of the estimator ^b1is given by the corresponding element of (7.74): var( ^b1)¼s2P i(1=xi)P ixiP i(1=xi)/C0n2: (7:77) Consider the following seven values of x: 1, 2, 3, 4, 5, 6, 7.",
    "2P i(1=xi)P ixiP i(1=xi)/C0n2: (7:77) Consider the following seven values of x: 1, 2, 3, 4, 5, 6, 7. Using (7.76), we obtain var( ^b/C3 1)¼:1429s2, and from (7.77), we have var( ^b1)¼:1099s2. Thus for these values of x, the use of ordinary least squares yields a slope estimator with a larger variance, as expected. A Further consequences of using a wrong model are discussed in the next section.",
    "iance, as expected. A Further consequences of using a wrong model are discussed in the next section. 7.9 MODEL MISSPECIFICATION In Section 7.8.2, we discussed some consequences of misspeciﬁcation of cov( y). We now consider consequences of misspeciﬁcation of E(y).",
    "consequences of misspeciﬁcation of cov( y). We now consider consequences of misspeciﬁcation of E(y). As a framework for discus- sion, let the model y¼Xbþ1be partitioned as y¼Xbþ1¼(X1,X2)b1 b2/C18/C19 þ1 ¼X1b1þX2b2þ1: (7:78)7.9 MODEL MISSPECIFICATION 169 --- Page 183 --- If we leave out X2b2when it should be included (i.e., when b2=0), we are under- ﬁtting . If we include X2b2when it should be excluded (i.e., when b2¼0), we are overﬁtting.",
    "e under- ﬁtting . If we include X2b2when it should be excluded (i.e., when b2¼0), we are overﬁtting. We discuss the effect of underﬁtting or overﬁtting on the bias and the variance of the ^bj,^y, and s2values. We ﬁrst consider estimation of b1when underﬁtting.",
    "ias and the variance of the ^bj,^y, and s2values. We ﬁrst consider estimation of b1when underﬁtting. We write the reduced model as y¼X1b/C3 1þ1/C3, ð7:79Þ usingb/C31to emphasize that these parameters (and their estimates ^b/C31) will be different fromb1(and ^b1) in the fullmodel (7.78) (unless the x’s are orthogonal; see Corollary 1 to Theorem 7.9a and Theorem 7.10). This was illustrated in Example 7.2.",
    "orthogonal; see Corollary 1 to Theorem 7.9a and Theorem 7.10). This was illustrated in Example 7.2. In the fol- lowing theorem, we discuss the bias in the estimator ^b/C3 1obtained from (7.79) and give the covariance matrix for ^b/C3 1. Theorem 7.9a.",
    "the estimator ^b/C3 1obtained from (7.79) and give the covariance matrix for ^b/C3 1. Theorem 7.9a. If we ﬁt the model y¼X1b/C31þ1/C3when the correct model is y¼X1b1þX2b2þ1with cov( y)¼s2I, then the mean vector and covariance matrix for the least-squares estimator ^b/C31¼(X0 1X1)/C01X01yare as follows: (i)E(^b/C3 1)¼b1þAb2,where A¼(X0 1X1)/C01X01X2, (7:80) (ii) cov( ^b/C3 1)¼s2(X0 1X1)/C01: (7:81) PROOF (i)E(^b/C3 1)¼E[(X0 1X1)/C01X01y]¼(X01X1)/C01X01E(y) ¼(X01X1)/C01X01(X1b1þX2b2) ¼b1þ(X01X1)/C01X01X2b2: (ii) cov( ^b/C3 1)¼cov[(X0 1X1)/C01X01y] ¼(X01X1)/C01X01(s2I)X1(X01X1)/C01[by (3 :44)] ¼s2(X01X1)/C01:A Thus, when underﬁtting, ^b/C31is biased by an amount that depends on the values of the x’s in both X1andX2.",
    "hen underﬁtting, ^b/C31is biased by an amount that depends on the values of the x’s in both X1andX2. The matrix A¼(X0 1X1)/C01X01X2in (7.81) is called the alias matrix. Corollary 1. IfX0 1X2¼O, that is, if the columns of X1are orthogonal to the columns of X2, then ^b/C3 1is unbiased: E(^b/C31)¼b1. A In the next three theorems, we discuss the effect of underﬁtting or overﬁtting on ^y,s2,a n d the variances of the ^bj’s. In some of the proofs we follow Hocking (1996, pp. 245–247).",
    "^y,s2,a n d the variances of the ^bj’s. In some of the proofs we follow Hocking (1996, pp. 245–247). Letx0¼(1,x01,x02,...,x0k)0be a particular value of xfor which we desire to estimate E(y0)¼x0 0b. If we partition x00into ( x001,x002) corresponding to the170 MULTIPLE REGRESSION: ESTIMATION --- Page 184 --- partitioning X¼(X1,X2) andb0¼(b0 1,b02),then we can use either ^y0¼x0 0^bor ^y01¼x001^b/C3 1to estimate x0 0b. In the following theorem, we consider the mean of ^y01. Theorem 7.9b.",
    "¼x001^b/C3 1to estimate x0 0b. In the following theorem, we consider the mean of ^y01. Theorem 7.9b. Let^y01¼x001^b/C3 1, where ^b/C311¼(X0 1X1)/C01X01y. Then, if b2=0,w e obtain E(x0 01^b/C31)¼x001(b1þAb2), (7:82) ¼x00b/C0(x02/C0A0x01)0b2=x00b: (7:83) PROOF. See Problem 7.43. A In Theorem 7.9b, we see that, when underﬁtting, x0 01^b/C3 1is biased for estimating x0 0b.",
    "7.43. A In Theorem 7.9b, we see that, when underﬁtting, x0 01^b/C3 1is biased for estimating x0 0b. [When overﬁtting, x00^bis unbiased since E(x00^b)¼x00b¼x001b1þx002b2, which is equal to x001b1ifb2¼0.] In the next theorem, we compare the variances of ^b/C3 jand ^bj, where ^b/C3jis from ^b/C3 1 and ^bjis from ^b1. We also compare the variances of x0 01^b/C3 1andx0 0^b. Theorem 7.9c.",
    "om ^b/C3 1 and ^bjis from ^b1. We also compare the variances of x0 01^b/C3 1andx0 0^b. Theorem 7.9c. Let ^b¼(X0X)/C01X0yfrom the full model be partitioned as ^b¼^b1^b2/C18/C19 , and let ^b/C3 1¼(X0 1X1)/C01X01ybe the estimator from the reduced model. Then (i) cov( ^b1)/C0cov( ^b/C3 1)¼s2AB/C01A0, which is a positive deﬁnite matrix, where A¼(X0 1X1)/C01X01X2andB¼X02X2/C0X02X1A. Thus var( ^bj).var( ^b/C3 j). (ii) var( x0 0^b)/C21var(x001^b/C3 1).",
    "1)/C01X01X2andB¼X02X2/C0X02X1A. Thus var( ^bj).var( ^b/C3 j). (ii) var( x0 0^b)/C21var(x001^b/C3 1). PROOF (i) Using X0Xpartitioned to conform to X¼(X1,X2), we have cov( ^b)¼cov^b1 ^b2 ! ¼s2(X0X)/C01¼s2X0 1X1X01X2 X02X1X02X2/C18/C19 /C01 ¼s2G11G12 G21G22/C18/C19 /C01 ¼s2G11G12 G21G22 ! , where Gij¼X0iXjandGijis the corresponding block of the partitioned inverse matrix ( X0X)/C01.T h u sc o v ( ^b1)¼s2G11. By (2.50), G11¼G/C01 11þ G/C01 11G12B/C01G21G/C01 11,w h e r e B¼G22/C0G21G/C01 11G12.",
    "( ^b1)¼s2G11. By (2.50), G11¼G/C01 11þ G/C01 11G12B/C01G21G/C01 11,w h e r e B¼G22/C0G21G/C01 11G12. By (7.81), cov( ^b/C3 1)¼ s2(X0 1X1)/C01¼s2G/C01 11.H e n c e cov( ^b1)/C0cov( ^b/C3 1)¼s2(G11/C0G/C01 11) ¼s2(G/C01 11þG/C01 11G12B/C01G21G/C01 11/C0G/C01 11) ¼s2AB/C01A0:7.9 MODEL MISSPECIFICATION 171 --- Page 185 --- (ii) var(x0 0^b)¼s2x00(X0X)/C01x0 ¼s2(x001,x002)G11G12 G21G22 !",
    "ISSPECIFICATION 171 --- Page 185 --- (ii) var(x0 0^b)¼s2x00(X0X)/C01x0 ¼s2(x001,x002)G11G12 G21G22 ! x01 x02/C18/C19 ¼s2(x001G11x01þx001G12x02þx002G21x01þx002G22x02): Using (2.50), it can be shown that var(x00^b)/C0var(x001^b/C31)¼s2(x02/C0A0x01)0G22(x02/C0A0x01)/C210 because G22is positive deﬁnite. A By Theorem 7.9c(i), var( ^bj) in the full model is greater than var( ^b/C3 j) in the reduced model. Thus underﬁtting reduces the variance of the ^bj’s but introduces bias.",
    "/C3 j) in the reduced model. Thus underﬁtting reduces the variance of the ^bj’s but introduces bias. On the other hand, overﬁtting increases the variance of the ^bj’s. In Theorem 7.9c (ii), var( ^y0) based on the full model is greater than var( ^y01) based on the reduced model. Again, underﬁtting reduces the variance of the estimate of E(y0) but introduces bias. Overﬁtting increases the variance of the estimate of E(y0). We now consider s2for the full model and for the reduced model.",
    "e variance of the estimate of E(y0). We now consider s2for the full model and for the reduced model. For the full model y¼Xbþ1¼X1b1þX2b2þ1, the sample variance s2is given by (7.23) as s2¼(y/C0X^b)0(y/C0X^b) n/C0k/C01: In Theorem 7.3f, we have E(s2)¼s2. The expected value of s2for the reduced model is given in the following theorem. Theorem 7.9d.",
    ")¼s2. The expected value of s2for the reduced model is given in the following theorem. Theorem 7.9d. Ify¼Xbþ1is the correct model, then for the reduced model y¼X1b/C3 1þ1/C3(underﬁtting), where X1isn/C2(pþ1) with p,k, the variance estimator s2 1¼(y/C0X1^b/C31)0(y/C0X1^b/C31) n/C0p/C01(7:84) has expected value E(s21)¼s2þb02X0 2[I/C0X1(X01X1)/C01X01]X2b2 n/C0p/C01: (7:85) PROOF.",
    "0p/C01(7:84) has expected value E(s21)¼s2þb02X0 2[I/C0X1(X01X1)/C01X01]X2b2 n/C0p/C01: (7:85) PROOF. We write the numerator of (7.84) as SSE 1¼y0y/C0^b/C3 1X0 1y¼y0y/C0y0X1(X01X1)/C01X01y ¼y0[I/C0X1(X01X1)/C01X01]y:172 MULTIPLE REGRESSION: ESTIMATION --- Page 186 --- Since E(y)¼Xbby assumption, we have, by Theorem 5.2a, E(SSE 1)¼tr{[I/C0X1(X0 1X1)/C01X01]s2I}þb0X0[I/C0X1(X01X1)/C01X01]Xb ¼(n/C0p/C01)s2þb0 2X0 2[I/C0X1(X01X1)/C01X01]X2b2 (see Problem 7.45).",
    "}þb0X0[I/C0X1(X01X1)/C01X01]Xb ¼(n/C0p/C01)s2þb0 2X0 2[I/C0X1(X01X1)/C01X01]X2b2 (see Problem 7.45). A Since the quadratic form in (7.85) is positive semideﬁnite, s2is biased upward when underﬁtting (see Fig. 7.6). We can also examine (7.85) from the perspective of over- ﬁtting, in which case b2¼0ands2is unbiased. To summarize the results in this section, underﬁtting leads to biased ^bj’s, biased ^y’s, and biased s2. Overﬁtting increases the variances of the ^bj’s and of the ^y’s.",
    "^bj’s, biased ^y’s, and biased s2. Overﬁtting increases the variances of the ^bj’s and of the ^y’s. We are thus compelled to seek an appropriate balance between a biased model and one with large variances. This is the task of the model builder and serves as motivation for seeking an optimum subset of x’s. Example 7.9a. Suppose that the model yi¼b/C3 0þb/C31xiþ1/C3 ihas been ﬁtted when the true model is yi¼b0þb1xiþb2x2iþ1i.",
    "se that the model yi¼b/C3 0þb/C31xiþ1/C3 ihas been ﬁtted when the true model is yi¼b0þb1xiþb2x2iþ1i. (This situation is similar to that illustrated in Figure 6.2.) In this case, ^b/C3 0,^b/C31, and s2 1would be biased by an amount dependent on the choice of the xi’s [see (7.80) and (7.86)].",
    "1, and s2 1would be biased by an amount dependent on the choice of the xi’s [see (7.80) and (7.86)]. The error term ^1/C3 iin the misspeciﬁed model yi¼b/C3 0þb/C31xiþ1/C3 idoes not have a mean of 0: E(1/C3 i)¼E(yi/C0b/C30/C0b/C31xi) ¼E(yi)/C0b/C30/C0b/C31xi ¼b0þb1xiþb2x2i/C0b/C30/C0b/C31xi ¼b0/C0b/C30þ(b1/C0b/C31)xiþb2x2i: AFigure 7.6 Straight-line ﬁt to a curved pattern of points.7.9 MODEL MISSPECIFICATION 173 --- Page 187 --- Example 7.9b.",
    "-line ﬁt to a curved pattern of points.7.9 MODEL MISSPECIFICATION 173 --- Page 187 --- Example 7.9b. Suppose that the true model is yi¼b0þb1xiþ1iand we ﬁt the model yi¼b/C3 1xiþ1/C3 i, as illustrated in Figure 7.7. For the model yi¼b/C3 1xiþ1/C3 i, the least-squares estimator is ^b/C3 1¼Pn i¼1xiyiPni¼1x2 i(7:86) (see Problem 7.46). Then, under the full model yi¼b0þb1xiþ1i,w eh a v e E(^b/C3 1)¼1P ix2 iX ixiE(yi) ¼1P ix2 iX ixi(b0þb1xi) ¼1P ix2ib0X ixiþb1X ix2 i !",
    "b1xiþ1i,w eh a v e E(^b/C3 1)¼1P ix2 iX ixiE(yi) ¼1P ix2 iX ixi(b0þb1xi) ¼1P ix2ib0X ixiþb1X ix2 i ! ¼b0P ixiP ix2 iþb1: (7:87) Thus ^b/C3 1is biased by an amount that depends on b0and the values of the x’s.A 7.10 ORTHOGONALIZATION In Section 7.9, we discussed estimation of b/C3 1in the model y¼X1b/C31þ1/C3when the true model is y¼X1b1þX2b2þ1.",
    "9, we discussed estimation of b/C3 1in the model y¼X1b/C31þ1/C3when the true model is y¼X1b1þX2b2þ1. By Theorem 7.9a, E(^b/C3 1)¼b1þ(X0 1X1)/C01X01X2b2,Figure 7.7 No-intercept model ﬁt to data from an intercept model.174 MULTIPLE REGRESSION: ESTIMATION --- Page 188 --- so that estimation of b1is affected by the presence of X2, unless X0 1X2¼O,i nw h i c h case, E(^b/C3 1)¼b1.",
    "estimation of b1is affected by the presence of X2, unless X0 1X2¼O,i nw h i c h case, E(^b/C3 1)¼b1. In the following theorem, we show that if X0 1X2¼O, the estimators ofb/C3 1andb1not only have the same expected value, but are exactly the same. Theorem 7.10. IfX0 1X2¼O, then the estimator of b1in the full model y¼X1b1þX2b2þ1is the same as the estimator of b/C3 1in the reduced model y¼X1b/C3 1þ1/C3. PROOF. The least-squares estimator of b/C31is^b/C31¼(X0 1X1)/C01X01y.",
    "reduced model y¼X1b/C3 1þ1/C3. PROOF. The least-squares estimator of b/C31is^b/C31¼(X0 1X1)/C01X01y. For the estimator of b1in the full model, we partition ^b¼(X0X)/C01X0yto obtain ^b1^b2/C18/C19 ¼X01X1X01X2 X02X1X02X2/C18/C19/C01X01y X02y/C18/C19 : Using the notation in the proof of Theorem 7.9c, this becomes ^b1 ^b2 ! ¼G11G12 G21G22/C18/C19 /C01X01y X02y/C18/C19 ¼G11G12 G21G22 !",
    "f Theorem 7.9c, this becomes ^b1 ^b2 ! ¼G11G12 G21G22/C18/C19 /C01X01y X02y/C18/C19 ¼G11G12 G21G22 ! X01y X02y/C18/C19 : By (2.50), we obtain ^b1¼G11X0 1yþG12X02y ¼(G/C01 11þG/C01 11G12B/C01G21G/C01 11)X01y/C0G/C01 11G12B/C01X02y, where B¼G22/C0G21G/C01 11G12.I fG12¼X0 1X2¼O, then ^b1reduces to ^b1¼G/C01 11X0 1y¼(X01X1)/C01X01y, which is the same as ^b/C3 1.",
    "I fG12¼X0 1X2¼O, then ^b1reduces to ^b1¼G/C01 11X0 1y¼(X01X1)/C01X01y, which is the same as ^b/C3 1. A Note that Theorem 7.10 will also hold if X1andX2are “essentially orthogonal,” that is, if the centered columns of X1are orthogonal to the centered columns of X2. In Theorem 7.9a, we discussed estimation of b/C31in the presence of b2when X0 1X2=O. We now consider a process of orthogonalization to give additional insights into the meaning of partial regression coefﬁcients.",
    "of orthogonalization to give additional insights into the meaning of partial regression coefﬁcients. In Example 7.2, we illustrated the change in the estimate of a regression coefﬁcient when another xwas added to the model. We now use the same data to further examine this change.The prediction equation obtained in Example 7.2 was ^y¼5:3754 þ3:0118 x1/C01:2855 x2, (7:88)7.10 ORTHOGONALIZATION 175 --- Page 189 --- and the negative partial regressions of yonx2were shown in Figure 7.2.",
    "LIZATION 175 --- Page 189 --- and the negative partial regressions of yonx2were shown in Figure 7.2. By means of orthogonalization, we can give additional meaning to the term 21.2855 x2. In order to addx2to the prediction equation containing only x1, we need to determine how much variation in yis due to x2after the effect of x1has been accounted for, and we must also correct for the relationship between x1andx2.",
    "e effect of x1has been accounted for, and we must also correct for the relationship between x1andx2. Our approach is to consider the relationship between the residual variation after regressing yonx1and the residual variation after regressing x2onx1. We follow a three-step process. 1. Regress yonx1, and calculate residuals [see (7.11)].",
    "sing x2onx1. We follow a three-step process. 1. Regress yonx1, and calculate residuals [see (7.11)]. The prediction equation is ^y¼1:8585 þ1:3019 x1, (7:89) and the residuals yi/C0^yi(x1) are given in Table 7.2, where ^yi(x1) indicates that ^y is based on a regression of yonx1as in (7.89). 2. Regress x2onx1and calculate residuals.",
    "tes that ^y is based on a regression of yonx1as in (7.89). 2. Regress x2onx1and calculate residuals. The prediction equation is ^x2¼2:7358 þ1:3302 x1, (7:90) and the residuals x2i/C0^x2i(x1) are given in Table 7.2, where ^x2i(x1) indicates that x2has been regressed on x1as in (7.90). 3. Now regress y/C0^y(x1)o n x2/C0^x2(x1), which gives dy/C0^y¼/C01:2855( x2/C0^x2): (7:91) There is no intercept in (7.91) because both sets of residuals have a mean of 0.",
    "( x2/C0^x2): (7:91) There is no intercept in (7.91) because both sets of residuals have a mean of 0. TABLE 7.2 Data from Table 7.1 and Residuals yx 1 x2 y/C0^y(x1) x2/C0^x2ðx1Þ 2 0 2 0.1415 20.7358 32 6 21.4623 0.6038 22 7 22.4623 1.6038 7 2 5 2.5377 20.3962 64 9 21.0660 0.9434 8 4 8 0.9340 20.0566 10 4 7 2.9340 21.0566 76 1 0 22.6698 20.7170 86 1 1 21.6698 0.2830 12 6 9 2.3302 21.7170 11 8 15 21.2736 1.6226 14 8 13 1.7264 20.3774176 MULTIPLE REGRESSION: ESTIMATION --- Page 190 --- In (7.91), we obtain a clearer insight into the meaning of the partial regression coef- ﬁcient21.2855 in (7.88).",
    "e obtain a clearer insight into the meaning of the partial regression coef- ﬁcient21.2855 in (7.88). We are using the “unexplained” portion of x2(after x1is accounted for) to predict the “unexplained” portion of y(after x1is accounted for). Since x2/C0^x2(x1) is orthogonal to x1[see Section 7.4.2, in particular (7.29)], ﬁtting y/C0^y(x1)t ox2/C0^x2(x1) yields the same coefﬁcient, 21.2855, as when ﬁtting ytox1 andx2together.",
    "g y/C0^y(x1)t ox2/C0^x2(x1) yields the same coefﬁcient, 21.2855, as when ﬁtting ytox1 andx2together. Thus 21.2855 represents the additional effect of x2beyond the effect ofx1and also after taking into account the overlap between x1andx2in their effect on y. The orthogonality of x1andx2/C0^x2(x1) makes this simpliﬁed breakdown of effects possible.",
    "ect on y. The orthogonality of x1andx2/C0^x2(x1) makes this simpliﬁed breakdown of effects possible. We can substitute ^y(x1) and ^x2(x1) in (7.91) to obtain dy/C0^y¼^y(x1,x2)/C0^y(x1)¼/C01:2855[ x2/C0^x2(x1)], or ^y/C0(1:8585 þ1:3019 x1)¼/C01:2855[ x2/C0(2:7358 þ1:3302 x1)], (7:92) which reduces to ^y¼5:3754 þ3:0118 x1/C01:2855 x2, (7:93) the same as (7.88). If we regress y(rather than y/C0^y)o n x2/C0^x2(x1), we will still obtain /C01:2855 x2, but we will not have 5.3754 þ3.0118 x1.",
    "y/C0^y)o n x2/C0^x2(x1), we will still obtain /C01:2855 x2, but we will not have 5.3754 þ3.0118 x1. The correlation between the residuals y/C0^y(x1) and x2/C0^x2(x1) is the same as the (sample) partial correlation of yandx2with x1held ﬁxed: ry2/C11¼ry/C0^y,x2/C0^x2: (7:94) This is discussed further in Section 10.8.",
    "yandx2with x1held ﬁxed: ry2/C11¼ry/C0^y,x2/C0^x2: (7:94) This is discussed further in Section 10.8. We now consider the general case with full model y¼X1b1þX2b2þ1 and reduced model y¼X1b/C3 1þ1/C3: We use an orthogonalization approach to obtain an estimator of b2, following the same three steps as in the illustration with x1andx2above: 1. Regress yon X1and calculate residuals y/C0^y(X1), where ^y(X1)¼X1^b/C3 1¼X1(X0 1X1)/C01X01y[see (7.11)]. 2.",
    "yon X1and calculate residuals y/C0^y(X1), where ^y(X1)¼X1^b/C3 1¼X1(X0 1X1)/C01X01y[see (7.11)]. 2. Regress the columns of X2onX1and obtain residuals X2/C11¼X2/C0^X2(X1). If X2is written in terms of its columns as X2¼(x21,...,x2j,...,x2p), then the7.10 ORTHOGONALIZATION 177 --- Page 191 --- regression coefﬁcient vector for x2jonX1isbj¼(X0 1X1)/C01X01x2j, and ^x2j¼X1bj¼X1(X0 1X1)/C01X01x2j.",
    "regression coefﬁcient vector for x2jonX1isbj¼(X0 1X1)/C01X01x2j, and ^x2j¼X1bj¼X1(X0 1X1)/C01X01x2j. For all columns of X2, this becomes ^X2(X1)¼X1(X01X1)/C01X01X2¼X1A, where A¼(X01X1)/C01X01X2is the alias matrix deﬁned in (7.80). Note that X2/C11¼X2/C0^X2(X1) is orthogonal to X1: X0 1X2/C11¼O: (7:95) Using the alias matrix A, the residual matrix can be expressed as X2/C11¼X2/C0^X2(X1)( 7 :96) ¼X2/C0X1(X01X1)/C01X01X2¼X2/C0X1A: (7:97) 3. Regress y/C0^y(X1)o nX2/C11¼X2/C0^X2(X1).",
    "2(X1)( 7 :96) ¼X2/C0X1(X01X1)/C01X01X2¼X2/C0X1A: (7:97) 3. Regress y/C0^y(X1)o nX2/C11¼X2/C0^X2(X1). Since X2/C11is orthogonal to X1,w e obtain the same ^b2as in the full model ^y¼X1^b1þX2^b2.",
    "^X2(X1). Since X2/C11is orthogonal to X1,w e obtain the same ^b2as in the full model ^y¼X1^b1þX2^b2. Adapting the nota- tion of (7.91) and (7.92), this can be expressed as ^y(X1,X2)/C0^y(X1)¼X2/C11^b2: (7:98) If we substitute ^y(X1)¼X1^b/C3 1andX2/C11¼X2/C0X1Ainto (7.98) and use ^b/C3 1¼^b1þA^b2from (7.80), we obtain ^y(X1,X2)¼X1^b/C3 1þ(X2/C0X1A)^b2 ¼X1(^b1þA^b2)þ(X2/C0X1A)^b2 ¼X1^b1þX2^b2 which is analogous to (7.93).",
    "1,X2)¼X1^b/C3 1þ(X2/C0X1A)^b2 ¼X1(^b1þA^b2)þ(X2/C0X1A)^b2 ¼X1^b1þX2^b2 which is analogous to (7.93). This conﬁrms that the orthogonality of X1andX2/C11 leads to the estimator ^b2in (7.98). For a formal proof, see Problem 7.50. PROBLEMS 7.1 Show thatPn i¼1(yi/C0x0 i^b)2¼(y/C0X^b)0(y/C0X^b), thus verifying (7.7). 7.2 Show that (7.10) follows from (7.9). Why is X0Xpositive deﬁnite, as noted below (7.10)?",
    "g (7.7). 7.2 Show that (7.10) follows from (7.9). Why is X0Xpositive deﬁnite, as noted below (7.10)? 7.3 Show that ^b0and ^b1in (7.12) in Example 7.3.1 are the same as in (6.5) and (6.6).178 MULTIPLE REGRESSION: ESTIMATION --- Page 192 --- 7.4 Obtain cov( ^b) in (7.16) from (7.15). 7.5 Show that var( ^b0)¼s2(P ix2 i=n)=P i(xi/C0/C22x)2in (7.16) in Example 7.3.2a is the same as var( ^b0) in (6.10).",
    "( ^b0)¼s2(P ix2 i=n)=P i(xi/C0/C22x)2in (7.16) in Example 7.3.2a is the same as var( ^b0) in (6.10). 7.6 Show that AA0can be expressed as AA0¼[A/C0(X0X)/C01X0] [A/C0(X0X)/C01X0]0þ(X0X)/C01as in (7.17) in Theorem 7.3d. 7.7 Prove Corollary 1 to Theorem 7.3d in the following two ways: (a) Use an approach similar to the proof of Theorem 7.3d. (b) Use the method of Lagrange multipliers (Section 2.14.3).",
    "h similar to the proof of Theorem 7.3d. (b) Use the method of Lagrange multipliers (Section 2.14.3). 7.8 Show that if the x’s are rescaled as zj¼cjxj,j¼1,2,...,k,t h e n ^bz¼D/C01^b, as in (7.18) in the proof of the Theorem 7.3e. 7.9 Verify (7.20) and (7.21) in the proof of Corollary 1 to Theorem 7.3e. 7.10 Show that s2is invariant to changes of scale on the x’s, as noted following Corollary 1 to Theorem 7.3e. 7.11 Show that ( y/C0X^b)0(y/C0X^b)¼y0y/C0^b0X0yas in (7.24).",
    "following Corollary 1 to Theorem 7.3e. 7.11 Show that ( y/C0X^b)0(y/C0X^b)¼y0y/C0^b0X0yas in (7.24). 7.12 Show that E(SSE) ¼s2(n/C0k/C01), as in Theorem 7.3f, using the following approach. Show that SSE ¼y0y/C0^b0X0X^b. Show that E(y0y)¼ns2þb0X0Xband that E(^b0X0X^b)¼(kþ1)s2þb0X0Xb. 7.13 Prove that an additional xreduces SSE, as noted following Theorem 7.3f. 7.14 Show that the noncentered model preceding (7.30) can be written in the cen- tered form in (7.30), with adeﬁned as in (7.31).",
    "d model preceding (7.30) can be written in the cen- tered form in (7.30), with adeﬁned as in (7.31). 7.15 Show that Xc¼[I/C0(1=n)J]X1as in (7.33), where X1is as given in (7.19). 7.16 Show that j0Xc¼00, as in (7.35), where Xcis the centered Xmatrix deﬁned in (7.33). 7.17 Show that the estimators ^a¼/C22yand ^b1¼(X0 cXc)/C01X0cyin (7.36) and (7.37) are the same as ^b¼(X0X)/C01X0yin (7.6). Use the following two methods: (a) Work with the normal equations in both cases.",
    "0X)/C01X0yin (7.6). Use the following two methods: (a) Work with the normal equations in both cases. (b) Use the inverse of X0X in partitioned form: (X0X)/C01¼[(j,X1)0(j,X1)]/C01. 7.18 Show that the ﬁtted regression plane ^y¼^aþ^b1(x1/C0/C22x1)þ/C1/C1/C1þ ^bk(xk/C0/C22xk) passes through the point ( /C22x1,/C22x2,...,/C22xk,/C22y), as noted below (7.38).",
    "^bk(xk/C0/C22xk) passes through the point ( /C22x1,/C22x2,...,/C22xk,/C22y), as noted below (7.38). 7.19 Show that SSE ¼P i(yi/C0/C22y)2/C0^b0 1X0 cyin (7.39) is the same as SSE¼y0y/C0^b0X0yin (7.24).PROBLEMS 179 --- Page 193 --- 7.20 (a) Show that Sxx¼X0 cXc=(n/C01) as in (7.44). (b) Show that syx¼X0cy=(n/C01) as in (7.45). 7.21 (a) Show that if yisNn(Xb,s2I), the likelihood function is L(b,s2)¼1 (2ps2)n=2e/C0(y/C0Xb)0(y/C0Xb)=2s2, as in (7.50) in the proof of Theorem 7.6a.",
    "function is L(b,s2)¼1 (2ps2)n=2e/C0(y/C0Xb)0(y/C0Xb)=2s2, as in (7.50) in the proof of Theorem 7.6a. (b) Differentiate lnL(b,s2) in (7.51) with respect to bto obtain ^b¼(X0X)/C01X0yin (7.48). (c) Differentiate lnL(b,s2) with respect to s2to obtain ^s2¼(y/C0X^b)0(y/C0 X^b)=nas in (7.49). 7.22 Prove parts (ii) and (iii) of Theorem 7.6b. 7.23 Show that ( y/C0Xb)0(y/C0Xb)¼(y/C0X^b)0(y/C0X^b)þ(^b/C0b)0X0X(^b/C0b) as in (7.52) in the proof of Theorem 7.6c.",
    "y/C0Xb)0(y/C0Xb)¼(y/C0X^b)0(y/C0X^b)þ(^b/C0b)0X0X(^b/C0b) as in (7.52) in the proof of Theorem 7.6c. 7.24 Explain why f(y;b,s2) does not factor into g1(^b,b)g2(^s2,s2)h(y), as noted following Theorem 7.6c. 7.25 Verify the equivalence of (7.55) and (7.56); that is, show that ^b0X0y/C0n/C22y2¼^b0 1X0 cXc^b1. 7.26 Verify the comments in property 1 in Section 7.7, namely, that if ^b1¼^b2¼/C1/C1/C1¼ ^bk¼0, then R2¼0, and if yi¼^yi,i¼1,2,...,n, then R2¼1.",
    "tion 7.7, namely, that if ^b1¼^b2¼/C1/C1/C1¼ ^bk¼0, then R2¼0, and if yi¼^yi,i¼1,2,...,n, then R2¼1. 7.27 Show that adding an xto the model increases (cannot decrease) the value of R2, as in property 3 in Section 7.7. 7.28 (a) Verify that R2is invariant to full-rank linear transformations on the x’s as in property 6 in Section 7.7. (b) Show that R2is invariant to a scale change z¼cyony. 7.29 (a) Show that R2in (7.55) can be written in the form R2¼1/C0SSE =P i(yi/C0/C22y)2.",
    "nge z¼cyony. 7.29 (a) Show that R2in (7.55) can be written in the form R2¼1/C0SSE =P i(yi/C0/C22y)2. (b) Replace SSE andP i(yi/C0/C22y)2in part (a) by variance estimators SSE =(n/C0k/C01) andP i(yi/C0/C22y)2=(n/C01) and show that the result is the same as R2 ain (7.56). 7.30 Show thatPn i¼1^yi=n¼Pni¼1yi=n, as noted following (7.59) in Section 7.7.",
    "ame as R2 ain (7.56). 7.30 Show thatPn i¼1^yi=n¼Pni¼1yi=n, as noted following (7.59) in Section 7.7. 7.31 Show that cosu¼Ras in (7.61), where R2is as given by (7.56).180 MULTIPLE REGRESSION: ESTIMATION --- Page 194 --- 7.32 (a) Show that E(^b)¼b, where ^b¼(X0V/C01X)/C01X0V/C01yas in (7.63). (b) Show that cov( ^b)¼s2(X0V/C01X)/C01as in (7.64). 7.33 (a) Show that the two forms of s2in (7.65) and (7.66) are equal. (b) Show that E(s2)¼s2, where s2is as given by (7.66).",
    "wo forms of s2in (7.65) and (7.66) are equal. (b) Show that E(s2)¼s2, where s2is as given by (7.66). 7.34 Complete the steps in the proof of Theorem 7.8b. 7.35 Show that for V¼(1/C0r)IþrJin (7.67), the inverse is given by V/C01¼a(I/C0brJ) as in (7.68), where a¼1=(1/C0r) and b¼1= [1þ(n/C01)r]. 7.36 (a) Show that X0V/C01X¼bn 00 0aX0 cXc/C18/C19 as in (7.69). (b) Show that X0V/C01y¼bn/C22y aX0cy/C18/C19 as in (7.70).",
    "C01X¼bn 00 0aX0 cXc/C18/C19 as in (7.69). (b) Show that X0V/C01y¼bn/C22y aX0cy/C18/C19 as in (7.70). 7.37 Show that cov( ^b/C3)¼s2(X0X)/C01X0VX(X0X)/C01as in (7.72), where ^b/C3¼(X0X)/C01X0yand cov( y)¼s2V. 7.38 (a) Show that the weighted least-squares estimator ^b¼(^b0,^b1)0for the model yi¼b0þb1xiþ1iwith var( yi)¼s2xihas the form given in (7.73). (b) Verify the expression for cov( ^b) in (7.74). 7.39 Obtain the expression for cov( ^b/C3) in (7.75).",
    "Verify the expression for cov( ^b) in (7.74). 7.39 Obtain the expression for cov( ^b/C3) in (7.75). 7.40 As an alternative derivation of var( ^b/C3 1) in (7.76), use the following two steps to ﬁnd var( ^b/C31) using ^b/C31¼P i(xi/C0/C22x)yi=P i(xi/C0/C22x)2from the answer to Problem 6.2: (a) Using var( yi)¼s2xi, show that var( ^b/C3 1)¼s2P i(xi/C0/C22x)2xi=/C2P i(xi/C0/C22x)2/C32. (b) Show that this expression for var( ^b/C3 1) is equal to that in (7.76).",
    "/C2P i(xi/C0/C22x)2/C32. (b) Show that this expression for var( ^b/C3 1) is equal to that in (7.76). 7.41 Using x¼2,3,5,7,8,10, compare var( ^b/C31) in (7.76) with var( ^b1) in (7.77). 7.42 Provide an alternative proof of cov( ^b/C3 1)¼s2(X0 1X1)/C01in (7.81) using the deﬁnition in (3.24), cov( ^b/C3 1)¼E{[^b/C31/C0E(^b/C31)][^b/C31/C0E(^b/C31)]0}. 7.43 Prove Theorem 7.9b. 7.44 Provide the missing steps in the proof of Theorem 7.9c(ii).",
    "b/C31)]0}. 7.43 Prove Theorem 7.9b. 7.44 Provide the missing steps in the proof of Theorem 7.9c(ii). 7.45 Show that x01^b/C3 1is biased for estimating x01b1ifb2=0andX0 1X2=O. 7.46 Show that var( x01^b1)/C21var(x01^b/C3 1). 7.47 Complete the steps in the proof of Theorem 7.9d.PROBLEMS 181 --- Page 195 --- 7.48 Show that for the no-intercept model yi¼b/C3 1xiþ1/C3 i, the least-squares estima- tor is ^b/C3 1¼P ixiyi=P ix2 ias in (7.86).",
    "cept model yi¼b/C3 1xiþ1/C3 i, the least-squares estima- tor is ^b/C3 1¼P ixiyi=P ix2 ias in (7.86). 7.49 Obtain E(^b/C3 1)¼b0P ixi=P ix2 iþb1 in (7.87) using (7.80), E(^b/C3 1)¼b1þAb2. 7.50 Suppose that we use the model yi¼b/C3 0þb/C31xiþ1/C3 iwhen the true model is yi¼b0þb1xiþb2x2iþb3x3iþ1i. (a) Using (7.80), ﬁnd E(^b/C3 0) and E(^b/C31) if observations are taken at x¼/C03,/C02,/C01,0,1,2,3. (b) Using (7.85), ﬁnd E(s2 1) for the same values of x.",
    "tions are taken at x¼/C03,/C02,/C01,0,1,2,3. (b) Using (7.85), ﬁnd E(s2 1) for the same values of x. 7.51 Show that X2/C11¼X2/C0^X2(X1) is orthogonal to X1, that is, X0 1X2/C11¼O,a si n (7.95). 7.52 Show that ^b2in (7.98) is the same as in the full ﬁtted model ^y¼X1^b1þX2^b2. 7.53 When gasoline is pumped into the tank of a car, vapors are vented into the atmosphere.",
    "1þX2^b2. 7.53 When gasoline is pumped into the tank of a car, vapors are vented into the atmosphere. An experiment was conducted to determine whether y, the amount of vapor, can be predicted using the following four variables based on initial conditions of the tank and the dispensed gasoline: x1¼tank temperature ( 8F) x2¼gasoline temperature ( 8F) x3¼vapor pressure in tank ( psi) x4¼vapor pressure of gasoline ( psi) The data are given in Table 7.3 (Weisberg 1985, p. 138). (a) Find ^bands2.",
    "essure of gasoline ( psi) The data are given in Table 7.3 (Weisberg 1985, p. 138). (a) Find ^bands2. (b) Find an estimate of cov( ^b). (c) Find ^b1and ^b0using Sxxandsyxas in (7.46) and (7.47). (d) Find R2andR2 a.",
    "estimate of cov( ^b). (c) Find ^b1and ^b0using Sxxandsyxas in (7.46) and (7.47). (d) Find R2andR2 a. 7.54 In an effort to obtain maximum yield in a chemical reaction, the values of the following variables were chosen by the experimenter: x1¼temperature ( 8C) x2¼concentration of a reagent (%) x3¼time of reaction (hours) Two different response variables were observed: y1¼percent of unchanged starting material y2¼percent converted to the desired product182 MULTIPLE REGRESSION: ESTIMATION --- Page 196 --- The data are listed in Table 7.4 (Box and Youle 1955, Andrews and Herzberg 1985, p.",
    "--- Page 196 --- The data are listed in Table 7.4 (Box and Youle 1955, Andrews and Herzberg 1985, p. 188). Carry out the following for y1: (a) Find ^bands2.",
    "nd Youle 1955, Andrews and Herzberg 1985, p. 188). Carry out the following for y1: (a) Find ^bands2. (b) Find an estimate of cov( ^b).TABLE 7.3 Gas Vapor Data yx 1 x2 x3 x4 yx 1 x2 x3 x4 29 33 53 3.32 3.42 40 90 64 7.32 6.70 24 31 36 3.10 3.26 46 90 60 7.32 7.20 26 33 51 3.18 3.18 55 92 92 7.45 7.45 22 37 51 3.39 3.08 52 91 92 7.27 7.26 27 36 54 3.20 3.41 29 61 62 3.91 4.0821 35 35 3.03 3.03 22 59 42 3.75 3.4533 59 56 4.78 4.57 31 88 65 6.48 5.8034 60 60 4.72 4.72 45 91 89 6.70 6.6032 59 60 4.60 4.41 37 63 62 4.30 4.3034 60 60 4.53 4.53 37 60 61 4.02 4.1020 34 35 2.90 2.95 33 60 62 4.02 3.8936 60 59 4.40 4.36 27 59 62 3.98 4.0234 60 62 4.31 4.42 34 59 62 4.39 4.5323 60 36 4.27 3.94 19 37 35 2.75 2.64 24 62 38 4.41 3.49 16 35 35 2.59 2.5932 62 61 4.39 4.39 22 37 37 2.73 2.59 TABLE 7.4 Chemical Reaction Data y1 y2 x1 x2 x3 41.5 45.9 162 23 3 33.8 53.3 162 23 827.7 57.5 162 30 521.7 58.8 162 30 819.9 60.6 172 25 5 15.0 58.0 172 25 812.2 58.6 172 30 5 4.3 52.4 172 30 8 19.3 56.9 167 27.5 6.5 6.4 55.4 177 27.5 6.5 37.6 46.9 157 27.5 6.518.0 57.3 167 32.5 6.526.3 55.0 167 22.5 6.5 9.9 58.9 167 27.5 9.5 25.0 50.3 167 27.5 3.5 14.1 61.1 177 20 6.515.2 62.9 177 20 6.515.9 60.0 160 34 7.519.6 60.6 160 34 7.5PROBLEMS 183 --- Page 197 --- (c) Find R2andR2 a.",
    "177 20 6.515.9 60.0 160 34 7.519.6 60.6 160 34 7.5PROBLEMS 183 --- Page 197 --- (c) Find R2andR2 a. (d) In order to ﬁnd the maximum yield for y1, a second-order model is of interest. Find ^bands2for the model y1¼b0þb1x1þb2x2þb3x3þ b4x21þb5x22þb6x23þb7x1x2þb8x1x3þb9x2x3þ1. (e) Find R2andR2afor the second-order model.",
    "x1þb2x2þb3x3þ b4x21þb5x22þb6x23þb7x1x2þb8x1x3þb9x2x3þ1. (e) Find R2andR2afor the second-order model. 7.55 The following variables were recorded for several counties in Minnesota in 1977: y¼average rent paid per acre of land with alfalfa x1¼average rent paid per acre for all land x2¼average number of dairy cows per square mile x3¼proportion of farmland in pasture The data for 34 counties are given in Table 7.5 (Weisberg 1985, p. 162).",
    "tion of farmland in pasture The data for 34 counties are given in Table 7.5 (Weisberg 1985, p. 162). Can rent for alfalfa land be predicted from the other three variables? (a) Find ^bands2. (b) Find ^b1and ^b0using Sxxandsyxas in (7.46) and (7.47).",
    "other three variables? (a) Find ^bands2. (b) Find ^b1and ^b0using Sxxandsyxas in (7.46) and (7.47). (c) Find R2andR2 a.TABLE 7.5 Land Rent Data yx 1 x2 x3 yx 1 x2 x3 18.38 15.50 17.25 .24 8.50 9.00 8.89 .08 20.00 22.29 18.51 .20 36.50 20.64 23.81 .24 11.50 12.36 11.13 .12 60.00 81.40 4.54 .05 25.00 31.84 5.54 .12 16.25 18.92 29.62 .72 52.50 83.90 5.44 .04 50.00 50.32 21.36 .1982.50 72.25 20.37 .05 11.50 21.33 1.53 .1025.00 27.14 31.20 .27 35.00 46.85 5.42 .0830.67 40.41 4.29 .10 75.00 65.94 22.10 .0912.00 12.42 8.69 .41 31.56 38.68 14.55 .1761.25 69.42 6.63 .04 48.50 51.19 7.59 .1360.00 48.46 27.40 .12 77.50 59.42 49.86 .1357.50 69.00 31.23 .08 21.67 24.64 11.46 .2131.00 26.09 28.50 .21 19.75 26.94 2.48 .1060.00 62.83 29.98 .17 56.00 46.20 31.62 .26 72.50 77.06 13.59 .05 25.00 26.86 53.73 .4360.33 58.83 45.46 .16 40.00 20.00 40.18 .5649.75 59.48 35.90 .32 56.67 62.52 15.89 .05184 MULTIPLE REGRESSION: ESTIMATION --- Page 198 --- 8Multiple Regression: Tests of Hypotheses and ConﬁdenceIntervals In this chapter we consider hypothesis tests and conﬁdence intervals for the parameters b0,b1,...,bkinbin the model y¼Xbþ1.",
    "ider hypothesis tests and conﬁdence intervals for the parameters b0,b1,...,bkinbin the model y¼Xbþ1. We also provide a conﬁ- dence interval for s2¼var(yi). We will assume throughout the chapter that yis Nn(Xb,s2I), where Xisn/C2(kþ1) of rank kþ1,n. 8.1 TEST OF OVERALL REGRESSION We noted in Section 7.9 that the problems associated with both overﬁtting and under- ﬁtting motivate us to seek an optimal model.",
    "the problems associated with both overﬁtting and under- ﬁtting motivate us to seek an optimal model. Hypothesis testing is a formal tool for, among other things, choosing between a reduced model and an associated full model. The hypothesis H0, expresses the reduced model in terms of values of a subset of the bj’s inb. The alternative hypothesis, H1, is associated with the full model.",
    "lues of a subset of the bj’s inb. The alternative hypothesis, H1, is associated with the full model. To illustrate this tool we begin with a common test, the test of the overall regression hypothesis that none of the xvariables predict y. This hypothesis (leading to the reduced model) can be expressed as H0:b1¼0, where b1¼(b1,b2,...,bk)0.",
    "his hypothesis (leading to the reduced model) can be expressed as H0:b1¼0, where b1¼(b1,b2,...,bk)0. Note that we wish to test H0:b1¼0, not H0:b¼0, where b¼b0 b1/C18/C19 : Sinceb0is usually not zero, we would rarely be interested in including b0¼0 in the hypothesis. Rejection of H0:b¼0might be due solely to b0, and we would not learn whether the xvariables predict y. For a test of H0:b¼0, see Problem 8.6.",
    "b0, and we would not learn whether the xvariables predict y. For a test of H0:b¼0, see Problem 8.6. We proceed by proposing a test statistic that is distributed as a central FifH0is true and as a noncentral Fotherwise. Our approach to obtaining a test statistic is somewhat Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc.",
    ",Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 185 --- Page 199 --- simpliﬁed if we use the centered model (7.32) y¼(j,Xc)a b1/C18/C19 þ1, where Xc¼[I/C0(1=n)J]X1is the centered matrix [see (7.33)] and X1contains all the columns of Xexcept the ﬁrst [see (7.19)].",
    "is the centered matrix [see (7.33)] and X1contains all the columns of Xexcept the ﬁrst [see (7.19)]. The corrected total sum of squares SST¼Pn i¼1(yi/C0/C22y)2can be partitioned as Xn i¼1(yi/C0/C22y)2¼^b0 1X0cyþXn i¼1(yi/C0/C22y)2/C0^b0 1X0cyhi [by (7 :53)] ¼^b01X0cXc^b1þSSE¼SSRþSSE [by (7 :54)], (8:1) where SSE is as given in (7.39). The regression sum of squares SSR ¼^b0 1X0 cXc^b1is clearly due to b1.",
    "re SSE is as given in (7.39). The regression sum of squares SSR ¼^b0 1X0 cXc^b1is clearly due to b1. In order to construct an Ftest, we ﬁrst express the sums of squares in (8.1) as quad- ratic forms in yso that we can use theorems from Chapter 5 to show that SSR and SSE have chi-square distributions and are independent.",
    "theorems from Chapter 5 to show that SSR and SSE have chi-square distributions and are independent. UsingP i(yi/C0/C22y)2¼ y0[I/C0(1=n)J]yin (5.2), ^b1¼(X0cXc)/C01X0cyin (7.37), and SSE ¼Pn i¼1(yi/C0/C22y)2/C0 ^b0 1X0 cyin (7.39), we can write (8.1) as y0I/C01 nJ/C18/C19 y¼SSRþSSE ¼y0Xc(X0 cXc)/C01X0cyþy0I/C01 nJ/C18/C19 y/C0y0Xc(X0 cXc)/C01X0cy ¼y0Hcyþy0I/C01 nJ/C0Hc/C18/C19 y, (8:2) where Hc¼Xc(X0 cXc)/C01X0c.",
    "C18/C19 y/C0y0Xc(X0 cXc)/C01X0cy ¼y0Hcyþy0I/C01 nJ/C0Hc/C18/C19 y, (8:2) where Hc¼Xc(X0 cXc)/C01X0c. In the following theorem we establish some properties of the three matrices of the quadratic forms in (8.2). Theorem 8.1a. The matrices I/C0(1=n)J,Hc¼Xc(X0 cXc)/C01X0c,a n dI/C0(1=n)J/C0Hc have the following properties: (i)Hc[I/C0(1=n)J]¼Hc: (8:3) (ii)Hcis idempotent of rank k. (iii)I/C0(1=n)J/C0Hcis idempotent of rank n/C0k/C01. (iv)Hc[I/C0(1=n)J/C0Hc]¼O: (8:4) PROOF.",
    "rank k. (iii)I/C0(1=n)J/C0Hcis idempotent of rank n/C0k/C01. (iv)Hc[I/C0(1=n)J/C0Hc]¼O: (8:4) PROOF. Part (i) follows from X0cj¼0, which was established in Problem 7.16. Part (ii) can be shown by direct multiplication. Parts (iii) and (iv) follow from (i) and (ii). A186 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 200 --- The distributions of SSR =s2and SSE =s2are given in the following theorem. Theorem 8.1b.",
    "age 200 --- The distributions of SSR =s2and SSE =s2are given in the following theorem. Theorem 8.1b. IfyisNn(Xb,s2I), then SSR =s2¼^b0 1X0 cXc^b1=s2and SSE=s2¼Pn i¼1(yi/C0/C22y)2/C0^b0 1X0 cXc^b1hi =s2have the following distributions: (i) SSR =s2isx2(k,l1), wherel1¼m0Am=2s2¼b0 1X0 cXcb1=2s2. (ii) SSE =s2isx2(n/C0k/C01). PROOF. These results follow from (8.2), Theorem 8.1a(ii) and (iii), and Corollary 2 to Theorem 5.5. A The independence of SSR and SSE is demonstrated in the following theorem.",
    "ollary 2 to Theorem 5.5. A The independence of SSR and SSE is demonstrated in the following theorem. Theorem 8.1c. IfyisNn(Xb,s2I), then SSR and SSE are independent, where SSR and SSE are deﬁned in (8.1) and (8.2). PROOF. This follows from Theorem 8.1a(iv) and Corollary 1 to Theorem 5.6b. A We can now establish an Ftest for H0:b1¼0versus H1:b1=0. Theorem 8.1d.",
    "orollary 1 to Theorem 5.6b. A We can now establish an Ftest for H0:b1¼0versus H1:b1=0. Theorem 8.1d. IfyisNn(Xb,s2I), the distribution of F¼SSR=(ks2) SSE=[(n/C0k/C01)s2]¼SSR=k SSE=(n/C0k/C01)(8:5) is as follows: (i) If H0:b1¼0is false, then Fis distributed as F(k,n/C0k/C01,l1), wherel1¼b0 1X0 cXcb1=2s2. (ii) If H0:b1¼0is true, then l1¼0 and Fis distributed as F(k,n/C0k/C01): PROOF (i) This result follows from (5.30) and Theorems 8.1b and 8.1c.",
    "distributed as F(k,n/C0k/C01): PROOF (i) This result follows from (5.30) and Theorems 8.1b and 8.1c. (ii) This result follows from (5.28) and Theorems 8.1b and 8.1c. A Note that l1¼0 if and only if b1¼0, since X0cXcis positive deﬁnite (see Corollary 1 to Theorem 2.6b).8.1 TEST OF OVERALL REGRESSION 187 --- Page 201 --- The test for H0:b1¼0is carried out as follows. Reject H0ifF/C21Fa,k,n/C0k/C01, where Fa,k,n/C0k/C01is the upper apercentage point of the (central) Fdistribution.",
    "21Fa,k,n/C0k/C01, where Fa,k,n/C0k/C01is the upper apercentage point of the (central) Fdistribution. Alternatively, a pvalue can be used to carry out the test. A pvalue is the tail area of the central Fdistribution beyond the calculated Fvalue, that is, the probability of exceeding the calculated Fvalue, assuming H0:b1¼0to be true. A pvalue less than ais equivalent to F>Fa,k,n/C0k/C01.",
    "alculated Fvalue, assuming H0:b1¼0to be true. A pvalue less than ais equivalent to F>Fa,k,n/C0k/C01. The analysis-of-variance (ANOVA) table (Table 8.1) summarizes the results and calculations leading to the overall Ftest. Mean squares are sums of squares divided by the degrees of freedom of the associated chi-square ( x2) distributions. The entries in the column for expected mean squares in Table 8.1 are simply E(SSR =k) and E[SSE =(n/C0k/C01)].",
    "s in the column for expected mean squares in Table 8.1 are simply E(SSR =k) and E[SSE =(n/C0k/C01)]. The ﬁrst of these can be established by Theorem 5.2a or by (5.20). The second was established by Theorem 7.3f. IfH0:b1¼0is true, both of the expected mean squares in Table 8.1 are equal to s2, and we expect Fto be near 1. If b1=0,t h e n E(SSR =k).s2since X0 cXcis posi- tive deﬁnite, and we expect Fto exceed 1. We therefore reject H0for large values of F.",
    "0 cXcis posi- tive deﬁnite, and we expect Fto exceed 1. We therefore reject H0for large values of F. The test of H0:b1¼0in Table 8.1 has been developed using the centered model (7.32). We can also express SSR and SSE in terms of the noncentered model y¼Xbþ1in (7.4): SSR ¼^b0X0y/C0n/C22y2,SSE¼y0y/C0^b0X0y: (8:6) These are the same as SSR and SSE in (8.1) [see (7.24), (7.39), (7.54), and Problems 7.19, 7.25]. Example 8.1.",
    "the same as SSR and SSE in (8.1) [see (7.24), (7.39), (7.54), and Problems 7.19, 7.25]. Example 8.1. Using the data in Table 7.1, we illustrate the test of H0:b1¼0where, in this case, b1¼(b1,b2)0. In Example 7.3.1(a), we found X0y¼(90,482,872)0and ^b¼(5:3754 ,3:0118 ,/C01:2855)0.",
    "se, b1¼(b1,b2)0. In Example 7.3.1(a), we found X0y¼(90,482,872)0and ^b¼(5:3754 ,3:0118 ,/C01:2855)0. The quantities y0y,^b0X0y, and n/C22y2are given by y0y¼X12 i¼1y2 i¼22þ32þ/C1/C1/C1þ 142¼840, ^b0X0y¼(5:3754 ,3:0118 ,/C01:2855)90 482 8720 B@1 CA¼814:5410 ,TABLE 8.1 ANOVA Table for the FTest of H0:b1¼0 Source of Variation df Sum of Squares Mean SquareExpected Mean Square Due tob1kSSR ¼^b0 1X0 cy¼^b0X0y/C0n/C22y2SSR /k s2þ1 kb0 1X0 cXcb1 Error n/C0k/C01SSE¼X iðyi/C0/C22yÞ2/C0^b0 1X0cySSE=(n/C0k/C01)s2 ¼y0y/C0^b0X0y Total n 21 SST ¼P i(yi/C0/C22y)2188 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 202 --- n/C22y2¼nP iyi n/C18/C192 ¼1290 12/C18/C192 ¼675: Thus, by (8.6), we obtain SSR ¼^b0X0y/C0n/C22y2¼139:5410 , SSE¼y0y/C0^b0X0y¼25:4590 , Xn i¼1(yi/C0/C22y)2¼y0y/C0n/C22y2¼165: TheFtest is given in Table 8.2.",
    ", SSE¼y0y/C0^b0X0y¼25:4590 , Xn i¼1(yi/C0/C22y)2¼y0y/C0n/C22y2¼165: TheFtest is given in Table 8.2. Since 24 :665.F:05,2,9¼4:26, we reject H0:b1¼0 and conclude that at least one of b1orb2is not zero. The pvalue is .000223. A 8.2 TEST ON A SUBSET OF THE b’S In more generality, suppose that we wish to test the hypothesis that a subset of the x’s is not useful in predicting y. A simple example is H0:bj¼0 for a single bj.I fH0is rejected, we would retain bjxjin the model.",
    "g y. A simple example is H0:bj¼0 for a single bj.I fH0is rejected, we would retain bjxjin the model. As another illustration, consider the model in (7.2) y¼b0þb1x1þb2x2þb3x2 1þb4x22þb5x1x2þ1, for which we may wish to test the hypothesis H0:b3¼b4¼b5¼0. If H0is rejected, we would choose the full second-order model over the reduced ﬁrst-order model. Without loss of generality, we assume that the b’s to be tested have been arranged last inb, with a corresponding arrangement of the columns of X.",
    "b’s to be tested have been arranged last inb, with a corresponding arrangement of the columns of X. ThenbandXcan be partitioned accordingly, and by (7.78), the model for all nobservations becomes y¼Xbþ1¼(X1,X2)b1 b2/C18/C19 þ1 ¼X1b1þX2b2þ1, (8:7)TABLE 8.2 ANOVA for Overall Regression Test for Data in Table 7.1 Source df SS MS F Due tob1 2 139.5410 69.7705 24.665 Error 9 25.4590 2.8288 Total 11 165.00008.2 TEST ON A SUBSET OF THE b’S 189 --- Page 203 --- whereb2contains the b’s to be tested.",
    "1 165.00008.2 TEST ON A SUBSET OF THE b’S 189 --- Page 203 --- whereb2contains the b’s to be tested. The intercept b0would ordinarily be included in b1. The hypothesis of interest is H0:b2¼0. If we designate the number of par- ameters in b2byh, then X2isn/C2h,b1is (k/C0hþ1)/C21, and X1is n/C2(k/C0hþ1). Thus b1¼(b0,b1,/C1/C1/C1,bk/C0h)0andb2¼(bk/C0hþ1,/C1/C1/C1,bk)0.I n terms of the illustration at the beginning of this section, we would have b1¼(b0,b1,b2)0andb2¼(b3,b4,b5)0.",
    "of the illustration at the beginning of this section, we would have b1¼(b0,b1,b2)0andb2¼(b3,b4,b5)0. Note that b1in (8.7) is different from b1in Section 8.1, in which bwas partitioned as b¼b0 b1/C18/C19 andb1constituted all ofbexceptb0. To test H0:b2¼0versus H1:b2=0, we use a full–reduced-model approach. The full model is given by (8.7).",
    "test H0:b2¼0versus H1:b2=0, we use a full–reduced-model approach. The full model is given by (8.7). Under H0:b2¼0, the reduced model becomes y¼X1b/C3 1þ1/C3: (8:8) We use the notation b/C3 1and1/C3as in Section 7.9, because in the reduced model, b/C31 and1/C3will typically be different from b1and1in the full model (unless X1andX2 are orthogonal; see Theorem 7.9a and its corollary).",
    "nt from b1and1in the full model (unless X1andX2 are orthogonal; see Theorem 7.9a and its corollary). The estimator of b/C3 1in the reduced model (8.8) is ^b/C3 1¼(X0 1X1)/C01X01y, which is, in general, not the same as the ﬁrst k2hþ1 elements of ^b¼(X0X)/C01X0yfrom the full model (8.7) (unless X1andX2are orthogonal; see Theorem 7.10).",
    "ements of ^b¼(X0X)/C01X0yfrom the full model (8.7) (unless X1andX2are orthogonal; see Theorem 7.10). In order to compare the ﬁt of the full model (8.7) to the ﬁt of the reduced model (8.8), we add and subtract ^b0X0yand ^b/C30 1X01yto the total corrected sum of squaresPn i¼1(yi/C0/C22y)2¼y0y/C0n/C22y2so as to obtain the partitioning y0y/C0n/C22y2¼(y0y/C0^b0Xy)þ(^b0X0y/C0^b/C30 1X0 1y)þ(^b/C30 1X1y/C0n/C22y2) (8:9) or SST¼SSEþSS(b2jb1)þSSR(reduced) , (8:10) where SS( b2jb1)¼^b0X0y/C0^b/C30 1X0 1yis the “extra” regression sum of squares due to b2after adjusting for b1.",
    "jb1)¼^b0X0y/C0^b/C30 1X0 1yis the “extra” regression sum of squares due to b2after adjusting for b1. Note that SS( b2jb1) can also be expressed as SS(b2jb1)¼^b0X0y/C0n/C22y2/C0(^b/C30 1X0 1y/C0n/C22y2) ¼SSR( full) /C0SSR(reduced) , which is the difference between the overall regression sum of squares for the full model and the overall regression sum of squares for the reduced model [see (8.6)].",
    "ares for the full model and the overall regression sum of squares for the reduced model [see (8.6)]. IfH0:b2¼0is true, we would expect SS( b2jb1) to be small so that SST in (8.10) is composed mostly of SSR(reduced) and SSE. If b2=0, we expect SS(b2jb1) to be larger and account for more of SST. Thus we are testing H0:b2¼0in the full model in which there are no restrictions on b1.",
    "more of SST. Thus we are testing H0:b2¼0in the full model in which there are no restrictions on b1. We are not ignoring b1(assuming b1¼0) but are testing H0:b2¼0in the presence of b1, that is, above and beyond whatever b1contributes to SST.190 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 204 --- To develop a test statistic based on SS( b2jb1), we ﬁrst write (8.9) in terms of quad- ratic forms in y.",
    "evelop a test statistic based on SS( b2jb1), we ﬁrst write (8.9) in terms of quad- ratic forms in y. Using ^b¼(X0X)/C01X0yand ^b/C3 1¼(X0 1X1)/C01X01yand (5.2), (8.9) becomes y0I/C01 nJ/C18/C19 y¼y0y/C0y0X(X0X)/C01X0yþy0X(X0X)/C01X0y /C0y0X1(X0 1X1)/C01X01yþy0X1(X01X1)/C01X01y/C0y01 nJy ¼y0I/C0X(X0X)/C01X0/C2/C3 yþy0[X(X0X)/C01X0/C0X1(X0 1X1)/C01X01]y þy0X1(X01X1)/C01X01/C01 nJ/C20/C21 y (8:11) ¼y0(I/C0H)yþy0(H/C0H1)yþy0H1/C01nJ/C18/C19 y, (8:12) where H¼X(X 0X)/C01X0andH1¼X1(X0 1X1)/C01X01.",
    ") ¼y0(I/C0H)yþy0(H/C0H1)yþy0H1/C01nJ/C18/C19 y, (8:12) where H¼X(X 0X)/C01X0andH1¼X1(X0 1X1)/C01X01. The matrix I/C0Hwas shown to be idempotent in Problem 5.32a, with rank n/C0k/C01, where kþ1 is the rank of X (kþ1 is also the number of elements in b). The matrix H/C0H1is shown to be idem- potent in the following theorem. Theorem 8.2a. The matrix H/C0H1¼X(X0X)/C01X0/C0X1(X0 1X1)/C01X01is idempotent with rank h, where his the number of elements in b2. PROOF.",
    ")/C01X0/C0X1(X0 1X1)/C01X01is idempotent with rank h, where his the number of elements in b2. PROOF. Premultiplying XbyH, we obtain HX¼X(X0X)/C01X0X¼X or X¼X(X0X)/C01X0/C2/C3 X: (8:13) Partitioning Xon the left side of (8.13) and the last Xon the right side, we obtain [by an extension of (2.28)] (X1,X2)¼X(X0X)/C01X0/C2/C3 (X1,X2) ¼X(X0X)/C01X0X1,X(X0X)/C01X0X2/C2/C3 : Thus X1¼X(X0X)/C01X0X1, X2¼X(X0X)/C01X0X2:(8:14)8.2 TEST ON A SUBSET OF THE b’S 191 --- Page 205 --- Simplifying HH 1andH1Hby (8.14) and its transpose, we obtain HH 1¼H1and H1H¼H1: (8:15) The matrices HandH1are idempotent (see Problem 5.32).",
    "transpose, we obtain HH 1¼H1and H1H¼H1: (8:15) The matrices HandH1are idempotent (see Problem 5.32). Thus (H/C0H1)2¼H2/C0HH 1/C0H1HþH2 1 ¼H/C0H1/C0H1þH1 ¼H/C0H1, andH/C0H1is idempotent.",
    "Problem 5.32). Thus (H/C0H1)2¼H2/C0HH 1/C0H1HþH2 1 ¼H/C0H1/C0H1þH1 ¼H/C0H1, andH/C0H1is idempotent. For the rank of H/C0H1, we have (by Theorem 2.13d) rank(H/C0H1)¼tr(H/C0H1) ¼tr(H)/C0tr(H1) [by (2 :86)] ¼trX(X0X)/C01X0/C2/C3 /C0trX1(X01X1)/C01X01/C2/C3 ¼trX0X(X0X)/C01/C2/C3 /C0trX01X1(X01X1)/C01/C2/C3 [by (2 :87)] ¼tr(Ikþ1)/C0tr(Ik/C0hþ1)¼kþ1/C0(k/C0hþ1)¼h: A We now ﬁnd the distributions of y0(I/C0H)yandy0(H/C0H1)yin (8.12) and show that they are independent. Theorem 8.2b.",
    "distributions of y0(I/C0H)yandy0(H/C0H1)yin (8.12) and show that they are independent. Theorem 8.2b. IfyisNn(Xb,s2I) and HandH1are as deﬁned in (8.11) and (8.12), then (i)y0(I/C0H)y=s2isx2(n/C0k/C01). (ii)y0(H/C0H1)y=s2isx2(h,l1),l1¼b0 2X0 2X2/C0X02X1(X01X1)/C01X01X2/C2/C3 b2=2s2: (iii) y0(I/C0H)yandy0(H/C0H1)yare independent. PROOF. Adding y0(1=n)Jyto both sides of (8.12), we obtain the decomposition y0y¼y0(I/C0H)yþy0(H/C0H1)yþy0H1y.",
    "ding y0(1=n)Jyto both sides of (8.12), we obtain the decomposition y0y¼y0(I/C0H)yþy0(H/C0H1)yþy0H1y. The matrices I/C0H,H/C0H1, and H1 were shown to be idempotent in Problem 5.32 and Theorem 8.2a. Hence by Corollary 1 to Theorem 5.6c, all parts of the theorem follow. See Problem 8.9 for the derivation of l1. A Ifl1¼0 in Theorem 8.2b(ii), then y0(H/C0H1)y=s2has the central chi-square distribution x2(h).",
    "l1. A Ifl1¼0 in Theorem 8.2b(ii), then y0(H/C0H1)y=s2has the central chi-square distribution x2(h). Since X0 2X2/C0X02X1(X01X1)/C01X01X2is positive deﬁnite (see Problem 8), l1¼0 if and only if b2¼0. AnFtest for H0:b2¼0versus H1:b2=0is given in the following theorem.192 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 206 --- Theorem 8.2c.",
    "he following theorem.192 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 206 --- Theorem 8.2c. LetybeNn(Xb,s2I) and deﬁne an Fstatistic as follows: F¼y0(H/C0H1)y=h y0(I/C0H)y=(n/C0k/C01)¼SS(b2jb1)=h SSE=(n/C0k/C01)(8:16) ¼(^b0X0y/C0^b/C30 1X0 1y)=h (y0y/C0^b0X0y)=(n/C0k/C01), (8:17) where ^b¼(X0X)/C01X0yis from the full model y¼Xbþ1and ^b/C3 1¼(X0 1X1)/C01X01y is from the reduced model y¼X1b1/C3þ1/C3.",
    "yis from the full model y¼Xbþ1and ^b/C3 1¼(X0 1X1)/C01X01y is from the reduced model y¼X1b1/C3þ1/C3. The distribution of Fin (8.17) is as follows: (i) If H0:b2¼0is false, then Fis distributed as F(h,n/C0k/C01,l1), wherel1¼b0 2X0 2X2/C0X02X1(X01X1)/C01X01X2/C2/C3 b2=2s2. (ii) If H0:b2¼0is true, then l1¼0 and Fis distributed as F(h,n/C0k/C01): PROOF (i) This result follows from (5.30) and Theorem 8.2b. (ii) This result follows from (5.28) and Theorem 8.2b.",
    "result follows from (5.30) and Theorem 8.2b. (ii) This result follows from (5.28) and Theorem 8.2b. A The test for H0:b2¼0is carried out as follows: Reject H0ifF/C21Fa,h,n/C0k/C01, where Fa,h,n/C0k/C01is the upper apercentage point of the (central) Fdistribution. Alternatively, we reject H0ifp,a, where pis the pvalue. Since X02X2/C0X02X1(X01X1)/C01X01X2is positive deﬁnite (see Problem 8.10), l1.0i f H0:b2¼0is false. This justiﬁes rejection of H0for large values of F.",
    "ite (see Problem 8.10), l1.0i f H0:b2¼0is false. This justiﬁes rejection of H0for large values of F. Results and calculations leading to this Ftest are summarized in the ANOVA table (Table 8.3), where b1is (k/C0hþ1)/C21,b2ish/C21,X1isn/C2(k/C0hþ1), and X2 isn/C2h. The entries in the column for expected mean squares are E[SS(b2jb1)=h] and E[SSE =(n/C0k/C01)]. For E[SS(b2jb1)=h], see Problem 8.11.",
    "ected mean squares are E[SS(b2jb1)=h] and E[SSE =(n/C0k/C01)]. For E[SS(b2jb1)=h], see Problem 8.11. Note that if H0is true, both expected mean squares (Table 8.3) are equal to s2, and if H0is false, E[SS(b2jb1)=h].ESSE=(n/C0k/C01) ½/C138 since X02X2/C0X02X1(X01X1)/C01X01X2is posi- tive deﬁnite.",
    "se, E[SS(b2jb1)=h].ESSE=(n/C0k/C01) ½/C138 since X02X2/C0X02X1(X01X1)/C01X01X2is posi- tive deﬁnite. This inequality provides another justiﬁcation for rejecting H0for large values of F.8.2 TEST ON A SUBSET OF THE b’S 193 --- Page 207 --- TABLE 8.3 ANOVA Table for F-Test of H0:b250 Source of Variation df Sum of Squares Mean Square Expected Mean Square Due tob2adjusted forb1 h SS(b2jb1)¼^b0X0y/C0^b/C3 1X0 1y SSðb2jb1Þ=hs2þ1 hb0 2[X0 2X2/C0X02X1(X01X1)/C01X01X2]b2 Error n/C0k/C01 SSE¼y0y/C0^b0X0y SSE=(n/C0k/C01)s2 Total n/C01 SST¼y0y/C0n/C22y2 194 --- Page 208 --- Example 8.2a.",
    "SSE¼y0y/C0^b0X0y SSE=(n/C0k/C01)s2 Total n/C01 SST¼y0y/C0n/C22y2 194 --- Page 208 --- Example 8.2a. Consider the dependent variable y2in the chemical reaction data in Table 7.4 (see Problem 7.52 for a description of the variables). In order to check the usefulness of second-order terms in predicting y2, we use as a full model, y2¼b0þb1x1þb2x2þb3x3þb4x2 1þb5x22þb6x23þb7x1x2þb8x1x3þb9x2x3þ1, and test H0:b4¼b5¼...¼b9¼0.",
    "l model, y2¼b0þb1x1þb2x2þb3x3þb4x2 1þb5x22þb6x23þb7x1x2þb8x1x3þb9x2x3þ1, and test H0:b4¼b5¼...¼b9¼0. For the full model, we obtain ^b0X0y/C0n/C22y2¼ 339:7888, and for the reduced model y2¼b/C3 0þb/C31x1þb/C32x2þb/C33x3þ1/C3,w e have ^b/C30 1X0 1y/C0n/C22y2¼151:0022. The difference is ^b0X0y/C0^b/C30 1X01y¼188:7866. The error sum of squares is SSE ¼60:6755, and the Fstatistic is given by (8.16) or Table 8.3 as F¼188:7866 =6 60:6755 =9¼31:4644 6:7417¼4:6671 , which has a pvalue of .0198.",
    "(8.16) or Table 8.3 as F¼188:7866 =6 60:6755 =9¼31:4644 6:7417¼4:6671 , which has a pvalue of .0198. Thus the second-order terms are useful in prediction of y2. In fact, the overall Fin (8.5) for the reduced model is 3.027 with p¼:0623, so that x1,x2,andx3are inadequate for predicting y2. The overall Ffor the full model is 5.600 with p¼.0086. A In the following theorem, we express SS( b2jb1) as a quadratic form in ^b2that corresponds to l1in Theorem 8.2b(ii). Theorem 8.2d.",
    "xpress SS( b2jb1) as a quadratic form in ^b2that corresponds to l1in Theorem 8.2b(ii). Theorem 8.2d. If the model is partitioned as in (8.7), then SS(b2jb1)¼^b0X0y/C0^b/C30 1X01ycan be written as SS(b2jb1)¼^b0 2X02X2/C0X02X1(X01X1)/C01X01X2/C2/C3^b2, (8:18) where ^b2is from a partitioning of ^bin the full model: ˆb¼^b1^b2/C18/C19 ¼(X0X)/C01X0y: (8:19) PROOF. We can write X^bin terms of ^b1and ^b2asX^b¼(X1,X2)^b1^b2/C18/C19 ¼X1^b1þ X2^b2.",
    "1X0y: (8:19) PROOF. We can write X^bin terms of ^b1and ^b2asX^b¼(X1,X2)^b1^b2/C18/C19 ¼X1^b1þ X2^b2. To write ^b/C3 1in terms of ^b1and ^b2, we note that by (7.80), E(^b/C31)¼b1þAb2, where A¼(X0 1X1)/C01X01X2is the alias matrix deﬁned in Theorem 7.9a. This can be estimated by ^b/C3 1¼^b1þA^b2, where ^b1and ^b2are from the full model, as in (8.19).",
    "a. This can be estimated by ^b/C3 1¼^b1þA^b2, where ^b1and ^b2are from the full model, as in (8.19). Then SS( b2jb1) in (8.10) or Table 8.3 can be written as SS(b2jb1)¼^b0X0y/C0^b/C30 1X0 1y ¼^b0X0X^b/C0^b/C30 1X01X1^b/C3[by (7 :8)] ¼(^b01X01þ^b02X02)(X1^b1þX2^b2)/C0(^b01þ^b02A0)X01X1(^b1þA^b2): Multiplying this out and substituting ( X0 1X1)/C01X01X2forA, we obtain (8.18). A8.2 TEST ON A SUBSET OF THE b’S 195 --- Page 209 --- In (8.18), it is clear that SS( b2jb1) is due to b2.",
    "EST ON A SUBSET OF THE b’S 195 --- Page 209 --- In (8.18), it is clear that SS( b2jb1) is due to b2. We also see in (8.18) a direct corre- spondence between SS( b2jb1) and the noncentrality parameter l1in Theorem 8.2b (ii) or the expected mean square in Table 8.3. Example 8.2b. The full–reduced-model test of H0:b2¼0in Table 8.3 can be used to test for signiﬁcance of a single ^bj. To illustrate, suppose that we wish to testH0:bk¼0, where bis partitioned as b¼b0 b1 ...",
    "single ^bj. To illustrate, suppose that we wish to testH0:bk¼0, where bis partitioned as b¼b0 b1 ... bk/C01 bk0 BBBBB@1 CCCCCA¼b1 bk/C18/C19 : Then Xis partitioned as X¼(X1,xk), where xkis the last column of XandX1contains all columns except xk. The reduced model is y¼X1b/C3 1þ1/C3, andb/C31is estimated as ^b/C3 1¼(X0 1X1)/C01X01y.",
    "s except xk. The reduced model is y¼X1b/C3 1þ1/C3, andb/C31is estimated as ^b/C3 1¼(X0 1X1)/C01X01y. In this case, h¼1, and the Fstatistic in (8.17) becomes F¼^b0X0y/C0^b/C30 1X01y (y0y/C0^b0X0y)=(n/C0k/C01), (8:20) which is distributed as F(1,n/C0k/C01) if H0:bk¼0 is true. A Example 8.2c. The test in Section 8.1 for overall regression can be obtained as a full– reduced-model test. In this case, the partitioning of Xand ofbisX¼(j,X1) and b¼b0 b1 ...",
    "as a full– reduced-model test. In this case, the partitioning of Xand ofbisX¼(j,X1) and b¼b0 b1 ... bk0 BBB@1 CCCA¼b0 b1/C18/C19 : The reduced model is y¼b/C3 0jþ1/C3, for which we have ^b/C3 0¼/C22yand SS( b/C30)¼n/C22y2(8:21) (see Problem 8.13). Then SS( b1jb0)¼^b0X0y/C0n/C22y2, which is the same as (8.6).",
    "C30)¼n/C22y2(8:21) (see Problem 8.13). Then SS( b1jb0)¼^b0X0y/C0n/C22y2, which is the same as (8.6). A 8.3 FTEST IN TERMS OF R2 TheFstatistics in Sections 8.1 and 8.2 can be expressed in terms of R2as deﬁned in (7.56).196 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 210 --- Theorem 8.3.",
    "R2as deﬁned in (7.56).196 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 210 --- Theorem 8.3. The Fstatistics in (8.5) and (8.17) for testing H0:b1¼0and H0:b2¼0, respectively, can be written in terms of R2as F¼(^b0X0y/C0n/C22y2)=k (y0y/C0^b0X0y)=(n/C0k/C01)(8:22) ¼R2=k (1/C0R2)=(n/C0k/C01)(8:23) and F¼(^b0X0y/C0^b/C30 1X0 1y)=h (y0y/C0^b0X0y)=(n/C0k/C01)(8:24) ¼(R2/C0R2 r)=h (1/C0R2)=(n/C0k/C01), (8:25) where R2for the full model is given in (7.56) as R2¼(^b0X0y/C0n/C22y2)=(y0y/C0n/C22y2) and R2rfor the reduced model y¼X1b/C3 1þ1in (8.8) is similarly deﬁned as R2 r¼^b/C30 1X0 1y/C0n/C22y2 y0y/C0n/C22y2: (8:26) PROOF.",
    "¼X1b/C3 1þ1in (8.8) is similarly deﬁned as R2 r¼^b/C30 1X0 1y/C0n/C22y2 y0y/C0n/C22y2: (8:26) PROOF. Adding and subtracting n/C22y2in the denominator of (8.22) gives F¼(^b0X0y/C0n/C22y2)=k [y0y/C0n/C22y2/C0(^b0X0y/C0n/C22y2)]=(n/C0k/C01): Dividing numerator and denominator by y0y/C0n/C22y2yields (8.23). For (8.25), see Problem 8.15. A In (8.25), we see that the Ftest for H0:b2¼0is equivalent to a test for signiﬁcant reduction in R2.",
    "In (8.25), we see that the Ftest for H0:b2¼0is equivalent to a test for signiﬁcant reduction in R2. Note also that since F/C210 in (8.25), we have R2/C21R2 r, which is an additional conﬁrmation of property 3 in Section 7.7, namely, that adding an xto the model increases R2. Example 8.3. For the dependent variable y2in the chemical reaction data in Table 7.4, a full model with nine x’s and a reduced model with three x’s were con- sidered in Example 8.2a.",
    ".4, a full model with nine x’s and a reduced model with three x’s were con- sidered in Example 8.2a. The values of R2for the full model and reduced model are .8485 and .3771, respectively. To test the signiﬁcance of the increase in R28.3 FTEST IN TERMS OF R2197 --- Page 211 --- from .3771 to .8485, we use (8.25) F¼(R2/C0R2 r)=h (1/C0R2)=(n/C0k/C01)¼(:8485 /C0:3771) =6 (1/C0:8485) =9 ¼:07857 :01683¼4:6671 , which is the same as the value obtained for Fin Example 8.2a.",
    "(1/C0:8485) =9 ¼:07857 :01683¼4:6671 , which is the same as the value obtained for Fin Example 8.2a. A 8.4 THE GENERAL LINEAR HYPOTHESIS TESTS FOR H0:Cb50 AND H0:Cb5t We discuss a test for H0:Cb¼0in Section 8.4.1 and a test for H0:Cb¼tin Section 8.4.2. 8.4.1 The Test for H0:Cb¼0 The hypothesis H0:Cb¼0, where Cis a known q/C2(kþ1) coefﬁcient matrix of rank q/C20kþ1, is known as the general linear hypothesis . The alternative hypothesis isH1:Cb=0.",
    "of rank q/C20kþ1, is known as the general linear hypothesis . The alternative hypothesis isH1:Cb=0. The formulation H0:Cb¼0includes as special cases the hypoth- eses in Sections 8.1 and 8.2. The hypothesis H0:b1¼0in Section 8.1 can be expressed in the form H0:Cb¼0as follows H0:Cb¼(0,Ik)b0 b1/C18/C19 ¼b1¼0[by (2 :36)], where 0is ak/C21 vector.",
    "in the form H0:Cb¼0as follows H0:Cb¼(0,Ik)b0 b1/C18/C19 ¼b1¼0[by (2 :36)], where 0is ak/C21 vector. Similarly, the hypothesis H0:b2¼0in Section 8.2 can be expressed in the form H0:Cb¼0: H0:Cb¼(O,Ih)b1 b2/C18/C19 ¼b2¼0, where the matrix Oish/C2(k/C0hþ1) and the vector 0ish/C21.",
    "H0:Cb¼0: H0:Cb¼(O,Ih)b1 b2/C18/C19 ¼b2¼0, where the matrix Oish/C2(k/C0hþ1) and the vector 0ish/C21. The formulation H0:Cb¼0also allows for more general hypotheses such as H0:2b1/C0b2¼b2/C02b3þ3b4¼b1/C0b4¼0, which can be expressed as follows: H0:02 /C0100 00 1 /C023 01 0 0 /C010 @1Ab0 b1 b2 b3 b40BBBB@1 CCCCA¼0 0 00 @1A:198 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 212 --- As another illustration, the hypothesis H0:b1¼b2¼b3¼b4can be expressed in terms of three differences, H0:b1/C0b2¼b2/C0b3¼b3/C0b4¼0, or, equivalently, asH0:Cb¼0: H0:01 /C0100 00 1 /C010 00 0 1 /C010 @1Ab0 b1 b2 b3 b40BBBB@1 CCCCA¼0 0 00 @1A: In the following theorem, we give the sums of squares used in the test of H 0:Cb¼0versus H1:Cb=0, along with the properties of these sums of squares.",
    "ares used in the test of H 0:Cb¼0versus H1:Cb=0, along with the properties of these sums of squares. We denote the sum of squares due to Cb(due to the hypothesis) as SSH. Theorem 8.4a. Ifyis distributed Nn(Xb,s2I) and Cisq/C2(kþ1) of rank q/C20kþ1, then (i)C^bisNq[Cb,s2C(X0X)/C01C0]: (ii) SSH =s2¼(C^b)0[C(X0X)/C01C0]/C01C^b=s2isx2(q,l), wherel¼(Cb)0[C(X0X)/C01C0]/C01Cb=2s2. (iii) SSE =s2¼y0[I/C0X(X0X)/C01X0]y=s2isx2(n/C0k/C01). (iv) SSH and SSE are independent.",
    "/C01Cb=2s2. (iii) SSE =s2¼y0[I/C0X(X0X)/C01X0]y=s2isx2(n/C0k/C01). (iv) SSH and SSE are independent. PROOF (i) By Theorem 7.6b (i), ^bisNkþ1[b,s2(X0X)/C01]. The result then follows by Theorem 4.4a (ii). (ii) Since cov( C^b)¼s2C(X0X)/C01C0ands2[C(X0X)/C01C]/C01C(X0X)/C01C0=s2¼ I, the result follows by Theorem 5.5. (iii) This was established in Theorem 8.1b(ii). (iv) Since ^band SSE are independent [see Theorem 7.6b(iii)], SSH ¼ ^bC0[C(X0X)/C01C0]C^band SSE are also independent (Seber 1977, pp.",
    "nt [see Theorem 7.6b(iii)], SSH ¼ ^bC0[C(X0X)/C01C0]C^band SSE are also independent (Seber 1977, pp. 17, 33–34). For a more formal proof, see Problem 8.16. A TheFtest for H0:Cb¼0versus H1:Cb=0is given in the following theorem. Theorem 8.4b. LetybeNn(Xb,s2I) and deﬁne the statistic F¼SSH=q SSE=(n/C0k/C01) ¼(C^b)0[C(X0X)/C01C0]/C01C^b=q SSE=(n/C0k/C01), (8:27) where Cisq/C2(kþ1) of rank q/C20kþ1 and ^b¼(X0X)/C01X0y.",
    "0X)/C01C0]/C01C^b=q SSE=(n/C0k/C01), (8:27) where Cisq/C2(kþ1) of rank q/C20kþ1 and ^b¼(X0X)/C01X0y. The distribution of F in (8.27) is as follows:8.4 THE GENERAL LINEAR HYPOTHESIS TESTS FOR H0:Cb¼0 AND H0:Cb¼t 199 --- Page 213 --- (i) If H0:Cb¼0is false, then Fis distributed as F(q,n/C0k/C01,l), wherel¼(Cb)0C(X0X)/C01C0/C2/C3/C01Cb=2s2. (ii) If H0:Cb¼0is true, then Fis distributed as F(q,n/C0k/C01): PROOF (i) This result follows from (5.30) and Theorem 8.4a.",
    "then Fis distributed as F(q,n/C0k/C01): PROOF (i) This result follows from (5.30) and Theorem 8.4a. (ii) This result follows from (5.28) and Theorem 8.4a. A TheFtest for H0:Cb¼0in Theorem 8.4b is usually called the general linear hypothesis test . The degrees of freedom qis the number of linear combinations in Cb. The test for H0:Cb¼0is carried out as follows.",
    "freedom qis the number of linear combinations in Cb. The test for H0:Cb¼0is carried out as follows. Reject H0if F/C21Fa,q,n/C0k/C01, where Fis as given in (8.27) and Fa,q,n/C0k/C01is the upper apercentage point of the (central) Fdistribution. Alternatively, we can reject H0ifp/C20awhere p is the pvalue for F.",
    "of the (central) Fdistribution. Alternatively, we can reject H0ifp/C20awhere p is the pvalue for F. [The pvalue is the probability that F(q,n/C0k/C01) exceeds the observed Fvalue.] Since C(X0X)/C01C0is positive deﬁnite (see Problem 8.17), l.0i f H0is false, where l¼(Cb)0[C(X0X)/C01C0]/C01Cb=2s2. Hence we reject H0:Cb¼0 for large values of F.",
    "f H0is false, where l¼(Cb)0[C(X0X)/C01C0]/C01Cb=2s2. Hence we reject H0:Cb¼0 for large values of F. In Theorems 8.4a and 8.4b, SSH could be written as ( C^b/C00)0[C(X0X)/C01C0]/C01 (C^b/C00), which is the squared distance between C^band the hypothesized value ofCb. The distance is standardized by the covariance matrix of C^b. Intuitively, if H0is true, C^btends to be close to 0so that the numerator of Fin (8.27) is small.",
    "b. Intuitively, if H0is true, C^btends to be close to 0so that the numerator of Fin (8.27) is small. On the other hand, if Cbis very different from 0, the numerator of Ftends to be large.",
    "27) is small. On the other hand, if Cbis very different from 0, the numerator of Ftends to be large. The expected mean squares for the Ftest are given by ESSH q/C18/C19 ¼s2þ1 q(Cb)0C(X0X)/C01C0/C2/C3 /C01Cb, ESSE n/C0k/C01/C18/C19 ¼s2:(8:28) These expected mean squares provide additional motivation for rejecting H0for large values of F.I fH0is true, both expected mean squares are equal to s2;i fH0is false, E(SSH =q).E[SSE =(n/C0q/C01)].",
    "fH0is true, both expected mean squares are equal to s2;i fH0is false, E(SSH =q).E[SSE =(n/C0q/C01)]. TheFstatistic in (8.27) is invariant to full-rank linear transformations on the x’s or ony. Theorem 8.4c. Letz¼cyandW¼XK, where Kis nonsingular (see Corollary 1 to Theorem 7.3e for the form of K). The Fstatistic in (8.27) is unchanged by these transformations.200 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 214 --- PROOF. See Problem 8.18.",
    "ormations.200 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 214 --- PROOF. See Problem 8.18. A In the ﬁrst paragraph of this section, it was noted that the hypothesis H0:b2¼0 can be expressed in the form H0:Cb¼0. Since we used a full–reduced-model approach to develop the test for H0:b2¼0, we expect that the general linear hypoth- esis test is also a full–reduced-model test. This is conﬁrmed in the following theorem. Theorem 8.4d.",
    "sis test is also a full–reduced-model test. This is conﬁrmed in the following theorem. Theorem 8.4d. The Ftest in Theorem 8.4b for the general linear hypothesis H0:Cb¼0is a full–reduced-model test. PROOF.",
    "Ftest in Theorem 8.4b for the general linear hypothesis H0:Cb¼0is a full–reduced-model test. PROOF. The reduced model under H0is y¼Xbþ1subject to Cb¼0: (8:29) Using Lagrange multipliers (Section 2.14.3), it can be shown (see Problem 8.19) that the estimator for bin this reduced model is ^bc¼^b/C0(X0X)/C01C0[C(X0X)/C01C0]/C01C^b, (8:30) where ^b¼(X0X)/C01X0yis estimated from the full model unrestricted by the hypoth- esis and the subscript cin^bcindicates that bis estimated subject to the constraint Cb¼0.",
    "he hypoth- esis and the subscript cin^bcindicates that bis estimated subject to the constraint Cb¼0. In (8.29), the Xmatrix for the reduced model is unchanged from the full model, and the regression sum of squares for the reduced model is therefore ^b0 cX0y (for a more formal justiﬁcation of ^b0 cX0y, see Problem 8.20).",
    "reduced model is therefore ^b0 cX0y (for a more formal justiﬁcation of ^b0 cX0y, see Problem 8.20). Hence, the regression sum of squares due to the hypothesis is SSH ¼^b0X0y/C0^b0cX0y: (8:31) By substituting ^bc[as given by (8.30)] into (8.31), we obtain SSH ¼(C^b)0[C(X0X)/C01C0]/C01C^b (8:32) (see Problem 8.21), thus establishing that the Ftest in Theorem 8.4b for H0:Cb¼0, is a full–reduced-model test. A Example 8.4.1a.",
    "blishing that the Ftest in Theorem 8.4b for H0:Cb¼0, is a full–reduced-model test. A Example 8.4.1a. In many cases, the hypothesis can be incorporated directly into the model to obtain the reduced model. Suppose that the full model is yi¼b0þb1xi1þb2xi2þb3xi3þ1i and the hypothesis is H0:b1¼2b2.",
    "ed model. Suppose that the full model is yi¼b0þb1xi1þb2xi2þb3xi3þ1i and the hypothesis is H0:b1¼2b2. Then the reduced model becomes yi¼b0þ2b2xi1þb2xi2þb3xi3þ1i ¼bc0þbc2(2xi1þxi2)þbc3xi3þ1i,8.4 THE GENERAL LINEAR HYPOTHESIS TESTS FOR H0:Cb¼0 AND H0:Cb¼t 201 --- Page 215 --- wherebciindicates a parameter subject to the constraint b1¼2b2. The full model and reduced model could be ﬁt, and the difference SS( b2jb1)¼^b0X0y/C0^b/C30X0 1y would be the same as SSH in (8.32).",
    "could be ﬁt, and the difference SS( b2jb1)¼^b0X0y/C0^b/C30X0 1y would be the same as SSH in (8.32). A IfCb=0, the estimator ^bcin (8.30) is a biased estimator of b, but the variances of the ^bcj’s in ^bcare reduced, as shown in the following theorem. Theorem 8.4e. The mean vector and covariance matrix of ^bcin (8.30) are as follows: (i)E(^bc)¼b/C0(X0X)/C01C0C(X0X)/C01C0/C2/C3 /C01Cb: (8:33) (ii)cov( ^bc)¼s2(X0X)/C01/C0s2(X0X)/C01C0C(X0X)/C01C0/C2/C3 /C01C(X0X)/C01: (8:34) PROOF.",
    "Cb: (8:33) (ii)cov( ^bc)¼s2(X0X)/C01/C0s2(X0X)/C01C0C(X0X)/C01C0/C2/C3 /C01C(X0X)/C01: (8:34) PROOF. See Problem 8.22. A Since the second matrix on the right side of (8.34) is positive semideﬁnite, the diagonal elements of cov( ^bc) are less than those of cov( ^b)¼s2(X0X)/C01; that is, var( ^bcj)/C20var( ^bj) for j¼0,1,2,/C1/C1/C1,k, where ^bcjis the jth diagonal element of cov( ^bc) in (8.34).",
    "/C20var( ^bj) for j¼0,1,2,/C1/C1/C1,k, where ^bcjis the jth diagonal element of cov( ^bc) in (8.34). This is analogous to the inequality var( ^b/C3 j),var( ^bj)i n Theorem 7.9c, where ^b/C3 jis from the reduced model. Example 8.4.1b. Consider the dependent variable y1in the chemical reaction data in Table 7.4. For the model y1¼b0þb1x1þb2x2þb3x3þ1, we test H0:2b1¼2b2¼b3using (8.27) in Theorem 8.4b.",
    "Table 7.4. For the model y1¼b0þb1x1þb2x2þb3x3þ1, we test H0:2b1¼2b2¼b3using (8.27) in Theorem 8.4b. To express H0in the form Cb¼0, the matrix Cbecomes C¼01 /C010 00 2 /C01/C18/C19 , and we obtain C^b¼/C0:1214 /C0:6118/C18/C19 , C(X0X)/C01C0¼:003366 /C0:006943 /C0:006943 :044974/C18/C19 , F¼/C0:1214 /C0:6118/C18/C19 0:003366 /C0:006943 /C0:006943 :044974/C18/C19 /C01/C0:1214 /C0:6118/C18/C19 =2 5:3449 ¼28:62301 =2 5:3449¼2:6776 , which has p¼:101.",
    "44974/C18/C19 /C01/C0:1214 /C0:6118/C18/C19 =2 5:3449 ¼28:62301 =2 5:3449¼2:6776 , which has p¼:101. A202 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 216 --- 8.4.2 The Test for H0:Cb¼t The test for H0:Cb¼tis a straightforward extension of the test for H0:Cb¼0. With the additional ﬂexibility provided by t, we can test hypotheses such as H0:b2¼b1þ5. We assume that the system of equations Cb¼tis consistent, that is, that rank(C)¼rank(C,t) (see Theorem 2.7).",
    "e that the system of equations Cb¼tis consistent, that is, that rank(C)¼rank(C,t) (see Theorem 2.7). The requisite sums of squares and their prop- erties are given in the following theorem, which is analogous to Theorem 8.4a. Theorem 8.4f.",
    "r prop- erties are given in the following theorem, which is analogous to Theorem 8.4a. Theorem 8.4f. IfyisNn(Xb,s2I) and Cisq/C2(kþ1) of rank q/C20kþ1, then (i)C^b/C0tisNq[Cb/C0t,s2C(X0X)/C01C0]: (ii) SSH =s2¼(C^b/C0t)0C(X0X)/C01C0/C2/C3/C01(C^b/C0t)=s2isx2(q,l) wherel¼(Cb/C0t)0[C(X0X)/C01C0]/C01(Cb/C0t)=2s2: (iii) SSE =s2¼y0[I/C0X(X0X)/C01X0]y=s2isx2(n/C0k/C01). (iv) SSH and SSE are independent. PROOF (i) By Theorem 7.6b (i), ^bisNkþ1[b,s2(X0X)/C01].",
    "/C0k/C01). (iv) SSH and SSE are independent. PROOF (i) By Theorem 7.6b (i), ^bisNkþ1[b,s2(X0X)/C01]. The result follows by Corollary 1 to Theorem 4.4a. (ii) By part (i), cov( C^b/C0t)¼s2C(X0X)/C01C0. The result follows as in the proof of Theorem 8.4a (ii). (iii) See Theorem 8.1b (ii). (iv) Since ^band SSE are independent [see Theorem 7.6b (iii)], SSH and SSE are independent [see Seber (1977, pp. 17, 33–34)]. For a more formal proof, see Problem 8.23.",
    "nd SSE are independent [see Seber (1977, pp. 17, 33–34)]. For a more formal proof, see Problem 8.23. A AnFtest for H0:Cb¼tversus H1:Cb=tis given in the following theorem, which is analogous to Theorem 8.4b. Theorem 8.4g. LetybeNn(Xb,s2I) and deﬁne an Fstatistic as follows: F¼SSH=q SSE=(n/C0k/C01) ¼(C^b/C0t)0C(X0X)/C01C0/C2/C3/C01(C^b/C0t)=q SSE=(n/C0k/C01), (8:35) where ^b¼(X0X)/C01X0y.",
    "C0k/C01) ¼(C^b/C0t)0C(X0X)/C01C0/C2/C3/C01(C^b/C0t)=q SSE=(n/C0k/C01), (8:35) where ^b¼(X0X)/C01X0y. The distribution of Fin (8.35) is as follows: (i) If H0:Cb¼tis false, then Fis distributed as F(q,n/C0k/C01,l), wherel¼(Cb/C0t)0[C(X0X)/C01C0]/C01(Cb/C0t)=2s2.8.4 THE GENERAL LINEAR HYPOTHESIS TESTS FOR H0:Cb¼0 AND H0:Cb¼t 203 --- Page 217 --- (ii) If H0:Cb¼tis true, then l¼0 and Fis distributed as F(q,n/C0k/C01): PROOF (i) This result follows from (5.28) and Theorem 8.4f.",
    "0 and Fis distributed as F(q,n/C0k/C01): PROOF (i) This result follows from (5.28) and Theorem 8.4f. (ii) This result follows from (5.30) and Theorem 8.4f. A The test for H0:Cb¼tis carried out as follows. Reject H0ifF/C21Fa,q,n/C0k/C01, where Fa,q,n/C0k/C01is the upper apercentage point of the central Fdistribution. Alternatively, we can reject H0ifp/C20a, where pis the pvalue for F.",
    "t of the central Fdistribution. Alternatively, we can reject H0ifp/C20a, where pis the pvalue for F. The expected mean squares for the Ftest are given by ESSH q/C18/C19 ¼s2þ1 q(Cb/C0t)0C(X0X)/C01C0/C2/C3 /C01(Cb/C0t), ESSE n/C0k/C01/C18/C19 ¼s2:(8:36) By extension of Theorem 8.4d, the Ftest for H0:Cb¼tin Theorem 8.4g is a full–reduced-model test (see Problem 8.24 for a partial result).",
    "est for H0:Cb¼tin Theorem 8.4g is a full–reduced-model test (see Problem 8.24 for a partial result). 8.5 TESTS ON bjAND a0b We consider tests for a single bjor a single linear combination a0bin Section 8.5.1 and tests for several bj’s or several a0 ib’s in Section 8.5.2.",
    "near combination a0bin Section 8.5.1 and tests for several bj’s or several a0 ib’s in Section 8.5.2. 8.5.1 Testing One bjor One a0b Tests for an individual bjcan be obtained using either the full–reduced- model approach in Section 8.2 or the general linear hypothesis approach in Section 8.4 The test statistic for H0:bk¼0 using a full–reduced–model is given in (8.20) as F¼^b0X0y/C0^b/C30 1X0 1y SSE=(n/C0k/C01), (8:37) which is distributed as F(1,n/C0k/C01) if H0is true.",
    "¼^b0X0y/C0^b/C30 1X0 1y SSE=(n/C0k/C01), (8:37) which is distributed as F(1,n/C0k/C01) if H0is true. In this case, bkis the last b,s o thatbis partitioned as b¼b1 bk/C18/C19 andXis partitioned as X¼(X1,xk), where xkis204 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 218 --- the last column of X. Then X1in the reduced model y¼X1b/C3 1þ1contains all the columns of Xexcept the last.",
    "t column of X. Then X1in the reduced model y¼X1b/C3 1þ1contains all the columns of Xexcept the last. To test H0:bj¼0 by means of the general linear hypothesis test of H0:Cb¼0 (Section 8.4.1), we ﬁrst consider a test of H0:a0b¼0 for a single linear combi- nation, for example, a0b¼(0,2,/C02,3,1)b. Using a0in place of the matrix Cin Cb¼0,w eh a v e q¼1, and (8.27) becomes F¼(a0^b)0a0(X0X)/C01a/C2/C3/C01a0^b SSE=(n/C0k/C01)¼(a0^b)2 s2a0(X0X)/C01a, (8:38) where s2¼SSE=(n/C0k/C01).",
    "0a0(X0X)/C01a/C2/C3/C01a0^b SSE=(n/C0k/C01)¼(a0^b)2 s2a0(X0X)/C01a, (8:38) where s2¼SSE=(n/C0k/C01). The Fstatistic in (8.38) is distributed as F(1,n/C0k/C01) if H0:a0b¼0 is true. To test H0:bj¼0 using (8.38), we deﬁne a0¼(0,...,0,1,0,...,0), where the 1 is in the jth position. This gives F¼^b2 j s2gjj, (8:39) where gjjis the jth diagonal element of ( X0X)/C01.I fH0:bj¼0 is true, Fin (8.39) is distributed as F(1,n/C0k/C01).",
    "jth diagonal element of ( X0X)/C01.I fH0:bj¼0 is true, Fin (8.39) is distributed as F(1,n/C0k/C01). We reject H0:bj¼0i f F/C21Fa,1,n/C0k/C01or, equiva- lently, if p/C20a, where pis the pvalue for F. By Theorem 8.4d (see also Problem 8.25), the Fstatistics in (8.37) and (8.39) are the same (for j¼k). This conﬁrms that (8.39) tests H0:bj¼0 adjusted for the other b’s.",
    "nd (8.39) are the same (for j¼k). This conﬁrms that (8.39) tests H0:bj¼0 adjusted for the other b’s. Since the Fstatistic in (8.39) has 1 and n/C0k/C01 degrees of freedom, we can equivalently use the tstatistic tj¼^bj sﬃﬃﬃﬃﬃ ﬃgjjp (8:40) to test the effect of bjabove and beyond the other b’s (see Problem 5.16). We reject H0:bj¼0i fjtjj/C21ta=2,n/C0k/C01or, equivalently, if p/C20a, where pis the pvalue.",
    "5.16). We reject H0:bj¼0i fjtjj/C21ta=2,n/C0k/C01or, equivalently, if p/C20a, where pis the pvalue. For a two-tailed ttest such as this one, the pvalue is twice the probability that t(n/C0k/C01) exceeds the absolute value of the observed t. For j¼1, (8.40) becomes t¼^b1=sﬃﬃﬃﬃﬃﬃg11p, which is not the same as t¼ ^b1=s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q/C20/C21 in (6.14). Unless the x’s are orthogonal, g/C01 11=P i(x1i/C0/C22x1)2.",
    "ﬃﬃﬃﬃP i(xi/C0/C22x)2q/C20/C21 in (6.14). Unless the x’s are orthogonal, g/C01 11=P i(x1i/C0/C22x1)2. 8.5.2 Testing Several bj’s or a0 ib0s We sometimes want to carry out several separate tests rather than a single joint test of the hypotheses. For example, the test in (8.40) might be carried out separately for eachbi,i¼1,...,krather than the joint test of H0:b1¼0 in (8.5).",
    "might be carried out separately for eachbi,i¼1,...,krather than the joint test of H0:b1¼0 in (8.5). Similarly, we might want to carry out separate tests for several (say, d)aib’s using (8.38)8.5 TESTS ON bjand a0b 205 --- Page 219 --- rather than the joint test of H0:Cb¼0 using (8.27), where C¼a1 a2 ... ad0 BBB@1 CCCA: In such situations there are two different alevels, the overall or familywise alevel (af) and the alevel for each test or comparisonwise alevel (ac).",
    "s, the overall or familywise alevel (af) and the alevel for each test or comparisonwise alevel (ac). In some cases researchers desire to control acwhen doing several tests (Saville 1990), and so no changes are needed in the testing procedure. In other cases, the desire is to control af.",
    "and so no changes are needed in the testing procedure. In other cases, the desire is to control af. In yet other cases, especially those involving thousands of separate tests (e.g., micro- array data), it makes sense to control other quantities such as the false discovery rate (Benjamini and Hochberg 1995, Benjamini and Yekutieli 2001). This will not be dis-cussed further here. We consider two ways to control afwhen several tests are made.",
    "will not be dis-cussed further here. We consider two ways to control afwhen several tests are made. The ﬁrst of these methods is the Bonferroni approach (Bonferroni 1936), which reducesacfor each test, so that afis less than the desired level of a/C3.A sa n example, suppose that we carry out the ktests of H0j:bj¼0,j¼1,2,...,k. Let Ejbe the event that the jth test rejects H0jwhen it is true, where P(Ej)¼ac.",
    "j:bj¼0,j¼1,2,...,k. Let Ejbe the event that the jth test rejects H0jwhen it is true, where P(Ej)¼ac. The overall afcan be deﬁned as af¼P(reject at least one H0jwhen all H0jare true) ¼P(E1orE2...orEk): Expressing this more formally and applying the Bonferroni inequality, we obtain af¼P(E1<E2</C1/C1/C1<Ek) /C20Xk j¼1P(Ej)¼Xk j¼1ac¼kac:(8:41) We can thus ensure that afis less than or equal to the desired a/C3by simply setting ac¼a/C3=k.",
    ":41) We can thus ensure that afis less than or equal to the desired a/C3by simply setting ac¼a/C3=k. Sinceafin (8.41) is at most a/C3, the Bonferroni procedure is a conserva- tive approach. To test H0j:bj¼0,j¼1,2,...,k, withaf/C20a/C3, we use (8.40) tj¼^bj sﬃﬃﬃﬃﬃ ﬃgjjp , (8:42) and reject H0jifjtjj/C21ta/C3=2k,n/C0k/C01. Bonferroni critical values ta/C3=2k,nare available in Bailey (1977). See also Rencher (2002, pp. 562–565).",
    "roni critical values ta/C3=2k,nare available in Bailey (1977). See also Rencher (2002, pp. 562–565). The critical values ta/C3=2k,ncan also be found using many software packages.",
    "r (2002, pp. 562–565). The critical values ta/C3=2k,ncan also be found using many software packages. Alternatively, we can carry out the test by the use of pvalues and reject H0jifp/C20a/C3=k.206 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 220 --- More generally, to test H0i:a0 ib¼0 for i¼1,2,...,dwithaf/C20a/C3, we use (8.38) Fi¼(a0i^b)0a0i(X0X)/C01ai/C2/C3/C01a0i^b s2(8:43) and reject H0iifFi/C21Fa/C3=d,1,n/C0k/C01. The critical values Fa/C3=dare available in many software packages.",
    "H0iifFi/C21Fa/C3=d,1,n/C0k/C01. The critical values Fa/C3=dare available in many software packages. To use pvalues, reject H0iifp/C20a/C3=d. The above Bonferroni procedures do not require independence of the ^bj’s; they are valid for any covariance structure on the ^bj’s. However, the logic of the Bonferroni procedure for testing H0i:a0ib¼0 for i¼1,2,...,drequires that the coefﬁcient vectors a1,a2,...,adbe speciﬁed before seeing the data.",
    "for i¼1,2,...,drequires that the coefﬁcient vectors a1,a2,...,adbe speciﬁed before seeing the data. If we wish to choose values of aiafter looking at the data, we must use the Scheffe ´procedure described below. Modiﬁcations of the Bonferroni approach have been proposed that are less conservative but still control af.",
    "tions of the Bonferroni approach have been proposed that are less conservative but still control af. For examples of these modiﬁed procedures, see Holm (1979), Shaffer (1986), Simes (1986), Holland and Copenhaver (1987), Hochberg (1988), Hommel (1988), Rom (1990), and Rencher (1995, Section 3.4.4). Comparisons of these procedures have been made by Holland (1991) and Broadbent (1993). A second approach to controlling afdue to Scheffe ´(1953; 1959, p.",
    "land (1991) and Broadbent (1993). A second approach to controlling afdue to Scheffe ´(1953; 1959, p. 68) yields simultaneous tests of H0:a0b¼0 for all possible values of aincluding those chosen after looking at the data. We could also test H0:a0b¼tfor arbitrary t.F o r any given a, the hypothesis H0:a0b¼0 is tested as usual by (8.38) F¼(a0^b)0a0(X0X)/C01a/C2/C3/C01a0^b s2 ¼(a0^b)2 s2a0(X0X)/C01a, (8:44) but the test proceeds by ﬁnding a critical value large enough to hold for all possible a.",
    "1a, (8:44) but the test proceeds by ﬁnding a critical value large enough to hold for all possible a. Accordingly, we now ﬁnd the distribution of max aF. Theorem 8.5 (i) The maximum value of Fin (8.44) is given by max a(a0^b)2 s2a0(X0X)/C01a¼^b0X0X^b s2: (8:45) (ii) If yisNn(Xb,s2I), then ^b0X0X^b=(kþ1)s2is distributed as F(kþ1,n/C0k/C01).",
    "¼^b0X0X^b s2: (8:45) (ii) If yisNn(Xb,s2I), then ^b0X0X^b=(kþ1)s2is distributed as F(kþ1,n/C0k/C01). Thus max a(a0^b)2 s2a0(X0X)/C01a(kþ1) is distributed as F(kþ1,n/C0k/C01).8.5 TESTS ON bjand a0b 207 --- Page 221 --- PROOF (i) Using the quotient rule, chain rule, and Section 2.14.1, we differentiate (a0^b)2=a0(X0X)/C01awith respect to aand set the result equal to 0: @ @a(a0^b)2 a0(X0X)/C01a¼[a0(X0X)/C01a]2(a0^b)^b/C0(a0^b)22(X0X)/C01a [a0(X0X)/C01a]2¼0: Multiplying by [ a0(X0X)/C01a/C1382=2a0^band treating 1 /C21 matrices as scalars, we obtain [a0(X0X)/C01a]^b/C0a0^b(X0X)/C01a¼0, a¼a0(X0X)/C01a a0^bX0X^b¼cX0X^b, where c¼a0(X0X)/C01a=a0^b.",
    "ain [a0(X0X)/C01a]^b/C0a0^b(X0X)/C01a¼0, a¼a0(X0X)/C01a a0^bX0X^b¼cX0X^b, where c¼a0(X0X)/C01a=a0^b. Substituting a¼cX0X^binto (8.44) gives max a(a0^b)2 s2a0(X0X)/C01a¼(c^b0X0X^b)2 s2c^b0X0X(X0X)/C01cX0X^b¼c2(^b0X0X^b)2 s2c2^b0X0X^b¼^b0X0X^b s2: (ii) Using C¼Ikþ1in (8.27), we have, by Theorem 8.4b (ii), that F¼^b0X0X^b (kþ1)s2is distributed as F(kþ1,n/C0k/C01): A By Theorem 8.5(ii), we have Pmax a(a0^b)2 s2a0(X0X)/C01a(kþ1)/C21Fa/C3,kþ1,n/C0k/C01\"# ¼a/C3, Pmax a(a0^b)2 s2a0(X0X)/C01a/C21(kþ1)Fa/C3,kþ1,n/C0k/C01\"# ¼a/C3: Thus, to test H0:a0b¼0 for any and all a(including values of achosen after seeing the data) with af/C20a/C3, we calculate Fin (8.44) and reject H0if F/C21(kþ1)Fa/C3,kþ1,n/C0k/C01.",
    "ng the data) with af/C20a/C3, we calculate Fin (8.44) and reject H0if F/C21(kþ1)Fa/C3,kþ1,n/C0k/C01. To test for individual bj’s using using Scheffe ´’s procedure, we set a0¼(0,...,0,1,0,...,0) with a 1 in the jth position. Then Fin (8.44) reduces to F¼^b2 j=s2gjjin (8.39), and the square root is tj¼^bj=sﬃﬃﬃﬃﬃ ﬃgjjpin (8.42). By Theorem 8.5, we reject H0:a0b¼bj¼0i fjtjj/C21ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ(kþ1)Fa/C3,kþ1,n/C0k/C01p.",
    "we reject H0:a0b¼bj¼0i fjtjj/C21ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ(kþ1)Fa/C3,kþ1,n/C0k/C01p. For practical purposes [ k/C20(n/C03)], we have ta/C3=2k,n/C0k/C01,ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ (kþ1)Fa/C3,kþ1,n/C0k/C01p ,208 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 222 --- and thus the Bonferroni tests for individual bj’s in (8.42) are usually more powerful than the Scheffe ´tests.",
    "he Bonferroni tests for individual bj’s in (8.42) are usually more powerful than the Scheffe ´tests. On the other hand, for a large number of linear combinations a0b, the Scheffe ´test is better since ( kþ1)Fa/C3,kþ1,n/C0k/C01is constant, while the critical value Fa/C3=d,1,n/C0k/C01for Bonferroni tests in (8.43) increases with the number of tests d and eventually exceeds the critical value for Scheffe ´tests.",
    ") increases with the number of tests d and eventually exceeds the critical value for Scheffe ´tests. It has been assumed that the tests in this section for H0:bj¼0a r ec a r r i e do u t without regard to whether the overall hypothesis H0:b1¼0is rejected.",
    "H0:bj¼0a r ec a r r i e do u t without regard to whether the overall hypothesis H0:b1¼0is rejected. However, if the test statistics tj¼^bj=sﬃﬃﬃﬃﬃgjjp,j¼1,2,...,k, in (8.42) are calculated only if H0:b1¼0is rejected using Fin (8.5), then clearly afis reduced and the conservative critical values ta/C3=2k,n/C0k/C01andﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ(kþ1)Fa/C3,kþ1,n/C0k/C01pbecome even more conserva- tive. Using this protected testing principle (Hocking 1996, p.",
    ",n/C0k/C01pbecome even more conserva- tive. Using this protected testing principle (Hocking 1996, p. 106), we can even use the critical value ta/C3=2,n/C0k/C01for all ktests andafwill still be close to a/C3. [For illustrations of this familywise error rate structure, see Hummel and sligo (1971) and Rencher and Scott (1990).] A similar statement can be made for testing the overall hypothesis H0:Cb¼0followed by ttests or Ftests of H0:c0 ib¼0 using the rows of C. Example 8.5.2.",
    "all hypothesis H0:Cb¼0followed by ttests or Ftests of H0:c0 ib¼0 using the rows of C. Example 8.5.2. We test H01:b1¼0 and H02:b2¼0 for the data in Table 7.1. Using (8.42) and the results in Examples 7.3.1(a), 7.33 and 8.1, we have t1¼^b1 sﬃﬃﬃﬃﬃﬃg11p ¼3:0118ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2:8288p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :16207p ¼3:0118 :67709¼4:448, t2¼^b2 sﬃﬃﬃﬃﬃﬃg22p ¼/C01:2855ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2:8288p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :08360p ¼/C01:2855 0:48629¼/C02:643: Usinga¼.05 for each test, we reject both H01andH02because t:025,9¼2:262.",
    "/C01:2855 0:48629¼/C02:643: Usinga¼.05 for each test, we reject both H01andH02because t:025,9¼2:262. The (two-sided) pvalues are .00160 and .0268, respectively. If we use a¼:05=2¼:025 for a Bonferroni test, we would not reject H02since p¼:0268.:025. However, using the protected testing principle, we would reject H02because the overall regression hypothesis H0:b1¼0was rejected in Example 8.1.",
    "le, we would reject H02because the overall regression hypothesis H0:b1¼0was rejected in Example 8.1. A 8.6 CONFIDENCE INTERVALS AND PREDICTION INTERVALS In this section we consider a conﬁdence region for b, conﬁdence intervals for bj,a0b,E(y), ands2, and prediction intervals for future observations. We assume throughout Section 8.6 that yisNn(Xb,s2I).",
    "d prediction intervals for future observations. We assume throughout Section 8.6 that yisNn(Xb,s2I). 8.6.1 Conﬁdence Region for b IfCis equal to Iandtis equal to bin (8.35), qbecomes kþ1, we obtain a central F distribution, and we can make the probability statement P[(^b/C0b)0X0X(^b/C0b)=(kþ1)s2/C20Fa,kþ1,n/C0k/C01]¼1/C0a,8.6 CONFIDENCE INTERVALS AND PREDICTION INTERVALS 209 --- Page 223 --- where s2¼SSE=(n/C0k/C01).",
    "C0a,8.6 CONFIDENCE INTERVALS AND PREDICTION INTERVALS 209 --- Page 223 --- where s2¼SSE=(n/C0k/C01). From this statement, a 100(1 /C0a)% joint conﬁdence region for b0,b1,...,bkinbis deﬁned to consist of all vectors bthat satisfy (^b/C0b)0X0X(^b/C0b)/C20(kþ1)s2Fa,kþ1,n/C0k/C01: (8:46) Fork¼1, this region can be plotted as an ellipse in two dimensions. For k.1, the ellipsoidal region in (8.46) is unwieldy to interpret and report, and we therefore con- sider intervals for the individual bj’s.",
    "is unwieldy to interpret and report, and we therefore con- sider intervals for the individual bj’s. 8.6.2 Conﬁdence Interval for bj Ifbj=0, we can subtract bjin (8.40) so that tj¼(^bj/C0bj)=sﬃﬃﬃﬃﬃ ﬃgjjphas the central t distribution, where gjjis the jth diagonal element of ( X0X)/C01.",
    "0bj)=sﬃﬃﬃﬃﬃ ﬃgjjphas the central t distribution, where gjjis the jth diagonal element of ( X0X)/C01. Then P/C0ta=2,n/C0k/C01/C20^bj/C0bj sﬃﬃﬃﬃﬃ ﬃg jjp /C20ta=2,n/C0k/C01\"# ¼1/C0a: Solving the inequality for bjgives P(^bj/C0ta=2,n/C0k/C01sﬃﬃﬃﬃﬃ ﬃgjjp/C20bj/C20^bjþta=2,n/C0k/C01sﬃﬃﬃﬃﬃ ﬃg jjp)¼1/C0a: Before taking the sample, the probability that the random interval will contain bjis 12a.After taking the sample, the 100(1 2a)% conﬁdence interval for bj ^bj+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃ ﬃgjjp(8:47) is no longer random, and thus we say that we are 100(1 2a)%conﬁdent that the interval contains bj.",
    ") is no longer random, and thus we say that we are 100(1 2a)%conﬁdent that the interval contains bj. Note that the conﬁdence coefﬁcient 1 2aholds only for a single conﬁdence inter- val for one of the bj’s. For conﬁdence intervals for all kþ1 of theb’s that hold simultaneously with overall conﬁdence coefﬁcient 1 2a, see Section 8.6.7. Example 8.6.2. We compute a 95% conﬁdence interval for each bjusing y2in the chemical reaction data in Table 7.4 (see Example 8.2a).",
    "conﬁdence interval for each bjusing y2in the chemical reaction data in Table 7.4 (see Example 8.2a). The matrix ( X0X)/C01(see the answer to Problem 7.52) and the estimate ^bhave the following values: (X0X)/C01¼65:37550 /C00:33885 /C00:31252 /C00:02041 /C00:33885 0 :00184 0 :00127 /C00:00043 /C00:31252 0 :00127 0 :00408 /C00:00176 /C00:02041 /C00:00043 /C00:00176 0 :021610 BB@1 CCA, ^b¼/C026:0353 0:4046 0:2930 1:03380 BB@1 CCA:210 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 224 --- Forb1, we obtain by (8.47), ^b1+t:025,15sﬃﬃﬃﬃﬃﬃg11p :4046+(2:1314)(4 :0781)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :00184p :4046+:3723 , (:0322 ,:7769) : For the other bj’s, we have b0:/C026:0353+70:2812 (/C096:3165 ,44:2459) , b2::2930+:5551 (/C0:2621 ,:8481) , b3:1:0338+1:27777 (/C0:2439 ,2:3115) : The conﬁdence coefﬁcient .95 holds for only one of the four conﬁdence intervals.",
    "/C0:2439 ,2:3115) : The conﬁdence coefﬁcient .95 holds for only one of the four conﬁdence intervals. For more than one interval, see Example 8.6.7. A 8.6.3 Conﬁdence Interval for a0b Ifa0b=0, we can subtract a0bfrom a0^bin (8.44) to obtain F¼(a0^b/C0a0b)2 s2a0(X0X)/C01a, which is distributed as F(1,n/C0k/C01).",
    "from a0^bin (8.44) to obtain F¼(a0^b/C0a0b)2 s2a0(X0X)/C01a, which is distributed as F(1,n/C0k/C01). Then by Problem 5.16, t¼a0^b/C0a0b sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ a0(X0X)/C01ap (8:48) is distributed as t(n/C0k/C01), and a 100(1 /C0a)% conﬁdence interval for a single value of a0bis given by a0^b+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ a0(X0X)/C01a:q (8:49) 8.6.4 Conﬁdence Interval for E(y) Letx0¼(1,x01,x02,...,x0k)0denote a particular choice of x¼(1,x1,x2,...,xk)0.",
    "dence Interval for E(y) Letx0¼(1,x01,x02,...,x0k)0denote a particular choice of x¼(1,x1,x2,...,xk)0. Note that x0need not be one of the x’s in the sample; that is, x00need not be a row ofX.I fx0is very far outside the area covered by the sample however, the prediction may be poor. Let y0be an observation corresponding to x0.",
    "ered by the sample however, the prediction may be poor. Let y0be an observation corresponding to x0. Then y0¼x0 0bþ1,8.6 CONFIDENCE INTERVALS AND PREDICTION INTERVALS 211 --- Page 225 --- and [assuming that the model is correct so that E(1)¼0] E(y0)¼x0 0b: (8:50) We wish to ﬁnd a conﬁdence interval for E(y0), that is, for the mean of the distri- bution of y-values corresponding to x0.",
    "nce interval for E(y0), that is, for the mean of the distri- bution of y-values corresponding to x0. By Corollary 1 to Theorem 7.6d, the minimum variance unbiased estimator of E(y0) is given by dE(y0)¼x00^b: (8:51) Since (8.50) and (8.51) are of the form a0banda0^b, respectively, we obtain a 100(12a)% conﬁdence interval for E(y0)¼x0 0bfrom (8.49): x0 0^b+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ x0 0(X0X)/C01x0q : (8:52) The conﬁdence coefﬁcient 1 2afor the interval in (8.52) holds only for a single choice of the vector x0.",
    "conﬁdence coefﬁcient 1 2afor the interval in (8.52) holds only for a single choice of the vector x0. For intervals covering several values of x0or all possible values of x0, see Section 8.6.7. We can express the conﬁdence interval in (8.52) in terms of the centered model in Section 7.5, yi¼aþb0 1(x01/C0/C22x1)þ1i, where x01¼(x01,x02,...,x0k)0and /C22x1¼ (/C22x1,/C22x2,...,/C22xk)0.",
    ".5, yi¼aþb0 1(x01/C0/C22x1)þ1i, where x01¼(x01,x02,...,x0k)0and /C22x1¼ (/C22x1,/C22x2,...,/C22xk)0. [We use the notation x01to distinguish this vector from x0¼ (1,x01,x02,...,x0k)0above.] For the centered model, (8.50), (8.51), and (8.52) become E(y0)¼aþb0 1(x01/C0/C22x1), (8:53) dE(y0)¼/C22yþ^b0 1(x01/C0/C22x1), (8:54) /C22yþ^b0 1(x01/C0/C22x1)+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 nþ(x01/C0/C22x1)0(X0 cXc)/C01(x01/C0/C22x1)r : (8:55) Note that in the form shown in (8.55), it is clear that if x01is close to /C22x1the interval is narrower; in fact, it is narrowest for x01¼/C22x.",
    "clear that if x01is close to /C22x1the interval is narrower; in fact, it is narrowest for x01¼/C22x. The width of the interval increases as the distance of x01from /C22x1increases. For the special case of simple linear regression, (8.50), (8.51), and (8.55) reduce to E(y0)¼b0þb1x0, (8:56) dE(y0)¼^b0þ^b1x0, (8:57) ^b0þ^b1x0+ta=2,n/C02sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 nþ(x0/C0/C22x)2 Pn i¼1(xi/C0/C22x)2s , (8:58) where sis given by (6.11).",
    "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 nþ(x0/C0/C22x)2 Pn i¼1(xi/C0/C22x)2s , (8:58) where sis given by (6.11). The width of the interval in (8.58) depends on how far x0is from /C22x.212 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 226 --- Example 8.6.4. For the grades data in Example 6.2, we ﬁnd a 95% conﬁdence interval for E(y0), where x0¼80.",
    "e 8.6.4. For the grades data in Example 6.2, we ﬁnd a 95% conﬁdence interval for E(y0), where x0¼80. Using (8.58), we obtain ^b0þ^b1(80)+t:025,16sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 18þ(80/C058:056)2 19530 :944s , 80:5386+2:1199(13 :8547)( :2832) , 80:5386+8:3183 , (72:2204 ,88:8569) : A 8.6.5 Prediction Interval for a Future Observation A “conﬁdence interval” for a future observation y0corresponding to x0is called a prediction interval .",
    "“conﬁdence interval” for a future observation y0corresponding to x0is called a prediction interval . We speak of a prediction interval rather than a conﬁdence interval because y0is an individual observation and is thereby a random variable rather than a parameter. To be 100(1 2a)% conﬁdent that the interval contains y0, the prediction interval will clearly have to be wider than a conﬁdence interval for the parameter E(y0).",
    "prediction interval will clearly have to be wider than a conﬁdence interval for the parameter E(y0). Since y0¼x00bþ10, we predict y0by^y0¼x0 0^b, which is also the estimator of E(y0)¼x00b. The random variables y0and ^y0are independent because y0is a future observation to be obtained independently of the nobservations used to compute ^y0¼x00^b.",
    "is a future observation to be obtained independently of the nobservations used to compute ^y0¼x00^b. Hence the variance of y0/C0^y0is var(y0/C0^y0)¼var(y0/C0x0 0^b)¼var(x00bþ10/C0x00^b): Since x0 0bis a constant, this becomes var(y0/C0^y0)¼var(10)þvar(x0 0^b)¼s2þs2x00(X0X)/C01x0 ¼s21þx00(X0X)/C01x0/C2/C3 , (8:59) which is estimated by s2[1þx0 0(X0X)/C01x0]. It can be shown that E(y0/C0^y0)¼0 and thats2is independent of both y0and ^y0¼x00^b.",
    "0(X0X)/C01x0]. It can be shown that E(y0/C0^y0)¼0 and thats2is independent of both y0and ^y0¼x00^b. Therefore, the tstatistic t¼y0/C0^y0/C00 sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01x0q (8:60) is distributed as t(n/C0k/C01), and P¼/C0 ta=2,n/C0k/C01/C20y0/C0^y0 sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01x0q /C20ta=2,n/C0k/C012 643 75¼1/C0a:8.6 CONFIDENCE INTERVALS AND PREDICTION INTERVALS 213 --- Page 227 --- The inequality can be solved for y0to obtain the 100(1 2a)% prediction interval ^y0/C0ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01x0q /C20y0/C20^y0þta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01x0q or, using ^y0¼x0 0^b,w eh a v e x0 0^b+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01x0q : (8:61) Note that the conﬁdence coefﬁcient 1 2afor the prediction interval in (8.61) holds for only one value of x0.",
    "t the conﬁdence coefﬁcient 1 2afor the prediction interval in (8.61) holds for only one value of x0. In 1þx0 0(X0X)/C01x0, the second term, x00(X0X)/C01x0, is typically much smaller than 1 (provided kis much smaller than n) because the variance of ^y0¼x0 0^bis much less than the variance of y0.",
    "ded kis much smaller than n) because the variance of ^y0¼x0 0^bis much less than the variance of y0. [To illustrate, if X0Xwere diagonal and x0were in the area covered by the rows of X, then x00(X0X)/C01x0would be a sum with kþ1 terms, each of the form x2 0j=Pn i¼1x2 ij, which is of the order of 1 /n.] Thus prediction intervals fory0are generally much wider than conﬁdence intervals for E(y0)¼x00b.",
    "n.] Thus prediction intervals fory0are generally much wider than conﬁdence intervals for E(y0)¼x00b. In terms of the centered model in Section 7.5, the 100(1 2a)% prediction interval in (8.61) becomes /C22yþ^b0 1(x01/C0/C22x1)+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ 1þ1 nþ(x01/C0/C22x1)0(X0 cXc)/C01(x01/C0/C22x1)r :(8:62) For the case of simple linear regression, (8.61) and (8.62) reduce to ^b0þ^b1x0+ta=2,n/C02sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þ1 nþ(x0/C0/C22x)2 Pn i¼1(xi/C0/C22x)2s , (8:63) where sis given by (6.11).",
    "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þ1 nþ(x0/C0/C22x)2 Pn i¼1(xi/C0/C22x)2s , (8:63) where sis given by (6.11). In (8.63), it is clear that the second and third terms within the square root are much smaller than 1 unless x0is far removed from the interval bounded by the smallest and largest x’s. For a prediction interval for the mean of qfuture observations, see Problem 8.30. Example 8.6.5. Using the data from Example 6.2, we ﬁnd a 95% prediction interval fory0when x0¼80.",
    "0. Example 8.6.5. Using the data from Example 6.2, we ﬁnd a 95% prediction interval fory0when x0¼80. Using (8.63), we obtain ^b0þ^b1(80)+t:025,16sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þ1 18þ(80/C058:056)2 19530 :944r , 80:5386+2:1199(13 :8547)(1 :0393) , 80:5386+30:5258 , (50:0128 ,111:0644) :214 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 228 --- Note that the prediction interval for y0here is much wider than the conﬁdence interval forE(y0) in Example 8.6.4.",
    "prediction interval for y0here is much wider than the conﬁdence interval forE(y0) in Example 8.6.4. A 8.6.6 Conﬁdence Interval for s2 By Theorem 7.6b(ii), ( n/C0k/C01)s2=s2isx2(n/C0k/C01). Therefore Px2 1/C0a=2,n/C0k/C01/C20(n/C0k/C01)s2 s2/C20x2 a=2,n/C0k/C01/C20/C21 ¼1/C0a, (8:64) wherex2 a=2,n/C0k/C01is the upper a=2 percentage point of the chi-square distribution and x2 1/C0a=2,n/C0k/C01is the lower a=2 percentage point.",
    "tage point of the chi-square distribution and x2 1/C0a=2,n/C0k/C01is the lower a=2 percentage point. Solving the inequality for s2yields the 100(1 2a)% conﬁdence interval (n/C0k/C01)s2 x2 a=2,n/C0k/C01/C20s2/C20(n/C0k/C01)s2 x2 1/C0a=2,n/C0k/C01: (8:65) A 100(1 2a)% conﬁdence interval for sis given by ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (n/C0k/C01)s2 x2 a=2,n/C0k/C01s /C20s/C20ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (n/C0k/C01)s2 x21/C0 a=2,n/C0k/C01s : (8:66) 8.6.7 Simultaneous Intervals By analogy to the discussion of testing several hypotheses (Section 8.5.2), when several intervals are computed, two conﬁdence coefﬁcients can be considered: familywise conﬁdence (1 2af) and individual conﬁdence (1 2ac).",
    "ﬁdence coefﬁcients can be considered: familywise conﬁdence (1 2af) and individual conﬁdence (1 2ac). Familywise conﬁ- dence of 1 2afmeans that we are 100(1 2af)% conﬁdent that every interval contains its respective parameter. In some cases, our goal is simply to control 1 2acfor each one of several conﬁ- dence or prediction intervals so that no changes are needed to expressions (8.47), (8.49), (8.52), and (8.61). In other cases the desire is to control 1 2af.",
    "ed to expressions (8.47), (8.49), (8.52), and (8.61). In other cases the desire is to control 1 2af. To do so, both the Bonferroni and Scheffe ´methods can be adapted to the situation of multiple intervals. In yet other cases we may want to control other properties of multiple inter-vals (Benjamini and Yekutieli 2005). The Bonferroni procedure increases the width of each individual interval so that 12 affor the set of intervals is greater than or equal to the desired value 1 2a/C3.",
    "nterval so that 12 affor the set of intervals is greater than or equal to the desired value 1 2a/C3. As an example suppose that it is desired to calculate the kconﬁdence intervals for b1,...,bk. Let Ejbe the event that the jth interval includes bj, and Ejcbe the comp- lement of that event.",
    ",bk. Let Ejbe the event that the jth interval includes bj, and Ejcbe the comp- lement of that event. Then by deﬁnition 1/C0af¼P(E1>E2>...>Ek) ¼1/C0P(Ec 1<Ec 2<...<Ec k):8.6 CONFIDENCE INTERVALS AND PREDICTION INTERVALS 215 --- Page 229 --- Assuming that P(Ec j)¼acforj¼1,...,k, the Bonferroni inequality now implies that 1/C0af/C211/C0kac: Hence we can ensure that 1 2afis greater than or equal to the desired 1 2a/C3by setting 1/C0ac¼1/C0a/C3=kfor the individual intervals.",
    "reater than or equal to the desired 1 2a/C3by setting 1/C0ac¼1/C0a/C3=kfor the individual intervals. Using this approach, Bonferroni conﬁdence intervals for b1,b2,...,bkare given by ^bj+ta/C3=2k,n/C0k/C01sﬃﬃﬃﬃﬃgjjp,j¼1,2,...,k, (8:67) where gjjis the jth element of ( X0X)/C01. Bonferroni tvalues ta/C3=2kare available in Bailey (1977) and can also be obtained in many software programs.",
    "i tvalues ta/C3=2kare available in Bailey (1977) and can also be obtained in many software programs. For example, a probability calculator for the t,t h e F, and other distributions is available free from NCSS (download at www.ncss.com).",
    "r for the t,t h e F, and other distributions is available free from NCSS (download at www.ncss.com). Similarly for dlinear functions a0 1b,a02b,...,a0db(chosen before seeing the data), Bonferroni conﬁdence intervals are given by a0 i^b+ta/C3=2d,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ a0 i(X0X)/C01aiq ,i¼1,2,...,d: (8:68) These intervals hold simultaneously with familywise conﬁdence of at least 1 /C0a/C3.",
    "2,...,d: (8:68) These intervals hold simultaneously with familywise conﬁdence of at least 1 /C0a/C3. Bonferroni conﬁdence intervals for E(y0)¼x0 0bfor a few values of x0,s a y , x01,x02,...,x0dare given by x0 0i^b+ta/C3=2d,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ x0 0i(X0X)/C01x0iq ,i¼1,2,...,d: (8:69) [Note that x01here differs from x01in (8.53)–(8.55).] For simultaneous prediction of dnew observations y01,y02,...,y0datdvalues of x0,s a y , x01,x02,...,x0d, we can use the Bonferroni prediction intervals x0 0i^b+ta/C3=2d,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0i(X0X)/C01x0iq i¼1,2,...,d (8:70) [see (8.61) and (8.69)].",
    "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0i(X0X)/C01x0iq i¼1,2,...,d (8:70) [see (8.61) and (8.69)]. Simultaneous Scheffe ´conﬁdence intervals for all possible linear functions a0b (including those chosen after seeing the data) can be based on the distribution of max aF[Theorem 8.5(ii)].",
    "ing those chosen after seeing the data) can be based on the distribution of max aF[Theorem 8.5(ii)]. Thus a conservative conﬁdence interval for any and all a0bis a0^b+sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (kþ1)Fa/C3,kþ1,n/C0k/C01a0(X0X)/C01aq : (8:71) The (potentially inﬁnite number of) intervals in (8.71) have an overall conﬁdence coefﬁcient of at least 1 2a/C3.",
    "lly inﬁnite number of) intervals in (8.71) have an overall conﬁdence coefﬁcient of at least 1 2a/C3. For a few linear functions, the intervals in (8.68) will be narrower, but for a large number of linear functions, the intervals in (8.71) will be narrower.",
    "l be narrower, but for a large number of linear functions, the intervals in (8.71) will be narrower. A comparison of ta/C3=2d,n/C0k/C01andﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ(kþ1)Fa/C3,kþ1,n/C0k/C01pwill show which is preferred in a given case.216 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 230 --- For conﬁdence limits for E(y0)¼x0 0bfor all possible values of x0, we use (8.71): x0 0^b+sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (kþ1)Fa/C3,kþ1,n/C0k/C01x0 0(X0X)/C01x0q : (8:72) These intervals hold simultaneously with a conﬁdence coefﬁcient of 1 /C0a/C3.",
    "0(X0X)/C01x0q : (8:72) These intervals hold simultaneously with a conﬁdence coefﬁcient of 1 /C0a/C3. Thus, (8.72) becomes a conﬁdence region that can be applied to the entire regression surface for all values of x0. The intervals in (8.71) and (8.72) are due to Scheffe ´ (1953; 1959, p. 68) and Working and Hotelling (1929).",
    "vals in (8.71) and (8.72) are due to Scheffe ´ (1953; 1959, p. 68) and Working and Hotelling (1929). Scheffe ´-type prediction intervals for y01,y02,...,y0dare given by x0 0i^b+sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ dFa/C3,d,h/C0k/C01[1þx0 0i(X0X)/C01x0i]q i¼1,2,...,d (8:73) (see Problem 8.32). These dprediction intervals hold simultaneously with overall conﬁdence coefﬁcient at least 1 /C0a/C3, but note that dFa/C3,d,n/C0k/C01is not constant.",
    "th overall conﬁdence coefﬁcient at least 1 /C0a/C3, but note that dFa/C3,d,n/C0k/C01is not constant. It depends on the number of predictions. Example 8.6.7. We compute 95% Bonferroni conﬁdence limits for b1,b2, andb3, using y2in the chemical reaction data in Table 7.4; see Example 8.6.2 for ( X0X)/C01 and ^b.",
    "andb3, using y2in the chemical reaction data in Table 7.4; see Example 8.6.2 for ( X0X)/C01 and ^b. By (8.67), we have ^b1+t:025=3,15sﬃﬃﬃﬃﬃﬃg11p :4056+(2:6937)(4 :0781)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :00184p :4056+:4706 (/C0:0660 ,:8751) , b2: :2930+:7016 (/C0:4086 ,:9946) , b3:1:0338+1:6147 (/C0:5809 ,2:6485) : These three intervals hold simultaneously with conﬁdence coefﬁcient at least .95.",
    "C0:5809 ,2:6485) : These three intervals hold simultaneously with conﬁdence coefﬁcient at least .95. A 8.7 LIKELIHOOD RATIO TESTS The tests in Sections 8.1, 8.2, and 8.4 were derived using informal methods based on ﬁnding sums of squares that have chi-square distributions and are independent. These same tests can be obtained more formally by the likelihood ratio approach. Likelihood ratio tests have some good properties and sometimes have optimalproperties.",
    "tio approach. Likelihood ratio tests have some good properties and sometimes have optimalproperties. We describe the likelihood ratio method in the simple context of testing H 0:b¼0versus H1:b=0. The likelihood function L(b,s2) was deﬁned in Section 7.6.2 as the joint density of the y’s.",
    "H1:b=0. The likelihood function L(b,s2) was deﬁned in Section 7.6.2 as the joint density of the y’s. For a random sample y¼(y1,y2,...,yn)0with density NnðXb,s2IÞ, the likelihood function is given8.7 LIKELIHOOD RATIO TESTS 217 --- Page 231 --- by (7.50) as L(b,s2)¼1 (2ps2)n=2e/C0(y/C0Xb)0(y/C0Xb)=2s2: (8:74) The likelihood ratio method compares the maximum value of L(b,s2) restricted byH0:b¼0to the maximum value of L(b,s2) under H1:b1=0, which is essen- tially unrestricted.",
    "stricted byH0:b¼0to the maximum value of L(b,s2) under H1:b1=0, which is essen- tially unrestricted. We denote the maximum value of L(b,s2) restricted by b¼0 asmax H0L(b,s2) and the unrestricted maximum as max H1L(b,s2). Ifbis equal (or close) to 0, then max H0L(b,s2) should be close to max H1L(b,s2). If max H0L(b,s2) is not close to max H1L(b,s2), we would conclude that y¼(y1,y2,...,yn)0apparently did not come from Nn(Xb,s2I) withb¼0.",
    "H1L(b,s2), we would conclude that y¼(y1,y2,...,yn)0apparently did not come from Nn(Xb,s2I) withb¼0. In this illustration, we can ﬁnd max H0L(b,s2) by setting b¼0and then estimating s2as the value that maximizes L(0,s2). Under H1:b=0, bothbands2are esti- mated without restriction as the values that maximize L(b,s2).",
    "2). Under H1:b=0, bothbands2are esti- mated without restriction as the values that maximize L(b,s2). [In designating the unrestricted maximum as max H1L(b,s2), we are ignoring the restriction in H1that b=0.] It is customary to describe the likelihood ratio method in terms of maximizing L subject to v, the set of all values of bands2satisfying H0, and subject to V, the set of all values of bands2without restrictions (other than natural restrictions such as s2.0).",
    "the set of all values of bands2without restrictions (other than natural restrictions such as s2.0). However, to simplify notation in cases such as this in which H1includes all values of bexcept 0, we refer to maximizing Lunder H0andH1.",
    "ses such as this in which H1includes all values of bexcept 0, we refer to maximizing Lunder H0andH1. We compare the restricted maximum under H0with the unrestricted maximum under H1by the likelihood ratio LR¼max H0L(b,s2) max H1L(b,s2) ¼max L(0,s2) max L(b,s2): (8:75) It is clear that 0 /C20LR/C201, because the maximum of Lrestricted to b¼0cannot exceed the unrestricted maximum. Smaller values of LR would favor H1, and larger values would favor H0.",
    "eed the unrestricted maximum. Smaller values of LR would favor H1, and larger values would favor H0. We thus reject H0if LR /C20c,w h e r e cis chosen so thatP(LR/C20c)¼aifH0is true. Wald (1943) showed that, under H0 /C02lnLR is approximately x2(n) for large n,w h e r enis the number of parameters estimated under H1minus the number estimated under H0. In the case of H0:b¼0versus H1:b=0,w eh a v e n¼kþ2/C01¼kþ1 because bands2are estimated under H1while only s2is estimated under H0.",
    "b=0,w eh a v e n¼kþ2/C01¼kþ1 because bands2are estimated under H1while only s2is estimated under H0. In some cases, the x2approximation is not needed because LR turns out to be a function of a familiar test statistic, such as torF, whose exact distribution is available.218 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 232 --- We now obtain the likelihood ratio test for H0:b¼0.",
    "THESES AND CONFIDENCE INTERVALS --- Page 232 --- We now obtain the likelihood ratio test for H0:b¼0. The resulting likelihood ratio is a function of the Fstatistic obtained in Problem 8.6 by partitioning the total sum of squares. Theorem 8.7a. IfyisNn(Xb,s2I), the likelihood ratio test for H0:b¼0can be based on F¼^b0X0y=(kþ1) (y0y/C0^b0X0y)=(n/C0k/C01): We reject H0ifF.Fa,kþ1,n/C0k/C01. PROOF.",
    "0can be based on F¼^b0X0y=(kþ1) (y0y/C0^b0X0y)=(n/C0k/C01): We reject H0ifF.Fa,kþ1,n/C0k/C01. PROOF. To ﬁnd max H1L(b,s2)¼max L(b,s2), we use the maximum likelihood estimators ^b¼(X0X)/C01X0yand ^s2¼(y/C0X^b)0(y/C0X^b)=nfrom Theorem 7.6a.",
    "use the maximum likelihood estimators ^b¼(X0X)/C01X0yand ^s2¼(y/C0X^b)0(y/C0X^b)=nfrom Theorem 7.6a. Substituting these in (8.74), we obtain max H1L(b,s2)¼max L(b,s2)¼L(^b,^s2) ¼1 (2p^s2)n=2e/C0(y/C0X^b)0(y/C0X^b)=2^s2 ¼nn=2e/C0n=2 (2p)n=2(y/C0X^b)0(y/C0X^b)hin=2: (8:76) To ﬁnd max H0L(b,s2)¼max L(0,s2), we solve @lnL(0,s2)=@s2¼0 to obtain ^s2 0¼y0y n: (8:77) Then max H0L(b,s2)¼max L(0,s2)¼L(0,^s2 0) ¼1 (2p^s2 0)n=2e/C0y0y=2^s2 0 ¼nn=2e/C0n=2 (2p)n=2(y0y)n=2: (8:78)8.7 LIKELIHOOD RATIO TESTS 219 --- Page 233 --- Substituting (8.76) and (8.78) into (8.75), we obtain LR¼max H0L(b,s2) max H1L(b,s2)¼(y/C0X^b)0(y/C0X^b) y0y\"#n=2 ¼1 1þ(kþ1)F=(n/C0k/C01)/C20/C21n=2 , (8:79) where F¼^b0X0y=(kþ1) (y0y/C0^b0X0y)=(n/C0k/C01): Thus, rejecting H0:b¼0for a small value of LR is equivalent to rejecting H0for a large value of F.",
    ": Thus, rejecting H0:b¼0for a small value of LR is equivalent to rejecting H0for a large value of F. A We now show that the Ftest in Theorem 8.4b for the general linear hypothesis H0:Cb¼0is a likelihood ratio test. Theorem 8.7b. IfyisNn(Xb,s2I), then the Ftest for H0:Cb¼0in Theorem 8.4b is equivalent to the likelihood ratio test. PROOF. Under H1:Cb=0, which is essentially unrestricted, max H1L(b,s2)i s given by (8.76).",
    "tio test. PROOF. Under H1:Cb=0, which is essentially unrestricted, max H1L(b,s2)i s given by (8.76). To ﬁnd max H0L(b,s2)¼max L(b,s2) subject to Cb¼0,w e use the method of Lagrange multipliers (Section 2.14.3) and work with L(b,s2) to simplify the differentiation: v¼lnL(b,s2)þl0(Cb/C00) ¼/C0n 2ln(2p)/C0n2ln s2/C0(y/C0Xb)0(y/C0Xb) 2s2þl0Cb: Expanding ( y/C0Xb)0(y/C0Xb) and differentiating with respect to b,l,ands2,w e obtain @v @b¼(2X0y/C02X0Xb)=2s2þC0l¼0, (8:80) @v @l¼Cb¼0, (8:81) @v @s2¼/C0n 2s2þ1 2(s2)2(y/C0Xb)0(y/C0Xb)¼0: (8:82)220 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 234 --- Eliminating land solving for bands2gives ^b0¼^b/C0(X0X)/C01C0[C(X0X)/C01C0]/C01C^b, (8:83) ^s2 0¼1 n(y/C0X^b0)0(y/C0X^b0)( 8 :84) ¼^s2þ1n(C^ b)0[C(X0X)/C01C0]/C01C^b (8:85) (Problems 8.35 and 8.36), where ^s2¼(y/C0X^b)0(y/C0X^b)=nand ^b¼(X0X)/C01X0y are the maximum likelihood estimates from Theorem 7.6a.",
    "^s2¼(y/C0X^b)0(y/C0X^b)=nand ^b¼(X0X)/C01X0y are the maximum likelihood estimates from Theorem 7.6a. Thus max H0L(b,s2)¼L(^b0,^s2 0) ¼1 (2p)n=2(^s2 0)n=2e/C0(y/C0X^b0)0(y/C0X^b0)=2^s2 0 ¼nn=2e/C0n=2 (2p)n=2SSEþ(C^b)0C(X0X)/C01C0/C2/C3/C01C^bnon=2, and LR¼max H0L(b,s2) max H1L(b,s2) ¼SSE SSEþ(C^b)0[C(X0X)/C01C0]/C01C^b/C20/C21n=2 ¼1 1þSSH=SSE/C20/C21n=2 ¼1 1þqF=(n/C0k/C01)/C20/C21n=2 , where SSH ¼(C^b)0[C(X0X)/C01C0]/C01C^b,SSE¼(y/C0X^b)0(y/C0X^b), and Fis given in (8.27).",
    "20/C21n=2 , where SSH ¼(C^b)0[C(X0X)/C01C0]/C01C^b,SSE¼(y/C0X^b)0(y/C0X^b), and Fis given in (8.27). A PROBLEMS 8.1 Show that SSR ¼^b0 1X0 cXc^b1in (8.1) becomes y0Xc(X0cXc)/C01X0cyas in (8.2). 8.2 (a) Show that Hc[I/C0(1=n)J]¼Hc, as in (8.3) in Theorem 8.1a(i), where Hc¼Xc(X0cXc)/C01X0c. (b) Prove Theorem 8.1a(ii). (c) Prove Theorem 8.1a(iii). (d) Prove Theorem 8.1a(iv).PROBLEMS 221 --- Page 235 --- 8.3 Show that l1¼b0 1XcXcb1=2s2as in Theorem 8.1b(i). 8.4 Prove Theorem 8.1b(ii).",
    "1 --- Page 235 --- 8.3 Show that l1¼b0 1XcXcb1=2s2as in Theorem 8.1b(i). 8.4 Prove Theorem 8.1b(ii). 8.5 Show that E(SSR =k)¼s2þ(1=k)b01X0 cXcb1, as in the expected mean square column of Table 8.1. Employ the following two approaches: (a) Use Theorem 5.2a. (b) Use the noncentrality parameter in (5.19). 8.6 Develop a test for H0:b¼0in the model y¼Xbþ1, where yis Nn(Xb,s2I).",
    "trality parameter in (5.19). 8.6 Develop a test for H0:b¼0in the model y¼Xbþ1, where yis Nn(Xb,s2I). (It was noted at the beginning of Section 8.1 that this hypothesis is of little practical interest because it includes b0¼0.) Use the partitioning y0y¼(y0y/C0^b0X0y)þ^b0X0y, and proceed as follows: (a) Show that ^b0X0y¼y0X(X0X)/C01X0yandy0y/C0^b0X0y¼y0[I/C0X(X0X)/C01 X0]y. (b) Let H¼X(X0X)/C01X0:Show that HandI/C0Hare idempotent of rank kþ1 and n/C0k/C01, respectively.",
    "y. (b) Let H¼X(X0X)/C01X0:Show that HandI/C0Hare idempotent of rank kþ1 and n/C0k/C01, respectively. (c) Show that y0Hy=s2isx2(kþ1,l1), wherel1¼b0X0Xb=2s2, and that y0(I/C0H)y=s2isx2(n/C0k/C01). (d) Show that y0Hyandy0(I/C0H)yare independent. (e) Show that ^b0X0y (kþ1)s2¼y0Hy=(kþ1) y0(I/C0H)y=(n/C0k/C01) is distributed as F(kþ1,n/C0k/C01,l1). 8.7 Show that HH 1¼H1andH1H¼H1, as in (8.15), where HandH1are as deﬁned in (8.11) and (8.12).",
    "1,l1). 8.7 Show that HH 1¼H1andH1H¼H1, as in (8.15), where HandH1are as deﬁned in (8.11) and (8.12). 8.8 Show that conditions (a) and (b) of Corollary 1 to Theorem 5.6c are satisﬁed for the sum of quadratic forms in (8.12), as noted in the proof of Theorem 8.2b. 8.9 Show that l1¼b0 2[X0 2X2/C0X02X1(X01X1)/C01X01X2]b2=2s2as in Theorem 8.2b(ii). 8.10 Show that X02X2/C0X02X1(X01X1)/C01X01X2is positive deﬁnite, as noted below Theorem 8.2b.",
    "b(ii). 8.10 Show that X02X2/C0X02X1(X01X1)/C01X01X2is positive deﬁnite, as noted below Theorem 8.2b. 8.11 Show that E[SS(b2jb1)=h]¼s2þb0 2[X0 2X2/C0X02X1(X01X1)/C01X01X2]b2=h as in Table 8.3. 8.12 Find the expected mean square corresponding to the numerator of the F statistic in (8.20) in Example 8.2b.",
    "he expected mean square corresponding to the numerator of the F statistic in (8.20) in Example 8.2b. 8.13 Show that ^b/C3 0¼/C22yand SS(b/C30)¼n/C22y2, as in (8.21) in Example 8.2c.222 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 236 --- 8.14 In the proof of Theorem 8.2d, show that ( ^b0 1X0 1þ^b0 2X0 2)(X1^b1þX2^b2)/C0 (^b0 1þ^b02A0)X0 1X1(^b1þA^b2)¼^b0 2[X0 2X2/C0X02X1(X01X1)/C01X01X2]^b2. 8.15 Express the test for H0:b2¼0in terms of R2, as in (8.25) in Theorem 8.3.",
    "1(X01X1)/C01X01X2]^b2. 8.15 Express the test for H0:b2¼0in terms of R2, as in (8.25) in Theorem 8.3. 8.16 Prove Theorem 8.4a(iv). 8.17 Show that C(X0X)/C01C0is positive deﬁnite, as noted following Theorem 8.4b. 8.18 Prove Theorem 8.4c. 8.19 Show that in the model y¼Xbþ1subject to Cb¼0in (8.29), the estimator ofbis^bc¼^b/C0(X0X)/C01C0[C(X0X)/C01C0]/C01C^bas in (8.30), where ^b¼ (X0X)/C01X0y.",
    "), the estimator ofbis^bc¼^b/C0(X0X)/C01C0[C(X0X)/C01C0]/C01C^bas in (8.30), where ^b¼ (X0X)/C01X0y. Use a Lagrange multiplier land minimize u¼ (y/C0Xb)0(y/C0Xb)þl0(Cb/C00) with respect to bandlas follows: (a) Differentiate uwith respect to land set the result equal to 0to obtain C^bc¼0. (b) Differentiate uwith respect to band set the result equal to 0to obtain ^bc¼^b/C01 2(X0X)/C01C0l, (1) where ^b¼(X0X)/C01X0y.",
    "pect to band set the result equal to 0to obtain ^bc¼^b/C01 2(X0X)/C01C0l, (1) where ^b¼(X0X)/C01X0y. (c) Multiply (1) in part (b) by C, use C^bc¼0from part (a), solve for l, and substitute back into (1). 8.20 Show that ^b0 cX0X^bc¼^b0cX0y, thus demonstrating directly that the sum of squares due to the reduced model is ^b0 cX0yand that (8.31) holds.",
    "strating directly that the sum of squares due to the reduced model is ^b0 cX0yand that (8.31) holds. 8.21 Show that for the general linear hypothesis H0:Cb¼0in Theorem 8.4d, we have ^b0X0y/C0^b0 cX0y¼(C^b)0[C(X0X)/C01C0]/C01C^bas in (8.32), where ^bcis as given in (8.30). 8.22 Prove Theorem 8.4e. 8.23 Prove Theorem 8.4f(iv) by expressing SSH and SSE as quadratic forms in the same normally distributed random vector.",
    ".4f(iv) by expressing SSH and SSE as quadratic forms in the same normally distributed random vector. 8.24 Show that the estimator for bin the reduced model y¼Xbþ1subject to Cb¼tis given by ^bc¼^b/C0(X0X)/C01C0[C(X0X)/C01C0]/C01(C^b/C0t), where ^b¼(X0X)/C01X0y. 8.25 Show that ^b0X0y/C0^b/C30 1X0 1yin (8.37) is equal to ^b2 k=gkkin (8.39) (for j¼k), as noted below (8.39). 8.26 Obtain the conﬁdence interval for a0bin (8.49) from the tstatistic in (8.48).",
    "ted below (8.39). 8.26 Obtain the conﬁdence interval for a0bin (8.49) from the tstatistic in (8.48). 8.27 Show that the conﬁdence interval for x0 0bin (8.52) is the same as that for the centered model in (8.55). 8.28 Show that the conﬁdence interval for b0þb1x0in (8.58) follows from (8.55).PROBLEMS 223 --- Page 237 --- 8.29 Show that t¼(y0/C0^y0)=sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01x0q in (8.60) is distributed as t(n/C0k/C01).",
    "^y0)=sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01x0q in (8.60) is distributed as t(n/C0k/C01). 8.30 (a) Given that /C22y0¼Pq i¼y0i=qis the mean of qfuture observations at x0, show that a 100(1 /C0a)% prediction interval for /C22y0is given by x0 0^b+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1=qþx0 0(X0X)/C01x0q .",
    "C22y0is given by x0 0^b+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1=qþx0 0(X0X)/C01x0q . (b) Show that for simple linear regression, the prediction interval for /C22y0in part (a) reduces to ^b0þ^b1x0+ta=2,n/C02sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ 1=qþ1=nþ(x0/C0/C22x)2=Pn i¼1(xi/C0/C22x)2q . 8.31 Obtain the conﬁdence interval for s2in (8.65) from the probability statement in (8.64).",
    "2x)2q . 8.31 Obtain the conﬁdence interval for s2in (8.65) from the probability statement in (8.64). 8.32 Show that the Scheffe ´prediction intervals for dfuture observations are given by (8.73). 8.33 Verify (8.76)–(8.79) in the proof of Theorem 8.7a. 8.34 Verify (8.80), @v=@b¼(2X0y/C02X0Xb)=2s2þC0l. 8.35 Show that the solution to (8.80)–(8.82) is given by ^b0and ^s2 0in (8.83) and (8.84). 8.36 Show that ( y/C0X^b0)0(y/C0X^b0)¼n^s2þ(C^b)0[C(X0X)/C01C0]/C01C^bas in (8.85).",
    ".83) and (8.84). 8.36 Show that ( y/C0X^b0)0(y/C0X^b0)¼n^s2þ(C^b)0[C(X0X)/C01C0]/C01C^bas in (8.85). 8.37 Use the gas vapor data in Table 7.3. (a) Test the overall regression hypothesis H0:b1¼0using (8.5) [or (8.22)] and (8.23). (b) Test H0:b1¼b3¼0, that is, that x1andx3do not signiﬁcantly contrib- ute above and beyond x2andx4. (c) Test H0:bj¼0 for j¼1,2,3,4 using tjin (8.40). Use t:05=2for each test and also use a Bonferroni approach based on t:05=8(or compare the pvalue to .05 /4).",
    "=2for each test and also use a Bonferroni approach based on t:05=8(or compare the pvalue to .05 /4). (d) Using general linear hypothesis tests, test H0:b1¼b2¼12b3¼12b4, H01:b1¼b2,H02:b2¼12b3,H03:b3¼b4,andH04:b1¼b2andb3¼b4. (e) Find conﬁdence intervals for b1,b2,b3andb4using both (8.47) and (8.67). 8.38 Use the land rent data in Table 7.5. (a) Test the overall regression hypothesis H0:b1¼0using (8.5) [or (8.22)] and (8.23). (b) Test H0:bj¼0 for j¼1,2,3 using tjin (8.40).",
    "ypothesis H0:b1¼0using (8.5) [or (8.22)] and (8.23). (b) Test H0:bj¼0 for j¼1,2,3 using tjin (8.40). Use t:05=2for each test and also use a Bonferroni approach based on t:05=6(or compare the pvalue to .05/3). (c) Find conﬁdence intervals for b1,b2,b3using both (8.47) and (8.67).",
    "compare the pvalue to .05/3). (c) Find conﬁdence intervals for b1,b2,b3using both (8.47) and (8.67). (d) Using (8.52), ﬁnd a 95% conﬁdence interval for E(y0)¼x00b, where x00¼(1,15,30,:5).224 TESTS OF HYPOTHESES AND CONFIDENCE INTERVALS --- Page 238 --- (e) Using (8.61), ﬁnd a 95% prediction interval for y0¼x0 0bþ1, where x00¼(1,15,30,:5). 8.39 Usey2in the chemical reaction data in Table 7.4. (a)Using (8.52), ﬁnd a 95% conﬁdence interval for E(y0)¼x00b, where x00¼(1,165,32,5).",
    "in Table 7.4. (a)Using (8.52), ﬁnd a 95% conﬁdence interval for E(y0)¼x00b, where x00¼(1,165,32,5). (b) Using (8.61), ﬁnd a 95% prediction interval for y0¼x00bþ1, where x00¼(1,165,32,5). (c) Test H0:2b1¼2b2¼b3using (8.27). (This was done for y1in Example 8.4.b.) 8.40 Usey1in the chemical reaction data in Table 7.4. The full model with second- order terms and the reduced model with only linear terms were ﬁt in Problem 7.52.",
    "model with second- order terms and the reduced model with only linear terms were ﬁt in Problem 7.52. (a)Test H0:b4¼b5¼/C1/C1/C1¼b9¼0, that is, that the second-order terms are not useful in predicting y1. (This was done for y2in Example 8.2a.) (b) Test the signiﬁcance of the increase in R2from the reduced model to the full model. (This was done for y2in Example 8.3. See Problem 7.52 for values of R2.) (c) Find a 95% conﬁdence interval for each of b0,b1,b2,b3using (8.47).",
    "oblem 7.52 for values of R2.) (c) Find a 95% conﬁdence interval for each of b0,b1,b2,b3using (8.47). (d) Find Bonferroni conﬁdence intervals for b1,b2,b3using (8.67). (e) Using (8.52), ﬁnd a 95% conﬁdence interval for E(y0)¼x0 0b, where x00¼(1,165,32,5).",
    "sing (8.67). (e) Using (8.52), ﬁnd a 95% conﬁdence interval for E(y0)¼x0 0b, where x00¼(1,165,32,5). (f) Using (8.61), ﬁnd a 95%, prediction interval for y0¼x00bþ1, where x00¼(1,165,32,5).PROBLEMS 225 --- Page 239 --- 9Multiple Regression: Model Validation and Diagnostics In Sections 7.8.2 and 7.9 we discussed some consequences of misspeciﬁcation of the model. In this chapter we consider various approaches to checking the model and the attendant assumptions for adequacy and validity.",
    "er various approaches to checking the model and the attendant assumptions for adequacy and validity. Some properties of the residuals [see (7.11)] and the hat matrix are developed in Sections 9.1 and 9.2. We discuss out- liers, the inﬂuence of individual observations, and leverage in Sections 9.3 and 9.4. For additional reading, see Snee (1977), Cook (1977), Belsley et al.",
    "verage in Sections 9.3 and 9.4. For additional reading, see Snee (1977), Cook (1977), Belsley et al. (1980), Draper and Smith (1981, Chapter 6), Cook and Weisberg (1982), Beckman and Cook(1983), Weisberg (1985, Chapters 5, 6), Chatterjee and Hadi (1988), Myers (1990, Chapters 5–8), Sen and Srivastava (1990, Chapter 8), Montgomery and Peck (1992, pp.",
    "8), Myers (1990, Chapters 5–8), Sen and Srivastava (1990, Chapter 8), Montgomery and Peck (1992, pp. 67–113, 159–192), Jørgensen (1993, Chapter 5), Graybill and Iyer(1994, Chapter 5), Hocking (1996, Chapter 9), Christensen (1996, Chapter 13), Ryan (1997, Chapters 2, 5), Fox (1997, Chapters 11–13) and Kutner et al. (2005, Chapter 10). 9.1 RESIDUALS The usual model is given by (7.4) as y¼X bþ1with assumptions E(1)¼0 and cov(1)¼s2I, where yisn/C21,Xisn/C2(kþ1) of rank kþ1,n, andbis (kþ1)/C21.",
    "with assumptions E(1)¼0 and cov(1)¼s2I, where yisn/C21,Xisn/C2(kþ1) of rank kþ1,n, andbis (kþ1)/C21. The error vector 1is unobservable unless bis known. To estimate 1 for a given sample, we use the residual vector ˆ1¼y/C0X^b¼y/C0^y (9:1) as deﬁned in (7.11). The nresiduals in (9.1), 1ˆ1,1ˆ2,...,1ˆn, are used in various plots and procedures for checking on the validity or adequacy of the model. We ﬁrst consider some properties of the residual vector 1ˆ.",
    "n the validity or adequacy of the model. We ﬁrst consider some properties of the residual vector 1ˆ. Using the least-squares estimator ^b¼(X0X)/C01X0yin (7.6), the vector of predicted values ^y¼X^bcan be Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 227 --- Page 240 --- written as ^y¼X^b¼X(X0X)/C01X0y ¼Hy, (9:2) where H¼X(X0X)/C01X0(see Section 8.2).",
    "7 --- Page 240 --- written as ^y¼X^b¼X(X0X)/C01X0y ¼Hy, (9:2) where H¼X(X0X)/C01X0(see Section 8.2). The n/C2nmatrix His called the hat matrix because it transforms ytoyˆ. We also refer to Has a projection matrix for essentially the same reason; geometrically it projects y(perpendicularly) onto yˆ (see Fig. 7.4). The hat matrix His symmetric and idempotent (see Problem 5.32a).",
    "ndicularly) onto yˆ (see Fig. 7.4). The hat matrix His symmetric and idempotent (see Problem 5.32a). Multiplying XbyH, we obtain HX¼X(X0X)/C01X0X¼X: (9:3) Writing Xin terms of its columns and using (2.28), we can write (9.3) as HX¼H(j,x1,...xk)¼(Hj,Hx1,...,Hxk), so that j¼Hj,xi¼Hxi,i¼1,2,...,k: (9:4) Using (9.2), the residual vector 1ˆ(9.1) can be expressed in terms of H: ^1¼y/C0^y¼y/C0Hy ¼(I/C0H)y: (9:5) We can rewrite (9.5) to express the residual vector 1ˆin terms of 1: ^1¼(I/C0H)y¼(I/C0H)(Xbþ1) ¼(Xb/C0HXb)þ(I/C0H)1 ¼(Xb/C0Xb)þ(I/C0H)1 [by (9 :3)] ¼(I/C0H)1: (9:6) In terms of the elements hijofH,w eh a v e ^1i¼1i/C0Pn j¼1hij1j,i¼1,2,...,n.",
    "9 :3)] ¼(I/C0H)1: (9:6) In terms of the elements hijofH,w eh a v e ^1i¼1i/C0Pn j¼1hij1j,i¼1,2,...,n. Thus, if the hij’s are small (in absolute value), 1ˆis close to 1. The following are some of the properties of 1ˆ(see Problem 9.1).",
    "n absolute value), 1ˆis close to 1. The following are some of the properties of 1ˆ(see Problem 9.1). For the ﬁrst four, we assume that E(y)¼Xband cov( y)¼s2I: E(^1)¼0( 9 :7)228 MULTIPLE REGRESSION: MODEL VALIDATION AND DIAGNOSTICS --- Page 241 --- cov( ^1)¼s2[I/C0X(X0X)/C01X0]¼s2(I/C0H)( 9 :8) cov( ^1,y)¼s2[I/C0X(X0X)/C01X0]¼s2(I/C0H)( 9 :9) cov( ^1,^y)¼O (9:10) /C22^1¼Xn i¼1^1i=n¼^10j=n¼0( 9 :11) ^10y¼SSE¼y0[I/C0X(X0X)/C01X0]y¼y0(I/C0H)y (9:12) ^10^y¼0( 9 :13) ^10X¼00(9:14) In (9.7), the residual vector 1ˆhas the same mean as the error term 1, but in (9.8) cov( ^1)¼s2(I/C0H) differs from the assumption cov( 1)¼s2I.",
    "e mean as the error term 1, but in (9.8) cov( ^1)¼s2(I/C0H) differs from the assumption cov( 1)¼s2I. Thus the residuals ^11,^12,...,^1nare not independent. However, in many cases, especially if nis large, thehij’s tend to be small (for i=j), and the dependence shown in s2(I2H) does not unduly affect plots and other techniques for model validation. Each 1ˆiis seen to be correlated with each yjin (9.9), but in (9.10) the 1ˆi’s are uncorrelated with theyˆj’s.",
    "seen to be correlated with each yjin (9.9), but in (9.10) the 1ˆi’s are uncorrelated with theyˆj’s. Some sample properties of the residuals are given in (9.11)–(9.14). The sample mean of the residuals is zero, as shown in (9.11).",
    "residuals are given in (9.11)–(9.14). The sample mean of the residuals is zero, as shown in (9.11). By (9.12), it can be seen that 1ˆ andyare correlated in the sample since 1ˆ0yis the numerator of r^1y¼^10(y/C0/C22yj)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (ˆ10ˆ1)(y/C0/C22yj)0(y/C0/C22yj)p ¼ˆ10yﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ(ˆ 10ˆ1)(y/C0/C22yj)0(y/C0/C22yj)p : However, ^1and ^yare orthogonal by (9.13), and therefore r^1^y¼0: (9:15) Similarly, by (9.14), ^1is orthogonal to each column of Xand r^1xi¼0,i¼1,2,...,k: (9:16)9.1 RESIDUALS 229 --- Page 242 --- If the model and attendant assumptions are correct, then by (9.15), a plot of the residuals versus predicted values, ( ^11,^y1),(^12,^y2),...,(^1n,^yn), should show no sys- tematic pattern.",
    "ls versus predicted values, ( ^11,^y1),(^12,^y2),...,(^1n,^yn), should show no sys- tematic pattern. Likewise, by (9.16), the kplots of the residuals versus each of x1,x2,...,xkshould show only random variation. These plots are therefore useful for checking the model. A typical plot of this type is shown in Figure 9.1. It may also be useful to plot the residuals on normal probability paper and to plot residuals in time sequence (Christensen 1996, Section 13.2).",
    "on normal probability paper and to plot residuals in time sequence (Christensen 1996, Section 13.2). If the model is incorrect, various plots involving residuals may show departures from the ﬁtted model such as outliers, curvature, or nonconstant variance. Theplots may also suggest remedial measures to improve the ﬁt of the model. For example, the residuals could be plotted versus any of the x i’s, and a simple curved pattern might suggest the addition of x2 ito the model.",
    "versus any of the x i’s, and a simple curved pattern might suggest the addition of x2 ito the model. We will consider various approaches for detecting outliers in Section 9.3 and for ﬁnding inﬂuential observations in Section 9.4. Before doing so, we discuss some properties of the hatmatrix in Section 9.2. 9.2 THE HAT MATRIX It was noted following (9.2) that the hat matrix H¼X(X 0X)/C01X0is symmetric and idempotent. We now present some additional properties of this matrix.",
    "¼X(X 0X)/C01X0is symmetric and idempotent. We now present some additional properties of this matrix. These prop- erties will be useful in the discussion of outliers and inﬂuential observations in Sections 9.3 and 9.4.",
    "es will be useful in the discussion of outliers and inﬂuential observations in Sections 9.3 and 9.4. For the centered model y¼ajþXcb1þ1 (9:17) in (7.32), ^ybecomes ^y¼^ajþXc^b1, (9:18) Figure 9.1 Ideal residual plot when model is correct.230 MULTIPLE REGRESSION: MODEL VALIDATION AND DIAGNOSTICS --- Page 243 --- and the hat matrix is Hc¼Xc(X0 cXc)/C01X0c, where Xc¼I/C01 nJ/C18/C19 X1¼x11/C0/C22x1x12/C0/C22x2/C1/C1/C1 x1k/C0/C22xk x21/C0/C22x1x22/C0/C22x2/C1/C1/C1 x2k/C0/C22xk .........",
    "/C0/C22x1x12/C0/C22x2/C1/C1/C1 x1k/C0/C22xk x21/C0/C22x1x22/C0/C22x2/C1/C1/C1 x2k/C0/C22xk ......... xn1/C0/C22x1xn2/C0/C22x2/C1/C1/C1 xnk/C0/C22xk0 BBBB@1 CCCCA: By (7.36) and (7.37), we can write (9.18) as ^y¼/C22yjþXc(X0 cXc)/C01X0cy¼1 nj0y/C18/C19 jþHcy ¼1nJþH c/C18/C19 y: (9:19) Comparing (9.19) and (9.2), we have H¼1nJþH c¼1nJþX c(X0 cXc)/C01X0c: (9:20) We now examine some properties of the elements hijofH. Theorem 9.2.",
    "c¼1nJþX c(X0 cXc)/C01X0c: (9:20) We now examine some properties of the elements hijofH. Theorem 9.2. IfXisn/C2(kþ1) of rank kþ1,n, and if the ﬁrst column of Xisj, then the elements hijofH¼X(X0X)/C01X0have the following properties: (i) (1 =n)/C20hii/C201 for i¼1,2,...,n: (ii)/C0:5/C20hij/C20:5 for all j=i: (iii) hii¼(1=n)þ(x1i/C0/C22x1)0(X0 cXc)/C01(x1i/C0/C22x1), where x0 1i¼(xi1,xi2,...,xik), ¯x01¼(/C22x1,/C22x2,...,/C22xk), and ( x1i/C0/C22x1)0is the ith row of the centered matrix Xc: (iv) tr( H)¼Pn i¼1hii¼kþ1: PROOF (i) The lower bound follows from (9.20), since X0 cXcis positive deﬁnite.",
    "r( H)¼Pn i¼1hii¼kþ1: PROOF (i) The lower bound follows from (9.20), since X0 cXcis positive deﬁnite. Since H is symmetric and idempotent, we use the relationship H¼H2to ﬁnd an upper bound on hii. Let hi0be the ith row of H. Then hii¼h0 ihi¼(hi1,hi2,...,hin)hi1 hi2 ... hin0 BBBB@1 CCCCA¼Xn j¼1h2 ij ¼h2 iiþX j=ih2ij: (9:21)9.2 THE HAT MATRIX 231 --- Page 244 --- Dividing both sides of (9.21) by hii[which is positive since hii/C21(1=n)], we obtain 1¼hiiþP j=ih2 ij hii, (9:22) which implies hii/C201.",
    "hich is positive since hii/C21(1=n)], we obtain 1¼hiiþP j=ih2 ij hii, (9:22) which implies hii/C201. (ii) (Chatterjee and Hadi 1988, p. 18.) We can write (9.21) in the form hii¼h2 iiþh2ijþX r=i,jh2ir or hii/C0h2ii¼h2ijþX r=i,jh2ir: Thus, h2 ij/C20hii/C0h2ii, and since the maximum value of hii/C0h2iiis1 4,w eh a v e h2 ij/C201 4forj=i: (iii) This follows from (9.20); see Problem 9.2b. (iv) See Problem 9.2c. A By Theorem 9.2(iv), we see that as nincreases, the values of hiiwill tend to decrease.",
    "oblem 9.2c. A By Theorem 9.2(iv), we see that as nincreases, the values of hiiwill tend to decrease. The function ( x1i/C0/C22x1)0(X0 cXc)/C01(x1i/C0/C22x1) in Theorem 9.2(iii) is a standardized distance. The standardized distance (Mahalanobis distance) deﬁned in (3.27) is for a population covariance matrix. The matrix X0 cXcis proportional to a sample covari- ance matrix [see (7.44)].",
    "on covariance matrix. The matrix X0 cXcis proportional to a sample covari- ance matrix [see (7.44)]. Thus, ( x1i/C0/C22x1)0(X0 cXc)/C01(x1i/C0/C22x1) is an estimated standar- dized distance and provides a good measure of the relative distance of each x1ifrom the center of the points as represented by /C22x1: 9.3 OUTLIERS In some cases, the model appears to be correct for most of the data, but one residual is much larger (in absolute value) than the others.",
    "e correct for most of the data, but one residual is much larger (in absolute value) than the others. Such an outlier may be due to an error in recording or may be from another population or may simply be an unusual obser- vation from the assumed distribution. For example, if the errors 1iare distributed as N(0,s2), a value of 1igreater than 3 sor less than 23swould occur with frequency .0027.",
    "stributed as N(0,s2), a value of 1igreater than 3 sor less than 23swould occur with frequency .0027. If no explanation for an apparent outlier can be found, the dataset could be ana- lyzed both with and without the outlying observation. If the results differ sufﬁciently to affect the conclusions, then both analyses could be maintained until additional data become available. Another alternative is to discard the outlier, even though no expla- nation has been found.",
    "ailable. Another alternative is to discard the outlier, even though no expla- nation has been found. A third possibility is to use robust methods that accommodate232 MULTIPLE REGRESSION: MODEL VALIDATION AND DIAGNOSTICS --- Page 245 --- the outlying observation (Huber 1973, Andrews 1974, Hampel 1974, Welsch 1975, Devlin et al. 1975, Mosteller and Turkey 1977, Birch 1980, Krasker and Welsch 1982).",
    "4, Welsch 1975, Devlin et al. 1975, Mosteller and Turkey 1977, Birch 1980, Krasker and Welsch 1982). One approach to checking for outliers is to plot the residuals ^1iversus ^yior versus i, the observation number. In our examination of residuals, we need to keep in mind that by (9.8), the variance of the residuals is not constant: var(^1i)¼s2(1/C0hii): (9:23) By Theorem 9.2(i), hii/C201; hence, var( ^1i) will be small if hiiis near 1.",
    "1i)¼s2(1/C0hii): (9:23) By Theorem 9.2(i), hii/C201; hence, var( ^1i) will be small if hiiis near 1. By Theorem 9.2(iii), hiiwill be large if x1iis far from /C22x1, where x1i¼(xi1,xi2,...,xik)0and /C22x1¼(/C22x1,/C22x2,...,/C22xk)0. By (9.23), such observations will tend to have small residuals, which seems unfortunate because the model is less likely to hold far from /C22x1.",
    "e small residuals, which seems unfortunate because the model is less likely to hold far from /C22x1. A small residual at a point where x1iis far from /C22x1may result because the ﬁtted model will tend to pass close to a point isolated from the bulk of the points, with a resulting poorer ﬁt to the bulk of the data. This may mask an inadequacy of the true model in the region of x1i.",
    "orer ﬁt to the bulk of the data. This may mask an inadequacy of the true model in the region of x1i. An additional veriﬁcation that large values of hiiare accompanied by small residuals is provided by the following inequality (see Problem 9.4): 1 n/C20hiiþ^12 i ˆ10ˆ1/C201: (9:24) For the reasons implicit in (9.23) and (9.24), it is desirable to scale the residuals so that they have the same variance. There are two common (and related) methods of scaling.",
    "esiduals so that they have the same variance. There are two common (and related) methods of scaling. For the ﬁrst method of scaling, we use var( ^1i)¼s2(1/C0hii) in (9.23) to obtain the standardized residuals ^1i=sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0hiip, which have mean 0 and variance 1. Replacing s bysyields the studentized residual ri¼^1i sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0hiip , (9:25) where s2¼SSE =(n/C0k/C01) is as deﬁned in (7.24).",
    "ed residual ri¼^1i sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0hiip , (9:25) where s2¼SSE =(n/C0k/C01) is as deﬁned in (7.24). The use of riin place of ^1i eliminates the location effect (due to hii) on the size of residuals, as discussed follow- ing (9.23).",
    "iminates the location effect (due to hii) on the size of residuals, as discussed follow- ing (9.23). A second method of scaling the residuals uses an estimate of sthat excludes the ith observation ti¼^1i s(i)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0hiip , (9:26) where s(i)is the standard error computed with the n21 observations remaining after omitting ( yi,x0 i)¼(yi1,xi1,...,xik), in which yiis the ith element of yandx0iis the ith9.3 OUTLIERS 233 --- Page 246 --- row of X.I ft h e ith observation is an outlier, it will more likely show up as such with the standardization in (9.26), which is called the externally studentized residual or the studentized deleted residual orR student .",
    "hich is called the externally studentized residual or the studentized deleted residual orR student . Another option is to examine the deleted residuals .",
    "the studentized deleted residual orR student . Another option is to examine the deleted residuals . The ith deleted residual, 1(i),i s computed with ^b(i)on the basis of n21 observations with ( yi,x0 i) deleted: ^1(i)¼yi/C0^y(i)¼yi/C0x0 i^b(i): (9:27) By deﬁnition ^b(i)¼(X0(i)X(i))/C01X0(i)y(i), (9:28) where X(i)is the ( n21)/C2(kþ1) matrix obtained by deleting x0 i¼(1,xi1,...,xik), theith row of X, and y(i)is the corresponding ( n/C01)/C21yvector after deleting yi.",
    "1,xi1,...,xik), theith row of X, and y(i)is the corresponding ( n/C01)/C21yvector after deleting yi. The deleted vector ^b(i)can also be found without actually deleting ( yi,x0i) since ^b(i)¼^b/C0^1i 1/C0hii(X0X)/C01xi (9:29) (see Problem 9.5). The deleted residual ^1(i)¼yi/C0x0 i^b(i)in (9.27) can be expressed in terms of ^1iand hiias ^1(i)¼^1i 1/C0hii(9:30) (see Problem 9.6). Thus the ndeleted residuals can be obtained without computing nregressions.",
    "9:30) (see Problem 9.6). Thus the ndeleted residuals can be obtained without computing nregressions. The scaled residual tiin (9.26) can be expressed in terms of ^1(i)in (9.30) as ti¼^1(i)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ cvar(1(i))p (9:31) (see Problem 9.7). The deleted sample variance s2 ðiÞused in (9.26) is deﬁned as s2 (i)¼SSE (i)= (n/C0k/C02), where SSE (i)¼y0(i)y(i)/C0^b0 (i)X0 (i)y(i).",
    "sed in (9.26) is deﬁned as s2 (i)¼SSE (i)= (n/C0k/C02), where SSE (i)¼y0(i)y(i)/C0^b0 (i)X0 (i)y(i). This can be found without exclud- ing the ith observation as s2 (i)¼SSE (i) n/C0k/C02¼SSE/C0^12 i=(1/C0hii) n/C0k/C02(9:32) (see Problem 9.8).234 MULTIPLE REGRESSION: MODEL VALIDATION AND DIAGNOSTICS --- Page 247 --- Another option for outlier detection is to plot the ordinary residuals ^1i¼yi/C0x0 i^b against the deleted residuals ^1(i)in (9.27) or (9.30).",
    "plot the ordinary residuals ^1i¼yi/C0x0 i^b against the deleted residuals ^1(i)in (9.27) or (9.30). If the ﬁt does not change substan- tially when the ith observation is deleted in computation of ^b, the plotted points should approximately follow a straight line with a slope of 1. Any points that are rela- tively far from this line are potential outliers. If an outlier is from a distribution with a different mean, the model can be expressed as E(yi)¼x0 ibþu, where x0iis the ith row of X.",
    "on with a different mean, the model can be expressed as E(yi)¼x0 ibþu, where x0iis the ith row of X. This is called the mean-shift outlier model . The distribution of tiin (9.26) or (9.31) is t(n2k21), andtican therefore be used in a test of the hypothesis H0:u¼0. Since ntests will be made, a Bonferroni adjustment to the critical values can be used, or we can simply focus on the largest tivalues.",
    "rroni adjustment to the critical values can be used, or we can simply focus on the largest tivalues. Thendeleted residuals in (9.30) can be used for model validation or selection by deﬁning the prediction sum of squares (PRESS): PRESS ¼Xn i¼1^12 (i)¼Xn i¼1^1i 1/C0hii/C18/C192 : (9:33) Thus, a residual ^1ithat corresponds to a large value of hiicontributes more to PRESS. For a given dataset, PRESS may be a better measure than SSE of how well the model will predict future observations.",
    "aset, PRESS may be a better measure than SSE of how well the model will predict future observations. To use PRESS to compare alternative models whenthe objective is prediction, preference would be shown to models with small values of PRESS. 9.4 INFLUENTIAL OBSERVATIONS AND LEVERAGE In Section 9.3, we emphasized a search for outliers that did not ﬁt the model. In this section, we consider the effect that deletion of an observation ( y i,x0 i) has on the estimates ^bandX^b.",
    "n, we consider the effect that deletion of an observation ( y i,x0 i) has on the estimates ^bandX^b. An observation that makes a major difference on these estimates is called an inﬂuential observation . A point ( yi,x0 i) is potentially inﬂuential if it is an outlier in the ydirection or if it is unusually far removed from the center of thex’s. We illustrate inﬂuential observations for the case of one xin Figure 9.2.",
    "from the center of thex’s. We illustrate inﬂuential observations for the case of one xin Figure 9.2. Points 1 and 3 are extreme in the xdirection; points 2 and 3 would likely appear as outliers in theydirection. Even though point 1 is extreme in x, it will not unduly inﬂuence the slope or intercept. Point 3 will have a dramatic inﬂuence on the slope and intercept since the regression line would pass near point 3. Point 2 is also inﬂuential, but much less so than point 3.",
    "regression line would pass near point 3. Point 2 is also inﬂuential, but much less so than point 3. Thus, inﬂuential points are likely to be found in areas where little or no other data were collected.",
    "hus, inﬂuential points are likely to be found in areas where little or no other data were collected. Such points may be ﬁtted very well, sometimes to the detriment of theﬁt to the other data.9.4 INFLUENTIAL OBSERVATIONS AND LEVERAGE 235 --- Page 248 --- To investigate the inﬂuence of each observation, we begin with ^y¼Hyin (9.2), the elements of which are ^yi¼Xn j¼1hijyj¼hiiyiþX j=ihijyi: (9:34) By (9.22), if hiiis large (close to 1), then the h0 ijs,j=i, are all small, and yicontrib- utes much more than the other y’s to ^yi.",
    "to 1), then the h0 ijs,j=i, are all small, and yicontrib- utes much more than the other y’s to ^yi. Hence, hiiis called the leverage ofyi. Points with high leverage have high potential for inﬂuencing regression results. In general, if an observation ( yi,x0 i) has a value of hiinear 1, then the estimated regression equation will be close to yi; that is, ^yi/C0yiwill be small. By Theorem 9.2(iv), the average value of the hii’s is ( kþ1)/n.",
    "yi; that is, ^yi/C0yiwill be small. By Theorem 9.2(iv), the average value of the hii’s is ( kþ1)/n. Hoaglin and Welsch (1978) suggest that a point with hii.2(kþ1)=nis a high leverage point. Alternatively, we can simply examine any observation whose value of hiiis unusually large relative to the other values of hii. In terms of ﬁtting the model to the bulk of the data, high leverage points can be either good or bad, as illustrated by points 1 and 3 in Figure 9.2.",
    "ata, high leverage points can be either good or bad, as illustrated by points 1 and 3 in Figure 9.2. Point 1 may reduce the variance of ^b0and ^b1. On the other hand, point 3 will drastically alter the ﬁtted model. If point 3 is not the result of a recording error, then the researcher must choose between two competing ﬁtted models. Typically, the model that ﬁts the bulk of the data might be preferred until additional points can be observed in other areas.",
    "ﬁts the bulk of the data might be preferred until additional points can be observed in other areas. To formalize the inﬂuence of a point ( yi,x0 i), we consider the effect of its deletion onband ^y¼X^b. The estimate of bobtained by deleting the ith observation ( yi,x0 i) is deﬁned in (9.28) as ^b(i)¼(X0 (i)X(i))/C01X0(i)y(i).",
    "d by deleting the ith observation ( yi,x0 i) is deﬁned in (9.28) as ^b(i)¼(X0 (i)X(i))/C01X0(i)y(i). We can compare ^b(i)to^bby means Figure 9.2 Simple linear regression showing three outliers.236 MULTIPLE REGRESSION: MODEL VALIDATION AND DIAGNOSTICS --- Page 249 --- ofCook’s distance , deﬁned as Di¼(^b(i)/C0^b)0X0X(^b(i)/C0^b) (kþ1)s2: (9:35) This can be rewritten as Di¼(X^b(i)/C0X^b)0(X^b(i)/C0X^b) (kþ1)s2 ¼(^y(i)/C0^y)0(^y(i)/C0^y) (kþ1)s2, (9:36) in which Diis proportional to the ordinary Euclidean distance between ^y(i)and ^y.",
    ") (kþ1)s2, (9:36) in which Diis proportional to the ordinary Euclidean distance between ^y(i)and ^y. Thus if Diis large, the observation ( yi,x0 i) has substantial inﬂuence on both ^band ^y.",
    "(i)and ^y. Thus if Diis large, the observation ( yi,x0 i) has substantial inﬂuence on both ^band ^y. A more computationally convenient form of Diis given by Di¼r2 i kþ1hii 1/C0hii/C18/C19 (9:37) TABLE 9.1 Residuals and Inﬂuence Measures for the Chemical Data with Dependent Variable y1 Observation yi ^yi ^1i hii ri ti Di 1 41.5 42.19 20.688 0.430 20.394 20.383 0.029 2 33.8 31.00 2.798 0.310 1.457 1.520 0.239 3 27.7 27.74 20.042 0.155 20.020 20.019 0.000 4 21.7 21.03 0.670 0.139 0.313 0.303 0.0045 19.9 19.40 0.495 0.129 0.230 0.222 0.002 6 15.0 12.69 2.307 0.140 1.076 1.082 0.0477 12.2 12.28 20.082 0.228 20.040 20.039 0.000 8 4.3 5.57 21.270 0.186 20.609 20.596 0.021 9 19.3 20.22 20.917 0.053 20.408 20.396 0.002 10 6.4 4.76 1.642 0.233 0.811 0.801 0.050 11 37.6 35.68 1.923 0.240 0.954 0.951 0.07212 18.0 13.09 4.906 0.164 2.320 2.800 0.26413 26.3 27.34 21.040 0.146 20.487 20.474 0.010 14 9.9 13.51 23.605 0.245 21.795 21.956 0.261 15 25.0 26.93 21.929 0.250 20.964 20.961 0.077 16 14.1 15.44 21.342 0.258 20.674 20.661 0.039 17 15.2 15.44 20.242 0.258 20.121 20.117 0.001 18 15.9 19.54 23.642 0.217 21.780 21.937 0.220 19 19.6 19.54 0.058 0.217 0.028 0.027 0.0009.4 INFLUENTIAL OBSERVATIONS AND LEVERAGE 237 --- Page 250 --- (see Problem 9.9).",
    "7 0.028 0.027 0.0009.4 INFLUENTIAL OBSERVATIONS AND LEVERAGE 237 --- Page 250 --- (see Problem 9.9). Muller and Mok (1997) discuss the distribution of Diand provide a table of critical values. Example 9.4. We illustrate several diagnostic tools for the chemical reaction data of Table 7.4 using y1. In Table 9.1, we give ^1i,hii, and some functions of these from Sections 9.3 and 9.4. The guideline for hiiin Section 9.4 is 2( kþ1)=n¼2(4) =19¼:421.",
    "of these from Sections 9.3 and 9.4. The guideline for hiiin Section 9.4 is 2( kþ1)=n¼2(4) =19¼:421. The only value of hiithat exceeds .421 is the ﬁrst, h11¼:430. Thus the ﬁrst observation has potential for inﬂuencing the model ﬁt, but this inﬂuence does not appear in t1¼/C0 :383 and D1¼:029. Other relatively large values of hiiare seen for obser- vations 2, 11, 14, 15, 16, and 17. Of these only observation 14 has a very large (absol- ute) value of ti.",
    "s 2, 11, 14, 15, 16, and 17. Of these only observation 14 has a very large (absol- ute) value of ti. Observation 12 has large values of ^1i,ri,tiandDiand is a potentially inﬂuential outlier. The value of PRESS as deﬁned in (9.33) is PRESS ¼130.76, which can be compared to SSE ¼80.17.",
    "tlier. The value of PRESS as deﬁned in (9.33) is PRESS ¼130.76, which can be compared to SSE ¼80.17. A PROBLEMS 9.1 Verify the following properties of the residual vector ^1as given in (9.7)–(9.14): (a)E(^1)¼0 (b) cov( ˆ1)¼s2(I/C0H) (c) cov( ˆ1,y)¼s2(I/C0H) (d) cov( ˆ1,^y)¼O (e)/C22ˆ1¼Pn i¼1^1i=n¼0 (f)ˆ10y¼y0(I/C0H)y (g)ˆ10^y¼0 (h)ˆ10X¼00 9.2 (a) In the proof of Theorem 9.2(ii), verify that the maximum value of hii/C0h2 ii is1 4. (b) Prove Theorem 9.2(iii). (c) Prove Theorem 9.2(iv).",
    "that the maximum value of hii/C0h2 ii is1 4. (b) Prove Theorem 9.2(iii). (c) Prove Theorem 9.2(iv). 9.3 Show that an alternative expression for hiiin Theorem 9.2(iii) is the following: hii¼1 nþ(x1i/C0/C22x1)0(x1i/C0/C22x1)Xk r¼11 lrcos2uir, whereuiris the angle between x1i/C0/C22x1andar, the rth eigenvector of X0 cXc (Cook and Weisberg 1982, p. 13).",
    "the angle between x1i/C0/C22x1andar, the rth eigenvector of X0 cXc (Cook and Weisberg 1982, p. 13). Thus hiiis large if ( x1i/C0/C22x1)0(x1i/C0/C22x1)i s large or if uiris small for some r.238 MULTIPLE REGRESSION: MODEL VALIDATION AND DIAGNOSTICS --- Page 251 --- 9.4 Show that1 n/C20hiiþ^12 i=^10^1/C201 as in (9.24). The following steps are suggested: (a) Let H/C3be the hat matrix corresponding to the augmented matrix ( X,y).",
    "ing steps are suggested: (a) Let H/C3be the hat matrix corresponding to the augmented matrix ( X,y). Then H/C3¼(X,y)[(X,y)0(X,y)]/C01(X,y)0 ¼(X,y)X0XX0y y0Xy0y/C18/C19 /C01X0 y0/C18/C19 : Use the inverse of a partitioned matrix in (2.50) with A11¼X0X, a12¼X0y, and a22¼y0yto obtain H/C3¼Hþ1 b[X(X0X)/C01X0yy0X(X0X)/C01X0/C0yy0X(X0X)/C01X0 /C0X(X0X)/C01X0yy0þyy0] ¼Hþ1b[Hyy 0H/C0yy0H/C0Hyy0þyy0], where b¼y0y/C0y0X(X0X)/C01X0y.",
    "X(X0X)/C01X0 /C0X(X0X)/C01X0yy0þyy0] ¼Hþ1b[Hyy 0H/C0yy0H/C0Hyy0þyy0], where b¼y0y/C0y0X(X0X)/C01X0y. (b) Show that the above expression factors into H/C3¼Hþ(I/C0H)yy0(I/C0H) y0(I/C0H)y¼Hþˆ1ˆ10 ˆ10ˆ1, which gives h/C3 ii¼hiiþ^12 i=^10^1. (c) The proof is easily completed by noting that H/C3is a hat matrix and there- fore (1 =n)/C20h/C3 ii/C201 by Theorem 9.2(i). 9.5 Show that ^b(i)¼^b/C0^1i(X0X)/C01xi=(1/C0hii) as in (9.29).",
    ")/C20h/C3 ii/C201 by Theorem 9.2(i). 9.5 Show that ^b(i)¼^b/C0^1i(X0X)/C01xi=(1/C0hii) as in (9.29). The following steps are suggested: (a) Show that X0X¼X0 (i)X(i)þxix0 iand that X0y¼X0 (i)y(i)þxiyi. (b) Show that ( X0X)/C01X0(i)y(i)¼^b/C0(X0X)/C01xiyi.",
    "X0 (i)X(i)þxix0 iand that X0y¼X0 (i)y(i)þxiyi. (b) Show that ( X0X)/C01X0(i)y(i)¼^b/C0(X0X)/C01xiyi. (c) Using the following adaptation of (2.53) (B/C0cc0)/C01¼B/C01þB/C01cc0B/C01 1/C0c0B/C01c: show that ^b(i)¼(X0X)/C01þ(X0X)/C01xix0 i(X0X)/C01 1/C0hii/C20/C21 X0 (i)y(i):PROBLEMS 239 --- Page 252 --- (d) Using the result of parts (b) and (c), show that ^b(i)¼^b/C0^1i 1/C0hii(X0X)/C01xi: 9.6 Show that ^1(i)¼^1i=(1/C0hii) as in (9.30).",
    "nd (c), show that ^b(i)¼^b/C0^1i 1/C0hii(X0X)/C01xi: 9.6 Show that ^1(i)¼^1i=(1/C0hii) as in (9.30). 9.7 Show that ti¼^1(i)/C14ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ cvar(^1(i))p in (9.31) is the same as ti¼^1i=s(i)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0hiipin (9.26). The following steps are suggested: (a) Using ^1(i)¼^1i=(1/C0hii) in (9.30), show that var( ^1(i))¼s2=(1/C0hii). (b) If var( ^1(i)) in part (a) is estimated by cvar(^1(i))¼s2 (i)=(1/C0hii), show that ^1(i)=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ cvar(1(i))p ¼^1i=s(i)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0hiip.",
    "^1(i))¼s2 (i)=(1/C0hii), show that ^1(i)=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ cvar(1(i))p ¼^1i=s(i)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0hiip. 9.8 Show that SSE (i)¼y0 (i)y(i)/C0y0(i)X(i)^b(i)can be written in the form SSE (i)¼SSE/C0^12 i=(1/C0hii) as in (9.32). One way to do this is as follows: (a) Show that y0 (i)y(i)¼y0y/C0y2i.",
    "0^12 i=(1/C0hii) as in (9.32). One way to do this is as follows: (a) Show that y0 (i)y(i)¼y0y/C0y2i. (b) Using Problem 9.5a,d, we have y0 (i)X(i)^b(i)¼(y0X/C0yix0i)^b/C0^1i 1/C0hii(X0X)/C01xi/C20/C21 : Show that this can be written as y0 (i)X(i)^b(i)¼y0X^b/C0y2iþ^12 i 1/C0hii: (c) Show that SSE (i)¼SSE/C0^12 i=(1/C0hii): 9.9 Show that Di¼r2 ihii=(kþ1)(1/C0hii) in (9.37) is the same as Diin (9.35). This may be done by substituting (9.29) into (9.35).",
    "(1/C0hii) in (9.37) is the same as Diin (9.35). This may be done by substituting (9.29) into (9.35). 9.10 For the gas vapor data in Table 7.3, compute the diagnostic measures ^yi,^1i,hii,ri,ti, and Di. Display these in a table similar to Table 9.1. Are there outliers or potentially inﬂuential observations? Calculate PRESS and compare to SSE. 9.11 For the land rent data in Table 7.5, compute the diagnostic measures ^yi,^1i,hii,ri,ti, and Di. Display these in a table similar to Table 9.1.",
    "te the diagnostic measures ^yi,^1i,hii,ri,ti, and Di. Display these in a table similar to Table 9.1. Are240 MULTIPLE REGRESSION: MODEL VALIDATION AND DIAGNOSTICS --- Page 253 --- there outliers or potentially inﬂuential observations? Calculate PRESS and compare to SSE. 9.12 For the chemical reaction data of Table 7.4 with dependent variable y2, compute the diagnostic measures ^yi,^1i,hii,ri,ti, and Di. Display these in a table similar to Table 9.1.",
    "te the diagnostic measures ^yi,^1i,hii,ri,ti, and Di. Display these in a table similar to Table 9.1. Are there outliers or potentially inﬂuential obser- vations? Calculate PRESS and compare to SSE.PROBLEMS 241 --- Page 254 --- 10 Multiple Regression: Random x’s Throughout Chapters 7–9 we assumed that the xvariables were ﬁxed; that is, that they remain constant in repeated sampling. However, in many regression appli- cations, they are random variables.",
    "onstant in repeated sampling. However, in many regression appli- cations, they are random variables. In this chapter we obtain estimators and test stat- istics for a regression model with random xvariables. Many of these estimators and test statistics are the same as those for ﬁxed x’s, but their properties are somewhat different. In the random- xcase, kþ1 variables y,x1,x2,...,xkare measured on each of thensubjects or experimental units in the sample.",
    "þ1 variables y,x1,x2,...,xkare measured on each of thensubjects or experimental units in the sample. These nobservation vectors yield the data y1x11x12 ... x1k y2x21x22 ... x2k ............ ynxn1xn2... xnk:(10:1) The rows of this array are random vectors of the second type described in Section 3.1. The variables y,x1,x2,...,xkin a row are typically correlated and have different var- iances; that is, for the random vector ( y,x1,...,xk)¼(y,x0), we have covy x1 ...",
    "ave different var- iances; that is, for the random vector ( y,x1,...,xk)¼(y,x0), we have covy x1 ... xk0 BBB@1 CCCA¼covy x/C18/C19 ¼S, whereSis not a diagonal matrix. The vectors themselves [rows of the array in (10.1)] are ordinarily mutually independent (uncorrelated) if they arise from a random sample. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc.",
    ",Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 243 --- Page 255 --- In Sections 10.1–10.5 we assume that yand the xvariables have a multivariate normal distribution. Many of the results in Sections 10.6–10.8 do not require a normality assumption. 10.1 MULTIVARIATE NORMAL REGRESSION MODEL The estimation and testing results in Sections 10.1–10.5 are based on the assumption that ( y,x1,...,xk)¼(y,x0) is distributed as Nkþ1(m,S) with m¼my m1 ...",
    "are based on the assumption that ( y,x1,...,xk)¼(y,x0) is distributed as Nkþ1(m,S) with m¼my m1 ... mk0 BBBB@1 CCCCA¼my mx/C18/C19 S¼syysy1...syk s1ys11/C1/C1/C1s1k ......... skysk1/C1/C1/C1skk0 BBBBBB@1 CCCCCCA¼syys0 yx syxSxx/C18/C19 , (10:3) wheremxis the mean vector for the x’s,syxis the vector of covariances between yand thex’s, andSxxis the covariance matrix for the x’s.",
    "x’s,syxis the vector of covariances between yand thex’s, andSxxis the covariance matrix for the x’s. From Corollary 1 to Theorem 4.4d, we have E(yjx)¼myþs0 yxS/C01 xx(x/C0mx) (10 :4) ¼b0þb0 1x, (10:5) where b0¼my/C0s0 yxS/C01 xxmx, (10:6) b1¼S/C01 xxsyx: (10:7) From Corollary 1 to Theorem 4.4d, we also obtain var(yjx)¼syy/C0s0yxS/C01 xxsyx¼s2: (10:8) The mean, E(yjx)¼myþs0 yxS/C01 xx(x/C0mx), is a linear function of x, but the variance, s2¼syy/C0s0yxS/C01 xxsyx, is not a function x.",
    "x/C0mx), is a linear function of x, but the variance, s2¼syy/C0s0yxS/C01 xxsyx, is not a function x. Thus under the multivariate normal244 MULTIPLE REGRESSION: RANDOM x’s --- Page 256 --- assumption, (10.4) and (10.8) provide a linear model with constant variance, which is analogous to the ﬁxed- xcase. Note, however, that E(yjx)¼b0þb0 1xin (10.5) does not allow for curvature such as E(y)¼b0þb1xþb2x2. Thus E(yjx)¼b0þb0 1x represents a model that is linear in the x’s as well as the b’s.",
    ")¼b0þb1xþb2x2. Thus E(yjx)¼b0þb0 1x represents a model that is linear in the x’s as well as the b’s. This differs from the linear model in the ﬁxed- xcase, which requires only linearity in the b’s. 10.2 ESTIMATION AND TESTING IN MULTIVARIATE NORMAL REGRESSION Before obtaining estimators of b0,b1, ands2in (10.6)–(10.8), we must ﬁrst estimate mandS. Maximum likelihood estimators of mandSare given in the following theorem. Theorme 10.2a.",
    "mate mandS. Maximum likelihood estimators of mandSare given in the following theorem. Theorme 10.2a. If (y1,x0 1),(y2,x02),...,(yn,x0n) [rows of the array in (10.1)] is a random sample from Nkþ1(m,S), withmandSas given in (10.2) and (10.3), the maximum likelihood estimators are ^m¼^my ^mx/C18/C19 ¼/C22y /C22x/C18/C19 , (10:9) ^S¼n/C01 nS¼n/C01 nsyys0 yx syxSxx/C18/C19 , (10:10) where the partitioning of mˆandSis analogous to the partitioning of mandSin (10.2) and (10.3).",
    "0:10) where the partitioning of mˆandSis analogous to the partitioning of mandSin (10.2) and (10.3). The elements of the sample covariance matrix Sare deﬁned in (7.40) and in (10.14). PROOF. Denote ( yi,x0 i)b y v0i,i¼1,2,...,n. As noted below (10.1), v1,v2,...,vn are independent because they arise from a random sample.",
    ",...,n. As noted below (10.1), v1,v2,...,vn are independent because they arise from a random sample. The likelihood function ( joint density) is therefore given by the product L(m,S)¼Yn i¼1f(vi;m,S) ¼Yn i¼11 (ﬃﬃﬃﬃﬃﬃ 2pp )kþ1jSj1=2e/C0(vi/C0m)0S/C01(vi/C0m)=2 ¼1 (ﬃﬃﬃﬃﬃﬃ2pp )n(kþ1)jSjn=2e/C0Pn i¼1(vi/C0m)0S/C01(vi/C0m)=2: (10:11) Note that L(m,S)¼Qn i¼1f(vi;m,S) is a product of nmultivariate normal densities, each involving kþ1 random variables.",
    "Qn i¼1f(vi;m,S) is a product of nmultivariate normal densities, each involving kþ1 random variables. Thus there are n(kþ1) random variables as compared to the likelihood L(b,s2) in (7.50) that involves nrandom variables y1,y2,...,yn[thex’s are ﬁxed in (7.50)].10.2 ESTIMATION AND TESTING IN MULTIVARIATE NORMAL REGRESSION 245 --- Page 257 --- To ﬁnd the maximum likelihood estimator for m, we expand and sum the exponent in (10.11) and then take the logarithm to obtain lnL(m,S)¼/C0n(kþ1)lnﬃﬃﬃﬃﬃﬃ 2pp /C0n 2lnjSj/C012X iv0 iS/C01vi þm0S/C01X ivi/C0n 2m0S/C01m: (10:12) Differentiating (10.12) with respect to musing (2.112) and (2.113) and setting the result equal to 0, we obtain @lnL(m,S) @m¼/C00/C00/C00þS/C01X ivi/C02n 2S/C01m¼0, which gives ^m¼1nX n i¼1vi¼/C22v¼/C22y /C22x/C18/C19 , where /C22x¼(/C22x1,/C22x2,...,/C22xk)0is the vector of sample means of the x’s.",
    "22y /C22x/C18/C19 , where /C22x¼(/C22x1,/C22x2,...,/C22xk)0is the vector of sample means of the x’s. To ﬁnd the maximum likelihood estimator of S, we rewrite the exponent of (10.11) and then take the logarithm to obtain lnL(m,S/C01)¼/C0n(kþ1)lnﬃﬃﬃﬃﬃﬃ 2pp þn 2lnjS/C01j/C012X i(vi/C0/C22v)0S/C01(vi/C0/C22v) /C0n 2(/C22v/C0m)0S/C01(/C22v/C0m) ¼/C0n(kþ1)lnﬃﬃﬃﬃﬃﬃ 2pp þn 2lnjS/C01j/C012trS /C01X i(vi/C0/C22v)(vi/C0/C22v)0\"# /C0n 2tr[S/C01(/C22v/C0m)(/C22v/C0m)0]: Differentiating this with respect to S/C01using (2.115) and (2.116), and setting the result equal to 0, we obtain @lnL(m,S/C01) @S/C01¼nS/C0n 2diag(S)/C0X i(vi/C0/C22v)(vi/C0/C22v)0þ1 2diagX i(vi/C0/C22v)(vi/C0/C22v)\"# /C0n(/C22v/C0m)(/C22v/C0m)0þn 2diag[( /C22v/C0m)(/C22v/C0m)0]¼0:246 MULTIPLE REGRESSION: RANDOM x’s --- Page 258 --- Since ^m¼/C22v, the last two terms disappear and we obtain ^S¼1 nXn i¼1(vi/C0/C22v)(vi/C0/C22v)0¼n/C01 nS: (10:13) See Problem 10.1 for veriﬁcation thatP i(vi/C0/C22v)(vi/C0/C22v)0¼(n/C01)S:A In partitioned form, the sample covariance matrix Scan be written as in (10.10) S¼syys0 yx syxSxx/C18/C19 ¼syy sy1...",
    "rm, the sample covariance matrix Scan be written as in (10.10) S¼syys0 yx syxSxx/C18/C19 ¼syy sy1... syk s1ys11 ... s1k ......... sky sk1... skk0 BBBB@1 CCCCA, (10:14) where s yxis the vector of sample covariances between yand the x’s and Sxxis the sample covariance matrix for the x’s. For example sy1¼Pn i¼1(yi/C0/C22y)(xi1/C0/C22x1) n/C01, s11¼Pni¼1(xi1/C0/C22x1)2 n/C01, s12¼Pni¼1(xi1/C0/C22x1)(xi2/C0/C22x2) (n/C01) [see (7.41)–(7.43)]. By (5.7), E(syy)¼syyandE(sjj)¼sjj.",
    "12¼Pni¼1(xi1/C0/C22x1)(xi2/C0/C22x2) (n/C01) [see (7.41)–(7.43)]. By (5.7), E(syy)¼syyandE(sjj)¼sjj. By (5.17), E(syj)¼syj andE(sij)¼sij. Thus E(S)¼S, whereSis given in (10.3). The maximum likeli- hood estimator^S¼(n/C01)S=nis therefore biased. In order to ﬁnd maximum likelihood estimators of b0,b1, ands2we ﬁrst note the invariance property of maximum likelihood estimators. Theorem 10.2b.",
    "of b0,b1, ands2we ﬁrst note the invariance property of maximum likelihood estimators. Theorem 10.2b. The maximum likelihood estimator of a function of one or more parameters is the same function of the corresponding estimators; that is, if uˆis the maximum likelihood estimator of the vector or matrix of parameters u, then g(^u)i s the maximum likelihood estimator of g(u). PROOF. See Hogg and Craig (1995, p. 265). A Example 10.2.",
    "the maximum likelihood estimator of g(u). PROOF. See Hogg and Craig (1995, p. 265). A Example 10.2. We illustrate the use of the invariance property in Theorem 10.2b by showing that the sample correlation matrix Ris the maximum likelihood estimator of the population correlation matrix Prwhen sampling from the multivariate normal10.2 ESTIMATION AND TESTING IN MULTIVARIATE NORMAL REGRESSION 247 --- Page 259 --- distribution.",
    "rmal10.2 ESTIMATION AND TESTING IN MULTIVARIATE NORMAL REGRESSION 247 --- Page 259 --- distribution. By (3.30), the relationship between PrandSis given by Pr¼D/C01 sSD/C01 s, where Ds¼[diag(S)]1=2, so that D/C01 s¼diag1ﬃﬃﬃsp 11,1ﬃﬃﬃsp 22,...,1ﬃﬃﬃsp pp ! : The maximum likelihood estimator of 1 =ﬃﬃﬃﬃﬃﬃsjjpis 1 =ﬃﬃﬃﬃﬃﬃ ^sjjp , where ^sjj¼(1=n)Sn i¼1(yij/C0/C22yj)2. Thus ^D/C01 s¼diag(1 =ﬃﬃﬃﬃﬃﬃﬃ^s11p,1=ﬃﬃﬃﬃﬃﬃﬃ^s22p,...,1=ﬃﬃﬃﬃ ^sp pp, and we obtain ^Pr¼^D/C01 s^S^D/C01 s¼^sjkﬃﬃﬃﬃﬃﬃ^sjjpﬃﬃﬃﬃﬃﬃﬃ^skkp !",
    "1=ﬃﬃﬃﬃﬃﬃﬃ^s22p,...,1=ﬃﬃﬃﬃ ^sp pp, and we obtain ^Pr¼^D/C01 s^S^D/C01 s¼^sjkﬃﬃﬃﬃﬃﬃ^sjjpﬃﬃﬃﬃﬃﬃﬃ^skkp ! ¼P i(yij/C0/C22yj)(yik/C0/C22yk)=nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(yij/C0/C22yj)2=nq ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(yik/C0/C22yk)2=nq0 B@1 CA ¼P i(yij/C0/C22yj)(yik/C0/C22yk)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(yij/C0/C22yj)2q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(yik/C0/C22yk)2q0 B@1 CA ¼(rjk)¼R:A Maximum likelihood estimators of b0,b1, ands2are now given in the following theorem. Theorem 10.2c.",
    "Maximum likelihood estimators of b0,b1, ands2are now given in the following theorem. Theorem 10.2c. If ( y1,x0 1),(y2,x02),...,(yn,x0n), is a random sample from Nkþ1(m,S), wheremandSare given by (10.2) and (10.3), the maximum likelihood estimators for b0,b1, ands2in (10.6)–(10.8) are as follows: ^b0¼/C22y/C0s0 yxS/C01 xx¯x, (10:15) ^b1¼S/C01 xxsyx, (10:16) ^s2¼n/C01 ns2where s2¼syy/C0s0yxS/C01 xxsyx: (10:17) The estimator s2is a bias-corrected estimator of s2. PROOF.",
    "2where s2¼syy/C0s0yxS/C01 xxsyx: (10:17) The estimator s2is a bias-corrected estimator of s2. PROOF. By the invariance property of maximum likelihood estimators (Theorem 10.2b), we insert (10.9) and (10.10) into (10.6), (10.7), and (10.8) to obtain the desired results (using the unbiased estimator Sin place of Sˆ).",
    "6), (10.7), and (10.8) to obtain the desired results (using the unbiased estimator Sin place of Sˆ). A248 MULTIPLE REGRESSION: RANDOM x’s --- Page 260 --- The estimators bˆ0,b1, and s2have a minimum variance property analogous to that of the corresponding estimators for the case of normal y’s and ﬁxed x’s in Theorem 7.6d. It can be shown that mˆandSin (10.9) and (10.10) are jointly sufﬁcient for m andS(see Problem 10.2).",
    "It can be shown that mˆandSin (10.9) and (10.10) are jointly sufﬁcient for m andS(see Problem 10.2). Then, with some additional properties that can be demon- strated, it follows that bˆ0,b1, and s2are minimum variance unbiased estimators for b0,b1, ands2(Graybill 1976, p. 380). The maximum likelihood estimators bˆ0andbˆ1in (10.15) and (10.16) are the same algebraic functions of the observations as the least-squares estimators given in (7.47) and (7.46) for the ﬁxed- xcase.",
    "of the observations as the least-squares estimators given in (7.47) and (7.46) for the ﬁxed- xcase. The estimators in (10.15) and (10.16) are also identical to the maximum likelihood estimators for normal y’s and ﬁxed x’s in Section 7.6.2 (see Problem 7.17). However, even though the estimators in the random- xcase and ﬁxed- xcase are the same, their distributions differ.",
    "though the estimators in the random- xcase and ﬁxed- xcase are the same, their distributions differ. When yand the x’s are multi- variate normal, bˆ1does not have a multivariate normal distribution as it does in the ﬁxed- xcase with normal y’s [Theorem 7.6b(i)]. For large n, the distribution is similar to the multivariate normal, but for small n, the distribution has heavier tails than the multivariate normal.",
    "ltivariate normal, but for small n, the distribution has heavier tails than the multivariate normal. In spite of the nonnormality of bˆ1in the random- xmodel, the Ftests and ttests and associated conﬁdence regions and intervals of Chapter 8 (ﬁxed- xmodel) are still appropriate.",
    "ts and associated conﬁdence regions and intervals of Chapter 8 (ﬁxed- xmodel) are still appropriate. To see this, note that since the conditional distribution of yfor a given value of xis normal (Corollary 1 to Theorem 4.4d), the conditional distribution of the vector of observations y¼(y1,y2,...,yn)0for a given value of the Xmatrix is multivariate normal. Therefore, a test statistic such as (8.35) is distributed conditionally as an Ffor the given value of Xwhen H0is true.",
    "statistic such as (8.35) is distributed conditionally as an Ffor the given value of Xwhen H0is true. However, the central Fdistribution depends only on degrees of freedom; it does not depend on X. Thus under H0, the statistic has (unconditionally) an Fdistri- bution for all values of X, and so tests can be carried out exactly as in the ﬁxed- xcase. The main difference is that when H0is false, the noncentrality parameter is a func- tion of X, which is random.",
    "fference is that when H0is false, the noncentrality parameter is a func- tion of X, which is random. Hence the noncentral Fdistribution does not apply to the random- xcase. This only affects such things as power calculations. Conﬁdence intervals for the bj’s in Section 8.6.2 and for linear functions of thebj’s in Section 8.6.3 are based on the central tdistribution [e.g., see (8.48)]. Thus they also remain valid for the random- xcase.",
    "on the central tdistribution [e.g., see (8.48)]. Thus they also remain valid for the random- xcase. However, the expected width of the interval differs in the two cases (random x’s and ﬁxed x’s) because of random- ness in X. In Section 10.5, we obtain the Ftest for H0:b1¼0using the likelihood ratio approach. 10.3 STANDARDIZED REGRESSION COEFFICENTS We now show that the regression coefﬁcient vector bˆ1in (10.16) can be expressed in terms of sample correlations.",
    "hat the regression coefﬁcient vector bˆ1in (10.16) can be expressed in terms of sample correlations. By analogy to (10.14), the sample correlation matrix10.3 STANDARDIZED REGRESSION COEFFICENTS 249 --- Page 261 --- can be written in partitioned form as R¼1r0 yx ryxRxx/C18/C19 ¼1ry1ry2... ryk r1y1r12 ... r1k r2yr21 1 ... r2k ............ rkyrk1rk2... 10 BBBBBB@1 CCCCCCA, (10:18) where r yxis the vector of correlations between yand the x’s and Rxxis the correlation matrix for the x’s.",
    "r yxis the vector of correlations between yand the x’s and Rxxis the correlation matrix for the x’s. For example ry2¼sy2ﬃﬃﬃﬃﬃﬃﬃﬃ s2 ys2 2q ¼Pn i¼1(yi/C0/C22y)(xi2/C0/C22x2)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(yi/C0/C22y)2Pni¼1(xi2/C0/C22x2)2q , r12¼s12ﬃﬃﬃﬃﬃﬃﬃﬃ s2 1s22p ¼Pn i¼1(xi1/C0/C22x1)(xi2/C0/C22x2)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(xi1/C0/C22x1)2Pni¼1(xi2/C0/C22x2)2q : By analogy to (3.31), Rcan be converted to Sby S¼DRD , where D¼[diag( S)]1=2,which can be written in partitioned form as D¼sy 00 ...",
    "nverted to Sby S¼DRD , where D¼[diag( S)]1=2,which can be written in partitioned form as D¼sy 00 ... 0 0ﬃﬃﬃﬃﬃﬃs11p0 ... 0 00ﬃﬃﬃﬃﬃﬃs 22p... 0 ............",
    "n be written in partitioned form as D¼sy 00 ... 0 0ﬃﬃﬃﬃﬃﬃs11p0 ... 0 00ﬃﬃﬃﬃﬃﬃs 22p... 0 ............ 00 0 ...ﬃﬃﬃﬃﬃﬃskkp0 BBBBBB@1 CCCCCCA¼s y00 0D x/C18/C19 : Using the partitioned form of Sin (10.14), S¼DRD can be written as S¼syys0 yx syxSxx/C18/C19 ¼s2y syr0yxDx syDxryxDxRxxDx/C18/C19 , (10:19) so that Sxx¼DxRxxDx, (10:20) syx¼syDxryx, (10:21)250 MULTIPLE REGRESSION: RANDOM x’s --- Page 262 --- where Dx¼diag( s1,s2,...,sk) and sy¼ﬃﬃﬃﬃ s2 yq ¼ﬃﬃﬃﬃﬃﬃsyypis the sample standard devi- ation of y.",
    "- where Dx¼diag( s1,s2,...,sk) and sy¼ﬃﬃﬃﬃ s2 yq ¼ﬃﬃﬃﬃﬃﬃsyypis the sample standard devi- ation of y. When (10.20) and (10.21) are substituted into (10.16), we obtain an expression for bˆ1in terms of correlations: ^b1¼syD/C01 xR/C01 xxryx: (10:22) The regression coefﬁcients ^b1,^b2,...,^bkin^b1can be standardized so as to show the effect of standardized xvalues (sometimes called z scores ). We illustrate this for k¼2.",
    "to show the effect of standardized xvalues (sometimes called z scores ). We illustrate this for k¼2. The model in centered form [see (7.30) and an expression following (7.38)] is ^yi¼/C22yþ^b1(xi1/C0/C22x1)þ^b2(xi2/C0/C22x2): This can be expressed in terms of standardized variables as ^yi/C0/C22y sy¼s1 sy^b1xi1/C0/C22x1 s1/C18/C19 þs2 sy^b2xi2/C0/C22x2 s2/C18/C19 , (10:23) where sj¼ﬃﬃﬃﬃﬃsjjpis the standard deviation of xj.",
    "18/C19 þs2 sy^b2xi2/C0/C22x2 s2/C18/C19 , (10:23) where sj¼ﬃﬃﬃﬃﬃsjjpis the standard deviation of xj. We thus deﬁne the standardized coef- ﬁcients as ^b/C3 j¼sj sy^bj: These coefﬁcients are often referred to as beta weights orbeta coefﬁcients . Since they are used with standardized variables ( xij/C0/C22xj)=sjin (10.23), the ^b/C3 j’s can be readily compared to each other, whereas the bˆj’s cannot be so compared.",
    "0.23), the ^b/C3 j’s can be readily compared to each other, whereas the bˆj’s cannot be so compared. [Division by sy in (10.23) is customary but not necessary; the relative values of s1^b1ands2^b2are the same as those of s1^b1=syands2^b2=sy.] The beta weights can be expressed in vector form as ^b/C3 1¼1 syDx^b1: Using (10.22), this can be written as ^b/C31¼R/C01 xxryx: (10:24) Note that ^b/C31in (10.24) is not the same as ^b/C31from the reduced model in (8.8).",
    "xxryx: (10:24) Note that ^b/C31in (10.24) is not the same as ^b/C31from the reduced model in (8.8). Note also the analogy of ^b/C3 1¼R/C01 xxryxin (10.24) to ^b1¼S/C01 xxsyxin (10.16). In effect, Rxx andrxyare the covariance matrix and covariance vector for standardized variables.10.3 STANDARDIZED REGRESSION COEFFICENTS 251 --- Page 263 --- Replacing S/C01 xxandsyxbyR/C01 xxandryxleads to regression coefﬁcients for standardized variables. Example 10.3.",
    "01 xxandsyxbyR/C01 xxandryxleads to regression coefﬁcients for standardized variables. Example 10.3. The following six hematology variables were measured on 51 workers (Royston 1983): y¼lymphocyte count x3¼white blood cell count ( /C2:01) x1¼hemoglobin concentration x4¼neutrophil count x2¼packed-cell volume x5¼serum lead concentration The data are given in Table 10.1.",
    "neutrophil count x2¼packed-cell volume x5¼serum lead concentration The data are given in Table 10.1. For /C22y,/C22x,Sxxandsyx,w eh a v e /C22y¼22:902,/C22x0¼(15:108,45:196,53:824,25:529,21:039) , Sxx¼0:691 1 :494 3 :255 0 :422 /C00:268 1:494 5 :401 10 :155 1 :374 1 :292 3:255 10 :155 200 :668 64 :655 4 :067 0:422 1 :374 64 :655 56 :374 0 :579 /C00:268 1 :292 4 :067 0 :579 18 :0780 BBBBBB@1 CCCCCCA, s yx¼1:535 4:880 106 :202 3:753 3:0640 BBBBBB@1 CCCCCCA: By (10.15) to (10.17), we obtain ^ b1¼S/C01 xxsyx¼/C00:491 /C00:316 0:837 /C00:882 0:0250 BBBBBB@1 CCCCCCA, ^ b0¼/C22y/C0s0 yxS/C01 xx/C22x¼22:902/C01:355¼21:547, s2¼syy/C0s0 yxS/C01 xxsyx¼90:2902 /C083:3542 ¼6:9360 :252 MULTIPLE REGRESSION: RANDOM x’s --- Page 264 --- TABLE 10.1 Hematology Data Observation Number yx 1 x2 x3 x4 x5 1 14 13.4 39 41 25 17 2 15 14.6 46 50 30 20 3 19 13.5 42 45 21 184 23 15.0 46 46 16 18 5 17 14.6 44 51 31 196 20 14.0 44 49 24 197 21 16.4 49 43 17 188 16 14.8 44 44 26 299 27 15.2 46 41 13 27 10 34 15.5 48 84 42 3611 26 15.2 47 56 27 2212 28 16.9 50 51 17 2313 24 14.8 44 47 20 2314 26 16.2 45 56 25 19 15 23 14.7 43 40 13 1716 9 14.7 42 34 22 1317 18 16.5 45 54 32 1718 28 15.4 45 69 36 2419 17 15.1 45 46 29 1720 14 14.2 46 42 25 2821 8 15.9 46 52 34 1622 25 16.0 47 47 14 1823 37 17.4 50 86 39 1724 20 14.3 43 55 31 1925 15 14.8 44 42 24 2926 9 14.9 43 43 32 1727 16 15.5 45 52 30 20 28 18 14.5 43 39 18 25 29 17 14.4 45 60 37 2330 23 14.6 44 47 21 2731 43 15.3 45 79 23 2332 17 14.9 45 34 15 2433 23 15.8 47 60 32 2134 31 14.4 44 77 39 23 35 11 14.7 46 37 23 23 36 25 14.8 43 52 19 2237 30 15.4 45 60 25 1838 32 16.2 50 81 38 1839 17 15.0 45 49 26 2440 22 15.1 47 60 33 1641 20 16.0 46 46 22 2242 20 15.3 48 55 23 23 (Continued )10.3 STANDARDIZED REGRESSION COEFFICENTS 253 --- Page 265 --- The correlations are given by Rxx¼1:000 0 :774 0 :277 0 :068 /C00:076 0:774 1 :000 0 :308 0 :079 0 :131 0:277 0 :308 1 :000 0 :608 0 :068 0:068 0 :079 0 :608 1 :000 0 :018 /C00:076 0 :131 0 :068 0 :018 1 :0000 BBBB@1 CCCCA,r yx¼0:194 0:221 0:789 0:053 0:0760 BBBB@1 CCCCA: By (10.24), the standardized coefﬁcient vector is given by ^ b/C3 1¼R/C01 xxryx¼/C00:043 /C00:077 1:248 /C00:697 0:0110 BBBB@1 CCCCA: A 10.4 R 2IN MULTIVARIATE NORMAL REGRESSION In the case of ﬁxed x’s, we deﬁned R2as the proportion of variation in ydue to regression [see (7.55)].",
    "the case of ﬁxed x’s, we deﬁned R2as the proportion of variation in ydue to regression [see (7.55)]. In the case of random x’s, we obtain Ras an estimate of a population multiple correlation between yand the x’s. Then R2is the square of this sample multiple correlation.",
    "multiple correlation between yand the x’s. Then R2is the square of this sample multiple correlation. The population multiple correlation coefﬁcient ryjxis deﬁned as the correlation between yand the linear function w¼myþs0 yxS/C01 xx(x/C0mx): ryjx¼corr( y,w)¼syw sysw: (10:25)TABLE 10.1 Continued Observation Number yx 1 x2 x3 x4 x5 43 20 14.5 41 62 36 21 44 26 14.2 41 49 20 20 45 40 15.0 45 72 25 2546 22 14.2 46 58 31 22 47 61 14.9 45 84 17 1748 12 16.2 48 31 15 1849 20 14.5 45 40 18 2050 35 16.4 49 69 22 2451 38 14.7 44 78 34 16254 MULTIPLE REGRESSION: RANDOM x’s --- Page 266 --- (We use the subscript yjxto distinguish ryjxfromr, the correlation between yandxin the bivariate normal case; see Sections 3.2, 6.4, and 10.5).",
    "xfromr, the correlation between yandxin the bivariate normal case; see Sections 3.2, 6.4, and 10.5). By (10.4), wis equal to E(yjx), which is the population analogue of ^y¼^b0þ^b0 1x1, the sample predicted value of y.A s xvaries randomly, the population predicted value w ¼myþ s0 yxS/C01 xx(x/C0mx) becomes a random variable.",
    "ies randomly, the population predicted value w ¼myþ s0 yxS/C01 xx(x/C0mx) becomes a random variable. It is easily established that cov( y,w) and var( w) have the same value: cov( y,w)¼var(w)¼s0 yxS/C01 xxsyx: (10:26) Then the population multiple correlation ryjxin (10.25) becomes ryjx¼cov( y,w)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃvar(y)var( w)p ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ s0 yxS/C01 xxsyx syys , and the population coefﬁcient of determination orpopulation squared multiple correlation r2 yjxis given by r2 yjx¼s0 yxS/C01 xxsyx syy: (10:27) We now list some properties of ryjxandr2 yjx.",
    "r2 yjxis given by r2 yjx¼s0 yxS/C01 xxsyx syy: (10:27) We now list some properties of ryjxandr2 yjx. 1.ryjxis the maximum correlation between yand any linear function of x: ryjx¼max ary,a0x: (10:28) This is an alternative deﬁnition of ryjxthat is not based on the multivariate normal distribution as is the deﬁnition in (10.25). 2.r2yjxcan be expressed in terms of determinants: r2 yjx¼1/C0jSj syyjSxxj, (10:29) whereSandSxxare deﬁned in (10.3).",
    "pressed in terms of determinants: r2 yjx¼1/C0jSj syyjSxxj, (10:29) whereSandSxxare deﬁned in (10.3). 3.r2 yjxis invariant to linear transformations on yor on the x’s; that is, if u¼ayand v¼Bx, where Bis nonsingular, then r2 ujv¼r2yjx: (10:30) (Note that vhere is not the same as viused in the proof of Theorem 10.2a.)10.4 R2IN MULTIVARIATE NORMAL REGRESSION 255 --- Page 267 --- 4.",
    "used in the proof of Theorem 10.2a.)10.4 R2IN MULTIVARIATE NORMAL REGRESSION 255 --- Page 267 --- 4. Using var( w)¼s0 yxS/C01 xxsyxin (10.26), r2 yjxin (10.27) can be written in the form r2 yjx¼var(w) var(y): (10:31) Since w¼myþs0 yxS/C01 xx(x/C0mx) is the population regression equation, r2 yjxin (10.31) represents the proportion of the variance of ythat can be attributed to the regression relationship with the variables in x. In this sense, r2 yjxis analo- gous to R2in the ﬁxed- xcase in (7.55).",
    "ship with the variables in x. In this sense, r2 yjxis analo- gous to R2in the ﬁxed- xcase in (7.55). 5. By (10.8) and (10.27), var( yjx) can be expressed in terms of r2yjx: var(yjx)¼syy/C0s0 yxS/C01 xxsyx¼syy/C0syyr2yjx ¼syy(1/C0r2yjx):(10:32) 6. If we consider y2was a residual or error term, then y2wis uncorrelated with thex’s cov( y/C0w,x)¼00(10:33) (see Problem 10.8).",
    "idual or error term, then y2wis uncorrelated with thex’s cov( y/C0w,x)¼00(10:33) (see Problem 10.8). We can obtain a maximum likelihood estimator for r2 yjxby substituting esti- mators from (10.14) for the parameters in (10.27): R2¼s0 yxS/C01 xxsyx syy(10:34) We use the notation R2rather than ^r2 yjxbecause (10.34) is recognized as having the same form as R2for the ﬁxed- xcase in (7.59). We refer to R2as the sample coefﬁcient of deter- mination or as the sample squared multiple correlation .",
    "fer to R2as the sample coefﬁcient of deter- mination or as the sample squared multiple correlation . The square root of R2 R¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ s0 yxS/C01 xxsyx syys (10:35) is the sample multiple correlation coefﬁcient . We now list several properties of RandR2, some of which are analogous to prop- erties ofr2 yjxabove.",
    "We now list several properties of RandR2, some of which are analogous to prop- erties ofr2 yjxabove. 1.Ris equal to the correlation between yand ^y¼^b0þ^b1x1þ/C1/C1/C1þ ^bkxk¼ ^b0þ^b0 1x: R¼ry^y: (10:36) 2.Ris equal to the maximum correlation between yand any linear combination of thex’s,a0x: R¼max ary,a0x: (10:37)256 MULTIPLE REGRESSION: RANDOM x’s --- Page 268 --- 3.R2can be expressed in terms of correlations: R2¼r0 yxR/C01 xxryx, (10:38) where ryxandRxxare from the sample correlation matrix Rpartitioned as in (10.18).",
    "C01 xxryx, (10:38) where ryxandRxxare from the sample correlation matrix Rpartitioned as in (10.18). 4.R2can be obtained from R/C01: R2¼1/C01 ryy, (10:39) where ryyis the ﬁrst diagonal element of R/C01. Using the other diagonal elements of R/C01, this relationship can be extended to give the multiple corre- lation of any xjwith the other x’s and y. Thus from R21we obtain multiple correlations, as opposed to the simple correlations in R.",
    "x’s and y. Thus from R21we obtain multiple correlations, as opposed to the simple correlations in R. 5.R2can be expressed in terms of determinants: R2¼1/C0jSj syyjSxxj(10:40) ¼1/C0jRj jRxxj, (10:41) where SxxandRxxare deﬁned in (10.14) and (10.18). 6. From (10.24) and (10.38), we can express R2in terms of beta weights: R2¼r0yx^b/C3 1, (10:42) where ^b/C31¼R/C01 xxryx. This equation does notimply that R2is the sum of squared partial correlations (Section 10.8). 7.",
    "yx. This equation does notimply that R2is the sum of squared partial correlations (Section 10.8). 7. Ifr2 yjx¼0, the expected value of R2is given by E(R2)¼k n/C01: (10:43) Thus R2is biased when r2yjxis 0 [this is analogous to (7.57)]. 8.R2/C21max jr2 yj, where ryjis an element of r0 yx¼(ry1,ry2,...,ryk). 9.R2is invariant to full rank linear transformations on yor on the x’s. Example 10.4. For the hematology data in Table 10.1, Sxx,syx,Rxx, and ryxwere obtained in Example 10.3.",
    "mple 10.4. For the hematology data in Table 10.1, Sxx,syx,Rxx, and ryxwere obtained in Example 10.3. Using either (10.34) or (10.38), we obtain R2¼:9232 : A10.4 R2IN MULTIVARIATE NORMAL REGRESSION 257 --- Page 269 --- 10.5 TESTS AND CONFIDENCE INTERVALS FOR R2 Note that by (10.27), r2 yjx¼0 becomes r2 yjx¼s0 yxS/C01 xxsyx syy¼0, which leads to syx¼0sinceSxxis positive deﬁnite. Then by (10.7), b1¼S/C01 xxsyx¼0, and H0:r2 yjx¼0 is equivalent to H0:b1¼0.",
    "eSxxis positive deﬁnite. Then by (10.7), b1¼S/C01 xxsyx¼0, and H0:r2 yjx¼0 is equivalent to H0:b1¼0. TheFstatistic for ﬁxed x’s is given in (8.5), (8.22), and (8.23) as F¼(^b0X0y/C0n/C22y2)=k (y0y/C0^b0X0y)=(n/C0k/C01) ¼R2=k (1/C0R2)=(n/C0k/C01): (10:44) The test statistic in (10.44) can be obtained by the likelihood ratio approach in the case of random x’s (Anderson 1984, pp. 140–142): Theorem 10.5.",
    "the likelihood ratio approach in the case of random x’s (Anderson 1984, pp. 140–142): Theorem 10.5. If ( y1,x0 1),(y2,x02),...,(yn,x0n) is a random sample from Nkþ1(m,S), wheremandSare given by (10.2) and (10.3), the likelihood ratio test forH0:b1¼0or equivalently H0:r2 yjx¼0 can be based on Fin (10.44). We reject H0ifF/C21Fa,k,n/C0k/C01. PROOF.",
    "¼0or equivalently H0:r2 yjx¼0 can be based on Fin (10.44). We reject H0ifF/C21Fa,k,n/C0k/C01. PROOF. Using the notation v0 i¼(yi,x0i), as in the proof of Theorem 10.2a, the likeli- hood function L(m,S)¼Qn i¼1f(vi;m,S) is given by (10.11), and the likelihood ratio is LR¼max H0L(m,S) max H1L(m,S): Under H1, the parameters mandSare essentially unrestricted, and we have max H1L(m,S)¼max L(m,S)¼L(^m,^S), wheremˆandSˆare the maximum likelihood estimators in (10.9) and (10.10).",
    "L(m,S)¼max L(m,S)¼L(^m,^S), wheremˆandSˆare the maximum likelihood estimators in (10.9) and (10.10). Since ( vi/C0m)0S/C01(vi/C0m) is a scalar, the exponent of L(m,S) in (10.11) can be258 MULTIPLE REGRESSION: RANDOM x’s --- Page 270 --- written as Pn i¼1tr (vi/C0m)0S/C01(vi/C0m)hi 2¼Pn i¼1trS/C01(vi/C0m)(vi/C0m)0hi 2 ¼trS/C01Pni¼1(vi/C0m)(vi/C0m)0hi 2: Then substitution of mˆandSˆformandSinL(m,S) gives max H1L(m,S)¼L(^m,^S)¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )n(kþ1)j^Sjn=2e/C0tr(^S/C01n^S=2) ¼e/C0n(kþ1)=2 (ﬃﬃﬃﬃﬃﬃ 2pp )n(kþ1)j^Sjn=2: Under H0:r2 yjx¼0, we have syx¼0, andSin (10.3) becomes S0¼syy00 0Sxx/C18/C19 ; (10:45) whose maximum likelihood estimator is ^S0¼^syy00 0^Sxx !",
    ".3) becomes S0¼syy00 0Sxx/C18/C19 ; (10:45) whose maximum likelihood estimator is ^S0¼^syy00 0^Sxx ! : (10:46) Using^S0in (10.46) and ^m¼/C22vin (10.9), we have max H0L(m,S)¼L(^m,^S0)¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )n(kþ1)j^S0jn=2e/C0tr(^S/C01 0n^S0=2): By (2.74), this becomes L(^m,^S0)¼e/C0n(kþ1)=2 (ﬃﬃﬃﬃﬃﬃ2pp )n(kþ1)^sn=2 yyj^Sxxjn=2: (10:47) Thus LR¼j^Sjn=2 ^sn=2 yyj^Sxxjn=2:(10:48)10.5 TESTS AND CONFIDENCE INTERVALS FOR R2259 --- Page 271 --- Substituting^S¼(n/C01)S=nand using (10.40), we obtain LR¼(1/C0R2)n=2: (10:49) We reject H0for (1 /C0R2)n=2/C20c,which is equivalent to F¼R2=k (1/C0R2)=(n/C0k/C01)/C21Fa,k,n/C0k/C01, since R2=(1/C0R2) is a monotone increasing function of R2andFis distributed as F(k,n2k21) when H0is true (Anderson 1984, pp.",
    "onotone increasing function of R2andFis distributed as F(k,n2k21) when H0is true (Anderson 1984, pp. 138–139). A When k¼1,Fin (10.44) reduces to F¼(n22)r2/(12r2). Then, by Problem 5.16 t¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C02p rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1/C0r2p [see (6.20)] has a tdistribution with n22 degrees of freedom (df) when ( y,x) has a bivariate normal distribution with r¼0.",
    "tribution with n22 degrees of freedom (df) when ( y,x) has a bivariate normal distribution with r¼0. If (y,x) is bivariate normal and r=0, then var( r)¼(1/C0r2)2=nand the function u¼ﬃﬃﬃnp(r/C0r) 1/C0r2(10:50) is approximately standard normal for large n. However, the distribution of u approaches normality very slowly as nincreases (Kendall and Stuart 1969, p. 236). Its use is questionable for n,500.",
    "lity very slowly as nincreases (Kendall and Stuart 1969, p. 236). Its use is questionable for n,500. Fisher (1921) found a function of rthat approaches normality much faster than does (10.50) and can thereby be used with much smaller nthan that required for (10.50). In addition, the variance is almost independent of r. Fisher’s function is z¼1 2ln1þr 1/C0r¼tanh/C01r, (10:51) where tanh21ris the inverse hyperbolic tangent of r.",
    "function is z¼1 2ln1þr 1/C0r¼tanh/C01r, (10:51) where tanh21ris the inverse hyperbolic tangent of r. The approximate mean and variance of zare E(z)ﬃ12ln1þr 1/C0r¼tanh/C01r, (10:52) var(z)ﬃ1 n/C03: (10:53)260 MULTIPLE REGRESSION: RANDOM x’s --- Page 272 --- We can use Fisher’s ztransformation in (10.51) to test hypotheses such as H0:r¼r0 orH0:r1¼r2.",
    "72 --- We can use Fisher’s ztransformation in (10.51) to test hypotheses such as H0:r¼r0 orH0:r1¼r2. To test H0:r¼r0vs.H1:r=r0, we calculate v¼z/C0tanh/C01r0ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1=(n/C03)p , (10:54) which is approximately distributed as the standard normal N(0, 1). We reject H0if jvj/C21za=2, where z¼tanh/C01randza=2is the upper a=2 percentage point of the stan- dard normal distribution.",
    "=2, where z¼tanh/C01randza=2is the upper a=2 percentage point of the stan- dard normal distribution. To test H0:r1¼r2vs.H1:r1=r2for two independent samples of sizes n1andn2yielding sample correlations r1andr2, we calculate v¼z1/C0z2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1=(n 1/C03)þ1=(n2/C03)p (10:55) and reject H0ifjvj/C21za=2, where z1¼tanh/C01r1and z2¼tanh/C01r2. To test H0:r1¼/C1/C1/C1¼rqforq.2, see Problem 10.18.",
    "/C21za=2, where z1¼tanh/C01r1and z2¼tanh/C01r2. To test H0:r1¼/C1/C1/C1¼rqforq.2, see Problem 10.18. To obtain a conﬁdence interval for r, we note that since zin (10.51) is approxi- mately normal, we can write P/C0za=2/C20z/C0tanh/C01r 1=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C03p /C20za=2/C18/C19 ﬃ1/C0a: (10:56) Solving the inequality for r, we obtain the approximate 100(1 2a)% conﬁdence interval tanh z/C0za=2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C03p/C18/C19 /C20r/C20tanh zþza=2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃn/C03p/C18/C19 : (10:57) A conﬁdence interval forr2 yjxwas given by Helland (1987).",
    "zþza=2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃn/C03p/C18/C19 : (10:57) A conﬁdence interval forr2 yjxwas given by Helland (1987). Example 10.5a. For the hematology data in Table 10.1, we obtained R2in Example 10.4. The overall Ftest of H0:b1¼0orH0:r2yjx¼0 is carried out using Fin (10.44): F¼R2=k (1/C0R2)=(n/C0k/C01) ¼:9232 =5 (1/C0:9232) =45¼108 :158 : Thepvalue is less than 10216. A10.5 TESTS AND CONFIDENCE INTERVALS FOR R2261 --- Page 273 --- Example 10.5b.",
    "e is less than 10216. A10.5 TESTS AND CONFIDENCE INTERVALS FOR R2261 --- Page 273 --- Example 10.5b. To illustrate Fisher’s ztransformation in (10.51) and its use to compare two independent correlations in (10.55), we divide the hematology data in Table 10.1 into two subsamples of sizes n1¼26 and n2¼25 (the ﬁrst 26 obser- vations and the last 25 observations). For the correlation between yandx1in each of the two subsamples, we obtain r1¼.4994 and r2¼.0424.",
    "). For the correlation between yandx1in each of the two subsamples, we obtain r1¼.4994 and r2¼.0424. The ztransformation in (10.51) for each of these two values is given by z1¼tanh/C01r1¼:5485 , z2¼tanh/C01r2¼:0425 : To test H0:r1¼r2, we use the approximate test statistic (10.55) to obtain v¼:5485 /C0:0425ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1=(26/C03)þ1=(25/C03)p ¼1:6969 : Since 1.6969 ,z.025¼1.96, we do not reject H0.",
    "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1=(26/C03)þ1=(25/C03)p ¼1:6969 : Since 1.6969 ,z.025¼1.96, we do not reject H0. To obtain approximate 95% conﬁdence limits for r1, we use (10.57): Lower limit for r1:tanh :5485 /C01:96ﬃﬃﬃﬃﬃ 23p/C18/C19 ¼:1389 , Upper limit for r1:tanh :5485 þ1:96ﬃﬃﬃﬃﬃ 23p/C18/C19 ¼:7430 : Forr2, the limits are given by Lower limit for r2:tanh :0425 /C01:96ﬃﬃﬃﬃﬃ 22p/C18/C19 ¼/C0 :3587 , Upper limit for r2:tanh :0425 þ1:96ﬃﬃﬃﬃﬃ 22p/C18/C19 ¼:4303 : A 10.6 EFFECT OF EACH VARIABLE ON R2 The contribution of a variable xjto the multiple correlation Rwill, in general, be different from its bivariate correlation with y; that is, the increase in R2when xjis added is not equal to r2 yxj.",
    "its bivariate correlation with y; that is, the increase in R2when xjis added is not equal to r2 yxj. This increase in R2can be either more or less than r2 yxj. It seems clear that relationships with other variables can render a variable partially redundant and thereby reduce the contribution of xjtoR2, but it is not intuitively apparent how the contribution of xjtoR2can exceed r2 yxj.",
    "ution of xjtoR2, but it is not intuitively apparent how the contribution of xjtoR2can exceed r2 yxj. The latter phenomenon has been illustrated numerically by Flury (1989) and Hamilton (1987).262 MULTIPLE REGRESSION: RANDOM x’s --- Page 274 --- In this section, we provide a breakdown of the factors that determine how much each variable adds to R2and show how the increase in R2can exceed r2 yxj (Rencher 1993). We ﬁrst introduce some notation.",
    "R2and show how the increase in R2can exceed r2 yxj (Rencher 1993). We ﬁrst introduce some notation. The variable of interest is denoted by z, which can be one of the x’s or a new variable added to the x’s.",
    "ariable of interest is denoted by z, which can be one of the x’s or a new variable added to the x’s. We make the following additional notational deﬁnitions: R2 yw¼squared multiple correlation between yandw¼(x1,x2,...,xk,z)0: R2yx¼squared multiple correlation between yandx¼(x1,x2,...,xk)0: R2zx¼s0zxS/C01 xxszx=s2z¼squared multiple correlation between zandx: ryz¼simple correlation between yandz: ryx¼(ryx1,ryx2,...,ryxk)0¼vector of correlations between yandx: rzx¼(rzx1,rzx2,...,rzxk)0¼vector of correlations between zandx: ^b/C3zx¼R/C01 xxrzxis the vector of standardized regression coefficients (beta weights) ofzregressed on x[see (10.24)] : The effect of zonR2is formulated in the following theorem.",
    "weights) ofzregressed on x[see (10.24)] : The effect of zonR2is formulated in the following theorem. Theorem 10.6. The increase in R2due to zcan be expressed as R2yw/C0R2yx¼(^ryz/C0ryz)2 1/C0R2 zx, (10:58) where ^ryz¼^b/C30 zxryxis a “predicted” value of ryzbased on the relationship of zto thex’s. PROOF. See Problem 10.19. A Since the right side of (10.58) is positive, R2cannot decrease with an additional variable, which is a veriﬁcation of property 3 in Section 7.7.",
    "R2cannot decrease with an additional variable, which is a veriﬁcation of property 3 in Section 7.7. If zis orthogonal to x (i.e., if rzx¼0), then ^b/C3 zx¼0, which implies that rˆyz¼0 and R2 zx¼0. In this case, (10.58) can be written as R2yw¼R2yxþr2 yz, which veriﬁes property 5 of Section 7.7. It is clear in Theorem 10.6 that the contribution of ztoR2can either be less than or greater than r2 yz.I frˆyzis close to ryz, the contribution of zis less than r2yz.",
    "r be less than or greater than r2 yz.I frˆyzis close to ryz, the contribution of zis less than r2yz. There are three ways in which the contribution of zcan exceed r2 yz: (1) rˆyzis substantially larger in absolute value than ryz, (2) rˆyzandryzare of opposite signs, and (3) R2 zx is large. In many cases, the researcher may ﬁnd it helpful to know why a variable contrib- uted more than expected or less than expected.",
    "er may ﬁnd it helpful to know why a variable contrib- uted more than expected or less than expected. For example, admission to a university or professional school may be based on previous grades and the score on a standar- dized national test. An applicant for admission to a university with limited enrollmentwould submit high school grades and a national test score.",
    "on to a university with limited enrollmentwould submit high school grades and a national test score. These might be entered10.6 EFFECT OF EACH VARIABLE ON R2263 --- Page 275 --- into a regression equation to obtain a predicted value of ﬁrst-year grade-point average at the university. It is typically found that the standardized test increases R2only slightly above that based on high school grades alone.",
    "d that the standardized test increases R2only slightly above that based on high school grades alone. This small increase in R2 would be disappointing to admissions ofﬁcials who had hoped that the nationaltest score might be a more useful predictor than high school grades. The designers of such standardized tests may ﬁnd it beneﬁcial to know precisely why the testmakes such an unexpectedly small contribution relative to high school grades.",
    "precisely why the testmakes such an unexpectedly small contribution relative to high school grades. In Theorem 10.6, we have available the speciﬁc information needed by the designer of the standardized test. To illustrate the use of (10.58), let ybe the grade-point average for the ﬁrst year at the university, let zbe the score on the stan- dardized test, and let x 1,x2,...,xkbe high school grades in key subject areas.",
    "e score on the stan- dardized test, and let x 1,x2,...,xkbe high school grades in key subject areas. By (10.58), the increase in R2due to zis (^ryz/C0ryz)2=(1/C0R2 zx), in which we see that z adds little to R2ifrˆyzis close to ryz. We could examine the coefﬁcients in ^ryz¼^b/C30 zxryxto determine which of the ryxj’s in ryxhave the most effect. This infor- mation could be used in redesigning the questions so as to reduce these particular ryxj’s.",
    "is infor- mation could be used in redesigning the questions so as to reduce these particular ryxj’s. It may also be possible to increase the contribution of ztoR2 ywby increasing R2zx(thereby reducing 1 2R2zx). This might be done by designing the questions in the standardized test so that the test score zis more correlated with high school grades, x1,x2,...,xq. Theil and Chung (1988) proposed a measure of the relative importance of a vari- able in multiple regression based on information theory.",
    "asure of the relative importance of a vari- able in multiple regression based on information theory. Example 10.6. For the hematology data in Table 10.1, the overall R2 ywwas found in Example 10.4 to be .92318. From Theorem 10.6, the increase in R2due to a variable z has the breakdown R2 yw/C0R2yx¼(^ryz/C0ryz)2=(1/C0R2zx), where zrepresents any one of x1,x2,...,x5, and xrepresents the other four variables.",
    "z)2=(1/C0R2zx), where zrepresents any one of x1,x2,...,x5, and xrepresents the other four variables. The values of ^ryz,ryz,R2 zx, R2yw/C0R2yx, and Fare given below for each variable in turn as z: zr ˆyz ryz R2 zx R2yw2R2yxFp value x1 .2101 .1943 .6332 .00068 0.4 .53 x2 .2486 .2210 .6426 .00213 1.25 .26 x3 .0932 .7890 .4423 .86820 508.6 0 x4 .4822 .0526 .3837 .29945 175.4 0 x5 .0659 .0758 .0979 .00011 0.064 .81 TheFvalue is from the partial Ftest in (8.25), (8.37), or (8.39) for the signiﬁcance of the increase in R2due to each variable.",
    "al Ftest in (8.25), (8.37), or (8.39) for the signiﬁcance of the increase in R2due to each variable. An interesting variable here is x4, whose value of ryzis .0526, the smallest among the ﬁve variables. Despite this small individual correlation with y,x4contributes much more to R2 ywthan do all other variables except x3because rˆyzis much greater forx4than for the other variables.",
    "wthan do all other variables except x3because rˆyzis much greater forx4than for the other variables. This illustrates how the contribution of a variable can be augmented in the presence of other variables as reﬂected in rˆyz. The difference between the two major contributors x3andx4may be very revealing to the researcher.",
    "z. The difference between the two major contributors x3andx4may be very revealing to the researcher. The contribution of x3toR2ywis due mostly to its own correlation264 MULTIPLE REGRESSION: RANDOM x’s --- Page 276 --- with y, whereas virtually all the effect of x4comes from its association with the other variables as reﬂected in rˆyz.",
    "rtually all the effect of x4comes from its association with the other variables as reﬂected in rˆyz. A 10.7 PREDICTION FOR MULTIVARIATE NORMAL OR NONNORMAL DATA In this section, we consider an approach to modeling and estimation in the random- x case that is somewhat reminiscent of least squares in the ﬁxed- xcase. Suppose that (y,x0)¼(y,x1,x2,...,xk) is not necessarily assumed to be multivariate normal and we wish to ﬁnd a function t(x) for predicting y.",
    "t necessarily assumed to be multivariate normal and we wish to ﬁnd a function t(x) for predicting y. In order to ﬁnd a predicted value t(x) that is expected to be “close” to y, we will choose the function t(x) that minimizes the mean squared error E[y/C0t(x)]2, where the expectation is in the joint distribution of y,x1,...,xk. This function is given in the following theorem. Theorem 10.7.",
    "he joint distribution of y,x1,...,xk. This function is given in the following theorem. Theorem 10.7. For the random vector ( y,x0), the function t(x) that minimizes the mean squared error E[y/C0t(x)]2is given by E(yjx). PROOF. For notational simplicity, we use k¼1. By (4.28), the joint density g(y,x) can be written as g(y,x)¼f(yjx)h(x).",
    "nal simplicity, we use k¼1. By (4.28), the joint density g(y,x) can be written as g(y,x)¼f(yjx)h(x). Then E[y/C0t(x)]2¼ðð [y/C0t(x)]2g(y,x)dy dx ¼ðð [y/C0t(x)]2f(yjx)h(x)dy dx ¼ð h(x)ð [y/C0t(x)]2f(yjx)dy/C26/C27 dx: To ﬁnd the function t(x) that minimizes E(y2t)2, we differentiate with respect to t and set the result equal to 0 [for a more general proof not involving differentiation, see Graybill (1976, pp. 432–434) or Christensen (1996, p. 119)].",
    "roof not involving differentiation, see Graybill (1976, pp. 432–434) or Christensen (1996, p. 119)]. Assuming that we can interchange integration and differentiation, we obtain @E[y/C0t(x)]2 @t¼ð h(x)ð 2(/C01)[y/C0t(x)]f(yjx)dy/C26/C27 dx¼0, which gives 2ð h(x)ð yf(yjx)dy/C0ð t(x)f(yjx)dy/C20/C21 dx¼0, 2ð h(x)[E(yjx)/C0t(x)/C138dx¼0: The left side is 0 if t(x)¼E(yjx):A10.7 PREDICTION FOR MULTIVARIATE NORMAL OR NONNORMAL DATA 265 --- Page 277 --- In the case of the multivariate normal, the prediction function E(yjx) is a linear function of x[see (10.4) and (10.5)].",
    "ultivariate normal, the prediction function E(yjx) is a linear function of x[see (10.4) and (10.5)]. However, in general, E(yjx) is not linear. For an illustration of a nonlinear E(yjx), see Example 3.2, in which we have E(yjx)¼1 2(1þ4x/C02x2). If we restrict t(x)t olinear functions of x, then the optimal result is the same linear function as in the multivariate normal case [see (10.6) and (10.7)]. Theorem 10.7b.",
    "the same linear function as in the multivariate normal case [see (10.6) and (10.7)]. Theorem 10.7b. The linear function t(x) that minimizes E[y/C0t(x)]2is given by t(x)¼b0þb0 1x, where b0¼my/C0s0 yxS/C01 xxmx, (10:59) b1¼S/C01 xxsyx: (10:60) PROOF. See Problem 10.21. A We can ﬁnd estimators bˆ0andbˆ1forb0andb1in (10.59) and (10.60) by mini- mizing the sample mean squared error,Pn i¼1(yi/C0^b0/C0^b0 1xi)2=n. The results are given in the following theorem. Theorem 10.7c.",
    "error,Pn i¼1(yi/C0^b0/C0^b0 1xi)2=n. The results are given in the following theorem. Theorem 10.7c. If (y1,x0 1),(y2,x02),...,(yn,x0n) is a random sample with mean vector and covariance matrix ^m¼/C22y /C22x/C18/C19 ;S¼syys0 yx syxSxx/C18/C19 , then the estimators bˆ0andbˆ1that minimizePn i¼1(yi/C0^b0/C0^b0 1xi)2=nare given by ^b0¼/C22y/C0s0yxS/C01 xx/C22x, (10:61) ^b1¼S/C01 xxsyx: (10:62) PROOF. See Problem 10.22.",
    "e given by ^b0¼/C22y/C0s0yxS/C01 xx/C22x, (10:61) ^b1¼S/C01 xxsyx: (10:62) PROOF. See Problem 10.22. A The estimators bˆ0andbˆ1in (10.61) and (10.62) are the same as the maximum like- lihood estimators in the normal case [see (10.15) and (10.16)]. 10.8 SAMPLE PARTIAL CORRELATIONS Partial correlations were introduced in Sections 4.5 and 7.10.",
    "6)]. 10.8 SAMPLE PARTIAL CORRELATIONS Partial correlations were introduced in Sections 4.5 and 7.10. Assuming multivariate normality, the population partial correlation rij/C1rs/C1/C1/C1qis the correlation between yiandyj in the conditional distribution of ygiven x, where yiand yjare in yand the266 MULTIPLE REGRESSION: RANDOM x’s --- Page 278 --- subscripts r,s,...,qrepresent all the variables in x.",
    "LTIPLE REGRESSION: RANDOM x’s --- Page 278 --- subscripts r,s,...,qrepresent all the variables in x. By (4.36), we obtain rij/C1rs...q¼sij/C1rs...qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃsii/C1rs...qsjj/C1rs...qp , (10:63) wheresij/C1rs/C1/C1/C1q;is the ( ij) element of Sy/C1x¼cov(yjx). For normal populations, Sy/C1xis given by (4.27) as Sy/C1x¼Syy/C0SyxS/C01 xxSxy, whereSyy,Syx,Sxx, andSyxare from the partitioned covariance matrix covy x/C18/C19 ¼S¼SyySyx SxySxx/C18/C19 [see (3.33)].",
    "dSyxare from the partitioned covariance matrix covy x/C18/C19 ¼S¼SyySyx SxySxx/C18/C19 [see (3.33)]. The matrix of (population) partial correlations rij/C1rs...qcan be found by (4.37): Py/C1x¼D/C01 y/C1xSy/C1xD/C01 y/C1x¼D/C01 y/C1x(Syy/C0SyxS/C01 xxSxy)D/C01 y/C1x, (10:64) where Dy/C1x¼[diag(Sy/C1x)]1=2: To obtain a maximum likelihood estimator Ry/C1x¼(rij/C1rs...q)o fPy/C1x¼(rij/C1rs...q)i n (10.64), we use the invariance property of maximum likelihood estimators (Theorem 10.2b) to obtain Ry/C1x¼D/C01 s(Syy/C0SyxS/C01 xxSxy)D/C01 s, (10:65) where Ds¼[diag( Syy/C0SyxS/C01 xxSxy)]1=2: The matrices Syy,Syx,Sxx, and Sxyare from the sample covariance matrix parti- tioned by analogy to Sabove S¼SyySyx SxySxx/C18/C19 , where Syy¼s2 y1sy1y2/C1/C1/C1 sy1yp sy2y1s2y 2/C1/C1/C1 sy2yp .........",
    "ve S¼SyySyx SxySxx/C18/C19 , where Syy¼s2 y1sy1y2/C1/C1/C1 sy1yp sy2y1s2y 2/C1/C1/C1 sy2yp ......... sypy1sypy2/C1/C1/C1 s2 yp0 BBBBB@1 CCCCCAand Syx¼sy1x1sy1x2/C1/C1/C1 sy1xq sy2x1sy2x2/C1/C1/C1 sy2xq ......... sypx1sypx2/C1/C1/C1 sypxq0 BBBB@1 CCCCA10.8 SAMPLE PARTIAL CORRELATIONS 267 --- Page 279 --- are estimators of SyyandSyx. Thus the maximum likelihood estimator of rij/C1rs...qin (10.63) is rij/C1rs...q, the ( ij) th element of Ry/C1xin (10.65).",
    "ihood estimator of rij/C1rs...qin (10.63) is rij/C1rs...q, the ( ij) th element of Ry/C1xin (10.65). We now consider two other expressions for partial correlation and show that they are equivalent to rij/C1rs...qin (10.65). To simplify exposition, we illustrate with r12/C13.",
    "t they are equivalent to rij/C1rs...qin (10.65). To simplify exposition, we illustrate with r12/C13. The sample partial correlation of y1andy2with y3held ﬁxed is usually given as r12/C13¼r12/C0r13r23ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (1/C0r2 13)(1/C0r2 23)p , (10:66) where r12,r13, and r23are the ordinary correlations between y1andy2,y1andy3, and y2andy3, respectively. In the following theorem, we relate r12/C13to two previous deﬁ- nitions of partial correlation. Theorem 10.8a.",
    "lowing theorem, we relate r12/C13to two previous deﬁ- nitions of partial correlation. Theorem 10.8a. The expression for r12/C13in (10.66) is equivalent to an element of Ry/C1xin (10.65) and is also equal to ry1/C0^y1,y2/C0^y2from (7.94), where y1/C0^y1and y2/C0^y2are residuals from regression of y1ony3andy2ony3. PROOF. We ﬁrst consider ry1/C0^y1,y2/C0^y2, which is not a maximum likelihood estimator and can therefore be used when the data are not normal.",
    "which is not a maximum likelihood estimator and can therefore be used when the data are not normal. We obtain yˆ1andyˆ1by regressing y1ony3andy2ony3. Using the notation in Section 7.10, we indicate the predicted value of y1based on regression of y1ony3as^y1(y3).",
    "tation in Section 7.10, we indicate the predicted value of y1based on regression of y1ony3as^y1(y3). With a similar deﬁnition of ^y2(y3), the residuals can be expressed as u1¼y1/C0^y1(y3)¼y1/C0(^b01þ^b11y3), u2¼y2/C0^y2(y3)¼y2/C0(^b02þ^b12y3), where, by (6.5), ^b11and ^b12are the usual least-squares estimators ^b11¼Pn i¼1(y1i/C0/C22y1)(y3i/C0/C22y3)Pn i¼1(y3i/C0/C22y3)2, (10:67) ^b12¼Pn i¼1(y2i/C0/C22y2)(y3i/C0/C22y3)Pni¼1(y3i/C0/C22y3)2: (10:68) Then the sample correlation between u1¼y1/C0^y1(y3) and u2¼y2/C0^y2(y3) [see (7.94)] is ru1u2¼ry1/C0^y1,y2/C0^y2 ¼dcov(u1,u2)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ cvar(u1)cvar(u2)p : (10:69)268 MULTIPLE REGRESSION: RANDOM x’s --- Page 280 --- Since the sample mean of the residuals u1andu2is 0 [see (9.11)], ru1u2can be written as ru1u2¼Pn i¼1u1iu2iﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1u2 1iPn i¼1u2 2ip ¼Pn i¼1(y1i/C0^y1i)(y2i/C0^y2i)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(y1i/C0^y1i)2Pni¼1(y2i/C0^y2i)2q : (10:70) We now show that ru1u2in (10.70) can be expressed as an element of Ry/C1xin (10.65).",
    "2i)2q : (10:70) We now show that ru1u2in (10.70) can be expressed as an element of Ry/C1xin (10.65). Note that in this illustration, Ry/C1xis 2/C22.",
    "can be expressed as an element of Ry/C1xin (10.65). Note that in this illustration, Ry/C1xis 2/C22. The numerator of (10.70) can be written as Xn i¼1u1iu2i¼Xn i¼1(y1i/C0^y1i)(y2i/C0^y2i) ¼Xn i¼1(y1i/C0^b01/C0^b11y3i)(y2i/C0^b02/C0^b12y3i): Using ^b01¼/C22y1/C0^b11/C22y3and ^b02¼/C22y2/C0^b12/C22y3, we obtain Xn i¼1u1iu2i¼Xn i¼1[y1i/C0/C22y1/C0^b11(y3i/C0/C22y3)][y2i/C0/C22y2/C0^b12(y3i/C0/C22y3)] ¼X i(y1i/C0/C22y1)(y2i/C0/C22y2)/C0^b11^b12X i(y3i/C0/C22y3)2: (10:71) The other two terms in (10.71) sum to zero.",
    "2y1)(y2i/C0/C22y2)/C0^b11^b12X i(y3i/C0/C22y3)2: (10:71) The other two terms in (10.71) sum to zero. Using (10.67) and (10.68), the second term on the right side of (10.71) can be written as ^b11^b12X i(y3i/C0/C22y3)2¼[Pn i¼1(y1i/C0/C22y1)(y3i/C0/C22y3)][Pni¼1(y2i/C0/C22y2)(y3i/C0/C22y3)]Pni¼1(y3i/C0/C22y3)2: (10:72) If we divide (10.71) by n/C01, divide numerator and denominator of (10.72) by n/C01, and substitute (10.72) into (10.71), we obtain dcov(u1,u2)¼dcov( y1/C0^y1,y2/C0^y2)¼s12/C0s13s23 s33:10.8 SAMPLE PARTIAL CORRELATIONS 269 --- Page 281 --- This is the element in the ﬁrst row and second column of Syy/C0SyxS/C01 xxSxyin (10.65), where Syy¼s11s12 s21s22/C18/C19 ,Syx¼syx¼s13 s23/C18/C19 ,Sxx¼s33, and Sxy¼s0 yx.",
    "1 xxSxyin (10.65), where Syy¼s11s12 s21s22/C18/C19 ,Syx¼syx¼s13 s23/C18/C19 ,Sxx¼s33, and Sxy¼s0 yx. In this case, the 2 /C22 matrix Syy/C0SyxS/C01 xxSxyis given by Syy/C0SyxS/C01 xxSxy¼s11s12 s21s22/C18/C19 /C01 s33s13 s23/C18/C19 (s13,s23) ¼s11s12 s21s22/C18/C19 /C01 s33s213 s13s23 s23s13 s223 ! : Thus ru1u2, as based on residuals in (10.69), is equivalent to the maximum likelihood estimator in (10.65).",
    "1u2, as based on residuals in (10.69), is equivalent to the maximum likelihood estimator in (10.65). We now use (10.71) to convert ru1u2in (10.69) into the familiar formula for r12/C13 given in (10.66).",
    "e now use (10.71) to convert ru1u2in (10.69) into the familiar formula for r12/C13 given in (10.66). By (10.70), we obtain ru1u2¼P iu1iu2iﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP iu2 1iP iu22ip : (10:73) By an extension of (10.71), we further obtain Xn i¼1u2 1i¼X i(y1i/C0/C22y1)2/C0^b211X i(y3i/C0/C22y3)2, (10:74) Xn i¼1u2 2i¼X i(y2i/C0/C22y2)2/C0^b212X i(y3i/C0/C22y3)2: (10:75) Then (10.73) becomes ru1u2¼P i(y1i/C0/C22y1)(y2i/C0/C22y2)/C0^b11^b12P i(y3i/C0/C22y3)2 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ [P i(y1i/C0/C22y1)2/C0^b2 11P i(y3i/C0/C22y3)2][P i(y2i/C0/C22y2)2/C0^b212P i(y3i/C0/C22y3)2]q : (10:76) We now substitute for ^b11and ^b12as deﬁned in (10.67) and (10.68) and divide numerator and denominator byﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(y1i/C0/C22y1)2P i(y2i/C0/C22y2)2q to obtain ru1u2¼r12/C13¼r12/C0r13r23ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (1/C0r2 13)(1/C0r2 23)p : (10:77) Thus ru1u2based on residuals as in (10.69) is equivalent to the usual formulation r12/C13 in (10.66).",
    "hus ru1u2based on residuals as in (10.69) is equivalent to the usual formulation r12/C13 in (10.66). A270 MULTIPLE REGRESSION: RANDOM x’s --- Page 282 --- For the general case rij/C1rs...q,w h e r e iandjare subscripts pertaining to yandr,s,...,q are all the subscripts associated with x, we deﬁne a residual vector yi/C0^yi(x), where ^yi(x) is the vector of predicted values from the regression of yonx.[ N o t et h a t iis used differently in rij/C1rs...qandyi/C0^yi(x).] In Theorem 10.8a, r12/C13was found to be equal tory1/C0^y1,y2/C0^y2, the ordinary correlation of the two residuals, and to be equivalent to the partial correlation deﬁned as an element of Ry/C1xin (10.65).",
    "residuals, and to be equivalent to the partial correlation deﬁned as an element of Ry/C1xin (10.65). In the following theorem, this is extended to the vectors yandx. Theorem 10.8b. The sample covariance matrix of the residual vector yi/C0^yi(x)i s equivalent to Syy/C0SyxS/C01 xxSxyin (10.65), that is, Sy/C0^y¼Syy/C0SyxS/C01 xxSxy. PROOF. The sample predicted value ^yi(x) is an estimator of E(yjxi)¼myþ SyxS/C01 xx(xi/C0mx) given in (4.26).",
    "e sample predicted value ^yi(x) is an estimator of E(yjxi)¼myþ SyxS/C01 xx(xi/C0mx) given in (4.26). For ^yi(x), we use the maximum likelihood estimator ofE(yjxi): ^yi(x)¼/C22yþSyxS/C01 xx(xi/C0/C22x): (10:78) [The same result can be obtained without reference to normality; see Rencher (1998, p.",
    "22x): (10:78) [The same result can be obtained without reference to normality; see Rencher (1998, p. 304).] Since the sample mean of yi/C0^yi(x)i s0(see Problem 10.26), the sample covari- ance matrix of yi/C0^yi(x) is deﬁned as Sy/C0^y¼1 n/C01Xn i¼1[yi/C0^yi(x)][yi/C0^yi(x)]0(10:79) (see Problem 10.1).We ﬁrst note that by extension of (10.13), we have Syy¼P i (yi/C0/C22y)(yi/C0/C22y)0=(n/C01),Syx¼P i(yi/C0/C22y)(xi/C0/C22x)0=(n/C01), and Sxx¼P i(xi/C0/C22x) (xi/C0/C22x)0=(n/C01) (see Problem 10.27).",
    "yi/C0/C22y)(xi/C0/C22x)0=(n/C01), and Sxx¼P i(xi/C0/C22x) (xi/C0/C22x)0=(n/C01) (see Problem 10.27). Using these expressions, after substituting (10.78) in (10.79), we obtain Sy/C0^y¼1 n/C01Xn i¼1[yi/C0/C22y/C0SyxS/C01 xx(xi/C0/C22x)][yi/C0/C22y/C0SyxS/C01 xx(xi/C0/C22x)]0 ¼1 n/C01Xn i¼1(yi/C0/C22y)(yi/C0/C22y)0/C0Xn i¼1(yi/C0/C22y)(xi/C0/C22x)0S/C01 xxSxy\" /C0SyxS/C01 xxXn i¼1(xi/C0/C22x)(yi/C0/C22y)0þSyxS/C01 xxXn i¼1(xi/C0/C22x)(xi/C0/C22x)0S/C01 xxSxy# ¼Syy/C0SyxS/C01 xxSxy/C0SyxS/C01 xxSxyþSyxS/C01 xxSxxS/C01 xxSxy ¼Syy/C0SyxS/C01 xxSxy: Thus the covariance matrix of residuals gives the same result as the maximum like- lihood estimator of conditional covariances and correlations in (10.65).",
    "result as the maximum like- lihood estimator of conditional covariances and correlations in (10.65). A10.8 SAMPLE PARTIAL CORRELATIONS 271 --- Page 283 --- Example 10.8. We illustrate some partial correlations for the hematology data in Table 10.1. To ﬁnd ry1/C12345, for example, we use (10.65), Ry/C1x¼D/C01 s(Syy/C0 SyxS/C01 xxSxy)D/C01 s. In this case, y¼(y,x1)0andx¼(x2,x3,x4,x5)0.",
    "e (10.65), Ry/C1x¼D/C01 s(Syy/C0 SyxS/C01 xxSxy)D/C01 s. In this case, y¼(y,x1)0andx¼(x2,x3,x4,x5)0. The matrix Sis therefore partitioned as S¼90:290 1 :535 4:880 106 :202 3 :753 3 :064 1:535 0 :691 1:494 3 :255 0 :422 /C00:268 4:880 1 :494 5:401 10 :155 1 :374 1 :292 106 :202 3 :255 10:155 200 :668 64 :655 4 :067 3:753 0 :422 1:374 64 :655 56 :374 0 :579 3:064 /C00:268 1:292 4 :067 0 :579 18 :0780 BBBBBBBBB@1 CCCCCCCCCA ¼S yySyx SxySxx/C18/C19 : The matrix Ds¼[diag( Syy/C0SyxS/C01 xxSxy)]1=2is given by Ds¼2:645 0 0 :503/C18/C19 , and we have Ry/C1x¼1:0000 /C00:0934 /C00:0934 1 :000/C18/C19 : Thus, ry1/C12345¼/C0 :0934.",
    "C18/C19 , and we have Ry/C1x¼1:0000 /C00:0934 /C00:0934 1 :000/C18/C19 : Thus, ry1/C12345¼/C0 :0934. On the other hand, ry1¼:1934. To ﬁnd ry2/C11345,w eh a v e y¼(y,x2)0andx¼(x1,x3,x4,x5)0. Thus Syy¼90:290 4 :880 4:880 5 :401/C18/C19 , and there are corresponding matrices for Syx,Sxy,andSxx.",
    "us Syy¼90:290 4 :880 4:880 5 :401/C18/C19 , and there are corresponding matrices for Syx,Sxy,andSxx. The diagonal matrix Dsis given by Ds¼diag(2 :670,1:389) ,and we have Ry/C1x¼1:000 /C00:164 /C00:164 1 :000/C18/C19 : Thus, ry2/C11345¼/C0 :164, which can be compared with ry2¼:221.272 MULTIPLE REGRESSION: RANDOM x’s --- Page 284 --- To ﬁnd ry3/C145,w eh a v e y¼(y,x1,x2,x3)0andx¼(x4,x5)0.",
    "PLE REGRESSION: RANDOM x’s --- Page 284 --- To ﬁnd ry3/C145,w eh a v e y¼(y,x1,x2,x3)0andx¼(x4,x5)0. Then, for example, we obtain Syy¼90:290 1 :535 4 :880 106 :202 1:535 0 :691 1 :494 3 :255 4:880 1 :494 5 :401 10 :155 106 :202 3 :255 10 :155 200 :6680 BB@1 CCA: The diagonal matrix Dsis given by Ds¼diag(9 :462,:827,2:297,11:219) , and we have Ry/C1x¼1:000 0 :198 0 :210 0 :954 0:198 1 :000 0 :792 0 :304 0:210 0 :792 1 :000 0 :324 0:954 0 :304 0 :324 1 :0000 BB@1 CCA: Thus, for example, ry1/C145¼:198,ry3/C145¼:954,r12/C145¼:792,andr23/C145¼:324.",
    "324 1 :0000 BB@1 CCA: Thus, for example, ry1/C145¼:198,ry3/C145¼:954,r12/C145¼:792,andr23/C145¼:324. In this case, Ry/C1xis little changed from Ryy: Ryy¼1:000 0 :194 0 :221 0 :789 0:194 1 :000 0 :774 0 :277 0:221 0 :774 1 :000 0 :308 0:789 0 :277 0 :308 1 :0000 BB@1 CCA: A PROBLEMS 10.1 Show that Sin (10.14) can be found as S¼Pn i¼1(vi/C0/C22v)(vi/C0/C22v)0=(n/C01) as in (10.13). 10.2 Show that mˆandSin (10.9) and (10.10) are jointly sufﬁcient for mandS,a s noted following Theorem 10.2c.",
    "that mˆandSin (10.9) and (10.10) are jointly sufﬁcient for mandS,a s noted following Theorem 10.2c. 10.3 Show that S¼DRD gives the partitioned result in (10.19). 10.4 Show that cov( y,w)¼s0 yxS/C01 xxsyxand var( w)¼s0yxS/C01 xxsyxas in (10.26), where w¼myþs0yxS/C01 xx(x/C0mx): 10.5 Show that r2 yjxin (10.27) is the maximum squared correlation between yand any linear function of x, as in (10.28). 10.6 Show that r2yjxcan be expressed as r2yjx¼1/C0jSj=(syyjSxxj) as in (10.29).",
    "of x, as in (10.28). 10.6 Show that r2yjxcan be expressed as r2yjx¼1/C0jSj=(syyjSxxj) as in (10.29). 10.7 Show that r2yjxis invariant to linear transformations u¼ayandv¼Bx, where Bis nonsingular, as in (10.30).PROBLEMS 273 --- Page 285 --- 10.8 Show that cov( y/C0w,x)¼00as in (10.33).",
    "nsingular, as in (10.30).PROBLEMS 273 --- Page 285 --- 10.8 Show that cov( y/C0w,x)¼00as in (10.33). 10.9 Verify that R2¼r2 y^y, as in (10.36), using the following two deﬁnitions of r2 y^y: (a)r2 y^y¼Pn i¼1(yi/C0^yi)(^yi/C0/C22^y)/C2/C32=Pn i¼1(yi/C0/C22y)2Pni¼1(^yi/C0/C22^y)2/C2/C3 (b)ry^y¼sy^y=(sys^y) 10.10 Show that R2¼max ar2 y;a0xas in (10 :37). 10.11 Show that R2¼r0 yxR/C01 xxryxas in (10 :38) .",
    "10.10 Show that R2¼max ar2 y;a0xas in (10 :37). 10.11 Show that R2¼r0 yxR/C01 xxryxas in (10 :38) . 10.12 Show that R2¼1/C01=ryyas in (10.39), where ryyis the upper left-hand diag- onal element of R/C01, with Rpartitioned as in (10.18). 10.13 Verify that R2can be expressed in terms of determinants as in (10.40) and (10.41). 10.14 Show that R2is invariant to full-rank linear transformations on yor the x’s, as in property 9 in Section 10.4.",
    "R2is invariant to full-rank linear transformations on yor the x’s, as in property 9 in Section 10.4. 10.15 Show that^S0in (10.46) is the maximum likelihood estimator of S0in (10.45) and that max H0Lðm;SÞis given by (10.47). 10.16 Show that LR in (10.48) is equal to LR¼ð1/C0R2Þn=2in (10.49). 10.17 Obtain the conﬁdence interval in (10.57) from the inequality in (10.56). 10.18 Suppose that we have three independent samples of bivariate normal data.",
    "nequality in (10.56). 10.18 Suppose that we have three independent samples of bivariate normal data. The three sample correlations are r1,r2, and r3based, respectively, on sample sizes n1,n2, and n3. (a) Find the covariance matrix Vofz¼(z1z2z3)0where zi¼1 2ln[(1 þri)=(1/C0ri)]. (b) Letm0 z¼(tanh/C01r1;tanh/C01r2;tanh/C01r3), and let C¼1/C010 10 /C01/C18/C19 : Find the distribution of [ C(z/C0mz)]0[CVC0]/C01[C(z/C0mz)]. (c) Using (b), propose a test for H0:r1¼r2¼r3or equivalently H0:Cmz¼0.",
    "C0mz)]0[CVC0]/C01[C(z/C0mz)]. (c) Using (b), propose a test for H0:r1¼r2¼r3or equivalently H0:Cmz¼0. 10.19 Prove Theorem 10.6. 10.20 Show that if zwere orthogonal to the x’s, (10.58) could be written in the form R2yw¼R2yxþr2 yz, as noted following Theorem 10.6. 10.21 Prove Theorem 10.7b. 10.22 Prove Theorem 10.7c.274 MULTIPLE REGRESSION: RANDOM x’s --- Page 286 --- 10.23 Show thatPn i¼1u1iu2i¼Pni¼1(y1i/C0/C22y1)(y2i/C0/C22y2)/C0^b11^b12Pni¼1(y3i/C0/C22y3)2 as in (10.71).",
    "how thatPn i¼1u1iu2i¼Pni¼1(y1i/C0/C22y1)(y2i/C0/C22y2)/C0^b11^b12Pni¼1(y3i/C0/C22y3)2 as in (10.71). 10.24 Show thatPni¼1u2 1i¼Pn i¼1(y1i/C0/C22y1)2/C0^b2 11Pn i¼1(y3i/C0/C22y3)2as in (10.74). 10.25 Obtain r12/C13in (10.77) from ru1u2in (10.76). 10.26 Show that Sni¼1[yi/C0^yi(x)]¼0, as noted following (10.78). 10.27 Show that Syy¼P i(yi/C0/C22y)(yi/C0/C22y)0=(n/C01);Syx¼P i(yi/C0/C22y)(xi/C0/C22x)0= (n/C01), and Sxx¼P i(xi/C0/C22x)(xi/C0/C22x)0=(n/C01), as noted following (10.79).",
    "2y)(xi/C0/C22x)0= (n/C01), and Sxx¼P i(xi/C0/C22x)(xi/C0/C22x)0=(n/C01), as noted following (10.79). 10.28 In an experiment with rats, the concentration of a particular drug in the liver was of interest. For 19 rats the following variables were observed: y¼percentage of the dose in the liver x1¼body weight x2¼liver weight x3¼relative dose The data are given in Table 10.2 (Weisberg 1985, p. 122). (a) Find Sxx;syx;^b1;^b0;ands2. (b) Find Rxx;ryx, and ^b/C3 1. (c) Find R2.",
    "(Weisberg 1985, p. 122). (a) Find Sxx;syx;^b1;^b0;ands2. (b) Find Rxx;ryx, and ^b/C3 1. (c) Find R2. (d) Test H0:b1¼0: 10.29 Use the hematology data in Table 10.1 as divided into two subsamples of sizes 26 and 25 in Example 10.5b (the ﬁrst 26 observations and the last 25 observations). For each pair of variables below, ﬁnd r1andr2for the two sub- samples, ﬁnd z1andz2as in (10.51), test H0:r1¼r2as in (10.55), and ﬁnd conﬁdence limits for r1andr2as in (10.57).",
    "z1andz2as in (10.51), test H0:r1¼r2as in (10.55), and ﬁnd conﬁdence limits for r1andr2as in (10.57). (a)yandx2 (b)yandx3 TABLE 10.2 Rat Data yx 1 x2 x3 yx 1 x2 x3 .42 176 6.5 0.88 .27 158 6.9 .80 .25 176 9.5 0.88 .36 148 7.3 .74.56 190 9.0 1.00 .21 149 5.2 .75 .23 176 8.9 0.88 .28 163 8.4 .81 .23 200 7.2 1.00 .34 170 7.2 .85.32 167 8.9 0.83 .28 186 6.8 .94.37 188 8.0 0.94 .30 164 7.3 .73.41 195 10.0 0.98 .37 181 9.0 .90 .33 176 8.0 0.88 .46 149 6.4 .75.38 165 7.9 0.84PROBLEMS 275 --- Page 287 --- (c)yandx4 (d)yandx5 10.30 For the rat data in Table 10.2, check the effect of each variable on R2as in Section 10.6.",
    "dx5 10.30 For the rat data in Table 10.2, check the effect of each variable on R2as in Section 10.6. 10.31 Using the rat data in Table 10.2. (a) Find ry1/C123and compare to ry1.",
    "n R2as in Section 10.6. 10.31 Using the rat data in Table 10.2. (a) Find ry1/C123and compare to ry1. (b) Find ry2/C113 (c) Find Ry/C1x, where y¼(y;x1;x2)0andx¼x3, in order to obtain ry1/C13;ry2/C13, andr12/C13.276 MULTIPLE REGRESSION: RANDOM x’s --- Page 288 --- 11 Multiple Regression: Bayesian Inference We now consider Bayesian estimation and prediction for the multiple linear regression model in which the xvariables are ﬁxed constants as in Chapters 7–9.",
    "the multiple linear regression model in which the xvariables are ﬁxed constants as in Chapters 7–9. The Bayesian statistical paradigm is conceptually simple and general because infer-ences involve only probability calculations as opposed to maximization of a function like the log likelihood. On the other hand, the probability calculations usually entail complicated or even intractable integrals.",
    "e other hand, the probability calculations usually entail complicated or even intractable integrals. The Bayesian approach has become popular more recently because of the development of computer-intensive approximations to these integrals (Evans and Swartz 2000) and user-friendly programs to carry out the computations (Gilks et al. 1998). We discuss both analytical and computer- intensive approaches to the Bayesian multiple regression model.",
    "iscuss both analytical and computer- intensive approaches to the Bayesian multiple regression model. Throughout Chapters 7 and 8 we assumed that the parameters bands2were unknown ﬁxed constants. We couldn’t really do otherwise because to this point (at least implicitly) we have only allowed probability distributions to represent variabilitydue to such things as random sampling or imprecision of measurement instruments.",
    "epresent variabilitydue to such things as random sampling or imprecision of measurement instruments. The Bayesian approach additionally allows probability distributions to represent con- jectural uncertainty. Thus bands2can be treated as if they are random variables because we are uncertain about their values. The technical property that allows one to treat parameters as random variables is exchangeability of the observational units in the study (Lindley and Smith 1972).",
    "andom variables is exchangeability of the observational units in the study (Lindley and Smith 1972). 11.1 ELEMENTS OF BAYESIAN STATISTICAL INFERENCE In Bayesian statistics, uncertainty about the value of a parameter is expressed using the tools of probability theory (e.g., a density function—see Section 3.2). Density functions of parameters like bands2reﬂect the current credibility of possible values for these parameters.",
    "ons of parameters like bands2reﬂect the current credibility of possible values for these parameters. The goal of the Bayesian approach is to use data to update the uncertainty distributions for parameters, and then draw sensible conclusions using these updated distributions. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 277 --- Page 289 --- The Bayesian approach can be used in any inference situation.",
    "iley & Sons, Inc. 277 --- Page 289 --- The Bayesian approach can be used in any inference situation. However, it seems especially natural in the following type of problem. Consider an industrial process in which it is desired to estimate b0andb1for the straight-line relationship in (6.1) between a response yand a predictor xfor a particular batch of product. Suppose that it is known from experience that b0andb1vary randomly from batch to batch.",
    "of product. Suppose that it is known from experience that b0andb1vary randomly from batch to batch. Bayesian inference allows historical (or prior) knowledge of the distributionsof b0andb1among batches to be expressed in probabilistic form, and then to be com- bined with ( x,y) data from a speciﬁc batch in order to give improved estimates of b0 andb1for that speciﬁc batch. Bayesian inference is based on two general equations.",
    "d estimates of b0 andb1for that speciﬁc batch. Bayesian inference is based on two general equations. In these equations as pre- sented below, uis a vector of mcontinuous parameters, yis a vector of ncontinuous observations, and f,g,h,k,p,q,randtare probability density functions. We begin with the deﬁnition of the conditional density of ugiven y[see (3.18)] g(ujy)¼k(y,u) h(y), (11 :1) where k(y,u) is the joint density of y1,y2,...,ynandu1,u2,...,um.",
    "3.18)] g(ujy)¼k(y,u) h(y), (11 :1) where k(y,u) is the joint density of y1,y2,...,ynandu1,u2,...,um. Using the deﬁ- nition of the conditional density f(yju), we can write k(y,u)¼f(yju)p(u), and (11.1) becomes g(ujy)¼f(yju)p(u) h(y), (11 :2) an expression that is commonly referred to as Bayes’ theorem .",
    "comes g(ujy)¼f(yju)p(u) h(y), (11 :2) an expression that is commonly referred to as Bayes’ theorem . By an extension of (3.13), the marginal density h(y) can be obtained by integrating uout of k(y,u)¼f(yju)p(u) so that (11.2) becomes g(ujy)¼f(yju)p(u) Ð1 /C01/C1/C1/C1Ð1 /C01f(yju)p(u)du ¼cf(yju)p(u), (11 :3) where du¼du1/C1/C1/C1dum. In this expression, p(u) is known as the prior density ofu, andg(ujy) is called the posterior density ofu.",
    "s expression, p(u) is known as the prior density ofu, andg(ujy) is called the posterior density ofu. The deﬁnite integral in the denomi- nator of (11.3) is often replaced by a constant ( c) because after integration, it no longer involves the random vector u. This deﬁnite integral is often very complicated, but can sometimes be obtained by noting that cis anormalizing constant , that is, a value such that the posterior density integrates to 1.",
    "g that cis anormalizing constant , that is, a value such that the posterior density integrates to 1. Rearranging this expression and reinterpreting the joint density function f(yju) of the data as the likelihood function L(ujy) (see Section 7.6.2), we obtain g(ujy)¼cp(u)L(ujy): (11 :4)278 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 290 --- Thus (11.2), the ﬁrst general equation of Bayesian inference, merely states that the posterior density of ugiven the data (representing the updated uncertainty in u)i sp r o - portional to the prior density of utimes the likelihood function.",
    "pdated uncertainty in u)i sp r o - portional to the prior density of utimes the likelihood function. Point and interval estimates of the parameters are taken as mathematical features of this joint posterior density or associated marginal posterior densities of individual parameters ui.F o r example, the mode or mean of the marginal posterior density of a parameter may be used as a point estimate of the parameter. A central or highest density interval (Gelman et al. 2004, pp.",
    "as a point estimate of the parameter. A central or highest density interval (Gelman et al. 2004, pp. 38–39) over which the marginal posterior density of a par- ameter integrates to 1 2vmay be taken as a 100(1 2v)% interval estimate of the parameter. For the second general equation of Bayesian inference, we consider a future obser- vation y0.",
    "meter. For the second general equation of Bayesian inference, we consider a future obser- vation y0. In the Bayesian approach, y0is not independent of yas was assumed in Section 8.6.5 because its density depends on u, a random vector whose current uncer- tainty depends on y.",
    "on 8.6.5 because its density depends on u, a random vector whose current uncer- tainty depends on y. Since y0,yanduare jointly distributed, the posterior predictive density of y0given yis obtained by integrating uout of the joint conditional density ofy0andugiven y: r(y0jy)¼ð1 /C01/C1/C1/C1ð1 /C01t(y0,ujy)du ¼ð1 /C01/C1/C1/C1ð1 /C01q(y0ju,y)g(ujy)du[by (4 :28)] where q(y0ju,y) is the conditional density function of the sampling distribution for a future observation y0.",
    "0ju,y) is the conditional density function of the sampling distribution for a future observation y0. Since y0is dependent on yonly through u,q(y0ju,y) simpliﬁes, and we have r(y0jy)¼ð1 /C01/C1/C1/C1ð1 /C01q(y0ju)g(ujy)du: (11 :5) Equation (11.5) expresses the intuitive idea that uncertainty associated with the pre- dicted value of a future observation has two components: sampling variability and uncertainty in the parameters.",
    "of a future observation has two components: sampling variability and uncertainty in the parameters. As before, point and interval predictions can be taken as mathematical features (such as the mean, mode, or speciﬁed integral) ofthis posterior predictive density.",
    "matical features (such as the mean, mode, or speciﬁed integral) ofthis posterior predictive density. 11.2 A BAYESIAN MULTIPLE LINEAR REGRESSION MODEL Bayesian multiple regression models are similar to the classical multiple regression model (see Section 7.6.1) except that they include speciﬁcations of the prior11.2 A BAYESIAN MULTIPLE LINEAR REGRESSION MODEL 279 --- Page 291 --- distributions for the parameters.",
    "2 A BAYESIAN MULTIPLE LINEAR REGRESSION MODEL 279 --- Page 291 --- distributions for the parameters. Prior speciﬁcation is an important part of the art and practice of Bayesian modeling, but since the focus of this text is the basic theory of linear models, we discuss only one set of prior speciﬁcations—one that is chosen forits mathematical convenience rather than actual prior information.",
    "eciﬁcations—one that is chosen forits mathematical convenience rather than actual prior information. 11.2.1 A Bayesian Multiple Regression Model with a Conjugate Prior Although not necessary, it is often convenient to parameterize Bayesian models using precision ( t) rather than variance ( s2), where t¼1 s2: Using this parameterization, as an example of a Bayesian linear regression model, let yjb,tbeNnXb,1 tI/C18/C19 , bjtbeNkþ1f,1 tV/C18/C19 , tbe gamma( a,d): The second and third distributions here are prior distributions, and we assume that f, V,a, andd(the parameters of the prior distributions), are known.",
    "istributions, and we assume that f, V,a, andd(the parameters of the prior distributions), are known. Although we will not do so here, this model could be extended by specifying hyperprior distributions forf,V,a, andd(Lindley and Smith 1972). As in previous chapters, the number of predictor variables is denoted by k(so that the rank of Xiskþ1) and the number of observations by n.",
    "predictor variables is denoted by k(so that the rank of Xiskþ1) and the number of observations by n. The prior density function for bjtis, using (4.9) p1(bjt)¼1 (2p)(kþ1)=2jt/C01Vj1 2e/C0t(b/C0f)0V/C01(b/C0f)=2: (11 :6) The prior density function for tis the gamma density (Gelman et al. 2004, pp. 574–575) p2(t)¼da G(a)ta/C01e/C0dt, (11 :7) wherea.0,d.0, and by deﬁnition G(a)¼ð1 0xa/C01e/C0xdx (11 :8)280 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 292 --- (see any advanced calculus text).",
    "11 :8)280 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 292 --- (see any advanced calculus text). For the gamma density in (11.7), E(t)¼a dand var( t)¼a d2: These prior distributions could be formulated with small enough variances that the prior knowledge strongly inﬂuences posterior distributions of the parameters in the model. If so, they are called informative priors .",
    "posterior distributions of the parameters in the model. If so, they are called informative priors . On the other hand, both of these priors could be formulated with large variances so that they have very little effect on the posterior distributions. If so, they are called diffuse priors . The priors would be diffuse if, for example, Vin (11.6) were a diagonal matrix with very large diagonal elements, and if din (11.7) were very close to zero.",
    "were a diagonal matrix with very large diagonal elements, and if din (11.7) were very close to zero. The prior speciﬁcations in (11.6) and (11.7) are ﬂexible and reasonable, and they also have nice mathematical properties, as will be shown in Theorem 11.2a. Other speciﬁcations for the prior distributions could be used. However, even the minor modiﬁcation of proposing a prior distribution for bthat is not conditional on t makes the model far less mathematically tractable.",
    "or distribution for bthat is not conditional on t makes the model far less mathematically tractable. The joint prior for bandtin our model is called a conjugate prior because its use results in a posterior distribution of the same form as the prior. We prove this in the following theorem. Theorem 11.2a. Consider the Bayesian multiple regression model in which yjb,t isNn(Xb,t/C01I),bjtisNkþ1(f,t/C01V),andtis gamma( a,d).",
    "ian multiple regression model in which yjb,t isNn(Xb,t/C01I),bjtisNkþ1(f,t/C01V),andtis gamma( a,d). The joint prior distri- bution is conjugate, that is, g(b,tjy) is of the same form as p(b,t). PROOF.",
    "The joint prior distri- bution is conjugate, that is, g(b,tjy) is of the same form as p(b,t). PROOF. Combining (11.6) and (11.7), the joint prior density is p(b,t)¼p1(bjt)p2(t) ¼c1t(kþ1=)2e/C0t(b/C0f)0V/C01(b/C0f)=2ta/C01e/C0dt ¼c1t(a/C3þkþ1)=2e/C0t[(b/C0f)0V/C01(b/C0f)þd/C3]=2, (11 :9) wherea/C3¼2a/C02,d/C3¼2dand all the factors not involving random variables are collected into the normalizing constant c1.",
    "2dand all the factors not involving random variables are collected into the normalizing constant c1. Using (11.4), the joint posterior density is then g(b,tjy)¼cp(b,t)L(b,tjy) ¼c2t(a/C3þkþ1)=2e/C0t[(b/C0f)0V/C01(b/C0f)þd/C3]=2tn=2e/C0t(y/C0Xb)0(y/C0Xb)=2 ¼c2t(a/C3/C3þkþ1)=2e/C0t[(b/C0f)0V/C01(b/C0f)þ(y/C0Xb)0(y/C0Xb)þd/C3]=2, wherea/C3/C3¼2a/C02þn, and all the factors not involving random variables are col- lected into the normalizing constant c2.",
    "and all the factors not involving random variables are col- lected into the normalizing constant c2. By expanding and completing the square in11.2 A BAYESIAN MULTIPLE LINEAR REGRESSION MODEL 281 --- Page 293 --- the exponent (Problem 11.1), we obtain g(b,tjy)¼c2t(a/C3/C3þkþ1)=2e/C0t[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3]=2, (11 :10) where V/C3¼(V/C01þX0X)/C01f/C3¼V/C3(V/C01fþX0y),andd/C3/C3¼/C0f0 /C3V/C01 /C3f/C3þ f0V/C01fþy0yþd/C3.",
    "here V/C3¼(V/C01þX0X)/C01f/C3¼V/C3(V/C01fþX0y),andd/C3/C3¼/C0f0 /C3V/C01 /C3f/C3þ f0V/C01fþy0yþd/C3. Hence the joint posterior density has exactly the same form as the joint prior density in (11.9). A It might seem odd to include terms like X0yandy0yin the “ constants ” of a prob- ability distribution, while considering parameters like bandtto be random, but this is completely characteristic of Bayesian inference.",
    "ring parameters like bandtto be random, but this is completely characteristic of Bayesian inference. In this sense, inference in a Bayesian linear model is opposite to inference in the classical linear model. 11.2.2 Marginal Posterior Density of b In order to carry out inferences for b, the marginal posterior density of b[see (3.13)] must be obtained by integrating tout of the joint posterior density in (11.10). The following theorem gives the form of this marginal distribution. Theorem 11.2b.",
    "nsity in (11.10). The following theorem gives the form of this marginal distribution. Theorem 11.2b. Consider the Bayesian multiple regression model in which yjb,t isNn(Xb,t/C01I),bjtisNkþ1(f,t/C01V), andtis gamma( a,d). The marginal posterior distribution u(bjy) is a multivariate tdistribution with parameters ( nþ2a,f/C3,W/C3), where f/C3¼(V/C01þX0X)/C01(V/C01fþX0y) (11 :11) and W/C3¼(y/C0Xf)0(IþXVX0)/C01(y/C0Xf)þ2d nþ2a/C20/C21 (V/C01þX0X)/C01: (11 :12) PROOF.",
    "0y) (11 :11) and W/C3¼(y/C0Xf)0(IþXVX0)/C01(y/C0Xf)þ2d nþ2a/C20/C21 (V/C01þX0X)/C01: (11 :12) PROOF. The marginal distribution of bjyis obtained by integration as u(bjy)¼ð1 0g(b,tjy)dt: By (11.10), this becomes u(bjy)¼c2ð1 0t(a/C3/C3þkþ1)=2e/C0t[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3]=2dt:282 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 294 --- Using (11.8) together with integration by substitution, the integral in this expression can be solved (Problem 11.2) to give the posterior distribution of bjyas u(bjy)¼c2Ga/C3/C3þ2þkþ1 2/C18/C19(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3 2/C20/C21 /C0(a/C3/C3þ2þkþ1)=2 ¼c3[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)/C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþd/C3]/C0(a/C3/C3þ2þkþ1)=2: To show that this is the multivariate tdensity, several algebraic steps are required as outlined in Problems 11.3a–c and 11.4.",
    "ultivariate tdensity, several algebraic steps are required as outlined in Problems 11.3a–c and 11.4. See also Seber and Lee (2003, pp. 100–110).",
    "s are required as outlined in Problems 11.3a–c and 11.4. See also Seber and Lee (2003, pp. 100–110). After these steps, the preceding expression becomes u(bjy)¼c3[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þ(y/C0Xf)0(IþXVX0)/C01(y/C0Xf) þ2d]/C0(a/C3/C3þ2þkþ1)=2: Dividing the expression in square brackets by ( y/C0Xf)0(IþXVX0)/C01(y/C0Xf)þ2d, modifying the normalizing constant accordingly, and replacing a/C3/C3by 2a/C02þn,w e obtain u(bjy)¼c41þ(b/C0f/C3)0V/C01 /C3(b/C0f/C3)=(nþ2a) [(y/C0Xf)0(IþXVX0)/C01(y/C0Xf)þ2d]=(nþ2a)/C20/C21 /C0(nþ2aþkþ1)=2 ¼c41þ(b/C0f/C3)0W/C01 /C3(b/C0f/C3) nþ2a/C18/C19 /C0(nþ2aþkþ1)=2 , (11 :13) where W/C3is as given in (11.12).",
    "0f/C3)0W/C01 /C3(b/C0f/C3) nþ2a/C18/C19 /C0(nþ2aþkþ1)=2 , (11 :13) where W/C3is as given in (11.12). The expression in (11.13) can now be recognized as the density function of the multivariate tdistribution (Gelman et al. 2004, pp. 576– 577; Rencher 1998, p. 56) with parameters ( nþ2a,f/C3,W/C3). Note that f/C3is the mean vector and [( nþ2a)=(nþ2a/C02)]W/C3is the covariance matrix of bjy.A As a historical note, the reasoning in this section is closely related to the work of W. S.",
    "of bjy.A As a historical note, the reasoning in this section is closely related to the work of W. S. Gosset or “Student” (Pearson et al. 1990, pp.",
    "n this section is closely related to the work of W. S. Gosset or “Student” (Pearson et al. 1990, pp. 49–53, 72–73) on the small- sample distribution of t¼/C22y s: Gosset used Bayesian reasoning (“inverse probability”) with a uniform prior distri- bution (“equal distribution of ignorance”) to show through a combination of proof, conjecture, and simulation that the posterior density of tis related to what we now call Student’s tdistribution with n21 degrees of freedom.11.2 A BAYESIAN MULTIPLE LINEAR REGRESSION MODEL 283 --- Page 295 --- 11.2.3 Marginal Posterior Densities of tands2 Inferences regarding tands2require knowledge of the marginal posterior distri- bution of tjy.",
    "tands2 Inferences regarding tands2require knowledge of the marginal posterior distri- bution of tjy. We derive the posterior density of tjyin the following theorem. Theorem 11.2c. Consider the Bayesian multiple regression model in which yjb,t isNn(Xb,t/C01I),bjtisNkþ1(f,t/C01V), andtis gamma ( a,d). The marginal posterior distribution v(tjy) is a gamma distribution with parameters aþn=2 and ( /C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2, where V/C3¼(V/C01þX0X)/C01and f/C3¼V/C3(V/C01fþX0y). PROOF.",
    "0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2, where V/C3¼(V/C01þX0X)/C01and f/C3¼V/C3(V/C01fþX0y). PROOF. The marginal distribution of tjyis obtained by integration as v(tjy)¼ð1 /C01/C1/C1/C1ð1 /C01g(b,tjy)db ¼c2ð1 /C01/C1/C1/C1ð1 /C01t(a/C3/C3þkþ1)=2e/C0t[(b/C0f/C3)0V/C3/C01(b/C0f/C3)þd/C3/C3]=2db ¼c2t(a/C3/C3þkþ1)=2e/C0td/C3/C3=2ð1 /C01/C1/C1/C1ð1 /C01e/C0t[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)]=2db where all the factors not involving random variables are collected into the normaliz- ing constant c2as in (11.10).",
    "actors not involving random variables are collected into the normaliz- ing constant c2as in (11.10). Since the integral in the preceding expression is proportional to the integral of a joint multivariate normal density, we obtain v(tjy)¼c2t(a/C3/C3þkþ1)=2e/C0(d/C3/C3=2)t(2p)(kþ1)=2jV/C3j1=2t/C0(kþ1)=2 ¼c5t(a/C3/C3þkþ1)=2/C0(kþ1)=2e/C0(d/C3/C3=2)t ¼c5t(aþn)=2/C01e/C0[(/C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2]t,(11 :14) which is the density function of the speciﬁed gamma distribution.",
    "f/C3þf0V/C01fþy0yþ2d)=2]t,(11 :14) which is the density function of the speciﬁed gamma distribution. A The marginal posterior density of s2can now be obtained by the univariate change-of-variable technique (4.2) as w(s2jy)¼c6(s2)/C0(aþn)=2/C01e/C0[(/C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2]=s2(11 :15) which is the density function of the inverse gamma distribution with parameters aþn=2 and ( /C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2 (Gelman et al. 2004, pp.",
    "ution with parameters aþn=2 and ( /C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2 (Gelman et al. 2004, pp. 574–575).284 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 296 --- 11.3 INFERENCE IN BAYESIAN MULTIPLE LINEAR REGRESSION 11.3.1 Bayesian Point and Interval Estimates of Regression Coefﬁcients A sensible Bayesian point estimator of bis the mean of the marginal posterior density in (11.13) f/C3¼(V/C01þX0X)/C01(V/C01fþX0y), (11 :16) and a sensible 100(1 2v)% Bayesian conﬁdence region for bis the highest-density regionVsuch that c4ð V/C1/C1/C1ð1þ(b/C0f/C3)0W/C01 /C3(b/C0f/C3) nþ2a/C20/C21 /C0(nþ2aþkþ1)=2 db¼1/C0v: (11 :17) A convenient property of the multivariate tdistribution is that linear functions of the random vector follow the (univariate) tdistribution.",
    "e tdistribution is that linear functions of the random vector follow the (univariate) tdistribution. Thus, given y, a0b/C0a0f/C3 a0W/C3aist(nþ2a) and, as an important special case, bi/C0f/C3i w/C3iiis t( nþ2a), (11 :18) wheref/C3iis the ith element of f/C3andw*iiis the ith diagonal element of W/C3.",
    "s t( nþ2a), (11 :18) wheref/C3iis the ith element of f/C3andw*iiis the ith diagonal element of W/C3. Thus a Bayesian point estimate of biisf/C3iand a 100(1 2v)% Bayesian conﬁdence interval forbiis f/C3i+tv=2,nþ2aw/C3ii: (11 :19) One very appealing aspect of Bayesian inference is that intervals like (11.19) have a natural interpretation.",
    "appealing aspect of Bayesian inference is that intervals like (11.19) have a natural interpretation. Instead of the careful classical interpretation of a conﬁdence interval in terms of hypothetical repeated sampling, one can simply and correctly say that the probability is 1 2vthatbiis in (11.19). An interesting ﬁnal note on Bayesian estimation of bis that the Bayesian estimator f/C3in (11.16) can be obtained as the generalized least-squares estimator of bin (7.63).",
    "n estimator f/C3in (11.16) can be obtained as the generalized least-squares estimator of bin (7.63). To see this, consider adding the prior information to the data as if it constituted a set of additional observations.",
    "ider adding the prior information to the data as if it constituted a set of additional observations. The idea is to augment ywithf, and to consider the mean vector and covariance matrix of the augmented vectory f/C18/C19 to be, respectively X Ikþ1/C18/C19 band1 tIO OV/C18/C19 :11.3 INFERENCE IN BAYESIAN MULTIPLE LINEAR REGRESSION 285 --- Page 297 --- Generalized least squares estimation expressed in terms of these partitioned matrices then gives f/C3in (11.16) as an estimate of b(Problem 11.6).",
    "in terms of these partitioned matrices then gives f/C3in (11.16) as an estimate of b(Problem 11.6). The implication of this is that prior information on the regression coefﬁcients can be incorporated into a multiple linear regression model by the intuitive informal process of “adding” observations. 11.3.2 Hypothesis Tests for Regression Coefﬁcients in Bayesian Inference Classical hypothesis testing is not a natural part of Bayesian inference (Gelman et al. 2004, p. 162).",
    "assical hypothesis testing is not a natural part of Bayesian inference (Gelman et al. 2004, p. 162). Nonetheless, if the question addressed by a classical hypothesis test is whether the data support the conclusion (i.e., alternative hypothesis) that biis greater thanbi0, a sensible approach is to use the posterior distribution (in this case the tdistribution with nþ2adegrees of freedom) to compute the probability P/C16 t(nþ2a).bi0/C0f/C3i w/C3ii/C17 : The larger this probability is, the more credible is the hypothesis that bi.bi0.",
    "0f/C3i w/C3ii/C17 : The larger this probability is, the more credible is the hypothesis that bi.bi0. If, alternatively, classical hypothesis testing is used to select a model from a set of candidate models, the corresponding Bayesian approach is to compute an information statistic for each model in question. For example, Schwarz (1978) proposed the Bayesian Information Criterion (BIC) for multiple linear regression models, andSpiegelhalter et al.",
    "Bayesian Information Criterion (BIC) for multiple linear regression models, andSpiegelhalter et al. (2002) proposed the Deviance Information Criterion (DIC) for more general Bayesian models. The model with the lowest value of the information criterion is selected. Model selection in Bayesian analysis is an area of current research. 11.3.3 Special Cases of Inference in Bayesian Multiple Regression Models Two special cases of inference in this Bayesian linear model are of particular interest.",
    "sion Models Two special cases of inference in this Bayesian linear model are of particular interest. First, consider the use of a diffuse prior. Let f¼0, letVbe a diagonal matrix with all diagonal elements equal to a large constant (say, 106), and let aanddboth be equal to a small constant (say, 1026). In this case, V21is close to O, and sof/C3, the Bayesian point estimate of bin (11.16), is approximately equal to (X0X)/C01X0y, the classical least-squares estimate.",
    "timate of bin (11.16), is approximately equal to (X0X)/C01X0y, the classical least-squares estimate. Also, since ( IþXVX0)/C01¼I/C0X(X0XþV/C01)/C01X0 (see Problem 11.3a), the covariance matrix W/C3approaches W/C3¼y0[I/C0X(X0X)/C01X0]y n(X0X)/C01 ¼n/C01 ns2(X0X)/C01[by (7 :26)] :286 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 298 --- Thus, in the case of diffuse priors, the Bayesian conﬁdence region (11.17) reduces to a region similar to (8.46), and Bayesian conﬁdence intervals for the regression coefﬁcients in (11.19) are similar to classical conﬁdence intervals in (8.47); the only differences are the multiplicative factor ( n21)/nand the use of thetdistribution with ndegrees of freedom rather than n2k21 degrees of freedom.",
    "n21)/nand the use of thetdistribution with ndegrees of freedom rather than n2k21 degrees of freedom. If a Bayesian multiple linear regression model with independent uniformly distributed priors for band ln(t/C01) is considered, Bayesian conﬁdence intervals for the regression coefﬁcients are exactly equal to classical conﬁdence intervals (Problem 11.5). One result of this is that simple Bayesian interpretations can be validly applied to conﬁdence intervals for the classical linear model.",
    "yesian interpretations can be validly applied to conﬁdence intervals for the classical linear model. In fact, most inferences for the classical linear model can be stated in terms of properties of pos- terior distributions. The second special case of inference in this Bayesian linear model is the case in whichf¼0andVis a diagonal matrix with a constant on the diagonal.",
    "yesian linear model is the case in whichf¼0andVis a diagonal matrix with a constant on the diagonal. Thus V¼aI, where ais a positive number, and the Bayesian estimator of bin (11.16) becomes X0Xþ1 aI/C18/C19/C01 X0y: For the centered model (Section 7.5) this estimator is also known as the “ridge estimator” (Hoerl and Kennard 1970).",
    "model (Section 7.5) this estimator is also known as the “ridge estimator” (Hoerl and Kennard 1970). It was originally proposed as a method for dealing with collinearity, the situation in which the columns of the Xmatrix have near-linear dependence so that X0Xis nearly singular. However, the estimator may also be understood as a “shrinkage estimator” in which prior informationcauses the estimates of the coefﬁcients to be shrunken toward zero (Seber and Lee 2003, pp. 321–322).",
    "auses the estimates of the coefﬁcients to be shrunken toward zero (Seber and Lee 2003, pp. 321–322). The use of a Bayesian linear model with hyperpriors (prior distributions for the parameters of the prior distributions) leads to a reasonable choice of value for ain terms of variances of the prior and hyperprior distributions (Lindley and Smith 1972).",
    "value for ain terms of variances of the prior and hyperprior distributions (Lindley and Smith 1972). 11.3.4 Bayesian Point and Interval Estimation of s2 A possible Bayesian point estimator of s2is the mean of the marginal inverse gamma density in (11.15) (/C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2 aþn=2/C01 and a 100(1 2v)% Bayesian conﬁdence interval for s2is given by the 1 2v/2 and v/2 quantiles of the appropriate inverse gamma distribution.",
    "terval for s2is given by the 1 2v/2 and v/2 quantiles of the appropriate inverse gamma distribution. As a special case, note that if aanddare both close to 0, f¼0, and Vis a diag- onal matrix with all diagonal elements equal to a large constant so that V21is close11.3 INFERENCE IN BAYESIAN MULTIPLE LINEAR REGRESSION 287 --- Page 299 --- toO, then the Bayesian point estimator of s2is approximately (y0y/C0f0 /C3V/C01 /C3f/C3)=2 n=2/C01¼y0y/C0y0X(X0X)/C01X0y n/C02 ¼y0[I/C0X(X0X)/C01X0]y n/C02 ¼n/C0k/C01 n/C02s2, and the centered Bayesian conﬁdence limits are the 1 2v/2 quantile and the v/2 quantile of the inverse gamma distribution with parameters n/2 and y0[I/C0X(X0X)/C01X0]y=2.",
    "the v/2 quantile of the inverse gamma distribution with parameters n/2 and y0[I/C0X(X0X)/C01X0]y=2. 11.4 BAYESIAN INFERENCE THROUGH MARKOV CHAIN MONTE CARLO SIMULATION The inability to derive a closed-form marginal posterior distribution for a parameter is extremely common in Bayesian inference (Gilks et al. 1998, p. 3).",
    "or distribution for a parameter is extremely common in Bayesian inference (Gilks et al. 1998, p. 3). For example, if the Bayesian multiple regression model of Section 11.2.1 had involved a prior distri- bution for bthat was notconditional on t, closed-form marginal distributions for the parameters could not have been derived (Lindley and Smith 1972). In actual prac- tice, the exception in Bayesian inference is to be able to derive closed-form marginal posterior distributions.",
    "xception in Bayesian inference is to be able to derive closed-form marginal posterior distributions. However, this difﬁculty turns out to be only a minor hindrance when modern computing resources are available. If it were possible, an ideal solution would be to draw a large number of samples from the joint posterior distribution. Then marginal means, marginal highest densityintervals, and other properties of the posterior distribution could be approximated using sample statistics.",
    "s, and other properties of the posterior distribution could be approximated using sample statistics. Furthermore, functions of the sampled values could be calcu- lated in order to approximate marginal posterior distributions of these functions. Thebig question, of course, is how it would be possible to draw samples from a distri- bution for which a familiar closed-form joint density function is not available.",
    "ples from a distri- bution for which a familiar closed-form joint density function is not available. A general approach for accomplishing this is referred to as Markov Chain Monte Carlo (MCMC) simulation (Gilks et al. 1998). A Markov Chain is a special sequence of random variables (Ross 2006, p. 185). Probability laws for general sequences of random variables are speciﬁed in terms of the conditional distribution of thecurrent value in the sequence, given all past values.",
    "in terms of the conditional distribution of thecurrent value in the sequence, given all past values. A Markov Chain is a simple sequence in which the conditional distribution of the current value is completely speciﬁed, given only the most recent value. Markov Chain Monte Carlo simulation in Bayesian inference is based on sequences of alternating random draws from conditional posterior distributions of each of the par-ameters in the model given the most recent values of the other parameters.",
    "utions of each of the par-ameters in the model given the most recent values of the other parameters. Thisprocess generates a Markov Chain for each parameter. Moreover, the unconditional dis- tribution for each parameter converges to the marginal posterior distribution of the288 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 300 --- parameter, and the unconditional joint distribution of the vector of parameters for any complete iteration of MCMC converges to the joint posterior distribution.",
    "ctor of parameters for any complete iteration of MCMC converges to the joint posterior distribution. Thus after dis- carding a number of initial draws (the “burn-in”), draws may be considered to constitutesequences of samples from marginal posterior distributions of the parameters. The samples are not independent, but the nonindependence can be ignored if the number of draws is sufﬁciently large.",
    "not independent, but the nonindependence can be ignored if the number of draws is sufﬁciently large. Plots of sample values can be examined to determine whether a sufﬁciently large number of draws has been obtained (Gilks et al. 1998). When the prior distributions are conjugate, closed-form density functions of the conditional posterior distributions of the parameters are available regardless ofwhether closed-form marginal posterior distributions can be derived.",
    "ters are available regardless ofwhether closed-form marginal posterior distributions can be derived. In the case of conjugate priors, a simple form of MCMC called “Gibbs sampling” (Gilks et al. 1998, Casella and George 1992) can be used by which draws are made successivelyfrom each of the conditional distributions of the parameters, given the current draws for the other parameters. We now illustrate this procedure.",
    "the parameters, given the current draws for the other parameters. We now illustrate this procedure. Consider again the Bayesian multiple regression model in which yj b,tisNn(Xb,t/C01I),bjtisNkþ1(f,t/C01V), andtis gamma( a,d). The joint posterior density function is given in (11.10). The conditional posterior density (or “full conditional”) of bjt,ycan be obtained by picking the terms out of (11.10) that involve b, and considering everything else to be part of the normal- izing constant.",
    "of (11.10) that involve b, and considering everything else to be part of the normal- izing constant. Thus, the conditional density of bjt,yis w(bjt,y)¼c6e/C0t(b/C0f/C3)0V/C01 /C3(b/C0f/C3)=2: Clearlybjt,yisNkþ1(f/C3,t/C01V/C3):Similarly, the conditional posterior density for tjb,yis c(tjb,y)¼c7t[(a/C3/C3þkþ3)=2]/C01e/C0t[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3]=2 so that tjb,y can be seen to be gamma {( a/C3/C3þkþ3)=2, [(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3]=2}: Gibbs sampling for this model proceeds as follows: †Specify a starting value t0[possibly 1 /s2from (7.23)].",
    "ampling for this model proceeds as follows: †Specify a starting value t0[possibly 1 /s2from (7.23)]. †For i¼1t o M:d r a wbifrom Nkþ1(f/C3,t/C01 i/C01V/C3),drawtifrom gamma {(a/C3/C3þkþ3)=2,[(bi/C0f/C3)0V/C01 /C3(bi/C0f/C3)þd/C3/C3]=2}: †Discard the ﬁrst Qdraws (as burn-in), and consider the last M2Qdraws (bi,ti) to be draws from the joint posterior distribution. For this model, using the start-ing value of 1 /s 2,Qwould usually be very small (say, 0), and Mwould be large (say, 10,000).",
    "start-ing value of 1 /s 2,Qwould usually be very small (say, 0), and Mwould be large (say, 10,000). Bayesian inferences for all parameters of the model could now be carried out using sample statistics of this empirical joint posterior distribution. For example, a Bayesian point estimate of tcould be calculated as the sample mean or median of the draws of t from the joint posterior distribution.",
    "be calculated as the sample mean or median of the draws of t from the joint posterior distribution. If we calculate (or “monitor”) 1 /ton each iter- ation, a Bayesian point estimate of s2¼1/tcould be calculated as the mean or11.4 BAYESIAN INFERENCE THROUGH MARKOV CHAIN 289 --- Page 301 --- median of 1 /t. A 95% Bayesian interval estimate of s2could be computed as the central 95% interval of the sample distribution of s2.",
    "terval estimate of s2could be computed as the central 95% interval of the sample distribution of s2. Other inferences could similarly be drawn on the basis of sample draws from the joint posterior distribution. Example 11.4. Table 11.1 contains body fat data for a sample of 20 females aged 25–34 (Kutner et al. 2005, p. 256). The response variable was body fat ( y), and two predictor variables were triceps skinfold thickness ( x1) and midarm circumfer- ence ( x2).",
    "and two predictor variables were triceps skinfold thickness ( x1) and midarm circumfer- ence ( x2). The data were analyzed using the Bayesian multiple regression model of Section 11.2.1 with diffuse priors in which f0¼(0, 0, 0), V¼106I3,a¼0.0001, andd¼0.0001. Density functions of the marginal posterior distributions of b0, b1, andb2from (11.13) as well as the marginal posterior density of s2from (11.15) are graphed in Figure 11.1.",
    "from (11.13) as well as the marginal posterior density of s2from (11.15) are graphed in Figure 11.1. Superimposed on these (and almost indistinguish- able from them) are smooth estimates (Silverman 1999) of the same posterior den-sities based on Gibbs sampling with Q¼0 and M¼10,000. A 11.5 POSTERIOR PREDICTIVE INFERENCE As a ﬁnal aspect of Bayesian inference for the multiple regression model, we consider Bayesian prediction of the value of the response variable for a future individual.",
    "odel, we consider Bayesian prediction of the value of the response variable for a future individual. If we again use the Bayesian multiple regression model of Section 11.2.1 in which yj b,tis Nn(Xb,t/C01I),bjtisNkþ1(f,t/C01V),andtis gamma( a,d), the posterior predictive density for a future observation y0with predictor variables x0can beTABLE 11.1 Body Fat Data yx 1 x2 11.9 19.5 29.1 22.8 24.7 28.2 18.7 30.7 37.0 20.1 29.8 31.1 12.9 19.1 30.921.7 25.6 23.727.1 31.4 27.625.4 27.9 30.621.3 22.1 23.219.3 25.5 24.825.4 31.1 30.027.2 30.4 28.311.7 18.7 23.017.8 19.7 28.6 12.8 14.6 21.323.9 29.5 30.122.6 27.7 25.725.4 30.2 24.614.8 22.7 27.121.1 25.2 27.5290 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 302 --- expressed using (11.5) as r(y0jy)¼ð1 0ð1 /C01/C1/C1/C1ð1 /C01q(y0jb,t)g(b,tjy)dbdt ¼cð1 0ð1 /C01/C1/C1/C1ð1 /C01t1=2e/C0t(y0/C0x0 0b)2=2t(a/C3/C3þkþ1)=2 /C2e/C0t[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3]=2dbdt ¼cð1 /C01/C1/C1/C1ð1 /C01[(b/C0f/C3)0V/C01 /C3(b/C0f/C3) þ(y0/C0x0 0b)2þd/C3/C3]/C0(a/C3/C3þkþ4)=2db: Further analytical progress with this integral is difﬁcult.",
    "/C0x0 0b)2þd/C3/C3]/C0(a/C3/C3þkþ4)=2db: Further analytical progress with this integral is difﬁcult. Nonetheless, Gibbs sampling as in Section 11.4 can be easily extended to simulate the posterior predictive distribution of y0as follows: †Specify a starting value t0[possibly 1 /s2from (7.23)].",
    "or predictive distribution of y0as follows: †Specify a starting value t0[possibly 1 /s2from (7.23)]. †For i¼1t o M:d r a w bifrom Nkþ1(f/C3,t/C01 i/C01V/C3),drawtifrom gamma{(a/C3/C3þkþ3)=2,[(bi/C0f/C3)0V/C01 /C3(bi/C0f/C3)þd/C3/C3]=2},draw y0ifrom N(x0 0bi,t/C01 i):Figure 11.1 Posterior densities of parameters for the fat data in Table 11.1.11.5 POSTERIOR PREDICTIVE INFERENCE 291 --- Page 303 --- †Discard the ﬁrst Qdraws (as burn-in), and consider the last M2Qdraws of y0i to be draws from the posterior predictive distribution.",
    "n-in), and consider the last M2Qdraws of y0i to be draws from the posterior predictive distribution. Example 11.5. Example 11.4(continued). Consider a new individual with x1¼20 and x2¼25. Thus x00¼(1, 20, 25). Figure 11.2 gives a smooth estimate of the posterior predictive density of y0based on Gibbs sampling with Q¼0 and M¼10,000.",
    "oth estimate of the posterior predictive density of y0based on Gibbs sampling with Q¼0 and M¼10,000. A The approximate Bayesian 95% prediction interval derived from this density is (11.83, 20.15), which may be compared to the 95% prediction interval (10.46, 21.57) for the same future individual using the non-Bayesian approach (8.62). This chapter gives a small taste of the calculations associated with the modern Bayesian multiple regression model.",
    "ves a small taste of the calculations associated with the modern Bayesian multiple regression model. With very little additional work, many aspects of the model can be modiﬁed and customized, especially if the MCMC approach is used. Versatility is one of the great advantages of the Bayesian approach.",
    "y if the MCMC approach is used. Versatility is one of the great advantages of the Bayesian approach. PROBLEMS 11.1 As in Theorem 11.2a, show that ( b/C0f)0V/C01(b/C0f)þ(y/C0Xb)0 (y/C0Xb)þd/C3¼(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3,where V/C3¼(V/C01þX0X)/C01, f/C3¼V/C3(V/C01fþX0y),andd/C3/C3¼/C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþd/C3: 11.2 As used in the proof to Theorem 11.2b, show that ð1 0tae/C0btdt¼b/C0(aþ1)G(aþ1):Figure 11.2 Approximate posterior predictive density using Gibbs sampling for a future observation y0with x00¼(1, 20, 25) for the fat data in Table 11.1.292 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 304 --- (a) Show that ( IþXVX0)/C01¼I/C0X(X0XþV/C01)/C01X0: (b) Show that ( IþXVX0)/C01X¼X(X0XþV/C01)/C01V/C01: (c) Show that V/C01/C0V/C01(X0XþV/C01)/C01V/C01¼X0(IþXVX0)/C01X: 11.4 As in the proof to Theorem 11.2b, show that y0yþf0V/C01f/C0f0 /C3V/C01 /C3f/C3 ¼ðy/C0XfÞ0ðIþXVX0Þ/C01ðy/C0XfÞ,where V/C3¼ðX0XþV/C01Þ/C01and f/C3¼V/C3ðX0yþV/C01fÞ: 11.5 Consider the Bayesian multiple linear regression model in which yjb,tis NnðXb,t/C01IÞ,bis uniform (Rkþ1) [i.e., uniform over ( kþ1) -dimensional space], and ln( t21) is uniform ( 21,1).",
    ",bis uniform (Rkþ1) [i.e., uniform over ( kþ1) -dimensional space], and ln( t21) is uniform ( 21,1). Show that the marginal posterior distribution of bjyis the multivariate tdistribution with parameters ½n/C0k/C01,^b,s2ðX0XÞ/C01/C138,where ^band s2are deﬁned in the usual way [see (7.6) and (7.23)]. These prior distributions are called improper priors because uniform distributions must be deﬁned for bounded sets of values.",
    "are called improper priors because uniform distributions must be deﬁned for bounded sets of values. Nonetheless, the sets can be very large, and so we can proceed as if they were unbounded.",
    "of values. Nonetheless, the sets can be very large, and so we can proceed as if they were unbounded. 11.6 Consider the augmented data vectory f/C18/C19 with mean vectorX Ikþ1/C18/C19 band covariance matrix 1 tIO O1 tV0 B@1 CA: Show that the generalized least-squares estimator of bis the Bayesian estima- tor in (11.16), ðV/C01þX0XÞ/C01ðV/C01fþX0yÞ: 11.7 Given that tis gamma( a,d) as in (11.7), ﬁnd E(t) and var( t).",
    "6), ðV/C01þX0XÞ/C01ðV/C01fþX0yÞ: 11.7 Given that tis gamma( a,d) as in (11.7), ﬁnd E(t) and var( t). 11.8 Use the Bayesian multiple regression model in which yjb,tis NnðXb,t/C01IÞ,bjtisNkþ1ðf,t/C01VÞ,andtis gamma( a,d). Derive the mar- ginal posterior density function for s2jy;wheres2¼1=t: 11.9 Consider the Bayesian simple linear regression model in which yijb0,b1, isNðb0þb1xi,1=tÞfor i¼1,...,n,b0jtisN(a,s2 0=t),b1jtis N(b,s21=t),cov(b0,b1jt)¼s12,andtis gamma( a,d).",
    "Nðb0þb1xi,1=tÞfor i¼1,...,n,b0jtisN(a,s2 0=t),b1jtis N(b,s21=t),cov(b0,b1jt)¼s12,andtis gamma( a,d). (a) Find the marginal posterior density of b1jy:(Do not simplify the results.) (b) Find Bayesian point and interval estimates of b1. 11.10 Consider the Bayesian multiple regression model in which yjb,tis Nn(Xb,t/C01I),bisNkþ1(f,V),andtis gamma( a,d). Note that this is similar to the model of Section 11.2 except that the prior distribution of b isnot conditional ont.",
    "similar to the model of Section 11.2 except that the prior distribution of b isnot conditional ont. (a) Find the joint posterior density of b,tjyup to a normalizing constant.11.3PROBLEMS 293 --- Page 305 --- (b) Find the conditional posterior density of bjt,yup to a normalizing constant. (c) Find the conditional posterior density of tjb,yup to a normalizing constant.",
    "rmalizing constant. (c) Find the conditional posterior density of tjb,yup to a normalizing constant. (d) Develop a Gibbs sampling procedure for estimating the marginal pos- terior distributions of bjyand (1 =t)jy: 11.11 Use the land rent data in Table 7.5. (a) Find 95% Bayesian conﬁdence intervals for b1,b2, andb3using (11.19) in connection with the model in which yjb,tisNn(Xb,t/C01I),bjtis Nkþ1(f,t/C01V),andtis gamma( a,d), where f¼0,V¼ 100I,a¼:0001 ,andd¼.0001.",
    "yjb,tisNn(Xb,t/C01I),bjtis Nkþ1(f,t/C01V),andtis gamma( a,d), where f¼0,V¼ 100I,a¼:0001 ,andd¼.0001. (b) Repeat part (a), but use Gibbs sampling to approximate the conﬁdence intervals.",
    "001 ,andd¼.0001. (b) Repeat part (a), but use Gibbs sampling to approximate the conﬁdence intervals. (c) Use Gibbs sampling to obtain a 95% Bayesian posterior prediction inter- val for a future individual with x0 0¼(1,15,30,:5): (d) Repeat part (b), but use the model in which yjb,tisNn(Xb,t/C01I), bisNkþ1(f,V), tisgamma(a,d) (11 :20) wheref¼0,V¼100I,a¼0:0001 ,andd¼0:0001 : 11.12 As in Section 11.5, show that ð1 0ð1 /C01/C1/C1/C1ð1 /C01t1=2e/C0t(y0/C0x0 0b)2=2t(a/C3/C3þkþ1)=2e/C0t[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3]=2dbdt ¼cð1 /C01/C1/C1/C1ð1 /C01[(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þ(y0/C0x0 0b)2þd/C3/C3](/C0a/C3/C3þkþ4)=2db:294 MULTIPLE REGRESSION: BAYESIAN INFERENCE --- Page 306 --- 12 Analysis-of-Variance Models In many experimental situations, a researcher applies several treatments or treatment combinations to randomly selected experimental units and then wishes to compare the treatment means for some response y.",
    "omly selected experimental units and then wishes to compare the treatment means for some response y. In analysis-of-variance (ANOVA), we use linear models to facilitate a comparison of these means. The model is often expressed with more parameters than can be estimated, which results in an X matrix that is not of full rank. We consider procedures for estimation and testing hypotheses for such models.",
    "t is not of full rank. We consider procedures for estimation and testing hypotheses for such models. The results are illustrated using balanced models, in which we have an equal number of observations in each cell or treatment combination. Unbalanced models are treated in more detail in Chapter 15. 12.1 NON-FULL-RANK MODELS In Section 12.1.1 we illustrate a simple one-way model, and in Section 12.1.2 we illustrate a two-way model without interaction.",
    "ate a simple one-way model, and in Section 12.1.2 we illustrate a two-way model without interaction. 12.1.1 One-Way Model Suppose that a researcher has developed two chemical additives for increasing the mileage of gasoline. To formulate the model, we might start with the notion that without additives, a gallon yields an average of mmiles.",
    "model, we might start with the notion that without additives, a gallon yields an average of mmiles. Then if chemical 1 is added, the mileage is expected to increase by t1miles per gallon, and if chemical 2 is added, the mileage would increase by t2miles per gallon. The model could be expressed as y1¼mþt1þ11,y2¼mþt2þ12, where y1is the miles per gallon from a tank of gasoline containing chemical 1 and 11 is a random error term. The variables y2and12are deﬁned similarly.",
    "line containing chemical 1 and 11 is a random error term. The variables y2and12are deﬁned similarly. The researcher Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 295 --- Page 307 --- would like to estimate the parameters m,t1, andt2and test hypotheses such as H0:t1¼t2. To make reasonable estimates, the researcher needs to observe the mileage per gallon for more than one tank of gasoline for each chemical.",
    "archer needs to observe the mileage per gallon for more than one tank of gasoline for each chemical. Suppose that the exper- iment consists of ﬁlling the tanks of six identical cars with gas, then adding chemical 1 to three tanks and chemical 2 to the other three tanks.",
    "ntical cars with gas, then adding chemical 1 to three tanks and chemical 2 to the other three tanks. We can write a model for eachof the six observations as follows: y 11¼mþt1þ111,y12¼mþt1þ112,y13¼mþt1þ113, y21¼mþt2þ121,y22¼mþt2þ122,y23¼mþt2þ123, (12 :1) or yij¼mþtiþ1ij,i¼1,2,j¼1,2,3 (12 :2) where yijis the observed miles per gallon of the jth car that contains the ith chemical in its tank and 1ijis the associated random error.",
    "lon of the jth car that contains the ith chemical in its tank and 1ijis the associated random error. The six equations in (12.1) can be written in matrix form as y11 y12 y13 y21 y22 y230 BBBBBB@1 CCCCCCA¼110 110110 101 1011010 BBBBBB@1 CCCCCCA m t1 t20 @1Aþ1 11 112 113 121 122 1230BBBBBB@1 CCCCCCA(12 :3) or y¼X bþ1: In (12.3), Xis a 6 /C23 matrix whose rank is 2 since the ﬁrst column is the sum of the second and third columns, which are linearly independent.",
    "2 since the ﬁrst column is the sum of the second and third columns, which are linearly independent. Since Xis not of full rank, the theorems of Chapters 7 and 8 cannot be used directly for estimating b¼(m,t1,t2)0and testing hypotheses. Thus, for example, the parameters m,t1, andt2cannot be estimated by ^b¼(X0X)/C01X0yin (7.6), because ( X0X)/C01does not exist. To further explore the reasons for the failure of (12.3) to be a full-rank model, let us reconsider the meaning of the parameters.",
    "for the failure of (12.3) to be a full-rank model, let us reconsider the meaning of the parameters. The parameter mwas introduced as the mean before adding chemicals, and t1andt2represented the increase due to chemi- cals 1 and 2, respectively. However, the model yij¼mþtiþ1ijin (12.2) cannot uniquely support this characterization.",
    "espectively. However, the model yij¼mþtiþ1ijin (12.2) cannot uniquely support this characterization. For example, if m¼15,t1¼1, andt2¼3, the model becomes y1j¼15þ1þ11j¼16þ11j,j¼1,2,3, y2j¼15þ3þ12j¼18þ12j,j¼1,2,3:(12 :4)296 ANALYSIS-OF-VARIANCE MODELS --- Page 308 --- However, from y1j¼16þ11jand y2j¼18þ12j, we cannot determine that m¼15,t1¼1, andt2¼3, because the model can also be written as y1j¼10þ6þ11j,j¼1,2,3, y2j¼10þ8þ12j,j¼1,2,3, or alternatively as y1j¼25/C09þ11j,j¼1,2,3, y2j¼25/C07þ12j,j¼1,2,3, or in inﬁnitely many other ways.",
    "or alternatively as y1j¼25/C09þ11j,j¼1,2,3, y2j¼25/C07þ12j,j¼1,2,3, or in inﬁnitely many other ways. Thus in (12.1) or (12.2), m,t1, andt2are not unique and therefore cannot be esti- mated. With three parameters and rank( X)¼2, the model is said to be overpara- meterized . Note that increasing the number of observations (replications) for each of the two additives will not change the rank of X.",
    "e number of observations (replications) for each of the two additives will not change the rank of X. There are various ways—each with its own advantages and disadvantages—to remedy this lack of uniqueness of the parameters in the overparameterized model.",
    "nd disadvantages—to remedy this lack of uniqueness of the parameters in the overparameterized model. Three such approaches are (1) redeﬁne the model using a smaller number of new par- ameters that are unique, (2) use the overparameterized model but place constraints on the parameters so that they become unique, and (3) in the overparameterized model,work with linear combinations of the parameters that are unique and can be unam- biguously estimated. We brieﬂy illustrate these three techniques. 1.",
    "hat are unique and can be unam- biguously estimated. We brieﬂy illustrate these three techniques. 1. To reduce the number of parameters, consider the illustration in (12.4): y 1j¼16þ11jand y2j¼18þ12j: The values 16 and 18 are the means after the two treatments have been applied. In general, these means could be labeled m1andm2and the model could be written as y1j¼m1þ11jand y2j¼m2þ12j: The means m1andm2are unique and can be estimated.",
    "odel could be written as y1j¼m1þ11jand y2j¼m2þ12j: The means m1andm2are unique and can be estimated. The redeﬁned model for all six observations in (12.1) or (12.2) takes the form y11 y12 y13 y21 y22 y230 BBBBBB@1 CCCCCCA¼10 10 10 01 01 010 BBBBBB@1 CCCCCCAm1 m2/C18/C19 þ111 112 113 121 122 1230 BBBBBB@1 CCCCCCA,12.1 NON-FULL-RANK MODELS 297 --- Page 309 --- which we write as y¼Wmþ1: The matrix Wis full-rank, and we can use (7.6) to estimate mas ^m¼^m1 ^m2/C18/C19 ¼(W0W)/C01W0y: This solution is called reparameterization .",
    "(7.6) to estimate mas ^m¼^m1 ^m2/C18/C19 ¼(W0W)/C01W0y: This solution is called reparameterization . 2. An alternative to reducing the number of parameters is to incorporate con- straints on the parameters m,t1, andt2. We denote the constrained parameters asm/C3,t/C3 1, andt/C32. In (12.1) or (12.2), the constraint t/C31þt/C32¼0 has the speciﬁc effect of deﬁning m/C3to be the new mean after the treatments are applied and t/C31 andt/C3 2to be deviations from this mean.",
    "be the new mean after the treatments are applied and t/C31 andt/C3 2to be deviations from this mean. With this constraint, y1j¼16þ11jand y2j¼18þ12jin (12.4) can be written only as y1j¼17/C01þ11j,y2j¼17þ1þ12j: This model is now unique because there is no other way to express it so that t/C3 1þt/C32¼0. Such constraints are often called side conditions .",
    "other way to express it so that t/C3 1þt/C32¼0. Such constraints are often called side conditions . The model yij¼m/C3þt/C3iþ1ijsubject to t/C31þt/C32¼0 can be expressed in a full-rank format by using t/C32¼/C0t/C31to obtain y1j¼m/C3þt/C31þ11jand y2j¼m/C3/C0t/C31þ1ij.",
    "d in a full-rank format by using t/C32¼/C0t/C31to obtain y1j¼m/C3þt/C31þ11jand y2j¼m/C3/C0t/C31þ1ij. The six observations can then be written in matrix form as y11 y12 y13 y21 y22 y230 BBBBBB@1 CCCCCCA¼11 1111 1/C01 1/C01 1/C010 BBBBBB@1 CCCCCCA m/C3 t/C3 1/C18/C19 þ111 112 113 121 122 1230 BBBBBB@1 CCCCCCA or y¼X /C3m/C3þ1: The matrix X/C3is full-rank, and the parameters m/C3andt/C3 1can be estimated.",
    "CCA or y¼X /C3m/C3þ1: The matrix X/C3is full-rank, and the parameters m/C3andt/C3 1can be estimated. It must be kept in mind, however, that speciﬁc constraints impose speciﬁc deﬁ- nitions on the parameters. 3. As we examine the parameters in the model illustrated in (12.4), we see some linear combinations that are unique. For example, t1/C0t2¼/C02,mþt1¼16, andmþt2¼18 remain the same for all alternative values of m,t1, andt2.",
    "example, t1/C0t2¼/C02,mþt1¼16, andmþt2¼18 remain the same for all alternative values of m,t1, andt2. Such unique linear combinations can be estimated.298 ANALYSIS-OF-VARIANCE MODELS --- Page 310 --- In the following example, we illustrate these three approaches to parameter deﬁ- nition in a simple two-way model without interaction.",
    "trate these three approaches to parameter deﬁ- nition in a simple two-way model without interaction. 12.1.2 Two-Way Model Suppose that a researcher wants to measure the effect of two different vitamins and two different methods of administering the vitamins on the weight gain of chicks. This leads to a two-way model. Let a1anda2be the effects of the two vitamins, and letb1andb2be the effects of the two methods of administration.",
    "the effects of the two vitamins, and letb1andb2be the effects of the two methods of administration. If the researcher assumes that these effects are additive (no interaction; see the last paragraph in this example for some comments on interaction), the model can be written as y11¼mþa1þb1þ111,y12¼mþa1þb2þ112, y21¼mþa2þb1þ121,y22¼mþa2þb2þ122, or as yij¼mþaiþbjþ1ij,i¼1,2,j¼1,2, (12 :5) where yijis the weight gain of the ijth chick and 1ijis a random error.",
    "þbjþ1ij,i¼1,2,j¼1,2, (12 :5) where yijis the weight gain of the ijth chick and 1ijis a random error. (To simplify exposition, we show only one replication for each vitamin–method combination.) In matrix form, the model can be expressed as y11 y12 y21 y220 BB@1 CCA¼11010 11001 10110 101010 BB@1 CCAm a1 a2 b1 b20 BBBB@1 CCCCAþ1 11 112 121 1220 BB@1 CCA(12 :6) or y¼X bþ1: In the Xmatrix, the third column is equal to the ﬁrst column minus the second column, and the ﬁfth column is equal to the ﬁrst column minus the fourth column.",
    "mn minus the second column, and the ﬁfth column is equal to the ﬁrst column minus the fourth column. Thus rank( X)¼3, and the 5 /C25 matrix X0Xdoes not have an inverse. Many of the theorems of Chapters 7 and 8 are therefore not applicable. Note that if there were replications leading to additional rows in the Xmatrix, the rank of Xwould still be 3. Since rank( X)¼3, there are only three possible unique parameters unless side conditions are imposed on the ﬁve parameters.",
    "are only three possible unique parameters unless side conditions are imposed on the ﬁve parameters. There are many ways to reparameter- ize in order to reduce to three parameters in the model.",
    "meters. There are many ways to reparameter- ize in order to reduce to three parameters in the model. For example, consider the par- ametersg1,g2, andg3deﬁned as g1¼mþa1þb1,g2¼a2/C0a1,g3¼b2/C0b1:12.1 NON-FULL-RANK MODELS 299 --- Page 311 --- The model can be written in terms of the gterms as y11¼(mþa1þb1)þ111¼g1þ111 y12¼(mþa1þb1)þ(b2/C0b1)þ112¼g1þg3þ112 y21¼(mþa1þb1)þ(a2/C0a1)þ121¼g1þg2þ121 y22¼(mþa1þb1)þ(a2/C0a1)þ(b2/C0b1)þ122¼g1þg2þg3þ122: In matrix form, this becomes y11 y12 y21 y220 BB@1 CCA¼100 101 110 1110 BB@1 CCAg1 g2 g30 @1Aþ1 11 112 121 1220 BB@1 CCA or y¼Zgþ1: (12 :7) The rank of Zis clearly 3, and we have a full-rank model for which gcan be esti- mated by ^g¼(Z0Z)/C01Z0y.",
    "nk of Zis clearly 3, and we have a full-rank model for which gcan be esti- mated by ^g¼(Z0Z)/C01Z0y. This provides estimates of g2¼a2/C0a1and g3¼b2/C0b1, which are typically of interest to the researcher. In Section 12.2.2, we will discuss methods for showing that linear functions such asmþa1þb1,a2/C0a1, andb2/C0b1are unique and estimable, even though m,a1,a2,b1,b2are not unique and not estimable. We now consider side conditions on the parameters.",
    "gh m,a1,a2,b1,b2are not unique and not estimable. We now consider side conditions on the parameters. Since rank( X)¼3 and there are ﬁve parameters, we need two (linearly independent) side conditions. If these two constraints are appropriately chosen, the ﬁve parameters become unique and thereby estimable. We denote the constrained parameters by m/C3,a/C3 i, andb/C3 jand con- sider the side conditions a/C3 1þa/C32¼0 andb/C3 1þb/C32¼0.",
    "meters by m/C3,a/C3 i, andb/C3 jand con- sider the side conditions a/C3 1þa/C32¼0 andb/C3 1þb/C32¼0. These lead to unique deﬁnitions of a/C3 iandb/C3 jas deviations from means. To show this, we start by writing the model as y11¼m11þ111,y12¼m12þ112, y21¼m21þ121,y22¼m22þ122,(12 :8) wheremij¼E(yij) is the mean weight gain with vitamin iand method j. The means are displayed in Table 12.1, and the parameters a/C3 1,a/C32,b/C3 1,b/C32are deﬁned as row ( a) and column ( b) effects.",
    "le 12.1, and the parameters a/C3 1,a/C32,b/C3 1,b/C32are deﬁned as row ( a) and column ( b) effects. The means in Table 12.1 are deﬁned as follows: /C22mi:¼mi1þmi2 2,/C22m:j¼m1jþm2j 2,/C22m::¼m11þm12þm21þm22 4:300 ANALYSIS-OF-VARIANCE MODELS --- Page 312 --- The ﬁrst row effect, a/C3 1¼/C22m1:/C0/C22m::, is the deviation of the mean for vitamin 1 from the overall mean (after treatments) and is unique. The parameters a/C3 2,b/C3 1,andb/C32are likewise uniquely deﬁned.",
    "(after treatments) and is unique. The parameters a/C3 2,b/C3 1,andb/C32are likewise uniquely deﬁned. From the deﬁnitions in Table 12.1, we obtain a/C3 1þa/C32¼/C22m1:/C0/C22m::þ/C22m2:/C0/C22m::¼/C22m1:þ/C22m2:/C02/C22m:: ¼2/C22m::/C02/C22m::¼0,(12 :9) and similarly, b/C3 1þb/C32¼0. Thus with the side conditions a/C3 1þa/C32¼0 and b/C3 1þb/C32¼0, the redeﬁned parameters are both unique and interpretable. In (12.5), it is assumed that the effects of vitamin and method are additive.",
    "que and interpretable. In (12.5), it is assumed that the effects of vitamin and method are additive. To make this notion more precise, we write the model (12.5) in terms of m/C3¼m::,a/C3 i¼/C22mi:/C0/C22m::, andb/C3 j¼/C22m:j/C0/C22m::: mij¼/C22m::þ(/C22mi:/C0/C22m::)þ(/C22m:j/C0/C22m::)þ(mij/C0/C22mi:/C0/C22m:jþ/C22m::) ¼m/C3þa/C3 iþb/C3j: The term mij/C0/C22mi:/C0/C22m:jþ/C22m::, which is required to balance the equation, is associ- ated with the interaction between vitamins and methods.",
    "required to balance the equation, is associ- ated with the interaction between vitamins and methods. In order for a/C3 iandb/C3 jto be additive effects, the interaction mij/C0/C22mi:/C0/C22m:jþ/C22m::must be zero. Interaction will be treated in Chapter 14. 12.2 ESTIMATION In this section, we consider various aspects of estimation of bin the non-full-rank model y¼Xbþ1. We do not reparameterize or impose side conditions.",
    "stimation of bin the non-full-rank model y¼Xbþ1. We do not reparameterize or impose side conditions. These two approaches to estimation are discussed in Sections 12.5 and 12.6, respectively.",
    "onditions. These two approaches to estimation are discussed in Sections 12.5 and 12.6, respectively. Normality of yis not assumed in the present section.TABLE 12.1 Means and Effects for the Model in (12.8) Columns ( b) Rows (a) 1 2 Row Means Row Effects Row 1 m11 m12 /C22m1: a/C3 1¼/C22m1:/C0/C22m:: Row 2 m21 m22 /C22m2: a/C3 2¼/C22m2:/C0/C22m:: Column means/C22m:1 /C22m:2 /C22m:: — Column effectsb/C3 1¼/C22m:1/C0/C22m::b/C32¼/C22m:2/C0/C22m:: ——12.2 ESTIMATION 301 --- Page 313 --- 12.2.1 Estimation of b Consider the model y¼Xbþ1, where E(y)¼Xb,cov(y)¼s2I, and Xisn/C2pof rank k,p/C20n.",
    "1 Estimation of b Consider the model y¼Xbþ1, where E(y)¼Xb,cov(y)¼s2I, and Xisn/C2pof rank k,p/C20n. [We will say “Xisn/C2pof rank k,p/C20n” to indicate that Xis not of full rank; that is, rank( X),pand rank( X),n. In some cases, we have k,n,p.] In this non- full-rank model, the pparameters in bare not unique. We now ascertain whether bcan be estimated.",
    "on- full-rank model, the pparameters in bare not unique. We now ascertain whether bcan be estimated. Using least-squares, we seek a value of ^bthat minimizes ^10^1¼(y/C0X^b)0(y/C0X^b): We can expand ^10^1to obtain ^10^1¼y0y/C02^b0X0yþ^b0X0X^b, (12 :10) which can be differentiated with respect to ^band set equal to 0to produce the familiar normal equations X0X^b¼X0y: (12 :11) Since Xis not full rank, X0Xhas no inverse, and (12.11) does not have a unique solution.",
    "y: (12 :11) Since Xis not full rank, X0Xhas no inverse, and (12.11) does not have a unique solution. However, X0X^b¼X0yhas (an inﬁnite number of) solutions: Theorem 12.2a. IfXisn/C2pof rank k,p/C20n, the system of equations X0X^b¼X0yis consistent. PROOF. By Theorem 2.8f, the system is consistent if and only if X0X(X0X)/C0X0y¼X0y, (12 :12) where ( X0X)/C0is any generalized inverse of X0X. By Theorem 2.8c(iii), X0X (X0X)/C0X0¼X0,and (12.12) therefore holds.",
    "any generalized inverse of X0X. By Theorem 2.8c(iii), X0X (X0X)/C0X0¼X0,and (12.12) therefore holds. (An alternative proof is suggested in Problem 12.3.) A Since the normal equations X0X^b¼X0yare consistent, a solution is given by Theorem 2.8d as ^b¼(X0X)/C0X0y, (12 :13)302 ANALYSIS-OF-VARIANCE MODELS --- Page 314 --- where ( X0X)/C0is any generalized inverse of X0X.",
    "3)302 ANALYSIS-OF-VARIANCE MODELS --- Page 314 --- where ( X0X)/C0is any generalized inverse of X0X. For a particular generalized inverse ( X0X)/C0, the expected value of ^bis E(^b)¼(X0X)/C0X0E(y) ¼(X0X)/C0X0Xb: (12 :14) Thus, ^bis an unbiased estimator of ( X0X)/C0X0Xb. Since ( X0X)/C0X0X=I,^bis not an unbiased estimator of b. The expression ( X0X)/C0X0Xbis not invariant to the choice of (X0X)/C0; that is, E(^b) is different for each choice of ( X0X)/C0.",
    "s not invariant to the choice of (X0X)/C0; that is, E(^b) is different for each choice of ( X0X)/C0. [An implication in (12.14) is that having selected a value of ( X0X)/C0, we would use that same value of (X0X)/C0in repeated sampling.] Thus, ^bin (12.13) does not estimate b. Next, we inquire as to whether there are any linear functions of ythat are unbiased estimators for the elements of b; that is, whether there exists a p/C2nmatrix Asuch that E(Ay)¼b.",
    "ed estimators for the elements of b; that is, whether there exists a p/C2nmatrix Asuch that E(Ay)¼b. If so, then b¼E(Ay)¼E[A(Xbþ1)]¼E(AXb)þAE(1)¼AXb: Since this must hold for all b,w eh a v e AX¼Ip[see (2.44)]. But by Theorem 2.4(i), rank(AX),psince the rank of Xis less than p. Hence AXcannot be equal to Ip, and there are no linear functions of the observations that yield unbiased estimators for the elements of b. Example 12.2.1. Consider the model yij¼mþtiþ1ij;i¼1,2;j¼1,2,3i n (12.2).",
    "tors for the elements of b. Example 12.2.1. Consider the model yij¼mþtiþ1ij;i¼1,2;j¼1,2,3i n (12.2). The matrix Xand the vector bare given in (12.3) as X¼110110110 101 101 1010 BBBBBB@1 CCCCCCA, b¼m t1 t20 @1A: By Theorem 2.2c(i), we obtain X 0X¼633 3303030 @1A: By Corollary 1 to Theorem 2.8b, a generalized inverse of X 0Xis given by (X0X)/C0¼000 01 30 00130 @1A:12.2 ESTIMATION 303 --- Page 315 --- The vector X0yis given by X0y¼111111 111000 0001110 @1Ay 11 y12 y13 y21 y22 y230 BBBBBB@1 CCCCCCA¼y :: y1: y2:0 @1A, where y ::¼P2 i¼1P3 j¼1yijandyi:¼P3 j¼1yij.",
    "y12 y13 y21 y22 y230 BBBBBB@1 CCCCCCA¼y :: y1: y2:0 @1A, where y ::¼P2 i¼1P3 j¼1yijandyi:¼P3 j¼1yij. Then ^b¼(X0X)/C0X0y¼000 01 30 001 30 @1Ay :: y1: y2:0@1A¼0 /C22y 1: /C22y2:0@1A, where /C22y i:¼P3 j¼1yij=3¼yi:=3. To ﬁnd E(^b), we need E(/C22yi:). Since E(1)¼0,w eh a v e E(1ij)¼0. Then E(/C22yi:)¼EX3 j¼1yij=3 !",
    ". To ﬁnd E(^b), we need E(/C22yi:). Since E(1)¼0,w eh a v e E(1ij)¼0. Then E(/C22yi:)¼EX3 j¼1yij=3 ! ¼1 3P3 j¼1E(yij) ¼13P3 j¼1E(mþtiþ1ij)¼13(3mþ3tiþ0) ¼mþti: Thus E(^b)¼0 mþt1 mþt20 @1A: The same result is obtained in (12.14): E(^ b)¼(X0X)/C0X0X^b ¼000 01 30 001 30 B@1 CA633 330 3030 B@1 CAm t1 t20 B@1 CA ¼0 mþt1 mþt20 B@1 CA: A304 ANALYSIS-OF-VARIANCE MODELS --- Page 316 --- 12.2.2 Estimable Functions of b Having established that we cannot estimate b, we next inquire as to whether we can estimate any linear combination of the b’s, say,l0b.",
    "stimate b, we next inquire as to whether we can estimate any linear combination of the b’s, say,l0b. For example, in Section 12.1.1, we considered the model yij¼mþtiþ1ij,i¼1,2, and found that m,t1, andt2in b¼(m,t1,t2)0are not unique but that the linear function t1/C0t2¼(0,1,/C01)bis unique. In order to show that functions such as t1/C0t2can be estimated, we ﬁrst give a deﬁnition of an estimable function l0b.",
    "at functions such as t1/C0t2can be estimated, we ﬁrst give a deﬁnition of an estimable function l0b. A linear function of parameters l0bis said to be estimable if there exists a linear combination of the observations with an expected value equal to l0b; that is,l0bis estimable if there exists a vector asuch that E(a0y)¼l0b. In the following theorem we consider three methods for determining whether a particular linear function l0bis estimable. Theorem 12.2b.",
    "r three methods for determining whether a particular linear function l0bis estimable. Theorem 12.2b. In the model y¼Xbþ1,w h e r e E(y)¼Xband Xisn/C2p of rank k,p/C20n, the linear function l0bis estimable if and only if any one of the following equivalent conditions holds: (i)l0is a linear combination of the rows of X; that is, there exists a vector asuch that a0X¼l0: (12 :15) (ii)l0is a linear combination of the rows of X0Xorlis a linear combination of the columns of X0X, that is, there exists a vector rsuch that r0X0X¼l0orX0Xr¼l: (12 :16) (iii)lorl0is such that X0X(X0X)/C0l¼lorl0(X0X)/C0X0X¼l0, (12 :17) where ( X0X)/C0is any (symmetric) generalized inverse of X0X.",
    "X0X)/C0l¼lorl0(X0X)/C0X0X¼l0, (12 :17) where ( X0X)/C0is any (symmetric) generalized inverse of X0X. PROOF. For (ii) and (iii), we prove the “if” part. For (i), we prove both “if” and “only if.” (i) If there exists a vector asuch that l0¼a0X, then, using this vector a,w eh a v e E(a0y)¼a0E(y)¼a0Xb¼l0b: Conversely, if l0bis estimable, then there exists a vector asuch that E(a0y)¼l0b.",
    ")¼a0E(y)¼a0Xb¼l0b: Conversely, if l0bis estimable, then there exists a vector asuch that E(a0y)¼l0b. Thus a0Xb¼l0b, which implies, among other things, that a0X¼l0.12.2 ESTIMATION 305 --- Page 317 --- (ii) If there exists a solution rforX0Xr¼l, then, by deﬁning a¼Xr, we obtain E(a0y)¼E(r0X0y)¼r0X0E(y) ¼r0X0Xb¼l0b: (iii) If X0X(X0X)/C0l¼l, then ( X0X)/C0lis a solution to X0Xr¼lin part(ii).",
    "y)¼r0X0E(y) ¼r0X0Xb¼l0b: (iii) If X0X(X0X)/C0l¼l, then ( X0X)/C0lis a solution to X0Xr¼lin part(ii). (For proof of the converse, see Problem 12.4.) A We illustrate the use of Theorem 12.2b in the following example. Example 12.2.2a. For the model yij¼mþtiþ1ij;i¼1,2;j¼1,2,3i n Example 12.2.1, the matrix Xand the vector bare given as X¼110 110 110 101 101 1010 BBBBBB@1 CCCCCCA, b¼m t1 t20 @1A: We noted in Section 12.1.1 that t1/C0t2is unique.",
    "110 101 101 1010 BBBBBB@1 CCCCCCA, b¼m t1 t20 @1A: We noted in Section 12.1.1 that t1/C0t2is unique. We now show that t1/C0t2¼ (0,1,/C01)b¼l0bis estimable, using all three conditions of Theorem 12.2b. (i) To ﬁnd a vector asuch that a0X¼l0¼(0,1,/C01), consider a0¼ (0,0,1,/C01,0,0), which gives a0X¼(0,0,1,/C01,0,0)X¼(1,1,0)/C0(1,0,1) ¼(0,1,/C01)¼l0: There are many other choices for a, of course, that will yield a0X¼l0, for example a0¼(1,0,0,0,0,/C01) or a0¼(2,/C01,0,0,1,/C02).",
    "for a, of course, that will yield a0X¼l0, for example a0¼(1,0,0,0,0,/C01) or a0¼(2,/C01,0,0,1,/C02). Note that we can likewise obtain l0bfrom E(y): l0b¼a0Xb¼a0E(y)¼(0,0,1,/C01,0,0)E(y) ¼(0,0,1,/C01,0,0)E(y11) E(y12) E(y13) E(y21) E(y22) E(y23)0 BBBBBBBB@1 CCCCCCCCA ¼E(y 13)/C0E(y21)¼mþt1/C0(mþt2)¼t1/C0t2:306 ANALYSIS-OF-VARIANCE MODELS --- Page 318 --- (ii) The matrix X0Xis given in Example 12.2.1 as X0X¼633 330 3030 @1A: To ﬁnd a vector rsuch that X 0Xr¼l¼(0,1,/C01)0, consider r¼(0,1 3,/C013)0, which gives X0Xr¼633 330 3030 @1A0 1 3 /C01 30 B@1 CA¼0 1 /C010 @1A¼ l: There are other possible values of r, of course, such as r¼(/C01 3,23,0)0.",
    "B@1 CA¼0 1 /C010 @1A¼ l: There are other possible values of r, of course, such as r¼(/C01 3,23,0)0. (iii) Using the generalized inverse ( X0X)/C0¼diag(0 ,1 3,13) given in Example 12.2.1, the product X0X(X0X)/C0becomes X0X(X0X)/C0¼011 010 0010 @1A: Then, for l¼(0,1,/C01)0, we see that X0X(X0X)/C0l¼lin (12.17) holds: 011 010 0010 B@1 CA0 1 /C010 B@1 CA¼0 1 /C010 B@1 CA: A A set of functions l0 1b,l02b,...,l0mbis said to be linearly independent if the coefﬁcient vectors l1,l2,...,lmare linearly independent [see (2.40)].",
    "be linearly independent if the coefﬁcient vectors l1,l2,...,lmare linearly independent [see (2.40)]. The number of linearly independent estimable functions is given in the next theorem. Theorem 12.2c. In the non-full-rank model y¼Xbþ1, the number of linearly independent estimable functions of bis the rank of X. PROOF. See Graybill (1976, pp. 485–486). A From Theorem 12.2b(i), we see that x0 ibis estimable for i¼1,2,...,n, where x0i is the ith row of X.",
    "From Theorem 12.2b(i), we see that x0 ibis estimable for i¼1,2,...,n, where x0i is the ith row of X. Thus every row (element) of Xbis estimable, and Xbitself can be said to be estimable. Likewise, from Theorem 12.2b(ii), every row (element) of X0Xb is estimable, and X0Xbis therefore estimable. Conversely, all estimable functions can be obtained from XborX0Xb: Thus we can examine linear combinations of the rows of Xor ofX0Xto see what functions of the parameters are estimable.",
    "e linear combinations of the rows of Xor ofX0Xto see what functions of the parameters are estimable. In the following example, we illustrate the12.2 ESTIMATION 307 --- Page 319 --- use of linear combinations of the rows of Xto obtain a set of estimable functions of the parameters. Example 12.2.2b.",
    "binations of the rows of Xto obtain a set of estimable functions of the parameters. Example 12.2.2b. Consider the model in (12.6) in Section 12.1.2 with X¼11010 11001 10110 101010 BB@1 CCA,b¼m a1 a2 b1 b20 BBBB@1 CCCCA: To examine what is estimable, we take linear combinations a 0Xof the rows of Xto obtain three linearly independent rows. For example, if we subtract the ﬁrst row of X from the third row and multiply by b, we obtain (0 /C01100 )b¼/C0a1þa2, which involves only the a’s.",
    "from the third row and multiply by b, we obtain (0 /C01100 )b¼/C0a1þa2, which involves only the a’s. Subtracting the ﬁrst row of Xfrom the third row can be expressed as a0X¼(/C01010 ) X¼/C0x0 1þx03,w h e r e x01andx03are the ﬁrst and third rows of X.",
    "an be expressed as a0X¼(/C01010 ) X¼/C0x0 1þx03,w h e r e x01andx03are the ﬁrst and third rows of X. Subtracting the ﬁrst row from each succeeding row in Xgives 11 01 0 00 0 /C011 0/C011 00 0/C011 /C0110 BB@1 CCA: Subtracting the second and third rows from the fourth row of this matrix yields 11 01 0 00 0 /C011 0/C011 00 00 00 00 BB@1 CCA: Multiplying the ﬁrst three rows byb, we obtain the three linearly independent esti- mable functions l0 1b¼mþa1þb1,l02b¼b2/C0b1,l03b¼a2/C0a1: These functions are identical to the functions g1,g2, andg3used in Section 12.1.2 to reparameterize to a full-rank model.",
    "dentical to the functions g1,g2, andg3used in Section 12.1.2 to reparameterize to a full-rank model. Thus, in that example, linearly independent esti- mable functions of the parameters were used as the new parameters. In Example 12.2.2.b, the two estimable functions b2/C0b1anda2/C0a1are such that the coefﬁcients of the b’s or of the a’s sum to zero.",
    "mable functions b2/C0b1anda2/C0a1are such that the coefﬁcients of the b’s or of the a’s sum to zero. A linear combination of this type is called a contrast .308 ANALYSIS-OF-VARIANCE MODELS --- Page 320 --- 12.3 ESTIMATORS 12.3.1 Estimators of l0b From Theorem 12.2b(i) and (ii) we have the estimators a0yandr0X0yforl0b, where a0andr0satisfyl0¼a0Xandl0¼r0X0X, respectively. A third estimator of l0bis l0^b, where ^bis a solution of X0X^b¼X0y.",
    "0¼a0Xandl0¼r0X0X, respectively. A third estimator of l0bis l0^b, where ^bis a solution of X0X^b¼X0y. In the following theorem, we discuss some properties of r0X0yandl0^b. We do not discuss the estimator a0ybecause it is not guaranteed to have minimum variance (see Theorem 12.3d). Theorem 12.3a. Letl0bbe an estimable function of bin the model y¼Xbþ1, where E(y)¼XbandXisn/C2pof rank k,p/C20n.L e t ^bbe any solution to the normal equations X0X^b¼X0y, and let rbe any solution to X0Xr¼l.",
    "/C20n.L e t ^bbe any solution to the normal equations X0X^b¼X0y, and let rbe any solution to X0Xr¼l. Then the two estimators l0^bandr0X0yhave the following properties: (i)E(l0^b)¼E(r0X0y)¼l0b. (ii)l0^bis equal to r0X0yfor any ^bor any r. (iii)l0^bandr0X0yare invariant to the choice of ^borr.",
    "(ii)l0^bis equal to r0X0yfor any ^bor any r. (iii)l0^bandr0X0yare invariant to the choice of ^borr. PROOF (i) By (12.14) E(l0^b)¼l0E(^b)¼l0(X0X)/C0X0Xb: By Theorem 12.2b(iii), l0(X0X)/C0X0X¼l0, and E(l0^b) becomes E(l0^b)¼l0b: By Theorem 12.2b(ii) E(r0X0y)¼r0X0E(y)¼r0X0Xb¼l0b: (ii) By Theorem 12.2b(ii), if l0bis estimable, l0¼r0X0Xfor some r.",
    "ii) E(r0X0y)¼r0X0E(y)¼r0X0Xb¼l0b: (ii) By Theorem 12.2b(ii), if l0bis estimable, l0¼r0X0Xfor some r. Multiplying the normal equations X0X^b¼X0ybyr0gives r0X0X^b¼r0X0y: Since r0X0X¼l0,w eh a v e l0^b¼r0X0y: (iii) To show that r0X0yis invariant to the choice of r, let r1andr2be such that X0Xr1¼X0Xr2¼l. Then r0 1X0X^b¼r01X0yand r02X0X^b¼r02X0y:12.3 ESTIMATORS 309 --- Page 321 --- Since r0 1X0X¼r02X0X,w eh a v e r01X0y¼r02X0y. It is clear that each is equal to l0^b.",
    "Page 321 --- Since r0 1X0X¼r02X0X,w eh a v e r01X0y¼r02X0y. It is clear that each is equal to l0^b. (For a direct proof that l0^bis invariant to the choice of ^b, see Problem 12.6.) A We illustrate the estimators r0X0yandl^bin the following example. Example 12.3.1. The linear function l0b¼t1/C0t2was shown to be estimable in Example 12.2.2a.",
    "xample. Example 12.3.1. The linear function l0b¼t1/C0t2was shown to be estimable in Example 12.2.2a. To estimate t1/C0t2withr0X0y, we use r0¼(0,1 3,/C013)f r o m Example 12.2.2a to obtain r0X0y¼0,1 3,/C013/C0/C1111111 111000 0001110 B@1 CAy11 y12 y13 y21 y22 y230 BBBBBBBB@1 CCCCCCCCA ¼0, 1 3,/C013/C0/C1y:: y1: y2:0 B@1 CA¼y1: 3/C0y2: 3¼/C22y1:/C0/C22y2:, where y::¼P2 i¼1P3 j¼1yij,yi:¼P3 j¼1yij, and /C22yi:¼yi:=3¼P3 j¼1yij=3.",
    "3/C0y2: 3¼/C22y1:/C0/C22y2:, where y::¼P2 i¼1P3 j¼1yij,yi:¼P3 j¼1yij, and /C22yi:¼yi:=3¼P3 j¼1yij=3. To obtain the same result using l0^b, we ﬁrst ﬁnd a solution to the normal equations X0X^b¼X0y 633 330 3030 @1A^ m ^t1 ^t20@1A¼y :: y1: y2:0@1A or 6^ mþ3^t1þ3^t2¼y:: 3^mþ3^t1¼y1: 3^mþ3^t2¼y2:: The ﬁrst equation is redundant since it is the sum of the second and third equations.",
    "3^mþ3^t2¼y2:: The ﬁrst equation is redundant since it is the sum of the second and third equations. We can take ^mto be an arbitrary constant and obtain ^t1¼1 3y1:/C0^m¼/C22y1:/C0^m,^t2¼13y2:/C0^m¼/C22y2:/C0^m: Thus ^b¼^m ^t1 ^t20 @1A¼0 /C22y 1: /C22y2:0@1Aþ^ m1 /C01 /C010@1A:310 ANALYSIS-OF-VARIANCE MODELS --- Page 322 --- To estimate t1/C0t2¼(0,1,/C01)b¼l0b, we can set ^m¼0 to obtain ^b¼(0,/C22y1:,/C22y2:)0andl0^b¼/C22y1:/C0/C22y2:.I fw el e a v e ^marbitrary, we likewise obtain l0^b¼(0,1,/C01)^m /C22y1:/C0^m /C22y2:/C0^m0 B@1 CA ¼/C22y1:/C0^m/C0(/C22y2:/C0^m)¼/C22y1:/C0/C22y2::A Since ^b¼(X0X)/C0X0yis not unique for the non-full-rank model y¼Xbþ1with cov(y)¼s2I, it does not have a unique covariance matrix.",
    "ique for the non-full-rank model y¼Xbþ1with cov(y)¼s2I, it does not have a unique covariance matrix. However, for a particular (symmetric) generalized inverse ( X0X)/C0, we can use Theorem 3.6d(i) to obtain the following covariance matrix: cov( ^b)¼cov[(X0X)/C0X0y] ¼(X0X)/C0X0(s2I)X[(X0X)/C0]0 ¼s2(X0X)/C0X0X(X0X)/C0: (12 :18) The expression in (12.18) is not invariant to the choice of ( X0X)/C0. The variance of l0^bor ofr0X0yis given in the following theorem. Theorem 12.3b.",
    "choice of ( X0X)/C0. The variance of l0^bor ofr0X0yis given in the following theorem. Theorem 12.3b. Letl0bbe an estimable function in the model y¼Xbþ1, where Xisn/C2pof rank k,p/C20nand cov( y)¼s2I. Let rbe any solution to X0Xr¼l, and let ^bbe any solution to X0X^b¼X0y. Then the variance of l0^borr0X0yhas the following properties: (i) var( r0X0y)¼s2r0X0Xr¼s2r0l. (ii) var(l0^b)¼s2l0(X0X)/C0l. (iii) var(l0^b) is unique, that is, invariant to the choice of ror (X0X)/C0.",
    "ar(l0^b)¼s2l0(X0X)/C0l. (iii) var(l0^b) is unique, that is, invariant to the choice of ror (X0X)/C0. PROOF (i) var(r0X0y)¼r0X0cov(y)Xr [by (3 :42)] ¼r0X0(s2I)Xr¼s2r0X0Xr ¼s2r0l: [by (12 :16)] : (ii) var(l0^b)¼l0cov( ^b)l ¼s2l0(X0X)/C0X0X(X0X)/C0l [by (12 :18)] :12.3 ESTIMATORS 311 --- Page 323 --- By (12.17), l0(X0X)/C0X0X¼l0, and therefore var(l0^b)¼s2l0(X0X)/C0l: (iii) To show that r0lis invariant to r, let r1andr2be such that X0Xr1¼land X0Xr2¼l.",
    "¼s2l0(X0X)/C0l: (iii) To show that r0lis invariant to r, let r1andr2be such that X0Xr1¼land X0Xr2¼l. Multiplying these two equations by r0 2andr01, we obtain r0 2X0Xr1¼r02land r01X0Xr2¼r01l: The left sides of these two equations are equal since they are scalars and are transposes of each other. Therefore the right sides are also equal: r0 2l¼r01l: To show that l0(X0X)/C0lis invariant to the choice of X0X/C0, letG1andG2be two generalized inverses of X0X.",
    "that l0(X0X)/C0lis invariant to the choice of X0X/C0, letG1andG2be two generalized inverses of X0X. Then by Theorem 2.8c(v), we have XG 1X0¼XG 2X0: Multiplying both sides by asuch that a0X¼l0[see Theorem 12.2b(i)], we obtain a0XG 1X0a¼a0XG 2X0a, l0G1l¼l0G2l: A The covariance of the estimators of two estimable functions is given in the follow- ing theorem. Theorem 12.3c.",
    "nce of the estimators of two estimable functions is given in the follow- ing theorem. Theorem 12.3c. Ifl0 1bandl02bare two estimable functions in the model y¼Xbþ1, where Xisn/C2pof rank k,p/C20nand cov( y)¼s2I, the covariance of their estimators is given by cov(l0 1^b,l02^b)¼s2r01l2¼s2l01r2¼s2l01(X0X)/C0l2, where X0Xr1¼l1andX0Xr2¼l2. PROOF. See Problem 12.12.",
    "(l0 1^b,l02^b)¼s2r01l2¼s2l01r2¼s2l01(X0X)/C0l2, where X0Xr1¼l1andX0Xr2¼l2. PROOF. See Problem 12.12. A The estimators l0^bandr0X0yhave an optimality property analogous to that in Corollary 1 to Theorem 7.3d.312 ANALYSIS-OF-VARIANCE MODELS --- Page 324 --- Theorem 12.3d. Ifl0bis an estimable function in the model y¼Xbþ1, where X isn/C2pof rank k,p/C20n, then the estimators l0^bandr0X0yare BLUE. PROOF.",
    "the model y¼Xbþ1, where X isn/C2pof rank k,p/C20n, then the estimators l0^bandr0X0yare BLUE. PROOF. Let a linear estimator of l0bbe denoted by a0y, where without loss of general- itya0y¼r0X0yþc0y, that is, a0¼r0X0þc0,w h e r e r0is a solution to l0¼r0X0X.F o r unbiasedness we must have l0b¼E(a0y)¼a0Xb¼r0X0Xbþc0Xb¼(r0X0Xþc0X)b: This must hold for all b, and we therefore have l0¼r0X0Xþc0X: Sincel0¼r0X0X, it follows that c0X¼00.",
    "This must hold for all b, and we therefore have l0¼r0X0Xþc0X: Sincel0¼r0X0X, it follows that c0X¼00. Using (3.42) and c0X¼00, we obtain var(a0y)¼a0cov(y)a¼a0s2Ia¼s2a0a ¼s2(r0X0þc0)(Xrþc) ¼s2(r0X0Xrþr0X0cþc0Xrþc0c) ¼s2(r0X0Xrþc0c): Therefore, to minimize var( a0y), we must minimize c0c¼P ic2 i. This is a minimum when c¼0, which is compatible with c0X¼00.",
    "ar( a0y), we must minimize c0c¼P ic2 i. This is a minimum when c¼0, which is compatible with c0X¼00. Hence a0is equal to r0X0, and the BLUE for the estimable function l0bisa0y¼r0X0y: A 12.3.2 Estimation of s2 By analogy with (7.23), we deﬁne SSE¼(y/C0X^b)0(y/C0X^b), (12 :19) where ^bis any solution to the normal equations X0X^b¼X0y.",
    "e deﬁne SSE¼(y/C0X^b)0(y/C0X^b), (12 :19) where ^bis any solution to the normal equations X0X^b¼X0y. Two alternative expressions for SSE are SSE¼y0y/C0^b0X0y, (12 :20) SSE¼y0[I/C0X(X0X)/C0X0]y: (12 :21)12.3 ESTIMATORS 313 --- Page 325 --- For an estimator of s2, we deﬁne s2¼SSE n/C0k, (12 :22) where nis the number of rows of Xandk¼rank(X). Two properties of s2are given in the following theorem. Theorem 12.3e.",
    "ber of rows of Xandk¼rank(X). Two properties of s2are given in the following theorem. Theorem 12.3e. Fors2deﬁned in (12.22) for the non-full-rank model, we have the following properties: (i)E(s2)¼s2. (ii)s2is invariant to the choice of ^bor to the choice of generalized inverse (X0X)/C0. PROOF (i) Using (12.21), we have E(SSE) ¼E{y0[I/C0X(X0X)/C0X0]y}.",
    "e of generalized inverse (X0X)/C0. PROOF (i) Using (12.21), we have E(SSE) ¼E{y0[I/C0X(X0X)/C0X0]y}. By Theorem 5.2a, this becomes E(SSE) ¼tr{[I/C0X(X0X)/C0X0](s2I)}þb0X0[I/C0X(X0X)/C0X0]Xb: It can readily be shown that the second term on the right side vanishes. For the ﬁrst term, we have, by Theorem 2.11(i), (ii), and (viii) s2tr[I/C0X(X0X)/C0X0]¼s2{tr(I)/C0tr[X0X(X0X)/C0]} ¼(n/C0k)s2, where k¼rank(X0X)¼rank(X).",
    "and (viii) s2tr[I/C0X(X0X)/C0X0]¼s2{tr(I)/C0tr[X0X(X0X)/C0]} ¼(n/C0k)s2, where k¼rank(X0X)¼rank(X). (ii) Since Xbis estimable, X^bis invariant to ^b[see Theorem 12.3a(iii)], and there- fore SSE ¼(y/C0X^b)0(y/C0X^b) in (12.19) is invariant. To show that SSE in (12.21) is invariant to choice of ( X0X)/C0, we note that X(X0X)/C0X0is invariant by Theorem 2.8c(v).",
    "12.21) is invariant to choice of ( X0X)/C0, we note that X(X0X)/C0X0is invariant by Theorem 2.8c(v). A 12.3.3 Normal Model For the non-full-rank model y¼Xbþ1, we now assume that yisNn(Xb,s2I)o r1isNn(0,s2I): With the normality assumption we can obtain maximum likelihood estimators.314 ANALYSIS-OF-VARIANCE MODELS --- Page 326 --- Theorem 12.3f.",
    "obtain maximum likelihood estimators.314 ANALYSIS-OF-VARIANCE MODELS --- Page 326 --- Theorem 12.3f. IfyisNn(Xb,s2I), where Xisn/C2pof rank k,p/C20n, then the maximum likelihood estimators for bands2are given by ^b¼(X0X)/C0X0y, (12 :23) ^s2¼1 n(y/C0X^b)0(y/C0X^b): (12 :24) PROOF.",
    "timators for bands2are given by ^b¼(X0X)/C0X0y, (12 :23) ^s2¼1 n(y/C0X^b)0(y/C0X^b): (12 :24) PROOF. For the non-full-rank model, the likelihood function L(b,s2) and its logar- ithm lnL(b,s2) can be written in the same form as those for the full-rank model in (7.50) and (7.51): L(b,s2)¼1 (2ps2)n=2e/C0(y/C0Xb)0(y/C0Xb)=2s2, (12 :25) lnL(b,s2)¼/C0n 2ln(2p)/C0n2ln s2/C01 2s2(y/C0Xb)0(y/C0Xb): (12 :26) Differentiation of lnL(b,s2) with respect to bands2and setting the results equal to zero gives X0X^b¼X0y, (12 :27) ^s2¼1n(y/C0X^ b)0(y/C0X^b), (12 :28) where ^bin (12.28) is any solution to (12.27).",
    "^b¼X0y, (12 :27) ^s2¼1n(y/C0X^ b)0(y/C0X^b), (12 :28) where ^bin (12.28) is any solution to (12.27). If ( X0X)/C0is any generalized inverse of X0X, a solution to (12.27) is given by ^b¼(X0X)/C0X0y: (12 :29) A The form of the maximum likelihood estimator ^bin (12.29) is the same as that of the least-squares estimator in (12.13). The estimator ^s2is biased. We often use the unbiased estimator s2given in (12.22).",
    "ator in (12.13). The estimator ^s2is biased. We often use the unbiased estimator s2given in (12.22). The mean vector and covariance matrix for ^bare given in (12.14) and (12.18) as E(^b)¼(X0X)/C0X0Xband cov( ^b)¼s2(X0X)/C0X0X(X0X)/C0. In the next theorem, we give some additional properties of ^bands2. Note that some of these follow because ^b¼(X0X)/C0X0yis a linear function of the observations.12.3 ESTIMATORS 315 --- Page 327 --- Theorem 12.3g.",
    "X)/C0X0yis a linear function of the observations.12.3 ESTIMATORS 315 --- Page 327 --- Theorem 12.3g. IfyisNn(Xb,s2I), where Xisn/C2pof rank k,p/C20n, then the maximum likelihood estimators ^bands2(corrected for bias) have the following properties: (i)^bisNp[(X0X)/C0X0Xb,s2(X0X)/C0X0X(X0X)/C0]. (ii) ( n/C0k)s2=s2isx2(n/C0k). (iii) ^bands2are independent. PROOF. Adapting the proof of Theorem 7.6b for the non-full-rank case yields the desired results.",
    "nt. PROOF. Adapting the proof of Theorem 7.6b for the non-full-rank case yields the desired results. A The expected value, covariance matrix, and distribution of ^bin Theorem 12.3g are valid only for a particular value of ( X0X)/C0, whereas, s2is invariant to the choice of ^b or (X0X)/C0[see Theorem 12.3e(ii)]. The following theorem is an adaptation of Corollary 1 to Theorem 7.6d. Theorem 12.3h.",
    "em 12.3e(ii)]. The following theorem is an adaptation of Corollary 1 to Theorem 7.6d. Theorem 12.3h. IfyisNn(Xb,s2I), where Xisn/C2pof rank k,p/C20n, and ifl0b is an estimable function, then l0^bhas minimum variance among all unbiased estimators. A In Theorem 12.3d, the estimator l0^bwas shown to have minimum variance among alllinear unbiased estimators. With the normality assumption added in Theorem 12.3g,l^bhas minimum variance among all unbiased estimators.",
    "e normality assumption added in Theorem 12.3g,l^bhas minimum variance among all unbiased estimators. 12.4 GEOMETRY OF LEAST-SQUARES IN THE OVERPARAMETERIZED MODEL The geometric approach to least-squares in the overparameterized model is similar to that for the full-rank model (Section 7.4), but there are crucial differences. The approach involves two spaces, a p-dimensional parameter space and an n-dimensional data space.",
    ". The approach involves two spaces, a p-dimensional parameter space and an n-dimensional data space. The unknown parameter vector bis an element of the parameter space with axes corresponding to the coefﬁcients, and the known data vector yis an element of the data space with axes corresponding to the observations (Fig. 12.1).",
    "ata vector yis an element of the data space with axes corresponding to the observations (Fig. 12.1). The n/C2ppartitioned Xmatrix of the overparameterized linear model (Section 12.2.1) is X¼(x1,x2,...,xp): The columns of Xare vectors in the data space, but since rank( X)¼k,p, the set of vectors is not linearly independent. Nonetheless, the set of all possible linear combi- nations of these column vectors constitutes the prediction space.",
    "set of all possible linear combi- nations of these column vectors constitutes the prediction space. The distinctive316 ANALYSIS-OF-VARIANCE MODELS --- Page 328 --- geometric characteristic of the overparameterized model is that the prediction space is of dimension k,pwhile the parameter space is of dimension p. Thus the product Xu, where uis any vector in the parameter space, deﬁnes a many-to-one relationship between the parameter space and the prediction space (Fig. 12.1).",
    "deﬁnes a many-to-one relationship between the parameter space and the prediction space (Fig. 12.1). An inﬁnite numberof vectors in the parameter space correspond to any particular vector in the prediction space. As was the case for the full-rank linear model, the overparameterized linear model states that yis equal to a vector in the prediction space, E(y)¼X b, plus a vector of random errors 1. Neither bnor1is known.",
    "a vector in the prediction space, E(y)¼X b, plus a vector of random errors 1. Neither bnor1is known. Geometrically, least-squares estimation for the overparametrized model is the process of ﬁnding a sensible guess of E(y)i n the prediction space and then determining the subset of the parameter space that is associated with this guess (Fig. 12.1). As in the full-rank model, a reasonable geometric idea is to estimate E(y) using ^y, the unique point in the prediction space that is closest to y.",
    "ic idea is to estimate E(y) using ^y, the unique point in the prediction space that is closest to y. This implies that the differ- ence vector ^1¼y/C0^ymust be orthogonal to the prediction space, and thus we seek ^y such that X0^1¼0, which leads to the normal equations X0X^b¼X0y: However, these equations do not have a single solution since X0Xis not full-rank.",
    "uations X0X^b¼X0y: However, these equations do not have a single solution since X0Xis not full-rank. Using Theorem 2.8e(ii), all possible solutions to this system of equations are given by^b¼(X0X)/C0X0yusing all possible values of ( X0X)/C0. These solutions constitute an inﬁnite subset of the parameter space (Fig.",
    "sible values of ( X0X)/C0. These solutions constitute an inﬁnite subset of the parameter space (Fig. 12.1), but this subset is not a subspace.Figure 12.1 A geometric view of least-squares estimation in the overparameterized model.12.4 GEOMETRY OF LEAST-SQUARES IN THE OVERPARAMETERIZED MODEL 317 --- Page 329 --- Since the solutions are inﬁnite in number, none of the ^bvalues themselves have any meaning.",
    "329 --- Since the solutions are inﬁnite in number, none of the ^bvalues themselves have any meaning. Nonetheless, ^y¼X^bis unique [see Theorem 2.8c(v)], and therefore, to be unambiguous, all further inferences must be restricted to linear functions of X^brather than of ^b. Also note that the nrows of Xgenerate a k-dimensional subspace of p-dimensional space. The matrix products of the row vectors in this space with bconstitute the set of all possible estimable functions.",
    "ducts of the row vectors in this space with bconstitute the set of all possible estimable functions. The matrix products of the row vectors in this space with any ^b(these products are invariant to the choice of a generalized inverse) con- stitute the unambiguous set of corresponding estimates of these functions. Finally, ^1¼y/C0X^b¼(I/C0H)ycan be taken as an unambiguous predictor of 1.",
    "mates of these functions. Finally, ^1¼y/C0X^b¼(I/C0H)ycan be taken as an unambiguous predictor of 1. Since ^1is now a vector in ( n/C0k)-dimensional space, it seems reasonable to estimate s2as the squared length (2.22) of ^1divided by n/C0k. In other words, a sensible esti- mator ofs2iss2¼y0(I/C0H)y=(n/C0k), which is equal to (12.22). 12.5 REPARAMETERIZATION Reparameterization was deﬁned and illustrated in Section 12.1.1.",
    "to (12.22). 12.5 REPARAMETERIZATION Reparameterization was deﬁned and illustrated in Section 12.1.1. We now formalize and extend this approach to obtaining a model based on estimable parameters. In reparameterization, we transform the non-full-rank model y¼Xbþ1, where Xisn/C2pof rank k,p/C20n, to the full-rank model y¼Zgþ1, where Zisn/C2kof rank kandg¼Ubis a set of klinearly independent estimable functions of b. Thus Zg¼Xb, and we can write Zg¼ZUb¼Xb, (12 :30) where X¼ZU.",
    "y independent estimable functions of b. Thus Zg¼Xb, and we can write Zg¼ZUb¼Xb, (12 :30) where X¼ZU. Since Uisk/C2pof rank k,p, the matrix UU0is nonsingular by Theorem 2.4(iii), and we can multiply ZU¼XbyU0to solve for Zin terms of X andU: ZUU0¼XU0 Z¼XU0(UU0)/C01: (12 :31) To establish that Zis full-rank, note that rank( Z)/C21rank(ZU)¼rank(X)¼kby Theorem 2.4(i). However, Zcannot have rank greater than ksince Zhas k columns. Thus rank( Z)¼k, and the model y¼Zgþ1is a full-rank model.",
    "rank greater than ksince Zhas k columns. Thus rank( Z)¼k, and the model y¼Zgþ1is a full-rank model. We can therefore use the theorems of Chapters 7 and 8; for example, the normal equations Z0Z^g¼Z0yhave the unique solution ^g¼(Z0Z)/C01Z0y.",
    "apters 7 and 8; for example, the normal equations Z0Z^g¼Z0yhave the unique solution ^g¼(Z0Z)/C01Z0y. In the reparameterized full-rank model y¼Zgþ1, the unbiased estimator of s2 is given by s2¼1 n/C0k(y/C0Z^g)0(y/C0Z^g): (12 :32)318 ANALYSIS-OF-VARIANCE MODELS --- Page 330 --- Since Zg¼Xb, the estimators Z^gandX^bare also equal Z^g¼X^b, and SSE in (12.19) and SSE in (12.32) are the same: (y/C0X^b)0(y/C0X^b)¼(y/C0Z^g)0(y/C0Z^g): (12 :33) The set Ub¼gis only one possible set of linearly independent estimable func- tions.",
    "C0Z^g): (12 :33) The set Ub¼gis only one possible set of linearly independent estimable func- tions. Let Vb¼dbe another set of linearly independent estimable functions. Then there exists a matrix Wsuch that y¼Wdþ1. Now an estimable function l0bcan be expressed as a function of gor ofd: l0b¼b0g¼c0d: (12 :34) Hence dl0b¼b0^g¼c0^d, and either reparameterization gives the same estimator of l0b. Example 12.5. We illustrate a reparameterization for the model yij¼mþtiþ 1ij,i¼1,2,j¼1,2.",
    "or of l0b. Example 12.5. We illustrate a reparameterization for the model yij¼mþtiþ 1ij,i¼1,2,j¼1,2. In matrix form, the model can be written as y¼Xbþ1¼110 110 101 1010 BB@1 CCAm t1 t20 @1Aþ1 11 112 121 1220BB@1 CCA: Since Xhas rank 2, there exist two linearly independent estimable functions (see Theorem 12.2c). We can choose these in many ways, one of which is mþt1and mþt2.",
    "mable functions (see Theorem 12.2c). We can choose these in many ways, one of which is mþt1and mþt2. Thus g¼g1 g2/C18/C19 ¼mþt1 mþt2/C18/C19 ¼110 101/C18/C19 m t1 t20 @1A¼U b: To reparameterize in terms of g, we can use Z¼10 10 01010 BB@1 CCA,12.5 REPARAMETERIZATION 319 --- Page 331 --- so that Za¼Xb: Zg¼10 10 01 010 BB@1 CCAg1 g2/C18/C19 ¼g1 g1 g2 g20 BB@1 CCA¼mþt1 mþt1 mþt2 mþt20 BB@1 CCA: [The matrix Zcan also be obtained directly using (12.31).] It is easy to verify thatZU¼X.",
    "BB@1 CCA: [The matrix Zcan also be obtained directly using (12.31).] It is easy to verify thatZU¼X. ZU¼10 10 01 010 BB@1 CCA110 101/C18/C19 ¼110 110 101 1010 BB@1 CCA¼X: A 12.6 SIDE CONDITIONS The technique of imposing side conditions was introduced and illustrated in Section 12.1 Side conditions provide (linear) constraints that make the parameters unique andindividually estimable, but side conditions also impose speciﬁc deﬁnitions on the parameters.",
    "que andindividually estimable, but side conditions also impose speciﬁc deﬁnitions on the parameters. Another use for side conditions is to impose arbitrary constraints on the estimates so as to simplify the normal equations. In this case the estimateshave exactly the same status as those based on a particular generalized inverse (12.13), and only estimable functions of bcan be interpreted. LetXben/C2pof rank k,p/C20n. Then, by Theorem 12.2b(ii), X0Xbrepresents a set of pestimable functions of b.",
    "/C2pof rank k,p/C20n. Then, by Theorem 12.2b(ii), X0Xbrepresents a set of pestimable functions of b. If a side condition were an estimable function of b, it could be expressed as a linear combination of the rows of X0Xband would con- tribute nothing to the rank deﬁciency in Xor to obtaining a solution vector ^bfor X0X^b¼X0y. Therefore, side conditions must be nonestimable functions of b. The matrix Xisn/C2pof rank k,p. Hence the deﬁciency in the rank of Xis p/C0k.",
    "imable functions of b. The matrix Xisn/C2pof rank k,p. Hence the deﬁciency in the rank of Xis p/C0k. In order for all the parameters to be unique or to obtain a unique solution vector ^b, we must deﬁne side conditions that make up this deﬁciency in rank. Accordingly, we deﬁne side conditions Tb¼0orT^b¼0, where Tis a (p/C0k)/C2pmatrix of rank p/C0ksuch that Tbis a set of nonestimable functions. In the following theorem, we consider a solution vector ^bfor both X0X^b¼X0y andT^b¼0. Theorem 12.6a.",
    "n the following theorem, we consider a solution vector ^bfor both X0X^b¼X0y andT^b¼0. Theorem 12.6a. Ify¼Xbþ1, where Xisn/C2pof rank k,p/C20n, and if Tis a (p/C0k)/C2pmatrix of rank p/C0ksuch that Tbis a set of nonestimable functions, then there is a unique vector ^bthat satisﬁes both X0X^b¼X0yandT^b¼0.320 ANALYSIS-OF-VARIANCE MODELS --- Page 332 --- PROOF.",
    "ector ^bthat satisﬁes both X0X^b¼X0yandT^b¼0.320 ANALYSIS-OF-VARIANCE MODELS --- Page 332 --- PROOF. The two sets of equations y¼Xbþ1 0¼Tbþ0 can be combined into y 0/C18/C19 ¼X T/C18/C19 bþ1 0/C18/C19 : (12 :35) Since the rows of Tare linearly independent and are not functions of the rows of X, the matrixX T/C18/C19 is (nþp/C0k)/C2pof rank p.",
    "independent and are not functions of the rows of X, the matrixX T/C18/C19 is (nþp/C0k)/C2pof rank p. ThusX T/C18/C190X T/C18/C19 isp/C2pof rank p, and the system of equations X T/C18/C190X T/C18/C19 ^b¼X T/C18/C190y 0/C18/C19 (12 :36) has the unique solution ^b¼X T/C18/C19 0X T/C18/C19\"#/C01X T/C18/C19 0y 0/C18/C19 ¼(X0,T0)X T/C18/C19/C20/C21 /C01 (X0,T0)y0/C18/C19 ¼(X 0XþT0T)/C01(X0yþT00) ¼(X0XþT0T)/C01X0y: (12 :37) This approach to imposing constraints on the parameters does not work for full-rank models [see (8.30) and Problem 8.19] or for overparameterized models if the con- straints involve estimable functions.",
    "and Problem 8.19] or for overparameterized models if the con- straints involve estimable functions. However if Tbis a set of nonestimable func- tions, the least-squares criterion guarantees that T^b¼0. The solution ^bin (12.37) also satisﬁes the original normal equations X0X^b¼X0y, since, by (12.36) (X0XþT0T)^b¼X0yþT00 X0X^bþT0T^b¼X0y:(12 :38) ButT^b¼0, and (12.38) reduces to X0X^b¼X0y. A12.6 SIDE CONDITIONS 321 --- Page 333 --- Example 12.6.",
    "ButT^b¼0, and (12.38) reduces to X0X^b¼X0y. A12.6 SIDE CONDITIONS 321 --- Page 333 --- Example 12.6. Consider the model yij¼mþtiþ1ij,i¼1,2,j¼1,2a si n Example 12.5. The function t1þt2was shown to be nonestimable in Problem 12.5b.",
    "1ij,i¼1,2,j¼1,2a si n Example 12.5. The function t1þt2was shown to be nonestimable in Problem 12.5b. The side condition t1þt2¼0 can be expressed as (0 ,1,1)b¼0, and X0XþT0Tbecomes 422 220 2020 @1Aþ0 1 10 @1A011ðÞ ¼422 231 2130 @1A: Then (X 0XþT0T)/C01¼1 42/C01/C01 /C0120 /C01020 @1A: With X 0y¼(y::,y1:,y2:)0, we obtain, by (12.37) ^b¼(X0XþT0T)/C01X0y ¼1 42y::/C0y1:/C0y2: 2y1:/C0y:: 2y2:/C0y::0 B@1 CA¼/C22y:: /C22y1:/C0/C22y:: /C22y2:/C0/C22y::0 B@1 CA,(12 :39) since y1:þy2:¼y::.",
    ": 2y2:/C0y::0 B@1 CA¼/C22y:: /C22y1:/C0/C22y:: /C22y2:/C0/C22y::0 B@1 CA,(12 :39) since y1:þy2:¼y::. We now show that ^bin (12.39) is also a solution to the normal equations X0X^b¼X0y: 422 220 2020 B@1 CA/C22y:: /C22y1:/C0/C22y:: /C22y2:/C0/C22y::0 B@1 CA¼y:: y1: y2:0 B@1 CA,or 4/C22y::þ2(/C22y1:/C0/C22y::)þ2(/C22y2:/C0/C22y::)¼y:: 2/C22y::þ2(/C22y1:/C0/C22y::)¼y1: 2/C22y::þ2(/C22y2:/C0/C22y::)¼y2: These simplify to 2/C22y1:þ2/C22y2:¼y:: 2/C22y1:¼y1: 2/C22y2:¼y2:, which hold because /C22y1:¼y1:=2,/C22y2:¼y2:=2 and y1:þy2:¼y::.",
    "22y2:¼y:: 2/C22y1:¼y1: 2/C22y2:¼y2:, which hold because /C22y1:¼y1:=2,/C22y2:¼y2:=2 and y1:þy2:¼y::. A322 ANALYSIS-OF-VARIANCE MODELS --- Page 334 --- 12.7 TESTING HYPOTHESES We now consider hypotheses about the b’s in the model y¼Xbþ1, where Xis n/C2pof rank k,p/C20n. In this section, we assume that yisNn(Xb,s2I). 12.7.1 Testable Hypotheses It can be shown that unless a hypothesis can be expressed in terms of estimable func- tions, it cannot be tested (Searle 1971, pp. 193–196).",
    "can be expressed in terms of estimable func- tions, it cannot be tested (Searle 1971, pp. 193–196). This leads to the following deﬁnition. A hypothesis such as H0:b1¼b2¼/C1/C1/C1¼bqis said to be testable if there exists a set of linearly independent estimable functions l0 1b,l02b,...,l0tbsuch that H0is true if and only if l01b¼l02b¼/C1/C1/C1¼l0tb¼0. Sometimes the subset of b0s whose equality we wish to test is such that every con- trastP icibiis estimable (P icibiis a contrast ifP ici¼0).",
    "y we wish to test is such that every con- trastP icibiis estimable (P icibiis a contrast ifP ici¼0). In this case, it is easy to ﬁnd a set of q/C01 linearly independent estimable functions that can be set equal to zero to express b1¼/C1/C1/C1¼bq. One such set is the following: l0 1b¼(q/C01)b1/C0(b2þb3þ/C1/C1/C1þbq) l0 2b¼(q/C02)b2/C0(b3þ/C1/C1/C1þbq) ...",
    "set is the following: l0 1b¼(q/C01)b1/C0(b2þb3þ/C1/C1/C1þbq) l0 2b¼(q/C02)b2/C0(b3þ/C1/C1/C1þbq) ... l0 q/C01b¼(1)bq/C01/C0(bq): These q/C01 contrasts l0 1b,...,l0q/C01bconstitute a set of linearly independent estimable functions such that l0 1b ... l0 q/C01b0 BB@1 CCA¼0 ... 00 B@1 CA if and only if b1¼b2¼/C1/C1/C1¼bq. To illustrate a testable hypothesis, suppose that we have the model yij¼mþaiþbjþ1ij,i¼1,2,3,j¼1,2,3, and a hypothesis of interest is H0:a1¼a2¼a3.",
    "that we have the model yij¼mþaiþbjþ1ij,i¼1,2,3,j¼1,2,3, and a hypothesis of interest is H0:a1¼a2¼a3. By taking linear combinations of the rows of Xb, we can obtain the two linearly independent estimable functions a1/C0a2anda1þa2/C02a3. The hypothesis H0:a1¼a2¼a3is true if and only if a1/C0a2anda1þa2/C02a3are simultaneously equal to zero (see Problem 12.21).",
    "¼a3is true if and only if a1/C0a2anda1þa2/C02a3are simultaneously equal to zero (see Problem 12.21). Therefore, H0is a testable12.7 TESTING HYPOTHESES 323 --- Page 335 --- hypothesis and is equivalent to H0:a1/C0a2 a1þa2/C02a3/C18/C19 ¼0 0/C18/C19 : (12 :40) We now discuss tests for testable hypotheses. In Section 12.7.2, we describe a pro- cedure that is based on the full-reduced-model methods of Section 8.2.",
    "on 12.7.2, we describe a pro- cedure that is based on the full-reduced-model methods of Section 8.2. Since (12.40) is of the form H0:Cb¼0, we could alternatively use a general linear hypoth- esis test (see Section 8.4.1). This approach is discussed in Section 12.7.3.",
    "general linear hypoth- esis test (see Section 8.4.1). This approach is discussed in Section 12.7.3. 12.7.2 Full-Reduced-Model Approach Suppose that we are interested in testing H0:b1¼b2¼/C1/C1/C1¼bqin the non-full-rank model y¼Xbþ1, wherebisp/C21 and Xisn/C2pof rank k,p/C20n.I fH0is tes- table, we can ﬁnd a set of linearly independent estimable functions l0 1b,l02b,...,l0tbsuch that H0:b1¼b2¼/C1/C1/C1¼bqis equivalent to H0:g1¼l01b l0 2b ... l0 tb0 BBBB@1 CCCCA¼0 0 ...",
    "l0tbsuch that H0:b1¼b2¼/C1/C1/C1¼bqis equivalent to H0:g1¼l01b l0 2b ... l0 tb0 BBBB@1 CCCCA¼0 0 ... 00 BBB@1 CCCA: It is also possible to ﬁnd g2¼l0 tþ1b ... l0 kb0 BB@1 CCA such that the kfunctions l0 1b,...,l0tb,l0tþ1b,...,l0kbare linearly independent and estimable, where k¼rank(X).",
    "he kfunctions l0 1b,...,l0tb,l0tþ1b,...,l0kbare linearly independent and estimable, where k¼rank(X). Let g¼g1 g2/C18/C19 : We can now reparameterize (see Section 12.5) from the non-full-rank model y¼Xbþ1to the full-rank model y¼Zgþ1¼Z1g1þZ2g2þ1, where Z¼(Z1,Z2) is partitioned to conform with the number of elements in g1 andg2.324 ANALYSIS-OF-VARIANCE MODELS --- Page 336 --- For the hypothesis H0:g1¼0, the reduced model is y¼Z2g/C3 2þ1/C3.",
    "F-VARIANCE MODELS --- Page 336 --- For the hypothesis H0:g1¼0, the reduced model is y¼Z2g/C3 2þ1/C3. By Theorem 7.10, the estimate of g/C32in the reduced model is the same as the estimate of g2in the full model if the columns of Z2are orthogonal to those of Z1, that is, if Z0 2Z1¼O. For the balanced models we are considering in this chapter, the orthogonality will typically hold (see Section 12.8.3). Accordingly, we refer to g2and ^g2rather than tog/C3 2and ^g/C32.",
    "pically hold (see Section 12.8.3). Accordingly, we refer to g2and ^g2rather than tog/C3 2and ^g/C32. Since y¼Zgþ1is a full-rank model, the hypothesis H0:g1¼0can be tested as in Section 8.2. The test is outlined in Table 12.2, which is analogous to Table 8.3. Note that the degrees of freedom tfor SS(g1jg2) is the number of linearly indepen- dent estimable functions required to express H0. In Table 12.2, the sum of squares ^g0Zyis obtained from the full model y¼Zgþ1.",
    "quired to express H0. In Table 12.2, the sum of squares ^g0Zyis obtained from the full model y¼Zgþ1. The sum of squares ^g0 2Z0 2yis obtained from the reduced model y¼Z2g2þ1,which assumes the hypothesis is true. The reparameterization procedure presented above seems straightforward. However, ﬁnding the matrix Zin practice can be time-consuming. Fortunately, this step is actually not necessary.",
    "ing the matrix Zin practice can be time-consuming. Fortunately, this step is actually not necessary. From (12.20) and (12.33), we obtain y0y/C0^b0X0y¼y0y/C0^g0Zy, which gives ^b0X0y¼^g0Z0y, (12 :41) where ^brepresents any solution to the normal equations X0X^b¼X0y. Similarly, cor- responding to y¼Zg/C32þ1/C3, we have a reduced model y¼X2b/C3 2þ1/C3obtained by settingb1¼b2¼/C1/C1/C1¼bq.",
    "ding to y¼Zg/C32þ1/C3, we have a reduced model y¼X2b/C3 2þ1/C3obtained by settingb1¼b2¼/C1/C1/C1¼bq. Then ^b/C30 2X0 2y¼^g/C30 2Z02y, (12 :42) where ^b/C3 2is any solution to the reduced normal equations X0 2X2^b/C3 2¼X0 2y. We can often use side conditions to ﬁnd ^band ^b/C3 2.",
    "reduced normal equations X0 2X2^b/C3 2¼X0 2y. We can often use side conditions to ﬁnd ^band ^b/C3 2. We noted above (see also Section 12.8.3) that if Z0 2Z1¼Oholds in a reparame- terized full-rank model, then by Theorem 7.10, the estimate of g/C3 2in the reduced TABLE 12.2 ANOVA for Testing H0:g150in Reparameterized Balanced Models Source of Variation df Sum of Squares FStatistic Due tog1adjusted for g2 t SS(g1jg2)¼^g0Z0y/C0^g0 2Z0 2y SSðg1jg2Þ=t SSE =ðn/C0kÞ Error n/C0k SSE¼y0y/C0^g0Z0y — Total n /C01 SST¼y0y/C0n/C22y212.7 TESTING HYPOTHESES 325 --- Page 337 --- model is the same as the estimate of g2in the full model.",
    ".7 TESTING HYPOTHESES 325 --- Page 337 --- model is the same as the estimate of g2in the full model. The following is an analo- gous theorem for the non-full-rank case. Theorem 12.7a. Consider the partitioned model y¼Xbþ1¼X1b1þX2b2þ1, where Xisn/C2pof rank k,p/C20n.I fX0 2X1¼O(see Section 12.8.3), any estimate ofb/C3 2in the reduced model y¼X2b/C32þ1/C3is also an estimate of b2in the full model. PROOF.",
    "timate ofb/C3 2in the reduced model y¼X2b/C32þ1/C3is also an estimate of b2in the full model. PROOF. There is a generalized inverse of X0X¼X0 1X1X01X2 X02X1X02X2/C18/C19 analogous to the inverse of a nonsingular symmetric partitioned matrix in (2.50) (Harville 1997, pp. 121–122). The proof then parallels that of Theorem 7.10. A In the balanced non-full-rank models we are considering in this chapter, the ortho-gonality of X 1andX2will typically hold.",
    "l-rank models we are considering in this chapter, the ortho-gonality of X 1andX2will typically hold. (This will be illustrated in Section 12.8.3) Accordingly, we refer to b2and ^b2, rather than to b/C3 2and ^b/C32. The test can be expressed as in Table 12.3, in which ^b0X0yis obtained from the full model y¼Xbþ1and ^b0 2X0 2yis obtained from the model y¼X2b2þ1, which has been reduced by the hypothesis H0:b1¼b2¼/C1/C1/C1¼bq.",
    "is obtained from the model y¼X2b2þ1, which has been reduced by the hypothesis H0:b1¼b2¼/C1/C1/C1¼bq. Note that the degrees of freedom tfor SS(b1jb2) is the same as for SS( g1jg2) in Table 12.2, namely, the number of linearly independent estimable functions required to express H0. Typically, this is given by t¼q/C01. A set of q/C01 linearly independent estimable functions was illustrated at the beginning of Section 12.7.1. The test in Table 12.3 will be illustrated in Section 12.8.2.",
    "ed at the beginning of Section 12.7.1. The test in Table 12.3 will be illustrated in Section 12.8.2. 12.7.3 General Linear Hypothesis As illustrated in (12.40), a hypothesis such as H0:a1¼a2¼a3can be expressed in the form H0:Cb¼0.",
    "sis As illustrated in (12.40), a hypothesis such as H0:a1¼a2¼a3can be expressed in the form H0:Cb¼0. We can test this hypothesis in a manner analogous to that used for the general linear hypothesis test for the full-rank model in Section 8.4.1 The fol- lowing theorem is an extension of Theorem 8.4a to the non-full-rank case.TABLE 12.3 ANOVA for Testing H0:b15b25/C1/C1/C15bqin Balanced Non-Full-Rank Models Source of Variation df Sum of Squares FStatistic Due tob1adjusted for b2 t SS(b1jb2)¼^b0X0y/C0^b0 2X0 2y SSðb1jb2Þ=t SSE =ðn/C0kÞ Error n/C0k SSE¼y0y/C0^b0X0y — Total n /C01 SST¼y0y/C0n/C22y2 —326 ANALYSIS-OF-VARIANCE MODELS --- Page 338 --- Theorem 12.7b.",
    "0y — Total n /C01 SST¼y0y/C0n/C22y2 —326 ANALYSIS-OF-VARIANCE MODELS --- Page 338 --- Theorem 12.7b. IfyisNn(Xb,s2I), where Xisn/C2pof rank k,p/C20n,i fC ism/C2pof rank m/C20ksuch that Cbis a set of mlinearly independent estimable functions, and if ^b¼(X0X)/C0X0y, then (i)C(X0X)/C0C0is nonsingular. (ii)C^bisNm[Cb,s2C(X0X)/C0C0]. (iii) SSH =s2¼(C^b)0[C(X0X)/C0C0]/C01C^b=s2isx2(m,l),wherel¼(Cb)0 [C(X0X)/C0C0]/C01Cb=2s2. (iv) SSE =s2¼y0[I/C0X(X0X)/C0X0]y=s2isx2(n/C0k).",
    "b=s2isx2(m,l),wherel¼(Cb)0 [C(X0X)/C0C0]/C01Cb=2s2. (iv) SSE =s2¼y0[I/C0X(X0X)/C0X0]y=s2isx2(n/C0k). (v) SSH and SSE are independent. PROOF (i) Since Cb¼c0 1b c02b ...",
    "[I/C0X(X0X)/C0X0]y=s2isx2(n/C0k). (v) SSH and SSE are independent. PROOF (i) Since Cb¼c0 1b c02b ... c0 mb0 BBBBB@1 CCCCCA is a set of mlinearly independent estimable functions, then by Theorem 12.2b(iii) we have c 0 i(X0X)/C0X0X¼c0ifori¼1,2,...,m:Hence C(X0X)/C0X0X¼C: (12 :43) Writing (12.43) as the product [C(X0X)/C0X0]X¼C, we can use Theorem 2.4(i) to obtain the inequalities rank(C)/C20rank[C(X0X)/C0X0]/C20rank(C): Hence rank[ C(X0X)/C0X0]¼rank(C)¼m.",
    "obtain the inequalities rank(C)/C20rank[C(X0X)/C0X0]/C20rank(C): Hence rank[ C(X0X)/C0X0]¼rank(C)¼m. Now, by Theorem 2.4(iii), which states that rank( A)¼rank(AA0), we can write rank(C)¼rank[C(X0X)/C0X0] ¼rank[C(X0X)/C0X0][C(X0X)/C0X0]0 ¼rank[C(X0X)/C0X0X(X0X)/C0C0]:12.7 TESTING HYPOTHESES 327 --- Page 339 --- By (12.43), C(X0X)/C0X0X¼C, and we have rank(C)¼rank[C(X0X)/C0C0]: Thus the m/C2mmatrix C(X0X)/C0C0is nonsingular. [Note that we are assuming that ( X0X)/C0is symmetric.",
    "us the m/C2mmatrix C(X0X)/C0C0is nonsingular. [Note that we are assuming that ( X0X)/C0is symmetric. See Problem 2.46 and a comment following Theorem 2.8c(v).] (ii) By (3.38) and (12.14), we obtain E(C^b)¼CE(^b)¼C(X0X)/C0X0Xb: By (12.43), C(X0X)/C0X0X¼C, and therefore E(C^b)¼Cb: (12 :44) By (3.44) and (12.18), we have cov(C^b)¼Ccov( ^b)C0¼s2C(X0X)/C0X0X(X0X)/C0C0: By (12.43), this becomes cov(C^b)¼s2C(X0X)/C0C0: (12 :45) By Theorem 12.3g(i), ^bisNp[(X0X)/C0X0Xb,s2(X0X)/C0X0X(X0X)/C0] for a par- ticular ( X0X)/C0.",
    "2 :45) By Theorem 12.3g(i), ^bisNp[(X0X)/C0X0Xb,s2(X0X)/C0X0X(X0X)/C0] for a par- ticular ( X0X)/C0. Then by (12.44), (12.45), and Theorem 4.4a(ii), we obtain C^bisNm[Cb,s2C(X0X)/C0C0]: (iii) By part (ii), cov( C^b)¼s2C(X0X)/C0C0. Since s2[C(X0X)/C0C0]/C01 C(X0X)/C0C0=s2¼I, the result follows by Theorem 5.5. (iv) This was established in Theorem 12.3g(ii). (v) By Theorem 12.3g(iii), ^band SSE are independent. Hence SSH ¼(C^b)0 [C(X0X)/C0C0]/C01C^band SSE are independent [see Seber (1977, pp.",
    "are independent. Hence SSH ¼(C^b)0 [C(X0X)/C0C0]/C01C^band SSE are independent [see Seber (1977, pp. 17–18) for a proof that continuous functions of independent random variables and vectors are independent]. For a more formal proof, see Problem 12.22. A Using the results in Theorem 12.7b, we obtain an Ftest for H0:Cb¼0, as given in the following theorem, which is analogous to Theorem 8.4b.328 ANALYSIS-OF-VARIANCE MODELS --- Page 340 --- Theorem 12.7c.",
    ", which is analogous to Theorem 8.4b.328 ANALYSIS-OF-VARIANCE MODELS --- Page 340 --- Theorem 12.7c. LetybeNn(Xb,s2I), where Xisn/C2pof rank k,p/C20n, and let C,Cb, and ^bbe deﬁned as in Theorem 12.7b. Then, if H0:Cb¼0is true, the statistic F¼SSH =m SSE =(n/C0k) ¼(C^b)0[C(X0X)/C0C0]/C01C^b=m SSE =(n/C0k)(12 :46) is distributed as F(m,n/C0k): PROOF. This follows from (5.28) and Theorem 12.7b.",
    "SE =(n/C0k)(12 :46) is distributed as F(m,n/C0k): PROOF. This follows from (5.28) and Theorem 12.7b. A 12.8 AN ILLUSTRATION OF ESTIMATION AND TESTING Suppose we have the additive (no-interaction) model yij¼mþaiþbjþ1ij,i¼1,2,3;j¼1,2, and that the hypotheses of interest are H0:a1¼a2¼a3andH0:b1¼b2.",
    "model yij¼mþaiþbjþ1ij,i¼1,2,3;j¼1,2, and that the hypotheses of interest are H0:a1¼a2¼a3andH0:b1¼b2. The six observations can be written in the form y¼Xbþ1as y11 y12 y21 y22 y31 y320 BBBBBB@1 CCCCCCA¼110010 110001 101010101001 100110 1001010 BBBBBB@1 CCCCCCAm a1 a2 a3 b1 b20 BBBBBB@1 CCCCCCAþ1 11 112 121 122 131 1320 BBBBBB@1 CCCCCCA: (12 :47) The matrix X 0Xis given by X0X¼622233 220011 202011200211 311130 3111030 BBBBBB@1 CCCCCCA: The rank of both XandX 0Xis 4.12.8 AN ILLUSTRATION OF ESTIMATION AND TESTING 329 --- Page 341 --- 12.8.1 Estimable Functions The hypothesis H0:a1¼a2¼a3can be expressed as H0:a1/C0a2¼0 and a1/C0a3¼0.",
    "12.8.1 Estimable Functions The hypothesis H0:a1¼a2¼a3can be expressed as H0:a1/C0a2¼0 and a1/C0a3¼0. Thus H0is testable if a1/C0a2anda1/C0a3are estimable. To check a1/C0a2for estimability, we write it as a1/C0a2¼(0,1,/C01,0,0,0)b¼l0 1b and then note that l0 1can be obtained from Xas (1,0,/C01,0,0,0)X¼(0,1,/C01,0,0,0) and from X0Xas (0,1 2,/C012,0,0,0)X0X¼(0,1,/C01,0,0,0) (see Theorem 12.2b).",
    "0,0,0)X¼(0,1,/C01,0,0,0) and from X0Xas (0,1 2,/C012,0,0,0)X0X¼(0,1,/C01,0,0,0) (see Theorem 12.2b). Alternatively, we can obtain a1/C0a2as a linear combination of the rows (elements) of E(y)¼Xb: E(y11/C0y21)¼E(y11)/C0E(y21) ¼mþa1þb1/C0(mþa2þb1) ¼a1/C0a2: Similarly, a1/C0a3can be expressed as a1/C0a3¼(0,1,0,/C01,0,0)b¼l0 2b, andl0 2can be obtained from XorX0X: (1,0,0,0,/C01,0)X¼(0,1,0,/C01,0,0), (0,1 2,0,/C012,0,0)X0X¼(0,1,0,/C01,0,0): It is also of interest to examine a complete set of linearly independent estimable functions obtained as linear combinations of the rows of X[see Theorem 12.2b(i) and Example 12.2.2b].",
    "unctions obtained as linear combinations of the rows of X[see Theorem 12.2b(i) and Example 12.2.2b]. If we subtract the ﬁrst row from each succeeding row of X, we obtain 11 0 01 0 00 0 0 /C011 0/C0110 00 0/C0110 /C011 0/C0101 00 0/C0101 /C0110 BBBBBB@1 CCCCCCA:330 ANALYSIS-OF-VARIANCE MODELS --- Page 342 --- We multiply the second and third rows by 21 and then add them to the fourth row, with similar operations involving the second, ﬁfth, and sixth rows.",
    "then add them to the fourth row, with similar operations involving the second, ﬁfth, and sixth rows. The result is 1 100 10 00 0 01 /C01 01 /C010 00 0 000 0001 0 /C010 0 0 000 000 BBBBBB@1 CCCCCCA: Multiplying this matrix by b, we obtain a complete set of linearly independent estimable functions: mþa1þb1,b1/C0b2,a1/C0a2,a1/C0a3. Note that the esti- mable functions not involving mare contrasts in the a0so rb’s.",
    ",a1/C0a2,a1/C0a3. Note that the esti- mable functions not involving mare contrasts in the a0so rb’s. 12.8.2 Testing a Hypothesis As noted at the beginning of Section 12.8.1, H0:a1¼a2¼a3is equivalent to H0:a1/C0a2¼a1/C0a3¼0. Since two linearly independent estimable functions of thea’s are needed to express H0:a1¼a2¼a3(see Theorems 12.7b and 12.7c), the sum of squares for testing H0:a1¼a2¼a3has 2 degrees of freedom. Similarly, H0:b1¼b2is testable with 1 degree of freedom.",
    "esting H0:a1¼a2¼a3has 2 degrees of freedom. Similarly, H0:b1¼b2is testable with 1 degree of freedom. The normal equations X0X^b¼X0yare given by 622233 220011 202011 200211311130 3111030 BBBBBB@1 CCCCCCA^ m ^a1 ^a2 ^a3 ^b1^b20 BBBBBB@1 CCCCCCA¼y :: y1: y2: y3: y:1 y:20 BBBBBB@1 CCCCCCA: (12 :48) If we impose the side conditions ^ a1þ^a2þ^a3¼0 and ^b1þ^b2¼0, we obtain the following solution to the normal equations: ^m¼/C22y::, ^a1¼/C22y1:/C0/C22y::,^a2¼/C22y2:/C0/C22y::, ^a3¼/C22y3:/C0/C22y::,^b1¼/C22y:1/C0/C22y::,^b2¼/C22y:2/C0/C22y::,(12 :49) where /C22y::¼P ijyij=6,/C22y1:¼P jy1j=2, and so on.",
    "C22y:1/C0/C22y::,^b2¼/C22y:2/C0/C22y::,(12 :49) where /C22y::¼P ijyij=6,/C22y1:¼P jy1j=2, and so on. If we impose the side conditions on both the parameters and the estimates, equations (12.49) are unique estimates of unique meaningful parameters. Thus, for example, a1becomes a/C3 1¼/C22m1:/C0/C22m::, the expected deviation from the mean due to treatment 1 (see Section 12.1.1), and /C22y1:/C0/C22y::is a reasonable estimate.",
    "rom the mean due to treatment 1 (see Section 12.1.1), and /C22y1:/C0/C22y::is a reasonable estimate. On the other hand, if the side conditions are used only to obtain estimates and are not imposed on the parameters, then a1is not unique, and /C22y1:/C0/C22y::does not estimate a parameter.",
    "imposed on the parameters, then a1is not unique, and /C22y1:/C0/C22y::does not estimate a parameter. In this case, ^a1¼/C22y1:/C0/C22y::can be used only together with other elements in^b[as given by (12.49)] to obtain estimates l0^bof estimable functions l0b.12.8 AN ILLUSTRATION OF ESTIMATION AND TESTING 331 --- Page 343 --- We now proceed to obtain the test for H0:a1¼a2¼a3following the outline in Table 12.3.",
    "- Page 343 --- We now proceed to obtain the test for H0:a1¼a2¼a3following the outline in Table 12.3. First, for the full model, we need ^b0X0y¼SS(m,a1,a2,a3,b1,b2), which we denote by SS( m,a,b). By (12.48) and (12.49), we obtain SS(m,a,b)¼^b0X0y¼(^m,^a1,...,^b2)y:: y1: ...",
    "denote by SS( m,a,b). By (12.48) and (12.49), we obtain SS(m,a,b)¼^b0X0y¼(^m,^a1,...,^b2)y:: y1: ... y:20 BBBB@1 CCCCA ¼^my::þ^a1y1:þ^a2y2:þ^a3y3:þ^b1y:1þ^b2y:2 ¼/C22y::y::þX3 i¼1(/C22yi:/C0/C22y::)yi:þX2 j¼1(/C22y:j/C0/C22y::)y:j ¼y2 :: 6þX3 i¼1yi: 2/C0y:: 6/C16/C17 yi:þX2 j¼1y:j 3/C0y:: 6/C16/C17 y:j ¼y2:: 6þX3 i¼1y2i: 2/C0y2:: 6 ! þX2 j¼1y2 :j 3/C0y2 :: 6 ! , (12 :50) sinceP iyi:¼y::andP jy:j¼y::.",
    "¼y2:: 6þX3 i¼1y2i: 2/C0y2:: 6 ! þX2 j¼1y2 :j 3/C0y2 :: 6 ! , (12 :50) sinceP iyi:¼y::andP jy:j¼y::. The error sum of squares SSE is given by y0y/C0^b0X0y¼X ijy2 ij/C0y2 :: 6/C0X3 i¼1y2i: 2/C0y2:: 6 ! /C0X2 j¼1y2:j 3/C0y2:: 6 ! : To obtain ^b2X0 2yin Table 12.3, we use the reduced model yij¼mþaþ bjþ1ij¼mþbjþ1ij, wherea1¼a2¼a3¼aandmþais replaced by m.",
    "able 12.3, we use the reduced model yij¼mþaþ bjþ1ij¼mþbjþ1ij, wherea1¼a2¼a3¼aandmþais replaced by m. The normal equations X02X2^b2¼X02yfor the reduced model are 6^mþ3^b1þ3^b2¼y:: 3^mþ3^b1¼y:1 3^mþ3^b2¼y:2: (12 :51) Using the side condition ^b1þ^b2¼0, the solution to the reduced normal equations in (12.51) is easily obtained as ^m¼/C22y::,^b1¼/C22y:1/C0/C22y::,^b2¼/C22y:2/C0/C22y::: (12 :52) By (12.51) and (12.52), we have SS(m,b)¼^b0 2X0 2y¼^my::þ^b1y:1þ^b2y:2¼y2 :: 6þX2 j¼1y2:j 3/C0y2:: 6 !",
    "(12.51) and (12.52), we have SS(m,b)¼^b0 2X0 2y¼^my::þ^b1y:1þ^b2y:2¼y2 :: 6þX2 j¼1y2:j 3/C0y2:: 6 ! : (12 :53)332 ANALYSIS-OF-VARIANCE MODELS --- Page 344 --- Abbreviating SS( a1,a2,a3jm,b1b2) as SS(ajm,a), we have SS(ajm,b)¼^b0X0y/C0^b0 2X0 2y¼X iy2 i: 2/C0y2:: 6: (12 :54) The test is summarized in Table 12.4.",
    "ve SS(ajm,b)¼^b0X0y/C0^b0 2X0 2y¼X iy2 i: 2/C0y2:: 6: (12 :54) The test is summarized in Table 12.4. [Note that SS( bjm,a) is not included.] 12.8.3 Orthogonality of Columns of X The estimates of m,b1, andb2given in (12.52) for the reduced model are the same as those ofm,b1, andb2given in (12.49) for the full model. The sum of squares ^b0 2X0 2y in (12.53) is clearly a part of ^b0X0yin (12.50). In fact, (12.54) can be expressed as SS(ajm,b)¼SS(a), and (12.50) becomes SS( m,a,b)¼SS(m)þSS(a)þSS(b).",
    "fact, (12.54) can be expressed as SS(ajm,b)¼SS(a), and (12.50) becomes SS( m,a,b)¼SS(m)þSS(a)þSS(b). These simpliﬁed results are due to the essential orthogonality in the Xmatrix in (12.47) as required by Theorem 12.7a. There are three groups of columns in the X matrix in (12.47), the ﬁrst column corresponding to m, the next three columns corre- sponding to a1,a2, anda3, and the last two columns corresponding to b1andb2.",
    "xt three columns corre- sponding to a1,a2, anda3, and the last two columns corresponding to b1andb2. The columns of Xin (12.47) are orthogonal within each group but not among groups as required by Theorem 12.7a.",
    "s of Xin (12.47) are orthogonal within each group but not among groups as required by Theorem 12.7a. However, consider the same Xmatrix if each column after the ﬁrst is centered using the mean of the column: (j,Xc)¼12 3/C013/C013 12/C012 12 3/C013/C013/C012 12 1/C01 323/C013 12/C012 1/C013 23/C013/C012 12 1/C01 3/C013 23 12/C012 1/C01 3/C013 23/C012 120 BBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCA: (12 :55)TABLE 12.4 ANOVA for Testing H0:a15a25a3 Source of Variation df Sum of Squares FStatistic Due toaadjusted for m,b 2SS(ajm,b)¼P iy2 i: 2/C0y2:: 6P iy2i: 2/C0y2:: 6/C18/C19 =2 SSE =2 Error 2 SSE¼P ijy2ij/C0^b0X0y — Total 5 SST¼P ijy2ij/C0y2::=6 —12.8 AN ILLUSTRATION OF ESTIMATION AND TESTING 333 --- Page 345 --- Now the columns are orthogonal among the groups.",
    "TION OF ESTIMATION AND TESTING 333 --- Page 345 --- Now the columns are orthogonal among the groups. For example, each of columns 2, 3, and 4 is orthogonal to each of columns 5 and 6, but columns 2, 3, and 4 are not orthogonal to each other. Note that rank( j,Xc)¼4 since the sum of columns 2, 3, and 4 is 0and the sum of columns 5 and 6 is 0. Thus rank( j,Xc) is the same as the rank of Xin (12.47).",
    "4 is 0and the sum of columns 5 and 6 is 0. Thus rank( j,Xc) is the same as the rank of Xin (12.47). We now illustrate the use of side conditions to obtain an orthogonalization that is full-rank (this was illustrated for a one-way model in Section 12.1.1.).",
    "n orthogonalization that is full-rank (this was illustrated for a one-way model in Section 12.1.1.). Consider the two-way model with interaction yijk¼mþaiþbjþgijþ1ijk,i¼1,2;j¼1,2;k¼1,2: (12 :56) In matrix form, the model is y111 y112 y121 y122 y211 y212 y221 y2220 BBBBBBBBBB@1 CCCCCCCCCCA¼110101000 110101000 110010100 110010100 101100010 101100010 101010001 1010100010 BBBBBBBBBB@1 CCCCCCCCCCAm a1 a2 b1 b2 g11 g12 g21 g220 BBBBBBBBBBBB@1 CCCCCCCCCCCCAþ1 111 1112 1121 1122 1211 1212 1221 12220 BBBBBBBBBB@1 CCCCCCCCCCA:(12 :57) Useful side conditions become apparent in the context of the normal equations, which are given by 8^ mþ4(^a1þ^a2)þ4(^b1þ^b2)þ2(^g11þ^g12þ^g21þ^g22)¼y:: 4^mþ4^aiþ2(^b1þ^b2)þ2(^gi1þ^gi2)¼yi::,i¼1,2 4^mþ2(^a1þ^a2)þ4^bjþ2(^g1jþ^g2j)¼y:j:,j¼1,2 2^mþ2^aiþ2^bjþ2^gij¼yij:,i¼1,2,j¼1,2(12 :58) Solution of the equations in (12.58) would be simpliﬁed by the following side con- ditions: ^a1þ^a2¼0,^b1þ^b2¼0, ^gi1þ^gi2¼0,i¼1,2, ^g1jþ^g2j¼0,j¼1,2:(12 :59) In (12.57), the Xmatrix is 8 /C29 of rank 4 since the ﬁrst ﬁve columns are all expressible as linear combinations of the last four columns, which are linearly inde- pendent.",
    "e all expressible as linear combinations of the last four columns, which are linearly inde- pendent. Thus X0Xis 9/C29 and has a rank deﬁciency of 9 24¼5. However, there are six side conditions in (12.59). This apparent discrepancy is resolved by noting that334 ANALYSIS-OF-VARIANCE MODELS --- Page 346 --- there are only three restrictions among the last four equations in (12.59). We can obtain any one of these four from the other three.",
    "among the last four equations in (12.59). We can obtain any one of these four from the other three. To illustrate, we obtain the ﬁrst equation from the last three. Adding the third and fourth equations gives ^g11þ^g21þ^g12þ^g22¼0. Then substitution of the second, ^g21þ^g22¼0, reduces this to the ﬁrst, ^g11þ^g12¼0.",
    "21þ^g12þ^g22¼0. Then substitution of the second, ^g21þ^g22¼0, reduces this to the ﬁrst, ^g11þ^g12¼0. We can obtain a full-rank orthogonalization by imposing the side conditions in (12.59) on the parameters and using these relationships to express redundant parameters in terms of the four parameters m,a1,b1, andg11.",
    "these relationships to express redundant parameters in terms of the four parameters m,a1,b1, andg11. (For exposi- tional convenience, we do not use *on the parameters subject to side conditions.) This gives a2¼/C0a1,b2¼/C0b1, g12¼/C0g11,g21¼/C0g11,g22¼g11:(12 :60) The last of these, for example, is obtained from the side condition g12þg22¼0.",
    "0g11,g22¼g11:(12 :60) The last of these, for example, is obtained from the side condition g12þg22¼0. Thusg22¼/C0g12¼/C0(/C0g11): Using (12.60), we can express the eight yijkvalues in (12.56) in terms of m,a1,b1, andg11: y11k¼mþa1þb1þg11þ111k,k¼1,2, y12k¼mþa1þb2þg12þ112k ¼mþa1/C0b1/C0g11þ112k,k¼1,2, y21k¼mþa2þb1þg21þ121k ¼m/C0a1þb1/C0g11þ121k,k¼1,2, y22k¼mþa2þb2þg22þ122k ¼m/C0a1/C0b1þg11þ122k,k¼1,2: The redeﬁned Xmatrix thus becomes 11111111 11 /C01/C01 11 /C01/C01 1/C011 /C01 1/C011 /C01 1/C01/C011 1/C01/C0110 BBBBBBBBBB@1 CCCCCCCCCCA, which is a full-rank matrix with orthogonal columns.",
    "/C01/C011 1/C01/C0110 BBBBBBBBBB@1 CCCCCCCCCCA, which is a full-rank matrix with orthogonal columns. The methods of Chapters 7 and 8 can now be used for estimation and testing hypotheses.12.8 AN ILLUSTRATION OF ESTIMATION AND TESTING 335 --- Page 347 --- PROBLEMS 12.1 Show that /C22m1:þ/C22m2:¼2/C22m::as in (12.9). 12.2 Show that ^10^1in (12.10) is minimized by ^b, the solution to X0X^b¼X0yin (12.11). 12.3 Use Theorem 2.7 to prove Theorem 12.2a.",
    "s minimized by ^b, the solution to X0X^b¼X0yin (12.11). 12.3 Use Theorem 2.7 to prove Theorem 12.2a. 12.4 (a) Give an alternative proof of Theorem 12.2b(iii) based on Theorem 2.8c(iii). (b) Give a second alternative proof of Theorem 12.2b(iii) based on Theorem 2.8f. 12.5 (a) Using all three conditions in Theorem 12.2b, show that l0b¼ mþt2¼(1,0,1)bis estimable (use the model in Example 12.2.2a). (b) Using all three conditions in Theorem 12.2b, show that l0b¼ t1þt2¼(0,1,1)bis not estimable.",
    "2a). (b) Using all three conditions in Theorem 12.2b, show that l0b¼ t1þt2¼(0,1,1)bis not estimable. 12.6 Ifl0bis estimable and ^b1and ^b2are two solutions to the normal equations, show that l0^b1¼l0^b2as in Theorem 12.3a(iii). 12.7 Obtain an estimate of mþt2using r0X0yandl0^bfrom the model in Example 12.3.1. 12.8 Consider the model yij¼mþtiþ1ij,i¼1,2,j¼1,2,3: (a)F o rl0b¼(1,1,0)b¼mþt1, show that r¼c/C01 1 10 @1Aþ0 1 3 00 B@1 CA, with arbitrary c, represents all solutions to X0Xr¼l.",
    "1, show that r¼c/C01 1 10 @1Aþ0 1 3 00 B@1 CA, with arbitrary c, represents all solutions to X0Xr¼l. (b) Obtain the BLUE [best linear unbiased estimator] for mþt1using r obtained in part (a). (c) Find the BLUE for t1/C0t2using the method of parts (a) and (b). 12.9 (a) In Example 12.2.2b, we found the estimable functions l0 1b¼mþa1þb1,l02b¼b1/C0b2,andl03b¼a1/C0a2. Find the BLUE for each of these using r0X0yin each case. (b) For each estimator in part (a), show that E(r0 iX0y)¼l0 ib.",
    "ch of these using r0X0yin each case. (b) For each estimator in part (a), show that E(r0 iX0y)¼l0 ib. 12.10 In the model yij¼mþtiþ1ij,i¼1,2,...,k;j¼1,2,...,n, show thatPk i¼1citiis estimable if and only ifPki¼1ci¼0, as suggested following Example 12.2.2b. Use the following two approaches: (a)I nl0b¼Pki¼1citi, expressl0as a linear combination of the rows of X.336 ANALYSIS-OF-VARIANCE MODELS --- Page 348 --- (b) ExpressPk i¼1citias a linear combination of the elements of E(y)¼Xb.",
    "NCE MODELS --- Page 348 --- (b) ExpressPk i¼1citias a linear combination of the elements of E(y)¼Xb. 12.11 In Example 12.3.1, ﬁnd all solutions rforX0Xr¼land show that all of them give r0X0y¼/C22y1:/C0/C22y2:. 12.12 Show that cov( l0 1^b,l02^b)¼s2r0 1l2¼s2l0 1r2¼s2l01(X0X)/C0l2as in Theorem 12.3c. 12.13 (a) Show that ( y/C0X^b)0(y/C0X^b)¼y0y/C0^b0X0yas in (12.20). (b) Show that y0y/C0^b0X0y¼y0[I/C0X(X0X)/C0X0]yas in (12.21).",
    ")0(y/C0X^b)¼y0y/C0^b0X0yas in (12.20). (b) Show that y0y/C0^b0X0y¼y0[I/C0X(X0X)/C0X0]yas in (12.21). 12.14 Show that b0X0[I/C0X(X0X)/C0X0]Xb¼0, as in the proof of Theorem 12.3e(i). 12.15 Differentiate lnL(b,s2) in (12.26) with respect to bands2to obtain (12.27) and (12.28). 12.16 Prove Theorem 12.3g. 12.17 Show that l0b¼b0g¼c0das in (12.34). 12.18 Show that the matrix Zin Example 12.5 can be obtained using (12.31), Z¼XU0(UU0)/C01.",
    "(12.34). 12.18 Show that the matrix Zin Example 12.5 can be obtained using (12.31), Z¼XU0(UU0)/C01. 12.19 Redo Example 12.5 with the parameterization g¼mþt1 t1/C0t2/C18/C19 : Find ZandUby inspection and show that ZU¼X. Then show that Zcan be obtained as Z¼XU0(UU0)/C01. 12.20 Show that ^bin (12.39) is a solution to the normal equations X0X^b¼X0y. 12.21 Show thata1/C0a2 a1þa2/C02a3/C18/C19 ¼0 0/C18/C19 in (12.40) implies a1¼a2¼a3,a s noted preceding (12.40). 12.22 Prove Theorem 12.7b(v).",
    "¼0 0/C18/C19 in (12.40) implies a1¼a2¼a3,a s noted preceding (12.40). 12.22 Prove Theorem 12.7b(v). 12.23 Multiply X0Xin (12.48) by ^bto obtain the six normal equations. Show that with the side conditions ^a1þ^a2þ^a3¼0 and ^b1þ^b2¼0,the solution is given by (12.49). 12.24 Obtain the reduced normal equations X0 2X2^b2¼X02yin (12.51) by writing X2andX02X2for the reduced model yij¼mþbjþ1ij,i¼1,2,3,j¼1,2.",
    "ions X0 2X2^b2¼X02yin (12.51) by writing X2andX02X2for the reduced model yij¼mþbjþ1ij,i¼1,2,3,j¼1,2. 12.25 Consider the model yij¼mþtiþ1ij,i¼1,2,3,j¼1,2,3: (a) Write X,X0X,X0y, and the normal equations.PROBLEMS 337 --- Page 349 --- (b) What is the rank of XorX0X? Find a set of linearly independent esti- mable functions. (c) Deﬁne an appropriate side condition, and ﬁnd the resulting solution to the normal equations. (d) Show that H0:t1¼t2¼t3is testable. Find ^b0X0y¼SS(m,t) and ^b0 2X0 2y¼SS(m).",
    "he normal equations. (d) Show that H0:t1¼t2¼t3is testable. Find ^b0X0y¼SS(m,t) and ^b0 2X0 2y¼SS(m). (e) Construct an ANOVA table for the test of H0:t1¼t2¼t3. 12.26 Consider the model yijk¼mþaiþbjþgijþ1ijk,i¼1,2,j¼1,2, k¼1,2,3. (a) Write X0X,X0y, and the normal equations. (b) Find a set of linearly independent estimable functions. Are a1/C0a2and b1/C0b2estimable? 12.27 Consider the model yijk¼mþaiþbjþgkþ1ijk,i¼1,2,j¼1,2, k¼1,2. (a) Write X0X,X0y, and the normal equations.",
    "ider the model yijk¼mþaiþbjþgkþ1ijk,i¼1,2,j¼1,2, k¼1,2. (a) Write X0X,X0y, and the normal equations. (b) Find a set of linearly independent estimable functions. (c) Deﬁne appropriate side conditions, and ﬁnd the resulting solution to the normal equations. (d) Show that H0:a1¼a2is testable. Find ^b0X0y¼SS(m,a,b,g) and ^b0 2X0 2y¼SS(m,b,g). (e) Construct an ANOVA table for the test of H0:a1¼a2.",
    "^b0X0y¼SS(m,a,b,g) and ^b0 2X0 2y¼SS(m,b,g). (e) Construct an ANOVA table for the test of H0:a1¼a2. 12.28 For the model yijk¼mþaiþbjþgijþ1ijk,i¼1,2,j¼1,2,k¼1,2i n (12.56), write X0Xand obtain the normal equations in (12.58).338 ANALYSIS-OF-VARIANCE MODELS --- Page 350 --- 13 One-Way Analysis-of-Variance: Balanced Case The one-way analysis-of-variance (ANOVA) model has been illustrated in Sections 12.1.1, 12.2.2, 12.3.1, 12.5, and 12.6. We now analyze this model more fully.",
    "llustrated in Sections 12.1.1, 12.2.2, 12.3.1, 12.5, and 12.6. We now analyze this model more fully. To solve the normal equations in Section 13.3, we use side conditions as well as a gen- eralized inverse approach. For hypothesis tests in Section 13.4, we use both the full– reduced-model approach and the general linear hypothesis. Expected mean squares are obtained in Section 13.5 using both a full–reduced-model approach and a general linear hypothesis approach.",
    "d in Section 13.5 using both a full–reduced-model approach and a general linear hypothesis approach. In Section 13.6, we discuss contrasts on the means, including orthogonal polynomials. Throughout this chapter, we consider only the balanced model. The unbalanced case is discussed in Chapter 15.",
    "t this chapter, we consider only the balanced model. The unbalanced case is discussed in Chapter 15. 13.1 THE ONE-WAY MODEL The one-way balanced model can be expressed as follows: yij¼mþaiþ1ij,i¼1,2,...,k,j¼1,2,...,n: (13 :1) Ifa1,a2,...,akrepresent the effects of ktreatments, each of which is applied to n experimental units, then yijis the response of the jth observation among the nunits that receive the ith treatment.",
    "its, then yijis the response of the jth observation among the nunits that receive the ith treatment. For example, in an agricultural experiment, the treat- ments may be different fertilizers or different amounts of a given fertilizer. On the other hand, in some experimental situations, the kgroups may represent samples from kpopulations whose means we wish to compare, populations that are not created by applying treatments.",
    "populations whose means we wish to compare, populations that are not created by applying treatments. For example, suppose that we wish to compare the average lifetimes of several brands of batteries or the mean grade-point averages for freshmen, sophomores, juniors, and seniors. Three additional assumptions that form part of the model in (13.1) are 1.E(1ij)¼0 for all i,j. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G.",
    "are 1.E(1ij)¼0 for all i,j. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 339 --- Page 351 --- 2. var(1ij)¼s2for all i,j. 3. cov(1ij,1rs)¼0 for all ( i,j)=(r,s). 4. We sometimes add the assumption that 1ijis distributed as N(0,s2). 5. In addition, we often use the constraint (side condition)Pk i¼1ai¼0. The mean for the ith treatment or population can be denoted by mi.",
    "raint (side condition)Pk i¼1ai¼0. The mean for the ith treatment or population can be denoted by mi. Thus Eij¼mi, and using assumption 1, we have mi¼mþai. We can thus write (13.1) in the form yij¼miþ1ij,i¼1,2,...,k,j¼1,2,...,n: (13 :2) In this form of the model, the hypothesis H0:m1¼m2¼/C1/C1/C1¼mkis of interest. In the context of design of experiments, the one-way layout is sometimes called a completely randomized design .",
    "xt of design of experiments, the one-way layout is sometimes called a completely randomized design . In this design, the experimental units are assigned at random to the ktreatments. 13.2 ESTIMABLE FUNCTIONS To illustrate the model (13.1) in matrix form, let k¼3 and n¼2.",
    "treatments. 13.2 ESTIMABLE FUNCTIONS To illustrate the model (13.1) in matrix form, let k¼3 and n¼2. The resulting six equations, yij¼mþaiþ1ij,i¼1,2,3,j¼1,2, can be expressed as y11 y12 y21 y22 y31 y320 BBBBBBBB@1 CCCCCCCCA¼mþa1 mþa1 mþa2 mþa2 mþa3 mþa30 BBBBBBBB@1 CCCCCCCCAþ1 11 112 121 122 131 1320 BBBBBBBB@1 CCCCCCCCA ¼1100 1100 1010 1010 1001 10010 BBBBBBBB@1 CCCCCCCCAm a1 a2 a30 BBB@1 CCCAþ1 11 112 121 122 131 1320 BBBBBBBB@1 CCCCCCCCA, (13 :3) or y¼X bþ1: In (13.3), Xis 6/C24 and is clearly of rank 3 because the ﬁrst column is the sum of the other three columns.",
    "), Xis 6/C24 and is clearly of rank 3 because the ﬁrst column is the sum of the other three columns. Thus b¼(m,a1,a2,a3)0is not unique and not estimable; hence340 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 352 --- the individual parameters m,a1,a2,a3cannot be estimated unless they are subject to constraints (side conditions). In general, the Xmatrix for the one-way balanced model iskn/C2(kþ1) of rank k. We discussed estimable functions l0bin Section 12.2.2.",
    "ne-way balanced model iskn/C2(kþ1) of rank k. We discussed estimable functions l0bin Section 12.2.2. It was shown in Problem 12.10 that for the one-way balanced model, contrasts in the a’s are esti- mable. ThusP iciaiis estimable if and only ifP ici¼0. For example, contrasts such asa1/C0a2anda1/C02a2þa3are estimable. If we impose a side condition on the ai’s and denote the constrained parameters as m/C3anda/C3 i, thenm/C3,a/C31,...,a/C3kare uniquely deﬁned and estimable.",
    "constrained parameters as m/C3anda/C3 i, thenm/C3,a/C31,...,a/C3kare uniquely deﬁned and estimable. Under the usual side condition,Pk i¼1a/C3 i¼0, the parameters are deﬁned as m/C3¼/C22m:and a/C3i¼mi/C0/C22m:, where /C22m:¼Pk i¼1mi=k.",
    "i¼1a/C3 i¼0, the parameters are deﬁned as m/C3¼/C22m:and a/C3i¼mi/C0/C22m:, where /C22m:¼Pk i¼1mi=k. To see this, we rewrite (13.1) and (13.2) in the form E(yij)¼mi¼m/C3þa/C3 ito obtain /C22m:¼Xk i¼1mi k¼X im/C3þa/C3i k ¼m/C3þX ia/C3i k¼m/C3: (13 :4) Then, from mi¼m/C3þa/C3i,w eh a v e a/C3 i¼mi/C0m/C3¼mi/C0/C22m:: (13 :5) 13.3 ESTIMATION OF PARAMETERS 13.3.1 Solving the Normal Equations Extending (13.3) to a general kandn, the one-way model can be written in matrix form as y1 y2 ...",
    "ns Extending (13.3) to a general kandn, the one-way model can be written in matrix form as y1 y2 ... yk0 BBB@1 CCCA¼jj0 /C1/C1/C1 0 j0j /C1/C1/C1 0 ............ j00 /C1/C1/C1 j0 BBB@1 CCCAm a1 a2 ... ak0 BBBBB@1 CCCCCAþ11 12 ... 1k0 BBB@1 CCCA, (13 :6) or y¼Xbþ1,13.3 ESTIMATION OF PARAMETERS 341 --- Page 353 --- where jand0are each of size n/C21, and yiand1iare deﬁned as yi¼yi1 yi2 ... yin0 BBB@1 CCCA,1i¼1i1 1i2 ...",
    "jand0are each of size n/C21, and yiand1iare deﬁned as yi¼yi1 yi2 ... yin0 BBB@1 CCCA,1i¼1i1 1i2 ... 1in0 BBB@1 CCCA: For (13.6), the normal equations X0X^b¼X0ytake the form kn n n /C1/C1/C1 n nn 0/C1/C1/C1 0 n0n/C1/C1/C1 0 ............ n00 /C1/C1/C1 n0 BBBBB@1 CCCCCA^ m ^a1 ^a2 ... ^ak0 BBBBB@1 CCCCCA¼y :: y1: y2: ... yk:0 BBBBB@1 CCCCCA, (13 :7) where y ::¼P ijyijandyi:¼P jyij.",
    ". ^ak0 BBBBB@1 CCCCCA¼y :: y1: y2: ... yk:0 BBBBB@1 CCCCCA, (13 :7) where y ::¼P ijyijandyi:¼P jyij. In Section 13.3.1.1, we ﬁnd a solution of (13.7) using side conditions, and in Section 13.3.1.2 we ﬁnd another solution using a generalized inverse of X0X.",
    "side conditions, and in Section 13.3.1.2 we ﬁnd another solution using a generalized inverse of X0X. 13.3.1.1 Side Conditions Thekþ1 normal equations in (13.7) can be expressed as kn^mþn^a1þn^a2þ/C1/C1/C1þ n^ak¼y::, n^mþn^ai¼yi:,i¼1,2,...,k: (13 :8) Using the side conditionP i^ai¼0, the solution to (13.8) is given by ^m¼y:: kn¼/C22y::, ^ai¼yi: n/C0^m¼/C22yi:/C0/C22y::,i¼1,2,...,k: (13 :9) In vector form, this solution ^bforX0X^b¼X0yis expressed as ^b¼/C22y:: /C22y1:/C0/C22y:: ...",
    "(13 :9) In vector form, this solution ^bforX0X^b¼X0yis expressed as ^b¼/C22y:: /C22y1:/C0/C22y:: ... /C22yk:/C0/C22y::0 BBB@1 CCCA: (13 :10) If the side conditionP ia/C3 i¼0 is imposed on the parameters, then the elements of ^bare unique estimators of the (constrained) parameters m/C3¼/C22m:and342 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 354 --- a/C3 i¼mi/C0/C22m:,i¼1,2,...,k, in (13.4) and (13.5). Otherwise, the estimators in (13.9) or (13.10) are to be used in estimable functions.",
    "4) and (13.5). Otherwise, the estimators in (13.9) or (13.10) are to be used in estimable functions. For example, by Theorem 12.3a(i), the estimator of l0b¼a1/C0a2is given by l0^b: l0^b¼da1/C0a2¼^a1/C0^a2¼/C22y1:/C0/C22y::/C0(/C22y2:/C0/C22y::)¼/C22y1:/C0/C22y2:: By Theorem 12.3d, such estimators are BLUE. If 1ijisN(0,s2), then, by Theorem 12.3h, the estimators are minimum variance unbiased estimators.",
    "E. If 1ijisN(0,s2), then, by Theorem 12.3h, the estimators are minimum variance unbiased estimators. 13.3.1.2 Generalized Inverse By Corollary 1 to Theorem 2.8b, a generalized inverse of X0Xin (13.7) is given by ðX0X)/C0¼00 /C1/C1/C1 0 01 n/C1/C1/C1 0 ......... 00 /C1/C1/C11 n0 BBBBBB@1 CCCCCCA: (13 :11) Then by (12.13) and (13.7), a solution to the normal equations is obtained as ^ b¼(X0X)/C0X0y¼0 /C22y1: ...",
    "(12.13) and (13.7), a solution to the normal equations is obtained as ^ b¼(X0X)/C0X0y¼0 /C22y1: ... /C22yk:0 BBB@1 CCCA: (13 :12) The estimators in (13.12) are different from those in (13.10), but they give the same estimates of estimable functions. For example, using bˆfrom (13.12) to estimate l0b¼a1/C0a2,w eh a v e l0^b¼da1/C0a2¼^a1/C0^a2¼/C22y1:/C0/C22y2:, which is the same estimate as that obtained above in Section 13.3.1.1 using bˆfrom (13.10).",
    "/C22y2:, which is the same estimate as that obtained above in Section 13.3.1.1 using bˆfrom (13.10). 13.3.2 An Estimator for s2 In assumption 2 for the one-way model in (13.1), we have var( 1ij)¼s2for all i,j.T o estimates2, we use (12.22) s2¼SSE k(n/C01),13.3 ESTIMATION OF PARAMETERS 343 --- Page 355 --- where SSE is as given by (12.20) or (12.21): SSE ¼y0y/C0^b0X0y¼y0½I/C0X(X0X)/C0X0/C138y: The rank of the idempotent matrix I/C0X(X0X)/C0X0iskn2kbecause rank( X)¼k, tr(I)¼kn, and tr[ X(X0X)/C0X0/C138¼k(see Theorem 2.13d).",
    "ix I/C0X(X0X)/C0X0iskn2kbecause rank( X)¼k, tr(I)¼kn, and tr[ X(X0X)/C0X0/C138¼k(see Theorem 2.13d). Then s2¼SSE =k(n/C01) is an unbiased estimator of s2[see Theorem 12.3e(i)].",
    "38¼k(see Theorem 2.13d). Then s2¼SSE =k(n/C01) is an unbiased estimator of s2[see Theorem 12.3e(i)]. Usingbˆfrom (13.12), we can express SSE ¼y0y/C0^b0Xyin the following form: SSE ¼y0y/C0^b0X0y¼Xk i¼1Xn j¼1y2 ij/C0Xk i¼1/C22yi:yi: ¼X ijy2 ij/C0X iy2 i: n: (13 :13) It can be shown (see Problem 13.4) that (13.13) can be written as SSE ¼X ij(yij/C0/C22yi:)2: (13 :14) Thus s2is given by either of the two forms s2¼P ij(yij/C0/C22yi:)2 k(n/C01)(13 :15) ¼P ijy2 ij/C0P iy2i:=n k(n/C01): (13 :16) 13.4 TESTING THE HYPOTHESIS H0:m15m25...5mk Using the model in (13.2), the hypothesis of equality of means can be expressed as H0:m1¼m2¼/C1/C1/C1¼mk.",
    "the model in (13.2), the hypothesis of equality of means can be expressed as H0:m1¼m2¼/C1/C1/C1¼mk. The alternative hypothesis is that at least two means are unequal. Using mi¼mþai[see (13.1) and (13.2)], the hypothesis can be expressed asH0:a1¼a2¼/C1/C1/C1¼ak, which is testable because it can be written in terms of k21 linearly independent estimable contrasts, for example, H0:a1/C0a2¼a1/C0 a3¼/C1/C1/C1¼a1/C0ak¼0 (see the second paragraph in Section 12.7.1).",
    ", for example, H0:a1/C0a2¼a1/C0 a3¼/C1/C1/C1¼a1/C0ak¼0 (see the second paragraph in Section 12.7.1). In Section 13.4.1 we develop the test using the full–reduced-model approach, and in Section 13.4.2 we use the general linear hypothesis approach. In the model y¼Xbþ1, the vector yiskn/C21 [see (13.6)]. Throughout Section 13.4, we assume that yis NknðXb,s2I).",
    "y¼Xbþ1, the vector yiskn/C21 [see (13.6)]. Throughout Section 13.4, we assume that yis NknðXb,s2I). 13.4.1 Full–Reduced-Model Approach The hypothesis H0:a1¼a2¼/C1/C1/C1¼ak ð13:17)344 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 356 --- is equivalent to H0:a/C3 1¼a/C32¼/C1/C1/C1¼a/C3k, (13 :18) where the a/C3 iterms are subject to the side conditionP ia/C3i¼0.",
    "1¼a/C32¼/C1/C1/C1¼a/C3k, (13 :18) where the a/C3 iterms are subject to the side conditionP ia/C3i¼0. With this constraint, H0in (13.18) is also equivalent to H0:a/C3 1¼a/C32¼/C1/C1/C1¼a/C3k¼0: (13 :19) The full model, yij¼mþaiþ1ij,i¼1,2,...,k,j¼1,2,...,n,is expressed in matrix form y¼Xbþ1in (13.6). If the full model is written in terms of m/C3 andai/C3asyij¼m/C3þa/C3 iþ1ij, then the reduced model under H0in (13.19) is yij¼m/C3þ1ij. In matrix form, this becomes y¼m/C3jþ1, where jiskn/C21.",
    "d model under H0in (13.19) is yij¼m/C3þ1ij. In matrix form, this becomes y¼m/C3jþ1, where jiskn/C21. To be consistent with the full model y¼Xbþ1, we write the reduced model as y¼mjþ1: (13 :20) For the full model, the sum of squares SS( m,a)¼^b0X0yis given as part of (13.13) as SS(m,a)¼^b0X0y¼Xk i¼1y2i: n, where the sum of squares SS( m,a1,...,ak) is abbreviated as SS( m,a).",
    "s SS(m,a)¼^b0X0y¼Xk i¼1y2i: n, where the sum of squares SS( m,a1,...,ak) is abbreviated as SS( m,a). For the reduced model in (13.20), the estimator “ ^b¼(X0X)/C01X0y” and the sum of squares “^b0X0y” become ^m¼(j0j)/C01j0y¼1 kny::¼/C22y::, (13 :21) SS(m)¼(^m)0j0y¼/C22y::y::¼y2:: kn, (13 :22) where jiskn/C21.",
    "¼(j0j)/C01j0y¼1 kny::¼/C22y::, (13 :21) SS(m)¼(^m)0j0y¼/C22y::y::¼y2:: kn, (13 :22) where jiskn/C21. From Table 12.3, the sum of squares for the a’s adjusted for mis given by SS(ajm)¼SS(m,a)/C0SS(m)¼^b0X0y/C0y2:: kn ¼1 nXk i¼1y2 i:/C0y2 :: kn(13 :23) ¼nXk i¼1(/C22yi:/C0/C22y::)2: (13 :24)13.4 TESTING THE HYPOTHESIS H0:m1¼m2¼...¼mk 345 --- Page 357 --- The test is summarized in Table 13.1 using SS( ajm) in (13.23) and SSE in (13.13).",
    "--- Page 357 --- The test is summarized in Table 13.1 using SS( ajm) in (13.23) and SSE in (13.13). The chi-square and independence properties of SS( ajm) and SSE follow from results established in Section 12.7.2. To facilitate comparison of (13.23) with the result of the general linear hypothesis approach in Section 13.4.2, we now express SS( ajm) as a quadratic form in y.B y (12.13), ^b¼ðX0X)/C0X0y, and therefore ^b0X0y¼y0XðX0X)/C0X0y.",
    "SS( ajm) as a quadratic form in y.B y (12.13), ^b¼ðX0X)/C0X0y, and therefore ^b0X0y¼y0XðX0X)/C0X0y. Then with (13.21) and (13.22), we can write SS(ajm)¼^b0X0y/C0y2 :: kn ¼y0X(X0X)/C0X0y/C0y0jkn(j0 knjkn)/C01j0kny ¼y0X(X0X)/C0X0y/C0y0jknj0 kn kn/C18/C19 y ¼y0[X(X0X)/C0X0/C01 knJkn/C21 y: (13 :25) Using some results in the answer to Problem 13.3, this can be expressed as SS(ajm)¼y01 nJO /C1/C1/C1 O OJ /C1/C1/C1 O ......... OO /C1/C1/C1 J0 BBBB@1 CCCCA/C01 knJJ /C1/C1/C1 J JJ /C1/C1/C1 J .........",
    "OJ /C1/C1/C1 O ......... OO /C1/C1/C1 J0 BBBB@1 CCCCA/C01 knJJ /C1/C1/C1 J JJ /C1/C1/C1 J ......... JJ /C1/C1/C1 J0 BBBB@1 CCCCA2 666643 77775y (13 :26) ¼1 kny0(k/C01)J /C0J /C1/C1/C1 /C0 J /C0J (k/C01)J/C1/C1/C1 /C0 J .........",
    "2 666643 77775y (13 :26) ¼1 kny0(k/C01)J /C0J /C1/C1/C1 /C0 J /C0J (k/C01)J/C1/C1/C1 /C0 J ......... /C0J /C0J /C1/C1/C1 (k/C01)J0 BBBB@1 CCCCAy, where each Jin (13.26) and (13.27) is n/C2n.TABLE 13.1 ANOVA for Testing H0:a1¼a2¼/C1/C1/C1¼akin the One-Way Model Source of Variation df Sum of SquaresMean Square FStatistic Treatments k21 SSðajmÞ¼1 nX iy2 i:/C0y2 :: knSSðajmÞ k/C01SSðajmÞ=ðk/C01Þ SSE =kðn/C01Þ Error k(n21) SSE ¼X ijy2 ij/C01 nX iy2 i:SSE kðn/C01Þ— Total kn 21SST ¼X ijy2ij/C0y2 :: kn346 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 358 --- Example 13.4.",
    "T ¼X ijy2ij/C0y2 :: kn346 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 358 --- Example 13.4. Three methods of packaging frozen foods were compared by Daniel (1974, p. 196). The response variable was ascorbic acid (mg /100g). The data are in Table 13.2.",
    "niel (1974, p. 196). The response variable was ascorbic acid (mg /100g). The data are in Table 13.2. To make the test comparing the means of the three methods, we calculate y2 :: kn¼(419 :95)2 (3)(7)¼8298 :0001 , 1 7X3 i¼1y2 i:¼1 7(120 :06)2þ(135 :18)2þ(164 :71)2/C2/C3 ¼17(59,817 :4201) ¼8545 :3457 , X 3 i¼1X7 j¼1y2 ij¼8600 :3127 : The sums of squares for treatments, error, and total are then SS(ajm)¼1 7X3 i¼1y2 i:/C0y2 :: 21¼8545 :3457 /C08398 :0001 ¼147 :3456 , SSE ¼X ijy2 ij/C01 7X iy2 i:¼8600 :3127 /C08545 :3457 ¼54:9670 , SST ¼X ijy2ij/C0y2 :: 21¼8600 :3127 /C08398 :0001 ¼202 :3126 : These sums of squares can be used to obtain an Ftest, as in Table 13.3.",
    "7 /C08398 :0001 ¼202 :3126 : These sums of squares can be used to obtain an Ftest, as in Table 13.3. The pvalue forF¼24.1256 is 8.07 /C21026. Thus we reject H0:m1¼m2¼m3.",
    "ain an Ftest, as in Table 13.3. The pvalue forF¼24.1256 is 8.07 /C21026. Thus we reject H0:m1¼m2¼m3. ATABLE 13.2 Ascorbic Acid (mg /100g) for Three Packaging Methods Method A B C 14.29 20.06 20.04 19.10 20.64 26.23 19.09 18.00 22.74 16.25 19.56 24.0415.09 19.47 23.3716.61 19.07 25.0219.63 18.38 23.27 Totals (y i.) 120.06 135.18 164.71 Means (/C22yi) 17.15 19.31 23.5313.4 TESTING THE HYPOTHESIS H0:m1¼m2¼...¼mk 347 --- Page 359 --- 13.4.2 General Linear Hypothesis For simplicity of exposition, we illustrate all results in this section with k¼4.",
    "Linear Hypothesis For simplicity of exposition, we illustrate all results in this section with k¼4. In this case,b¼(m,a1,a2,a3,a4)0, and the hypothesis is H0:a1¼a2¼a3¼a4. Using three linearly independent estimable contrasts, the hypothesis can be written in the form H0:a1/C0a2 a1/C0a3 a1/C0a40 @1A¼0 0 00 @1A, which can be expressed as H 0:Cb¼0,where C¼01 /C0100 01 0 /C010 01 0 0 /C010@1A: (13 :28) The matrix Cin (13.28) used to express H 0:a1¼a2¼a3¼a4is not unique.",
    "/C010 01 0 0 /C010@1A: (13 :28) The matrix Cin (13.28) used to express H 0:a1¼a2¼a3¼a4is not unique. Other contrasts could be used in C, for example C1¼01 /C0100 00 1 /C010 00 0 1 /C010@1A orC 2¼01 1 /C01/C01 01 /C0100 00 0 1 /C010@1A: From (12.13) and Theorem 12.7b(iii), we have SSH ¼(C^ b)0[C(X0X)/C0C0]/C01C^b ¼y0X(X0X)/C0C0[C(X0X)/C0C0]/C01C(X0X)/C0X0y: (13 :29)TABLE 13.3 ANOVA for the Ascorbic Acid Data in Table 13.2 Source dfSum of SquaresMean Square F Method 2 147.3456 73.6728 24.1256 Error 18 54.9670 3.0537 — Total 20 202.3126348 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 360 --- Using Cin (13.28) and ( X0X)/C0in (13.11), we obtain C(X0X)/C0C0¼1 n01 /C0100 01 0 /C010 0 100 /C010 B@1 CA00000 0100000100 00010 000010 BBBBBB@1 CCCCCCA00 0 11 1 /C0100 0/C010 00 /C010 BBBBBB@1 CCCCCCA ¼ 1 n211 121 1120 B@1 CA: (13 :30) To ﬁnd the inverse of (13.30), we write it in the form C(X0X)/C0C0¼1 n100 0100010 @1Aþ111 1111110 @1A2435¼ 1 n(I3þj3j0 3): Then by (2.53), the inverse is [C(X0X)/C0C0]/C01¼nI3/C0I/C01 3j3j0 3I/C01 3 1þj0 3I/C01 3j3/C18/C19 ¼nI3/C01 4J3/C18/C19 , (13 :31) where J3is 3/C23.",
    "nI3/C0I/C01 3j3j0 3I/C01 3 1þj0 3I/C01 3j3/C18/C19 ¼nI3/C01 4J3/C18/C19 , (13 :31) where J3is 3/C23. ForC(X0X)/C0X0in (13.29), we obtain C(X0X)/C0X0¼1nj 0 n/C0j0n0000 j0 n 00/C0j0n 00 j0 n 0000/C0j0n0 @1A¼ 1 nA, (13 :32) where j0nand00are 1 /C2n.",
    "X0¼1nj 0 n/C0j0n0000 j0 n 00/C0j0n 00 j0 n 0000/C0j0n0 @1A¼ 1 nA, (13 :32) where j0nand00are 1 /C2n. Using (13.31) and (13.32), the matrix of the quadratic form for SSH in (13.29) can be expressed as X(X0X)/C0C0[C(X0X)/C0C0/C138/C01C(X0X)/C0X0¼1nA 0nI3/C014J 3/C18/C191nA ¼1nA 0I3A/C01 4nA0J3A: (13 :33)13.4 TESTING THE HYPOTHESIS H0:m1¼m2¼...¼mk 349 --- Page 361 --- The ﬁrst term of (13.33) is given by 1 nA0A¼1nj n jn jn /C0jn00 0/C0jn0 00 /C0jn0 BBB@1 CCCAj 0 n/C0j0n0000 j0 n 00/C0j0n 00 j0 n 0000/C0j0n0 B@1 CA ¼1 n3Jn/C0Jn/C0Jn/C0Jn /C0Jn Jn OO /C0Jn OJ n O /C0Jn OOJ n0 BBB@1 CCCA, (13 :34) since j nj0 n¼Jnandjn00¼O, where Oisn/C2n.",
    "C0Jn Jn OO /C0Jn OJ n O /C0Jn OOJ n0 BBB@1 CCCA, (13 :34) since j nj0 n¼Jnandjn00¼O, where Oisn/C2n. Similarly (see Problem 13.10), the second term of (13.33) is given by 1 4nA0J3A¼1 4n9Jn/C03Jn/C03Jn/C03Jn /C03Jn Jn Jn Jn /C03Jn Jn Jn Jn /C03Jn Jn Jn Jn0 BB@1 CCA: (13 :35) Then (13.33) becomes 1 4n(4A0A)/C01 4nA0J3A¼1 4n12Jn/C04Jn/C04Jn/C04Jn /C04Jn 4Jn OO /C04Jn O 4Jn O /C04Jn OO 4Jn0 BBB@1 CCCA /C01 4n9Jn/C03Jn/C03Jn/C03Jn /C03Jn Jn Jn Jn /C03Jn Jn Jn Jn /C03Jn Jn Jn Jn0 BBB@1 CCCA ¼1 4n3Jn/C0Jn/C0Jn/C0Jn /C0Jn3Jn/C0Jn/C0Jn /C0Jn/C0Jn3Jn/C0Jn /C0Jn/C0Jn/C0Jn3Jn0 BBB@1 CCCA¼1 4nB: (13 :36) Note that the matrix for SSH in (13.36) is the same as the matrix for SS(ajm)i n (13.27) with k¼4.350 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 362 --- For completeness, we now express SSH in (13.29) in terms of the yij’s.",
    "ALANCED CASE --- Page 362 --- For completeness, we now express SSH in (13.29) in terms of the yij’s. We begin by writing (13 .36) in the form 1 4nB¼1 4n4JnOOO O 4JnOO OO 4JnO OO O 4Jn0 BBB@1 CCCA/C01 4nJnJnJnJn JnJnJnJn JnJnJnJn JnJnJnJn0 BBB@1 CCCA ¼1 nJnOOO OJ nOO OO J nO OOO J n0 BBB@1 CCCA/C01 4nJ4n: Using y0¼(y0 1,y02,y03,y04) as deﬁned in (13.6), SSH in (13.29) becomes SSH ¼y0X(X0X)/C0C0[C(X0X)/C0C0/C138/C01C(X0X)/C0X0y ¼y01 4nB/C18/C19 y ¼1 n(y0 1;y02;y03;y04)JnOOO OJ nOO OO J nO OOO J n0 BBB@1 CCCAy 1 y2 y3 y40 BBB@1 CCCA/C01 4ny0J4ny ¼1 nX4 i¼1y0 iJnyi/C01 4ny0J4ny ¼1 nX4 i¼1y0 ijnj0nyi/C01 4ny0j4nj04ny ¼1 nX4 i¼1y2 i:/C01 4ny2::, which is the same as SS( ajm) in (13.23).",
    "¼1y0 ijnj0nyi/C01 4ny0j4nj04ny ¼1 nX4 i¼1y2 i:/C01 4ny2::, which is the same as SS( ajm) in (13.23). 13.5 EXPECTED MEAN SQUARES The expected mean squares for a one-way ANOVA are given in Table 13.4. The expected mean squares are deﬁned as E[SS(ajm)=(k/C01)] and ESSE =k(n/C01)/C138 ½ .",
    "n Table 13.4. The expected mean squares are deﬁned as E[SS(ajm)=(k/C01)] and ESSE =k(n/C01)/C138 ½ . The result is given in terms of parameters a/C3 isuch thatP ia/C3i¼0.13.5 EXPECTED MEAN SQUARES 351 --- Page 363 --- IfH0:a/C3 1¼a/C32¼/C1/C1/C1¼a/C3k¼0 is true, both of the expected mean squares are equal to s2, and we expect Fto be close to 1. On the other hand, if H0is false, E[SS(ajm)=(k/C01)].E[SSE =k(n/C01)], and we expect Fto exceed 1. We therefore reject H0for large values of F.",
    "=(k/C01)].E[SSE =k(n/C01)], and we expect Fto exceed 1. We therefore reject H0for large values of F. The expected mean squares in Table 13.4 can be derived using the model yij¼m/C3þa/C3iþ1ijinE[SS(ajm)] and E(SSE) (see Problem 13.11). In Sections 13.5.1 and 13.5.2, we obtain the expected mean squares using matrix methods similar to those in Sections 13.4.1 and 13.4.2.",
    "btain the expected mean squares using matrix methods similar to those in Sections 13.4.1 and 13.4.2. 13.5.1 Full–Reduced-Model Approach For the error term in Table 13.4, we have E(SSE) ¼Efy0[I/C0X(X0X)/C0X0]yg¼k(n/C01)s2; (13 :37) which was proved in Theorem 12.3e(i). Using a full–reduced-model approach the sum of squares for the a’s adjusted for mis given by (13.25) as SS( ajm)¼y0X(X0X)/C0X0y/C0y0[(1=kn)Jkn]y.",
    "f squares for the a’s adjusted for mis given by (13.25) as SS( ajm)¼y0X(X0X)/C0X0y/C0y0[(1=kn)Jkn]y. Thus E½SS(ajm)/C138¼E[y0X(X0X)/C0X0y]/C0Ey01 knJkn/C18/C19 y/C20/C21 : (13 :38) Using Theorem 5.2a, the ﬁrst term on the right side of (13.38) becomes E[y0X(X0X)/C0X0y]¼tr[X(X0X)/C0X0s2I]þ(Xb)0X(X0X)/C0X0(Xb) ¼s2tr[X(X0X)/C0X0]þb0X0X(X0X)/C0X0Xb ¼s2tr[X(X0X)/C0X0]þb0X0Xb [by (2 :58)] : (13 :39) By Theorem 2.13f, the matrix X(X0X)/C0X0is idempotent.",
    "r[X(X0X)/C0X0]þb0X0Xb [by (2 :58)] : (13 :39) By Theorem 2.13f, the matrix X(X0X)/C0X0is idempotent. Hence, by Theorems 2.13d and 2.8c(v), we obtain tr[X(X0X)/C0X0]¼rank[X(X0X)/C0X]¼rank(X)¼k: (13 :40)TABLE 13.4 Expected Mean Squares for One-Way ANOVA Source of Variation dfSum of SquaresMean SquareExpected Mean Squares Treatments k21S S ðajmÞ SSðajmÞ k/C01s2þn k/C01Pk i¼1a/C32 i Error k(n21) SSE SSE kðn/C01Þs2 Total kn 21P ijy2 ij/C0y2:: kn352 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 364 --- To evaluate the second term on the right side of (13.39), we use X0Xin (13.7) and useb0¼(m/C3,a/C3 1,...,a/C3k) subject toP ia/C3i¼0.",
    "e right side of (13.39), we use X0Xin (13.7) and useb0¼(m/C3,a/C3 1,...,a/C3k) subject toP ia/C3i¼0. Then b0X0Xb¼n(m/C3,a/C3 1,...,a/C3k)k11 ... 1 110 ... 0 101 ... 0 ............ 100 ... 10 BBBBBBBBB@1 CCCCCCCCCAm/C3 a/C3 1 ... a/C3 k0 BBBBBB@1 CCCCCCA ¼nk m/C3þX ia/C3 i,m/C3þa/C31,...,m/C3þa/C3k !m/C3 a/C3 1 ... a/C3 k0 BBBBBB@1 CCCCCCA ¼nk m/C32þX i(m/C3þa/C3 i)a/C3i\"# ¼nkm/C32þm/C3X ia/C3iþX ia/C32 i !",
    "1 ... a/C3 k0 BBBBBB@1 CCCCCCA ¼nk m/C32þX i(m/C3þa/C3 i)a/C3i\"# ¼nkm/C32þm/C3X ia/C3iþX ia/C32 i ! ¼knm/C32þnX ia/C32 i: (13 :41) Hence, using (13.40) and (13.41), E[y0X(X0X)/C0X0y] in (13.39) becomes E[y0X(X0X)/C0X0y]¼ks2þknm/C32þnX ia/C32 i: (13 :42) For the second term on the right side of (13.38), we obtain Ey01 knJkn/C18/C19 y/C20/C21 ¼s2tr1 knJkn/C18/C19 þb0X01 knJkn/C18/C19 Xb ¼s2kn knþ1 knb0X0jknj0knXb ¼s2þ1 kn(b0X0jkn)(j0knXb): (13 :43)13.5 EXPECTED MEAN SQUARES 353 --- Page 365 --- Using Xas given in (13.6), j0 knXbbecomes j0 knXb¼(j0n,j0n,...,j0n)jnjn0/C1/C1/C1 0 jn0jn/C1/C1/C1 0 ............",
    "en in (13.6), j0 knXbbecomes j0 knXb¼(j0n,j0n,...,j0n)jnjn0/C1/C1/C1 0 jn0jn/C1/C1/C1 0 ............ jn00 /C1/C1/C1 jn0 BBBB@1 CCCCAm/C3 a/C3 1 ... a/C3 k0 BBBB@1 CCCCA ¼(kn,n,n,...,n)m/C3 a/C3 1 ...",
    ". jn00 /C1/C1/C1 jn0 BBBB@1 CCCCAm/C3 a/C3 1 ... a/C3 k0 BBBB@1 CCCCA ¼(kn,n,n,...,n)m/C3 a/C3 1 ... a/C3 k0 BBBB@1 CCCCA(since j 0 njn¼n) ¼knm/C3þnXk i¼1a/C3i¼knm/C3/C18 sinceX ia/C3i¼0/C19 : The second term on the right side of (13.43) is then given by 1 kn(b0X0jkn)(j0knXb)¼1 kn(j0Xb)2¼k2n2m/C32 kn¼knm/C32, so that (13.43) becomes Ey01 knJkn/C18/C19 y/C20/C21 ¼s2þknm/C32: (13 :44) Now, using (13.42) and (13.44), E[SS(ajm)] in (13.38) becomes E[SS(ajm)]¼ks2þknm/C32þnXk i¼1a/C32 i/C0(s2þknm/C32) ¼(k/C01)s2þnX ia/C32 i: (13 :45) 13.5.2 General Linear Hypothesis To simplify exposition, we use k¼4 to illustrate results in this section, as was done in Section 13.4.2.",
    "implify exposition, we use k¼4 to illustrate results in this section, as was done in Section 13.4.2. It was shown in Section 13.4.2 that SSH ¼(C^b)0[C(X0X)/C0C0]/C01C^b is the same as SS( ajm)¼P iy2 i:=n/C0y2::=knin (13.23). Note that for k¼4,Cis 3/C25 [see (13.28)] and C(X0X)/C0C0is 3 /C23 [see (13.30)].",
    "y2::=knin (13.23). Note that for k¼4,Cis 3/C25 [see (13.28)] and C(X0X)/C0C0is 3 /C23 [see (13.30)]. To obtain E[SS(ajm)], we ﬁrst note that by (12.44), (12.45), and (13.31), E(C^b)¼Cb, cov( C^b)¼ s2C(X0X)/C0C0, and [ C(X0X)/C0C0]/C01¼n(I3/C01 4J3).354 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 366 --- Then, by Theorem 5.2a, we have E[SS(ajm)]¼E{(C^b)0[C(X0X)/C0C0]/C01C^b} ¼tr{[C(X0X)/C0C0]/C01cov(C^b)}þ[E(C^b)]0[C(X0X)/C0C0]/C01E(C^b) ¼tr{[C(X0X)/C0C0]/C01s2C(X0X)/C0C0}þn(Cb)0[I3/C01 4J3]Cb ¼s2tr(I3)þnb0C0(I3/C014J3)Cb ¼3s2þnb0(C0C/C01 4C0J3C)b: (13 :46) Using Cin (13.28), we obtain C0C¼00000 03 /C01/C01/C01 0/C01100 0/C01010 0/C010010 BBBBBB@1 CCCCCCA, (13 :47) C 0J3C¼00000 09 /C03/C03/C03 0/C03111 0/C03111 0/C031110 BBBB@1 CCCCA: (13 :48) From (13.47) and (13.48), we have C0C/C01 4C0J3C¼14(4C0C/C0C0J3C) ¼1 400000 03 /C01/C01/C01 0/C013 /C01/C01 0/C01/C013 /C01 0/C01/C01/C0130 BBBBBB@1 CCCCCCA ¼ 1 400000 04000 00400 00040 000040 BBBBBB@1 CCCCCCA/C0 1 400000 01111 01111 01111 011110 BBBBBB@1 CCCCCCA ¼00 0 0I 4/C18/C19 /C01 4000 0J 4/C18/C19 :13.5 EXPECTED MEAN SQUARES 355 --- Page 367 --- Thus the second term on the right side of (13.46) is given by nb0(C0C/C01 4C0J3C)b ¼nb0000 0I 4/C18/C19 b/C014nb0000 0J 4/C18/C19 b ¼n(m/C3,a/C3 1,a/C32,a/C33,a/C34)000 0I 4/C18/C19m/C3 a/C3 1 a/C32 a/C33 a/C340 BBBBBB@1 CCCCCCA /C0 1 4n(m/C3,a/C3 1,a/C32,a/C33,a/C34)000 0J 4/C18/C19m/C3 a/C31 a/C32 a/C33 a/C340 BBBBBB@1 CCCCCCA ¼nX 4 i¼1a/C32 i/C01 4n0,X ia/C3 i,X ia/C3i,X ia/C3i,X ia/C3i !m/C3 a/C3 1 a/C32 a/C33 a/C340 BBBBBB@1 CCCCCCA ¼nX 4 i¼1a/C32 i: Hence, (13.46) becomes E[SS(ajm)]¼3s2þnX4 i¼1a/C32 i: (13 :49) This result is for the special case k¼4.",
    "ce, (13.46) becomes E[SS(ajm)]¼3s2þnX4 i¼1a/C32 i: (13 :49) This result is for the special case k¼4. For a general k, (13.49) becomes E[SS(ajm)]¼(k/C01)s2þnXk i¼1a/C32 i: For the case in which b0¼(m,a1,...,ak) is not subject toP iai¼0, see Problem 13.14.356 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 368 --- 13.6 CONTRASTS We noted in Section 13.2 that a linear combinationPk i¼1ciaiin thea’s is estimable if and only ifPki¼1ci¼0.",
    "d in Section 13.2 that a linear combinationPk i¼1ciaiin thea’s is estimable if and only ifPki¼1ci¼0. In Section 13.6.1, we develop a test of signiﬁcance for such contrasts. In Section 13.6.2, we show that if the contrasts are formulated appropri- ately, the sum of squares for treatments can be partitioned into k21 independent sums of squares for contrasts.",
    "sum of squares for treatments can be partitioned into k21 independent sums of squares for contrasts. In Section 13.6.3, we develop orthogonal polynomial contrasts for the special case in which the treatments have equally spaced quantitative levels.",
    "mial contrasts for the special case in which the treatments have equally spaced quantitative levels. 13.6.1 Hypothesis Test for a Contrast For the one-way model, a contrastP iciai, whereP ici¼0, is equivalent toP icimi since X cimi¼X ici(mþai)¼mX iciþX iciai¼X iciai: A hypothesis of interest is H0:X ciai¼0o r H0:X cimi¼0, (13 :50) which represents a comparison of means ifP ici¼0.",
    "f interest is H0:X ciai¼0o r H0:X cimi¼0, (13 :50) which represents a comparison of means ifP ici¼0. For example, the hypothesis H0:3m1/C0m2/C0m3/C0m4¼0 can be written as H0:m1¼1 3(m2þm3þm4), which compares m1with the average of m2,m3, andm4. The hypothesis in (13.50) can be expressed as H0:c0b¼0, where c0¼(0,c1,c2,...,ck) andb¼(m,a1,...,ak)0. Assuming that yisNkn(Xb,s2I), H0can be tested using Theorem 12.7c.",
    "1,c2,...,ck) andb¼(m,a1,...,ak)0. Assuming that yisNkn(Xb,s2I), H0can be tested using Theorem 12.7c. In this case, we have m¼1, and the test statistic becomes F¼(c0^b)0[c0(X0X)/C0c]/C01c0^b SSE =k(n/C01) ¼(c0^b)2 s2c0(X0X)/C0c(13 :51) ¼Pk i¼1ci/C22yi:/C16/C172 s2Pki¼1c2 i=n, (13 :52) where s2¼SSE =k(n/C01), and ( X0X)/C0andbˆare as given by (13.11) and (13.12).",
    "Pki¼1c2 i=n, (13 :52) where s2¼SSE =k(n/C01), and ( X0X)/C0andbˆare as given by (13.11) and (13.12). The sum of squares for the contrast is ( c0^b)2=c0(X0X)/C0corn(P ici/C22yi:)2=(P ic2 i).13.6 CONTRASTS 357 --- Page 369 --- 13.6.2 Orthogonal Contrasts Two contrasts c0 i^bandc0j^bare said to be orthogonal ifc0icj¼0. We now show that if c0i^bandc0j^bare orthogonal, they are independent.",
    "d to be orthogonal ifc0icj¼0. We now show that if c0i^bandc0j^bare orthogonal, they are independent. Since we are assuming normality, c0i^bandc0j^bare independent if cov(c0 i^b,c0j^b)¼0 (13 :53) (see Problem 13.16). By Theorem 12.3c, cov( c0 i^b,c0j^b)¼s2c0i(X0X)/C0cj. By (13.11), (X0X)/C0¼diag[0 ,(1=n),...,(1=n)], and therefore cov(c0 i^b,c0j^b)¼c0i(X0X)/C0cj¼0i f c0icj¼0 (13 :54) (assuming that the ﬁrst element of ciis 0 for all i).",
    "i^b,c0j^b)¼c0i(X0X)/C0cj¼0i f c0icj¼0 (13 :54) (assuming that the ﬁrst element of ciis 0 for all i). By an argument similar to that used in the proofs of Corollary 1 to Theorem 5.6b and in Theorem 12.7b(v), the sums of squares ( c0 i^b)2=c0i(X0X)/C0ciand (c0j^b)2=c0j(X0X)/C0cjare also independent. Thus, if two contrasts are orthogonal, they are independent and their corresponding sums of squares are independent.",
    "trasts are orthogonal, they are independent and their corresponding sums of squares are independent. We now show that if the rows of C(Section 13.4.2) are mutually orthogonal con- trasts, SSH is the sum of ( c0 i^b)2=c0i(X0X)/C0cifor all rows of C. Theorem 13.6a. In the balanced one-way model, if yisNkn(Xb,s2I) and if H0:a1¼a2¼/C1/C1/C1¼akis expressed as Cb¼0, where the rows of C¼c0 1 c02 ...",
    "if yisNkn(Xb,s2I) and if H0:a1¼a2¼/C1/C1/C1¼akis expressed as Cb¼0, where the rows of C¼c0 1 c02 ... c0 k/C010 BBB@1 CCCA are mutually orthogonal contrasts, then SSH ¼(C^ b)0[C(X0X)/C0C0]/C01C^bcan be expressed (partitioned) as SSH ¼Xk/C01 i¼1(c0 i^b)2 c0 i(X0X)/C0ci, (13 :55) where the sums of squares ( c0 i^b)2=c0i(X0X)/C0ci,i¼1,2,...,k/C01, are independent. PROOF. By (13.54), C(X0X)/C0C0is a diagonal matrix with c0i(X0X)/C0ci, i¼1,2,...,k/C01, on the diagonal.",
    "F. By (13.54), C(X0X)/C0C0is a diagonal matrix with c0i(X0X)/C0ci, i¼1,2,...,k/C01, on the diagonal. Thus, with ( C^b)0¼(c01^b,c02^b,...,c0k/C01^b), (13.55) follows. Since the rows c01,c02,...,c0k/C01ofCare orthogonal, the indepen- dence of the sums of squares for the contrasts follows from (13.53) and (13.54).",
    "gonal, the indepen- dence of the sums of squares for the contrasts follows from (13.53) and (13.54). A358 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 370 --- An interesting implication of Theorem 13.6a is that the overall Ffor treatments (Table 13.1) is the average of the Fstatistics for each of the orthogonal contrasts: F¼SSH =(k/C01) s2¼1 k/C01Xk/C01 i¼1(ci^b)2 s2c0 i(XX)/C0ci ¼1 k/C01Xk/C01 i¼1Fi: It is possible that the overall Fwould lead to rejection of the overall H0while some of theFi’s for individual contrasts would not lead to rejection of the corresponding H0’s.",
    "hile some of theFi’s for individual contrasts would not lead to rejection of the corresponding H0’s. Likewise, since one or more of the Fi’s will be larger than the overall F, it is possible that an individual H0would be rejected, while the overall H0is not rejected. Example 13.6a. We illustrate the use of orthogonal contrasts with the ascorbic acid data of Table 13.2. Consider the orthogonal contrasts 2 m1/C0m2/C0m3andm2/C0m3.",
    "th the ascorbic acid data of Table 13.2. Consider the orthogonal contrasts 2 m1/C0m2/C0m3andm2/C0m3. By (13.50), these can be expressed as 2m1/C0m2/C0m3¼2a1/C0a2/C0a3¼(0,2,/C01,/C01)b¼c0 1b, m2/C0m3¼a2/C0a3¼(0,0,1,/C01)b¼c02b: The hypotheses H01:c0 1b¼0 and H02:c02b¼0 compare the ﬁrst treatment versus the other two and the second treatment versus the third. The means are given in Table 13.2 as /C22y1:¼17:15,/C22y2:¼19:31, and /C22y3:¼23:53.",
    "rsus the third. The means are given in Table 13.2 as /C22y1:¼17:15,/C22y2:¼19:31, and /C22y3:¼23:53. Then by (13.52), the sums of squares for the two contrasts are SS1¼n(P3 i¼1ci/C22yi:)2 P3 i¼1c2 i¼7[2(17 :15)/C019:31/C023:53]2 4þ1þ1¼85:0584 , SS2¼7(19 :31/C023:53)2 1þ1¼62:2872 : By (13.52), the corresponding Fstatistics are F1¼SS1 s2¼85:0584 3:0537¼27:85, F2¼SS2 s2¼62:2872 3:0537¼20:40, where s2¼3:0537 is from Table 13.3. Both F1andF2exceed F.05,1,18¼4.41.",
    "¼SS2 s2¼62:2872 3:0537¼20:40, where s2¼3:0537 is from Table 13.3. Both F1andF2exceed F.05,1,18¼4.41. The pvalues are .0000511and .000267, respectively. Note that the sums of squares for the two orthogonal contrasts add to the sum of squares for treatments given in Example 13.4; that is, 147.3456 ¼85.0584 þ 62.2872, as in (13.55). A The partitioning of the treatment sum of squares in Theorem 13.6a is always poss- ible. First note that SSH ¼y0Ayas in (13.29), where Ais idempotent.",
    "in Theorem 13.6a is always poss- ible. First note that SSH ¼y0Ayas in (13.29), where Ais idempotent. We now show that any such quadratic form can be partitioned into independent components.13.6 CONTRASTS 359 --- Page 371 --- Theorem 13.6b. Lety0Aybe a quadratic form, let Abe symmetric and idempotent of rank r, letN¼kn, and let the N/C21 random vector ybeNN(Xb,s2I). Then there exist ridempotent matrices A1,A2,...,Arsuch that A¼Pr i¼1Ai, rank( Ai)¼1 for i¼1,2,...,r, and AiAj¼Ofori=j.",
    "idempotent matrices A1,A2,...,Arsuch that A¼Pr i¼1Ai, rank( Ai)¼1 for i¼1,2,...,r, and AiAj¼Ofori=j. Furthermore, y0Aycan be partitioned as y0Ay¼Xr i¼1y0Aiy, (13 :56) where each y0Aiyin (13.56) is x2(1,li) and y0Aiyandy0Ajyare independent for i=j (note that liis a noncentrality parameter). PROOF. Since AisN/C2Nof rank rand is symmetric and idempotent, then by Theorem 2.13c, rof its eigenvalues are equal to 1 and the others are 0.",
    "tric and idempotent, then by Theorem 2.13c, rof its eigenvalues are equal to 1 and the others are 0. Using the spectral decomposition (2.104), we can express Ain the form A¼Xr i¼1viv0 i¼Xr i¼1Ai, (13 :57) where v1,v2,...,vrare normalized orthogonal eigenvectors corresponding to the nonzero eigenvalues and Ai¼viv0 i. It is easily shown that rank( Ai)¼1,AiAj¼O fori=j, and Aiis symmetric and idempotent (see Problem 13.17).",
    "easily shown that rank( Ai)¼1,AiAj¼O fori=j, and Aiis symmetric and idempotent (see Problem 13.17). Then by Corollary 2 to Theorem 5.5 and Corollary 1 to Theorem 5.6b, y0Aiyisx2(1,li) andy0Aiyandy0Ajyare independent. A Ify0Ayin Theorem 13.6b is used to represent SSH, the eigenvectors correspond- ing to nonzero eigenvalues of Aalways deﬁne contrasts of the cell means. In other words, the partitioning of y0Ayin (13.56) is always in terms of orthogonal contrasts.",
    "eans. In other words, the partitioning of y0Ayin (13.56) is always in terms of orthogonal contrasts. To see this, note that SST ¼SSH þSSE , which, in the case of the one-way balanced model, implies that y0I/C01 knJ/C18/C19 y¼Xk i¼1y0Aiyþy0[I/C0X(X0X)/C0X0]y: (13 :58) If we let K¼1 nJ0 /C1/C1/C1 0 0J /C1/C1/C1 0 ............",
    "Xk i¼1y0Aiyþy0[I/C0X(X0X)/C0X0]y: (13 :58) If we let K¼1 nJ0 /C1/C1/C1 0 0J /C1/C1/C1 0 ............ 00 /C1/C1/C1 J0 BBB@1 CCCA as in (13.26), then (13.58) can be rewritten as y0y¼y01 knJyþXk i¼1y0(viv0 i)yþy0(I/C0K)y: (13 :59) By Theorem 2.13h, each vimust be orthogonal to the columns of (1 /n)Jand I2K. Orthogonality to (1 /n)Jimplies that vij¼0; that is, videﬁnes a contrast360 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 372 --- in the elements of y.",
    "nes a contrast360 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 372 --- in the elements of y. Orthogonality to I2Kimplies that the elements of vicor- responding to units associated with a particular treatment are constants. Together these results imply that videﬁnes a contrast of the estimated treatment means. Example 13.6b.",
    "gether these results imply that videﬁnes a contrast of the estimated treatment means. Example 13.6b. Using a one-way model, we demonstrate that orthogonal contrasts in the treatment means can be expressed in terms of contrasts in the observations andthat the coefﬁcients in these contrasts form eigenvectors. For simplicity of exposition, letk¼4.",
    "andthat the coefﬁcients in these contrasts form eigenvectors. For simplicity of exposition, letk¼4. The model is then y ij¼mþaiþ1ij,i¼1,2,3,4,j¼1,2,...,n: The sums of squares in (13.59) can be written in the form y0y¼SS(m)þSS(ajm)þSSE ¼y2 :: knþ ^b0X0y/C0y2:: kn/C18/C19 þ(y0y/C0^b0X0y): With k¼4, the sum of squares for treatments, y0Ay¼^b0X0y/C0y2::=4n, has 3 degrees of freedom. Any set of three orthogonal contrasts in the treatment means will serve to illustrate.",
    "s of freedom. Any set of three orthogonal contrasts in the treatment means will serve to illustrate. As an example, consider c0 1b¼(0,1,/C01,0,0)b,c02b¼(0,1,1,/C02,0)b, andc03b¼(0,1,1,1,/C03)b,w h e r eb¼(m,a1,a2,a3,a4)0. Thus, we are comparing the ﬁrst mean to the second, the ﬁrst two means to the third, and the ﬁrst three to the fourth (see a comment at the beginning of Section 13.4 for the equivalence of H0:a1¼a2¼a3¼a4andH0:m1¼m2¼m3¼m4).",
    "a comment at the beginning of Section 13.4 for the equivalence of H0:a1¼a2¼a3¼a4andH0:m1¼m2¼m3¼m4). Using the format in (13.55), we can write the three contrasts as c0 1^bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ c0 1(X0X)/C0c1p ¼/C22y1:/C0/C22y2:ﬃﬃﬃﬃﬃﬃﬃﬃ 2=np c0 2^bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ c0 2(X0X)/C0c2p ¼/C22y1:þ/C22y2:/C02/C22y3:ﬃﬃﬃﬃﬃﬃﬃﬃ 6=np c0 3^bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ c0 3(X0X)/C0c3p ¼/C22y1:þ/C22y2:þ/C22y3:/C03/C22y4:ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 12=np , where ( X0X)/C0¼diag[0 ,(1=n),...,(1=n)] is given in (13.11) and ^b¼(0,/C22y1:,...,/C22y4:)0 is from (13.12).",
    "0X)/C0¼diag[0 ,(1=n),...,(1=n)] is given in (13.11) and ^b¼(0,/C22y1:,...,/C22y4:)0 is from (13.12). To write these in the form v0 1y,v02y, and v03y[as in (13.59)] we start with the ﬁrst: /C22y1:/C0/C22y2:ﬃﬃﬃﬃﬃﬃﬃﬃ 2=np ¼1ﬃﬃﬃﬃﬃﬃﬃﬃ 2=npPn j¼1y1j n/C0Pn j¼1y2j n0 BB@1 CCA ¼1=nﬃﬃﬃﬃﬃﬃﬃﬃ 2=np (1,1,...,1,/C01,/C01,...,/C01,0,0,...,0)y ¼v0 1y,13.6 CONTRASTS 361 --- Page 373 --- where the number of 1s is n, the number of 21s is n, and the number of 0s is 2 n.",
    "1 --- Page 373 --- where the number of 1s is n, the number of 21s is n, and the number of 0s is 2 n. Thus v0 1¼(1=ﬃﬃﬃﬃﬃ 2np )(j0 n,/C0j0n,00,00), and v0 1v1¼2n 2n¼1: Similarly, v0 2andv03can be expressed as v02¼(1=ﬃﬃﬃﬃﬃ 6np )(j0 n,j0n/C02j0n,00) and v0 3¼(1=ﬃﬃﬃﬃﬃﬃﬃﬃ 12np )(j0 n,j0n,j0n,/C03j0n). We now show that v1,v2, and v3serve as eigenvec- tors in the spectral decomposition [see (2.104)] of the matrix Ain SS(ajm)¼y0Ay.",
    "3serve as eigenvec- tors in the spectral decomposition [see (2.104)] of the matrix Ain SS(ajm)¼y0Ay. Since Ais idempotent of rank 3, it has three nonzero eigenvalues, each equal to 1.",
    "Ain SS(ajm)¼y0Ay. Since Ais idempotent of rank 3, it has three nonzero eigenvalues, each equal to 1. Thus the spectral decomposition of Ais A¼v1v0 1þv2v02þv3v03 ¼1 2njn /C0jn 0 00 BBB@1 CCCA(j0 n,/C0j0n,00,00)þ1 6njn jn /C02jn 00 BBB@1 CCCA(j 0 n,j0n,/C02j0n,00) þ1 12njn jn jn /C03jn0 BBB@1 CCCA(j0 n,j0n,j0n,/C03j0n) ¼1 2nJn /C0JnOO /C0Jn JnOO OO O O OO O O0 BBB@1 CCCAþ1 6nJn Jn/C02JnO Jn Jn/C02JnO /C02Jn/C02Jn 4JnO OO O O0 BBB@1 CCCA þ1 12nJn Jn Jn/C03Jn Jn Jn Jn/C03Jn Jn Jn Jn/C03Jn /C03Jn/C03Jn/C03Jn 9Jn0 BBB@1 CCCA ¼1 4n3Jn/C0Jn/C0Jn/C0Jn /C0Jn3Jn/C0Jn/C0Jn /C0Jn/C0Jn3Jn/C0Jn /C0Jn/C0Jn/C0Jn3Jn0 BBB@1 CCCA, which is the matrix of the quadratic form for SS( ajm) in (13.27) with k¼4.",
    "0Jn/C0Jn3Jn0 BBB@1 CCCA, which is the matrix of the quadratic form for SS( ajm) in (13.27) with k¼4. For SS(m)¼y2 ::=4n,w eh a v e y2:: 4n¼y0j4nj0 4n 4n/C18/C19 y¼(v0 0y)2,362 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 374 --- where v0 0¼j0 4n=2ﬃﬃﬃnp. It is easily shown that v0 0v0¼1 and that v00v1¼0.",
    "ED CASE --- Page 374 --- where v0 0¼j0 4n=2ﬃﬃﬃnp. It is easily shown that v0 0v0¼1 and that v00v1¼0. It is also clear that v0is an eigenvector of j4nj0 4n=4n, because j4nj04n=4nhas one eigenvalue equal to 1 and the others equal to 0, so that j4nj04n=4nis already in the form of a spec- tral decomposition with j4n=2ﬃﬃﬃnpas the eigenvector corresponding to the eigenvalue 1 (see Problem 13.18b).",
    "omposition with j4n=2ﬃﬃﬃnpas the eigenvector corresponding to the eigenvalue 1 (see Problem 13.18b). A 13.6.3 Orthogonal Polynomial Contrasts Suppose the treatments in a one-way analysis of variance have equally spaced quan- titative levels, for example, 5, 10, 15, and 20lb of fertilizer per plot of ground. The researcher may then wish to investigate how the response varies with the level of fer- tilizer.",
    "The researcher may then wish to investigate how the response varies with the level of fer- tilizer. We can check for a linear trend, a quadratic trend, or a cubic trend by ﬁtting a third-order polynomial regression model yij¼b0þb1xiþb2x2 iþb3x3iþ1ij, (13 :60) i¼1,2,3,4,j¼1,2,...,n, where x1¼5,x2¼10,x3¼15,andx4¼20.",
    "model yij¼b0þb1xiþb2x2 iþb3x3iþ1ij, (13 :60) i¼1,2,3,4,j¼1,2,...,n, where x1¼5,x2¼10,x3¼15,andx4¼20. We now show that tests on the b0si n (13.60) can be carried out using orthogonal contrasts on the means /C22yi:that are esti- mates ofmiin the ANOVA model yij¼mþaiþ1ij¼miþ1ij,i¼1,2,3,4,j¼1,2,...,n: (13 :61) The sum of squares for the full–reduced-model test of H0:b3¼0i s ^b0X0y/C0^b/C30 1X01y, (13 :62) where ^bis from the full model in (13.60) and ^b/C31is from the reduced model with b3¼0 [see (8.9), (8.20), and Table 8.3].",
    "l model in (13.60) and ^b/C31is from the reduced model with b3¼0 [see (8.9), (8.20), and Table 8.3]. The Xmatrix is of the form X¼1x1x2 1x31 ............ 1x1x2 1x31 1x2x22x32 ............ 1x2x2 2x32 1x3x2 3x33 ............ 1x3x2 3x33 1x4x24x34 ............",
    "1x2 1x31 1x2x22x32 ............ 1x2x2 2x32 1x3x2 3x33 ............ 1x3x2 3x33 1x4x24x34 ............ 1x4x2 4x340 BBBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCCA: (13 :63)13.6 CONTRASTS 363 --- Page 375 --- For testing H0:b3¼0, we can use (8.37) F¼^b0X0y/C0^b/C30 1X0 1y s2, or (8.39) F¼^b2 3 s2g33, (13 :64) where X1consists of the ﬁrst three columns of Xin (13.63), s2¼SSE =(n/C03/C01), andg33is the last diagonal element of ( X0X)/C01.",
    "three columns of Xin (13.63), s2¼SSE =(n/C03/C01), andg33is the last diagonal element of ( X0X)/C01. We now carry out this full–reduced- model test using contrasts. Since the columns of Xare not orthogonal, the sums of squares for the b’s ana- logous to ^b23=g33in (13.64) are not independent. Thus, the interpretation in terms of the degree of curvature for E(yij) is more difﬁcult. We therefore orthogonalize the columns of Xso that the sums of squares become independent.",
    "difﬁcult. We therefore orthogonalize the columns of Xso that the sums of squares become independent. To simplify computations, we ﬁrst transform x1¼5,x2¼10,x3¼15, and x4¼20 by dividing by 5, the common distance between them. The x’s then become x1¼1,x2¼2,x3¼3, and x4¼4. The transformed 4 n/C24 matrix Xin (13.63) is given by X¼111213 ............ 111213 122223 ............ 122223 133233 ............ 133233 144243 ............",
    "111213 ............ 111213 122223 ............ 122223 133233 ............ 133233 144243 ............ 1442430 BBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCA¼(j,x 1,x2,x3), where jis 4n/C21. Note that by Theorem 8.4c, the resulting Fstatistics such as (13.64) will be unaffected by this transformation. To obtain orthogonal columns, we use the orthogonalization procedure in Section 7.10 based on regressing columns of Xon other columns and taking residuals. We begin by orthogonalizing x1.",
    "sed on regressing columns of Xon other columns and taking residuals. We begin by orthogonalizing x1. Denoting the ﬁrst column by x0, we use364 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 376 --- (7.97) to obtain x1/C10¼x1/C0x0(x0 0x0)/C01x00x1 ¼x1/C0j(j0j)/C01j0x1¼x1/C0j(4n)/C01nX4 i¼1xi ¼x1/C0/C22xj: (13 :65) The residual vector x1/C10is orthogonal to x0¼j: j0x1/C10¼j0(x1/C0/C22xj)¼j0x1/C0/C22xj0j¼4n/C22x/C04n/C22x¼0: (13 :66) We apply this procedure successively to the other two columns of X.",
    "0j¼4n/C22x/C04n/C22x¼0: (13 :66) We apply this procedure successively to the other two columns of X. To transform the third column, x2, so that it is orthogonal to the ﬁrst two columns, we use (7.97) to obtain x2/C101¼x2/C0Z1(Z01Z1)/C01Z01x2, (13 :67) where Z1¼(j,x1/C10). We use the notation Z1instead of X1because x1/C10, the second column of Z1, is different from x1, the second column of X1.",
    "instead of X1because x1/C10, the second column of Z1, is different from x1, the second column of X1. The matrix Z0 1Z1is given by Z0 1Z1¼j0 x0 1/C10/C18/C19 (j,x1/C10) ¼j0j 0 0x01/C10x1/C10/C18/C19 [by (13.66)] , and (13.67) becomes x2/C101¼x2/C0Z1(Z0 1Z1)/C01Z01x2 ¼x2/C0(j,x1/C10)j0j 0 0x0 1/C10x1/C10/C18/C19 /C01j0 x01/C10/C18/C19 x2 ¼x2/C0j0x2 j0jj/C0x01/C10x2 x0 1/C10x1/C10x1/C10: (13 :68) The residual vector x2/C101is orthogonal to x0¼jand to x1/C10: j0x2/C101¼0,x0 1/C10x2/C101¼0: (13 :69)13.6 CONTRASTS 365 --- Page 377 --- The fourth column of Zbecomes x3/C1012¼x3/C0j0x3 j0jj/C0x0 1/C10x3 x0 1/C10x1/C10x1/C10/C0x0 2/C101x3 x0 2/C101x2/C101x2/C101, (13 :70) which is orthogonal to the ﬁrst three columns, j,x1/C10, and x2/C101.",
    "2/C101x2/C101x2/C101, (13 :70) which is orthogonal to the ﬁrst three columns, j,x1/C10, and x2/C101. We have thus transformed y¼Xbþ1to y¼Zuþ1, (13 :71) where the columns of Zare mutually orthogonal and the elements of uare functions of theb’s. The columns of Zare given in (13.65), (13.68), and (13.70): z0¼j,z1¼x1/C10,z2¼x2/C101,z3¼x3/C1012: We now evaluate z1,z2, and z3for our illustration, in which x1¼1,x2¼2,x3¼3, and x4¼4.",
    "1,z3¼x3/C1012: We now evaluate z1,z2, and z3for our illustration, in which x1¼1,x2¼2,x3¼3, and x4¼4. By (13.65), we obtain z1¼x1/C10¼x1/C0/C22xj¼x1/C02:5j ¼/C0 1:5,...,/C01:5,/C0:5,...,/C0:5,:5,...,:5,1:5,...,1:5 ðÞ0, which we multiply by 2 so as to obtain integer values: z1¼x1/C10¼/C0 3,...,/C03,/C01,...,/C01,1,...,1,3,...,3 ðÞ0: (13 :72) Note that multiplying by 2 preserves the orthogonality and does not affect the F values.",
    "0: (13 :72) Note that multiplying by 2 preserves the orthogonality and does not affect the F values. To obtain z2, by (13.68), we ﬁrst compute j0x2 j0j¼nP4 i¼1x2 i 4n¼P4 i¼1i2 4¼30 4¼7:5, x0 1/C10x2 x0 1/C10x1/C10¼n[/C03(12)/C01(22)þ1(32)þ3(42)] n[(/C03)2þ(/C01)2þ12þ32]¼50 20¼2:5:366 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 378 --- Then, by (13.68), we obtain z2¼x2/C0j0x2 j0jj/C0x0 1/C10x2 x0 1/C10x1/C10x1/C10 ¼x2/C07:5j/C02:5x1/C10 ¼12 ... 12 22 ... 22 32 ... 32 42 ...",
    "j0jj/C0x0 1/C10x2 x0 1/C10x1/C10x1/C10 ¼x2/C07:5j/C02:5x1/C10 ¼12 ... 12 22 ... 22 32 ... 32 42 ... 420 BBBBBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCCCCA/C07:51 ... 1 1 ... 1 1 ... 1 1 ... 10 BBBBBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCC CCCCCCCCCCCCCCCCCCA/C02:5/C03 ... /C03 /C01 ... /C01 1 ... 1 3 ... 30 BBBBBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCCCCA¼1 ... 1 /C01 ... /C01 /C01 ... /C01 1 ...",
    "0 BBBBBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCCCCA¼1 ... 1 /C01 ... /C01 /C01 ... /C01 1 ... 10 BBBBBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCCCCA: (13 :73) Similarly, using (13.70), we obtain z 3¼/C0 1,...,/C01,3,...,3,/C03,...,/C03,1,...,1 ðÞ0: (13 :74) Thus Zis given by Z¼1/C031 /C01 ............ 1/C031 /C01 1/C01/C013 ............ 1/C01/C013 11 /C01/C03 ............ 11 /C01/C03 1311 ............",
    "/C031 /C01 1/C01/C013 ............ 1/C01/C013 11 /C01/C03 ............ 11 /C01/C03 1311 ............ 13110 BBBBBBBBBBBBB BBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCA:13.6 CONTRASTS 367 --- Page 379 --- Since Xb¼Zu, we can ﬁnd the u’s in terms of the b’s or theb’s in terms of the u’s.",
    "--- Page 379 --- Since Xb¼Zu, we can ﬁnd the u’s in terms of the b’s or theb’s in terms of the u’s. For our illus- tration, these relationships are given by (see Problem 13.24) b0¼u0/C05u1þ5u2/C035u3,b1¼2u1/C05u2þ16:7 :3u3, b2¼u2/C025u3,b3¼u3 :3:(13 :75) Since the columns of Z¼(j,z1,z2,z3) are orthogonal ( z0 izj¼0 for all i=j), we have Z0Z¼diag(j0j,z01z1,z02z2,z03z3).",
    "s of Z¼(j,z1,z2,z3) are orthogonal ( z0 izj¼0 for all i=j), we have Z0Z¼diag(j0j,z01z1,z02z2,z03z3). Thus ^u¼(Z0Z)/C01Z0y¼j0y=j0j z01y=z01z1 z02y=z02z2 z03y=z03z30 BB@1 CCA: (13 :76) The regression sum of squares (uncorrected for u0)i s SS(u)¼^u0Z0y¼X3 i¼0(z0 iy)2 z0 izi, (13 :77) where z0¼j. By an argument similar to that following (13.54), the sums of squares on the right side of (13.77) are independent.",
    "similar to that following (13.54), the sums of squares on the right side of (13.77) are independent. Since the sums of squares SS( ui)¼(z0 iy)2=z0izi,i¼1,2,3, are independent, each SS(ui) tests the signiﬁcance of ^uiby itself (regressing yonzialone) as well as in the presence of the other ^ui’s; that is, for a general k,w eh a v e SS(uiju0,...,ui/C01,uiþ1,...,uk)¼SS(u0,...,uk)/C0SS(u0,...,ui/C01,uiþ1,...,uk) ¼Xk j¼0(z0 jy)2 z0 jzj/C0X j=i(z0 jy)2 z0 jzj ¼(z0 iy)2 z0 izi¼SS(ui): In terms of the ^bi’s, it can be shown that each SS( ui) tests the signiﬁcance of ^biin the presence of ^b0,^b1,...,^bi/C01.",
    "t can be shown that each SS( ui) tests the signiﬁcance of ^biin the presence of ^b0,^b1,...,^bi/C01. For example, for bk(the lastb), the sum of squares368 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 380 --- can be written as SS(uk)¼(z0 ky)2 z0 kzk¼^b0X0y/C0^b/C30 1X0 1y (13 :78) (see Problem 13.26), where ^bis from the full model y¼Xbþ1and ^b/C31is from the reduced model y¼X1b/C3 1þ1, in which b1contains all the b’s exceptbkandX1con- sists of all columns of Xexcept the last.",
    "/C3 1þ1, in which b1contains all the b’s exceptbkandX1con- sists of all columns of Xexcept the last. The sum of squares SS( ui)¼(z0 iy)2=z0iziis equivalent to a sum of squares for a contrast on the means /C22y1:,/C22y2:,...,/C22yk:as in (13.52).",
    "equivalent to a sum of squares for a contrast on the means /C22y1:,/C22y2:,...,/C22yk:as in (13.52). For example z0 1y¼/C03y11/C03y12/C0/C1/C1/C1/C0 3y1n/C0y21/C0/C1/C1/C1/C0 y2n þy31þ/C1/C1/C1þ y3nþ3y41þ/C1/C1/C1þ 3y4n ¼/C03Xn j¼1y1j/C0Xn j¼1y2jþXn j¼1y3jþ3Xn j¼1y4j ¼/C03y1:/C0y2:þy3:þ3y4: ¼n(/C03/C22y1:/C0/C22y2:þ/C22y3:þ3/C22y4:) ¼nX4 i¼1ci/C22yi:, where c1¼/C03,c2¼/C01,c3¼1, and c4¼3.",
    "¼n(/C03/C22y1:/C0/C22y2:þ/C22y3:þ3/C22y4:) ¼nX4 i¼1ci/C22yi:, where c1¼/C03,c2¼/C01,c3¼1, and c4¼3. Similarly z0 1z1¼n(/C03)2þn(/C01)2þn(1)2þn(3)2 ¼n[(/C03)2þ(/C01)2þ12þ32] ¼nX4 i¼1c2i: Then (z0 1y)2 z0 1z1¼(nP4 i¼1ci/C22yi:)2 nP4 i¼1c2 i¼n(P4 i¼1ci/C22yi:)2 P4 i¼1c2 i, which is the sum of squares for the contrast in (13.52). Note that the coefﬁcients /C03,/C01,1,and 3 correspond to a linear trend.",
    "r the contrast in (13.52). Note that the coefﬁcients /C03,/C01,1,and 3 correspond to a linear trend. Likewise, z0 2ybecomes z0 2y¼n(/C22y1:/C0/C22y2:/C0/C22y3:þ/C22y4:),13.6 CONTRASTS 369 --- Page 381 --- whose coefﬁcients show a quadratic trend, and z0 3ycan be written as z0 3y¼n(/C0/C22y1:þ3/C22y2:/C03/C22y3:þ/C22y4:) with coefﬁcients that exhibit a cubic pattern. These contrasts in the /C22yi:’s have a meaningful interpretation in terms of the shape of the response curve.",
    "rasts in the /C22yi:’s have a meaningful interpretation in terms of the shape of the response curve. For example, suppose that the /C22yi:’s fall on a straight line. Then, for some b0andb1,w eh a v e /C22yi:¼b0þb1xi¼b0þb1i,i¼1,2,3,4, since xi¼i.",
    "on a straight line. Then, for some b0andb1,w eh a v e /C22yi:¼b0þb1xi¼b0þb1i,i¼1,2,3,4, since xi¼i. In this case, the linear contrast is nonzero and the quadratic and cubic contrasts are zero: /C03/C22y1:/C0/C22y2:þ/C22y3:þ3/C22y4:¼ /C03(b0þb1)/C0(b0þ2b1)þb0þ3b1þ3(b0þ4b1)¼10b1, b0þb1/C0(b0þ2b1)/C0(b0þ3b1)þ(b0þ4b1)¼0, /C0(b0þb1)þ3(b0þ2b1)/C03(b0þ3b1)þ(b0þ4b1)¼0: This demonstration could be simpliﬁed by choosing the linear trend /C22y1:¼1,/C22y2:¼2,/C22y3:¼3, and /C22y4:¼4.",
    "ration could be simpliﬁed by choosing the linear trend /C22y1:¼1,/C22y2:¼2,/C22y3:¼3, and /C22y4:¼4. Similarly, if the /C22yi:’s follow a quadratic trend, say /C22y1:¼1,/C22y2:¼2,/C22y3:¼2,/C22y4:¼1, then the linear and cubic contrasts are zero. In many cases it is not necessary to ﬁnd the orthogonal polynomial coefﬁcients by the orthogonalization process illustrated in this section. Tables of orthogonal polynomials are available [see, e.g., Rencher (2002, p. 587) or Guttman (1982,pp. 349–354)].",
    "hogonal polynomials are available [see, e.g., Rencher (2002, p. 587) or Guttman (1982,pp. 349–354)]. We give a brief illustration of some orthogonal polynomial coefﬁ- cients in Table 13.5, including those we found above for k¼4.",
    "n of some orthogonal polynomial coefﬁ- cients in Table 13.5, including those we found above for k¼4. TABLE 13.5 Orthogonal Polynomial Coefﬁcients for k53, 4, 5 k¼3 k¼4 k¼5 Linear 210 1 23211 3 222101 2 Quadratic 1 221 1 21211 2 2122212 Cubic 213 231 2120 221 Quartic 1 246 241370 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 382 --- In Table 13.5, we can see some relationships among the coefﬁcients for each value ofk.",
    "Page 382 --- In Table 13.5, we can see some relationships among the coefﬁcients for each value ofk. For example, if k¼3 and the three means /C22y1:,/C22y2:,/C22y3:have a linear relationship, then /C22y2:/C0/C22y1:is equal to /C22y3:/C0/C22y2:; that is /C22y3:/C0/C22y2:¼/C22y2:/C0/C22y1: or /C22y3:/C0/C22y2:/C0(/C22y2:/C0/C22y1:)¼0, /C22y3:/C02/C22y2:þ/C22y1:¼0: If this relationship among the three means fails to hold, we have a quadratic com- ponent of curvature.",
    "this relationship among the three means fails to hold, we have a quadratic com- ponent of curvature. Similarly, for k¼4, the cubic component, /C0/C22y1:þ3/C22y2:/C03/C22y3:þ/C22y4:, is equal to the difference between the quadratic component for /C22y1:,/C22y2:,/C22y3:and the quadratic com- ponent for /C22y2:,/C22y3:,/C22y4:: /C0/C22y1:þ3/C22y2:/C03/C22y3:þ/C22y4:¼/C22y2:/C02/C22y3:þ/C22y4:/C0(/C22y1:/C02/C22y2:þ/C22y3:): PROBLEMS 13.1 Obtain the normal equations in (13.7) from the model in (13.6).",
    ":/C02/C22y2:þ/C22y3:): PROBLEMS 13.1 Obtain the normal equations in (13.7) from the model in (13.6). 13.2 Obtainbˆin (13.12) using ( X0X)/C0in (13.11) and X0yin (13.7). 13.3 Show that SSE ¼y0[I/C0X(X0X)/C0X0]yin (12.21) is equal to SSE ¼P ijy2 ij/C0P iy2i:=nin (13.13). 13.4 Show that the expressions for SSE in (13.13) and (13.14) are equal. 13.5 (a) Show that H0:a1¼a2¼/C1/C1/C1¼akin (13.17) is equivalent to H0:a/C31¼a/C32¼/C1/C1/C1¼a/C3kin (13.18).",
    "Show that H0:a1¼a2¼/C1/C1/C1¼akin (13.17) is equivalent to H0:a/C31¼a/C32¼/C1/C1/C1¼a/C3kin (13.18). (b)Show that H0:a/C31¼a/C32¼/C1/C1/C1¼a/C3kin (13.18) is equivalent to H0:a/C31¼a/C32¼/C1/C1/C1¼a/C3k¼0 in (13.19). 13.6 Show that nPk i¼1(/C22yi:/C0/C22y::)2in (13.24) is equal toP iy2 i:=n/C0y2::=knin (13.23). 13.7 Using (13.6) and (13.11), show that X(X0X)/C0X0in (13.25) can be written in terms of JandOas in (13.26). 13.8 Show that for Cin (13.28), C(X0X)/C0C0is given by (13.30).",
    "tten in terms of JandOas in (13.26). 13.8 Show that for Cin (13.28), C(X0X)/C0C0is given by (13.30). 13.9 Show that C(X0X)/C0X0is given by the matrix in (13.32).",
    "13.28), C(X0X)/C0C0is given by (13.30). 13.9 Show that C(X0X)/C0X0is given by the matrix in (13.32). 13.10 Show that the matrix (1 =4n)A0J3Ain (13.33) has the form shown in (13.35).PROBLEMS 371 --- Page 383 --- 13.11 Using the model yij¼m/C3þa/C3 iþ1ijwith the assumptions E(1ij)¼ 0,var(1ij)¼s2,covð1ij,1i0j0)¼0,and the side conditionPk i¼1a/C3 i¼0, obtain the following results used in Table 13.4: (a)E(12ij)¼s2for all i,jandE(1ij1i0j0)¼0 for i,j=i0,j0. (b)E½SS(ajm)/C138¼(k/C01)s2þnPk i¼1a/C32 i.",
    "(a)E(12ij)¼s2for all i,jandE(1ij1i0j0)¼0 for i,j=i0,j0. (b)E½SS(ajm)/C138¼(k/C01)s2þnPk i¼1a/C32 i. (c)(SSE) ¼k(n/C01)s2. 13.12 Using Cin (13.28), show that C0Cis given by the matrix in (13.47) 13.13 Show that C0J3Chas the form shown in (13.48). 13.14 Show that if the constraintP4i¼1ai¼0 is not imposed, (13.49) becomes E[SS(ajm)]¼3s2þ4X4 i¼1(ai/C0/C22a)2: 13.15 Show that Fin (13.52) can be obtained from (13.51).",
    "comes E[SS(ajm)]¼3s2þ4X4 i¼1(ai/C0/C22a)2: 13.15 Show that Fin (13.52) can be obtained from (13.51). 13.16 Express the sums of squares ( c0 i^b)2=c0i(X0X)/C0ciand (c0j^b)2=c0j(X0X)/C0cjbelow (13.54) in Section 13.6.2 as quadratic forms in y, and show that these sums of squares are independent if cov( c0i^b,c0j^b)¼0 as in (13.53). 13.17 In the proof of Theorem 13.6b, show that Aiis symmetric and idempotent that rank(Ai)¼1, and that AiAj¼O.",
    "he proof of Theorem 13.6b, show that Aiis symmetric and idempotent that rank(Ai)¼1, and that AiAj¼O. 13.18 (a) Show that J/knin the ﬁrst term on the right side of (13.59) is idempotent with one eigenvalue equal to 1 and the others equal to 0. (b)Show that jis an eigenvector corresponding to the nonzero eigenvalue of J/kn. 13.19 In Example 13.6b, show that v00v0¼1 and v00v1¼0. 13.20 Show that j0x2/C101¼0 and x00/C11x2/C101¼0 as in (13.69). 13.21 Show that x3/C1012has the form given in (13.70).",
    "x2/C101¼0 and x00/C11x2/C101¼0 as in (13.69). 13.21 Show that x3/C1012has the form given in (13.70). 13.22 Show that x3/C1012is orthogonal to each of j;x1/C10, and x2/C101, as noted following (13.70). 13.23 Show that z3¼(/C01,...,/C01,3,...,3,/C03,...,/C03,1,...,1)0as in (13.74). 13.24 Show that b0¼u0/C05u1þ5u2/C035u3,b1¼2u1/C05u2þ(16 :7=:3)u3, b2¼u2/C025u3, andb3¼u3=:3, as in (13.75). 13.25 Show that the elements of ^u¼(Z0Z)/C01Z0yare of the form z0iy=z0izias in (13.76).",
    "in (13.75). 13.25 Show that the elements of ^u¼(Z0Z)/C01Z0yare of the form z0iy=z0izias in (13.76). 13.26 Show that SS( uk)¼^b0X0y/C0^b/C30X0 1yas in (13.78).",
    "yare of the form z0iy=z0izias in (13.76). 13.26 Show that SS( uk)¼^b0X0y/C0^b/C30X0 1yas in (13.78). 13.27 If the means /C22y1:,/C22y2:,/C22y3:, and /C22y4:have the quadratic trend /C22y1:¼1, /C22y2:¼2,/C22y3:¼2,/C22y4:¼1, show that the linear and cubic contrasts are zero, but the quadratic contrast is not zero.372 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 384 --- 13.28 Blood sugar levels (mg /100g) were measured on 10 animals from each of ﬁve breeds (Daniel 1974, p. 197).",
    "d sugar levels (mg /100g) were measured on 10 animals from each of ﬁve breeds (Daniel 1974, p. 197). The results are presented in Table 13.6. (a)Test the hypothesis of equality of means for the ﬁve breeds. (b)Make the following comparisons by means of orthogonal contrasts: A,B,C,vs. D ,E; A,B,vs. C ; A vs. B ; D vs. E. 13.29 In Table 13.7, we have the amount of insulin released from specimens of pancreatic tissue treated with ﬁve concentrations of glucose (Daniel 1974, p. 182).",
    "rom specimens of pancreatic tissue treated with ﬁve concentrations of glucose (Daniel 1974, p. 182). (a)Test the hypothesis of equality of means for the ﬁve glucoseconcentrations. (b)Assuming that the levels of glucose concentration are equally spaced, use orthogonal polynomial contrasts to test for linear, quadratic, cubic, and quartic trends. 13.30 A different stimulus was given to each of three groups of 14 animals (Daniel1974, p. 196). The response times in seconds are given in Table 13.8.",
    "ee groups of 14 animals (Daniel1974, p. 196). The response times in seconds are given in Table 13.8. (a)Test the hypothesis of equal mean response times. (b)Using orthogonal contrasts, make the two comparisons of stimuli: 1versus 2, 3; and 2 versus 3. 13.31 The tensile strength (kg) was measured for 12 wires from each of nine cables(Hald 1952, p. 434). The results are given in Table 13.9. (a)Test the hypothesis of equal mean strengths for the nine cables.",
    "results are given in Table 13.9. (a)Test the hypothesis of equal mean strengths for the nine cables. (b)The ﬁrst four cables were made from one type of raw material and the other ﬁve from another type.",
    "(b)The ﬁrst four cables were made from one type of raw material and the other ﬁve from another type. Compare these two types by means of a contrast.TABLE 13.6 Blood Sugar Levels (mg /100 g) for 10 Animals from Each of Five Breeds (A–E) AB C D E 124 111 117 104 142 116 101 142 128 139 101 130 121 130 133 118 108 123 103 120118 127 121 121 127120 129 148 119 149110 122 141 106 150127 103 122 107 149106 122 139 107 120130 127 125 115 116PROBLEMS 373 --- Page 385 --- TABLE 13.7 Insulin Released at Five Different Glucose Concentrations (1–5) 1 2345 1.53 3.15 3.89 8.18 5.86 1.61 3.96 4.80 5.64 5.46 3.75 3.59 3.69 7.36 5.96 2.89 1.89 5.70 5.33 6.493.26 1.45 5.62 8.82 7.812.83 3.49 5.79 5.26 9.032.86 1.56 4.75 8.75 7.492.59 2.44 5.33 7.10 8.98 TABLE 13.8 Response Times (in seconds) to Three Stimuli Stimulus Stimulus 1 231 23 16 6 8 17 6 9 1 4 71 0 781 114 7 9 17 6 1113 8 10 19 4 913 4 6 14 9 1012 8 7 15 5 912 9 10 20 5 5 TABLE 13.9 Tensile Strength (kg) of Wires from Nine Cables (1–9) 1 23456789 345 329 340 328 347 341 339 339 342 327 327 330 344 341 340 340 340 346335 332 325 342 345 335 342 347 347338 348 328 350 340 336 341 345 348 330 337 338 335 350 339 336 350 355 334 328 332 332 346 340 342 348 351335 328 335 328 345 342 347 341 333340 330 340 340 342 345 345 342 347333 328 335 337 330 346 336 340 348335 330 329 340 338 347 342 345 341374 ONE-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 386 --- 13.32 Four groups of physical therapy patients were given different treatments (Daniel 1974, p.",
    "--- 13.32 Four groups of physical therapy patients were given different treatments (Daniel 1974, p. 195). The scores measuring treatment effectiveness are given in Table 13.10. (a)Test the hypothesis of equal mean treatment effects. (b)Using contrasts, compare treatments 1, 2 versus 3, 4; 1 versus 2; and 3 versus 4. 13.33 Weight gains in pigs subjected to ﬁve different treatments are given in Table 13.11 (Crampton and Hopkins 1934). (a)Test the hypothesis of equal mean treatment effects.",
    "in Table 13.11 (Crampton and Hopkins 1934). (a)Test the hypothesis of equal mean treatment effects. (b)Using contrasts, compare treatments 1, 2, 3 versus 4; 1, 2 versus 3; and 1 versus 2.TABLE 13.10 Scores for Physical Therapy Patients Subjected to Four Treatment Programs (1–14) 12 3 4 64 76 58 95 88 70 74 90 72 90 66 80 80 80 60 8779 75 82 8871 82 75 85 TABLE 13.11 Weight Gain of Pigs Subjected to Five Treatments (1–5) 1 2345 165 168 164 185 201 156 180 156 195 189 159 180 156 195 189159 180 189 184 173167 166 138 201 193170 170 153 165 164146 161 190 175 160130 171 160 187 200151 169 172 177 142164 179 142 166 184 158 191 155 165 149PROBLEMS 375 --- Page 387 --- 14 Two-Way Analysis-of-Variance: Balanced Case The two-way model without interaction has been illustrated in Section 12.1.2, Example 12.2.2b, and Section 12.8.",
    "model without interaction has been illustrated in Section 12.1.2, Example 12.2.2b, and Section 12.8. In this chapter, we consider the two-way ANOVA model with interaction. In Section 14.1 we discuss the model and attendant assumptions. In Section 14.2 we consider estimable functions involving main effects and interactions. In Section 14.3 we discuss estimation of the parameters, including solutions to the normal equations using side conditions and also using a generalized inverse.",
    "luding solutions to the normal equations using side conditions and also using a generalized inverse. In Section 14.4 we develop a hypothesis test for the interaction using a full– reduced model, and we obtain tests for main effects using the general linear hypoth- esis as well as the full–reduced-model approach. In Section 14.5 we derive expected mean squares from the basic deﬁnition and also using a general linear hypothesisapproach.",
    "e expected mean squares from the basic deﬁnition and also using a general linear hypothesisapproach. Throughout this chapter we consider only the balanced two-way model. The unbalanced case is covered in Chapter 15. 14.1 THE TWO-WAY MODEL The two-way balanced model can be speciﬁed as follows: y ijk¼mþaiþbjþgijþ1ijk (14 :1) i¼1,2,... ,a,j¼1,2,... ,b,k¼1,2,... ,n: The effect of factor Aat the ith level is ai, and the term bjis due to the jth level of factor B.",
    "n: The effect of factor Aat the ith level is ai, and the term bjis due to the jth level of factor B. The term gijrepresents the interaction ABbetween the ith level of Aand the jth level of B. If an interaction is present, the difference a1/C0a2, for example, is not estimable and the hypothesis H0:a1¼a2¼...¼aacannot be tested. In Section 14.4, we discuss modiﬁcations of this hypothesis that are testable. There are two experimental situations in which the model in (14.1) may arise.",
    "sis that are testable. There are two experimental situations in which the model in (14.1) may arise. In the ﬁrst setup, factors AandBrepresent two types of treatment, for example, various amounts of nitrogen and potassium applied in an agricultural experiment. We apply Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc.",
    ",Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 377 --- Page 388 --- each of the abcombinations of the levels of AandBtonrandomly selected exper- imental units. In the second situation, the populations exist naturally, for example, gender (males and females) and political preference (Democrats, Republicans, andIndependents). A random sample of nobservations is obtained from each of the ab populations.",
    "ans, andIndependents). A random sample of nobservations is obtained from each of the ab populations. Additional assumptions that form part of the model are the following: 1.E(1 ijk)¼0 for all i,j,k. 2. var(1ijk)¼s2for all i,j,k. 3. cov(1ijk,1rst)¼0 for ( i,j,k)=(r,s,t). 4. Another assumption that we sometimes add to the model is that 1ijkisN(0,s2) for all i,j,k. From assumption 1, we have E(yijk)¼mij¼mþaiþbjþgij, and we can rewrite the model in the form yijk¼mijþ1ijk, (14 :2) i¼1,2,...",
    "e E(yijk)¼mij¼mþaiþbjþgij, and we can rewrite the model in the form yijk¼mijþ1ijk, (14 :2) i¼1,2,... ,a,j¼1,2,... ,b,k¼1,2,... ,n, wheremij¼E(yijk) is the mean of a random observation in the ( ij)th cell. In the next section, we consider estimable functions of the parameters ai,bj, andgij. 14.2 ESTIMABLE FUNCTIONS In the ﬁrst part of this section, we use a¼3,b¼2, and n¼2 for expositional pur- poses.",
    "BLE FUNCTIONS In the ﬁrst part of this section, we use a¼3,b¼2, and n¼2 for expositional pur- poses. For this special case, the model in (14.1) becomes yijk¼mþaiþbjþgijþ1ijk,i¼1,2,3,j¼1,2,k¼1,2: (14 :3) The 12 observations in (14.3) can be expressed in matrix form as y111 y112 y121 y122 y211 y212 y221 y222 y311 y312 y321 y3220 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCA¼110010100000 110010100000 110001010000 110001010000 101010001000 101010001000 101001000100 101001000100 100110000010 100110000010100101000001 1001010000010 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCAm a1 a2 a3 b1 b2 g11 g12 g21 g22 g31 g320 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCAþ1 111 1112 1121 1122 1211 1212 1221 1222 1311 g312 1321 13220 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCA(14 :4)378 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 389 --- or y¼Xbþ1, where yis 12 /C21,Xis 12 /C212, andbis 12 /C21.",
    "-VARIANCE: BALANCED CASE --- Page 389 --- or y¼Xbþ1, where yis 12 /C21,Xis 12 /C212, andbis 12 /C21. (If we added another replication, so thatn¼3, then ywould be 18 /C21,Xwould be 18 /C212, butbwould remain 12 /C21.) The matrix X0Xis given by X0X¼1 244466222222 4 40022220000 4 04022002200 4 00422000022 6 22260202020 6 222060202022 20020200000 2 20002020000 2 02020002000 2 02002000200 2 00220000020 2 002020000020 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCA: (14 :5) The partitioning in X 0Xcorresponds to that in Xin (14.4), where there is a column for m, three columns for the three a’s, two columns for the two b’s, and six columns for the sixg’s.",
    "for m, three columns for the three a’s, two columns for the two b’s, and six columns for the sixg’s. In both XandX0X, the ﬁrst six columns can be obtained as linear combinations of the last six columns, which are clearly linearly independent. Hence rank( X)¼ rank( X0X)¼6 [in general, rank( X)¼ab]. Since rank( X)¼6, we can ﬁnd six linearly independent estimable functions of the parameters (see Theorem 12.2c). By Theorem 12.2b, we can obtain these estimable functions from Xb.",
    "e parameters (see Theorem 12.2c). By Theorem 12.2b, we can obtain these estimable functions from Xb. Using rows 1, 3, 5, 7, 9, and 11 of E(y)¼Xb, we obtain E(yijk)¼mij¼mþaiþbjþgijfori¼1,2,3 and j¼1, 2: m11¼mþa1þb1þg11,m12¼mþa1þb2þg12 m21¼mþa2þb1þg21,m22¼mþa2þb2þg22 m31¼mþa3þb1þg31,m32¼mþa3þb2þg32:(14 :6) These can also be obtained from the last six rows of X0Xb(see Theorem 12.2b).",
    "32¼mþa3þb2þg32:(14 :6) These can also be obtained from the last six rows of X0Xb(see Theorem 12.2b). By taking linear combinations of the six functions in (14.6), we obtain the follow- ing estimable functions (e.g., u1¼m11/C0m21andu0 1¼m12/C0m22): m11¼mþa1þb1þg11 u1¼a1/C0a2þg11/C0g21oru0 1¼a1/C0a2þg12/C0g22 u2¼a1/C0a3þg11/C0g31oru0 2¼a1/C0a3þg12/C0g32 u3¼b1/C0b2þg11/C0g12oru0 3¼b1/C0b2þg21/C0g22(14 :7)14.2 ESTIMABLE FUNCTIONS 379 --- Page 390 --- oru00 3¼b1/C0b2þg31/C0g32 u4¼g11/C0g12/C0g21þg22 u5¼g11/C0g12/C0g31þg32: The alternative expressions for u4andu5are of the form gij/C0gij0/C0gi0jþgi0j0,i,i0¼1,2,3,j,j0¼1,2,i=i0,j=j0: (14 :8) [For general aandb, we likewise obtain estimable functions of the form of (14.7) and (14.8).] Inu4andu5in (14.7), we see that there are estimable contrasts in the gij’s, but in u1,u2, andu3(and in the alternative expressions u10,u20,u30, andu300) there are no estimable contrasts in the a’s alone or b’s alone.",
    "ve expressions u10,u20,u30, andu300) there are no estimable contrasts in the a’s alone or b’s alone. (This is also true for the case of general aandb.) To obtain a single expression involving a1/C0a2for later use in comparing the a values in a hypothesis test (see Section 14.4.2b), we average u1andu10: 1 2(u1þu0 1)¼a1/C0a2þ12(g11þg12)/C012(g21þg22) ¼a1/C0a2þ/C22g1:/C0/C22g2:: (14 :9) Fora1/C0a3,w eh a v e 12(u2þu0 2)¼a1/C0a3þ1 2(g11þg12)/C012(g31þg32) ¼a1/C0a3þ/C22g1:/C0/C22g3:: (14 :10) Similarly, the average of u3,u0 3, andu00 3yields 1 3(u3þu0 3þu00 3)¼b1/C0b2þ1 3(g11þg21þg31)/C013(g12þg22þg32) ¼b1/C0b2þ/C22g:1/C0/C22g:2: (14 :11) From (14.1) and assumption 1 in Section 14.1, we have E(yijk)¼E(mþaiþbjþgijþ1ijk), i¼1,2,...",
    "14 :11) From (14.1) and assumption 1 in Section 14.1, we have E(yijk)¼E(mþaiþbjþgijþ1ijk), i¼1,2,... ,a,j¼1,2,... ,b,k¼1,2,... ,n or mij¼mþaiþbjþgij (14 :12)380 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 391 --- [see also (14.2) and (14.6)]. In Section 12.1.2, we demonstrated that for a simple additive (no-interaction) model the side conditions on the a’s andb’s led to redeﬁned a/C3’s andb/C3’s that could be expressed as deviations from means, for example, a/C3 i¼/C22mi:/C0/C22m::.",
    "s andb/C3’s that could be expressed as deviations from means, for example, a/C3 i¼/C22mi:/C0/C22m::. We now extend this formulation to an interaction model formij: mij¼/C22m::þ(/C22mi:/C0/C22m::)þ(/C22m:j/C0/C22m::)þ(mij/C0/C22mi:/C0/C22m:jþ/C22m::) ¼m/C3þa/C3 iþb/C3jþg/C3ij,(14 :13) where m/C3¼/C22m::,a/C3i¼/C22mi:/C0/C22m::,b/C3j¼/C22m:j/C0/C22m::, g/C3 ij¼mij/C0/C22mi:/C0/C22m:jþ/C22m:::(14 :14) With these deﬁnitions, it follows that Xa i¼1a/C3 i¼0,Xb j¼1b/C3j¼0, Xa i¼1g/C3ij¼0 for all j¼1,2,...",
    "ith these deﬁnitions, it follows that Xa i¼1a/C3 i¼0,Xb j¼1b/C3j¼0, Xa i¼1g/C3ij¼0 for all j¼1,2,... ,b, (14 :15) Xb j¼1g/C3ij¼0 for all i¼1,2,... ,a: Using (14.12), we can write a/C3 i,b/C3 j,andg/C3 ijin (14.14) in terms of the original parameters; for example, a/C3ibecomes a/C3 i¼/C22mi:/C0/C22m::¼1 bXb j¼1mij/C01 abX ijmij ¼1bX j(mþaiþbjþgij)/C01 abX ij(mþaiþbjþgij) ¼1 bbmþbaiþX jbjþX jgij ! /C01 ababmþbX iaiþaX jbjþX ijgij !",
    "j(mþaiþbjþgij)/C01 abX ij(mþaiþbjþgij) ¼1 bbmþbaiþX jbjþX jgij ! /C01 ababmþbX iaiþaX jbjþX ijgij ! ¼mþaiþ/C22b:þ/C22gi:/C0m/C0/C22a:/C0/C22b:/C0/C22g:: ¼ai/C0/C22a:þ/C22gi:/C0/C22g::: (14 :16)14.2 ESTIMABLE FUNCTIONS 381 --- Page 392 --- Similarly b/C3 j¼bj/C0/C22b:þ/C22g:j/C0/C22g::, (14 :17) g/C3ij¼gij/C0/C22gi:/C0/C22g:jþ/C22g::: (14 :18) 14.3 ESTIMATORS OF l0bANDs2 We consider estimation of estimable functions l0bin Section 14.3.1 and estimation ofs2in Section 14.3.2.",
    "onsider estimation of estimable functions l0bin Section 14.3.1 and estimation ofs2in Section 14.3.2. 14.3.1 Solving the Normal Equations and Estimating l0b We discuss two approaches for solving the normal equations X0X^b¼X0yand for obtaining estimates of an estimable function l0b.",
    "for solving the normal equations X0X^b¼X0yand for obtaining estimates of an estimable function l0b. 14.3.1.1 Side Conditions From Xandyin (14.4), we obtain X0yfor the special case a¼3,b¼2, and n¼2: X0y¼(y...,y1::,y2::,y3::,y1::,y:2:,y11:,y12:,y21:,y22:,y31:,y32:)0: (14 :19) On the basis of X0yin (14.19) and X0Xin (14.5), we write the normal equations X0X^b¼X0yin terms of general a,b, and n: abn ^mþbnXa i¼1^aiþanXb j¼1^bjþnXa i¼1Xb j¼1^gij¼y..., bn^mþbn^aiþnXb j¼1^bjþnXb j¼1^gij¼yi::,i¼1,2,...",
    "^mþbnXa i¼1^aiþanXb j¼1^bjþnXa i¼1Xb j¼1^gij¼y..., bn^mþbn^aiþnXb j¼1^bjþnXb j¼1^gij¼yi::,i¼1,2,... ,a, an^mþnXa i¼1^aiþan^bjþnXa i¼1^gij¼y:j:,j¼1,2,... ,b, n^mþn^aiþn^bjþn^gij¼yij:,i¼1,2,... ,a, j¼1,2,... ,b:(14 :20)382 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 393 --- With the side conditionsP i^ai¼0,P j^bj¼0,P i^gij¼0, andP j^gij¼0, the solution of the normal equations in (14.20) is given by ^m¼y...",
    "j^bj¼0,P i^gij¼0, andP j^gij¼0, the solution of the normal equations in (14.20) is given by ^m¼y... abn¼/C22y..., ^ai¼yi:: bn/C0^m¼/C22yi::/C0/C22y..., ^bj¼y:j: an/C0^m¼/C22y:j:/C0/C22y..., (14 :21) ^gij¼yij: n/C0yi:: bn/C0y:j: anþy... abn, ¼/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...: These are unbiased estimators of the parameters m/C3,a/C3 i,b/C3 j,g/C3 ijin (14.14), subject to the side conditions in (14.15).",
    "s of the parameters m/C3,a/C3 i,b/C3 j,g/C3 ijin (14.14), subject to the side conditions in (14.15). If side conditions are not imposed on the parameters, then the estimators in (14.21) are not unbiased estimators of individual parameters, but these estimators can still be used in estimable functions.",
    "estimators of individual parameters, but these estimators can still be used in estimable functions. For example, consider the estimable function l0bin (14.9) (for a¼3,b¼2): l0b¼a1/C0a2þ1 2(g11þg12)/C012(g21þg22): By Theorem 12.3a and (14.21), the estimator is given by l0^b¼^a1/C0^a2þ1 2(^g11þ^g12)/C012(^g21þ^g22) ¼/C22y1::/C0/C22y.../C0(/C22y2::/C0/C22y...)þ1 2(/C22y11:/C0/C22y1::/C0/C22y:1:þ/C22y...) þ1 2(/C22y12:/C0/C22y1::/C0/C22y:2:þ/C22y...)/C012(/C22y21:/C0/C22y2::/C0/C22y:1:þ/C22y...) /C01 2(/C22y22:/C0/C22y2::/C0/C22y:2:þ/C22y...): Since /C22y11:þ/C22y12:¼2/C22y1::and /C22y21:þ/C22y22:¼2/C22y2::, the estimator l0^b¼^a1/C0^a2þ 12ð^g11þ^g12Þ/C012ð^g21þ^g22Þreduces to l0^b¼^a1/C0^a2þ12(^g11þ^g12)/C012(^g21þ^g22)¼/C22y1::/C0/C22y2::: (14 :22) This estimator of a1/C0a2þ12(g11þg12)/C012(g21þg22) is the same as the estimator we would have for a/C3 1/C0a/C32, using ^a1and ^a2as estimators of a/C31anda/C32: da/C3 1/C0a/C32¼^a1/C0^a2¼/C22y1::/C0/C22y.../C0(/C22y2::/C0/C22y...)¼/C22y1::/C0/C22y2:::14.3 ESTIMATORS OF l0bANDs2383 --- Page 394 --- By Theorem 12.3d, such estimators are BLUE.",
    "/C22y2:::14.3 ESTIMATORS OF l0bANDs2383 --- Page 394 --- By Theorem 12.3d, such estimators are BLUE. If we also assume that 1ijkis N(0,s2), then by Theorem 12.3h, the estimators are minimum variance unbiased estimators. 14.3.1.2 Generalized Inverse By Corollary 1 to Theorem 2.8b, a generalized inverse of X0Xin (14.5) is given by (X0X)/C0¼1 2OO OI 6/C18/C19 , (14 :23) where the Osa r e6 /C26.",
    "inverse of X0Xin (14.5) is given by (X0X)/C0¼1 2OO OI 6/C18/C19 , (14 :23) where the Osa r e6 /C26. Then by (12.13) and (14.19), a solution to the normal equations for a¼3 and b¼2 is given by ^b¼(X0X)/C0X0y ¼(0,0,0,0,0,0,/C22y11:,/C22y12:,/C22y21:,/C22y22:,/C22y31:,/C22y32:)0: (14 :24) The estimators in (14.24) are different from those in (14.21), but they give the same estimators of estimable functions.",
    "4.24) are different from those in (14.21), but they give the same estimators of estimable functions. For example, for l0b¼a1/C0a2þ1 2(g11þg12)/C0 12(g21þg22) in (14.9), we have l0^b¼^a1/C0^a2þ12^g11þ^g12/C0(^g21þ^g22) ½/C138 ¼0/C00þ1 2/C22y11:þ/C22y12:/C0(/C22y21:þ/C22y22:) ½/C138 : It was noted preceding (14.22) that /C22y11:þ/C22y12:¼2/C22y1:and /C22y21:þ/C22y22:¼2/C22y2:.",
    "C138 : It was noted preceding (14.22) that /C22y11:þ/C22y12:¼2/C22y1:and /C22y21:þ/C22y22:¼2/C22y2:. Thusl0^b becomes l0^b¼1 2(2/C22y1:/C02/C22y2:)¼/C22y1::/C0/C22y2::, which is the same estimator as that obtained in (14.22) using ^bin (14.21). 14.3.2 An Estimator for s2 For the two-way model in (14.1), assumption 2 states that var( 1ijk)¼s2for all i,j,k. To estimate s2, we use (12.22), s2¼SSE =ab(n/C01), where abnis the number of rows of Xandabis the rank of X.",
    "ate s2, we use (12.22), s2¼SSE =ab(n/C01), where abnis the number of rows of Xandabis the rank of X. By (12.20) and (12.21), we have SSE ¼y0y/C0^b0X0y¼y0I/C0X(X0X)/C0X0½/C138 y:384 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 395 --- With ^bfrom (14.24) and X0yfrom (14.19), SSE can be written as SSE ¼y0y/C0^b0X0y ¼Xa i¼1Xb j¼1Xn k¼1y2 ijk/C0Xa i¼1Xb j¼1/C22yij:yij: ¼X ijky2ijk/C0nX ij/C22y2ij:: (14 :25) It can also be shown (see Problem 14.10) that this is equal to SSE ¼X ijk(yijk/C0/C22yij:)2: (14 :26) Thus, s2is given by either of the two forms s2¼P ijk(yijk/C0/C22yij:)2 ab(n/C01)(14 :27) ¼P ijky2 ijk/C0nP ij/C22y2ij: ab(n/C01): (14 :28) By Theorem 12.3e, E(s2)¼s2.",
    "j:)2 ab(n/C01)(14 :27) ¼P ijky2 ijk/C0nP ij/C22y2ij: ab(n/C01): (14 :28) By Theorem 12.3e, E(s2)¼s2. 14.4 TESTING HYPOTHESES In this section, we consider tests of hypotheses for the main effects AandBand for the interaction AB. Throughout this section, we assume that yisNabn(Xb,s2I). For expositional convenience, we sometimes illustrate with a¼3 and b¼2.",
    "assume that yisNabn(Xb,s2I). For expositional convenience, we sometimes illustrate with a¼3 and b¼2. 14.4.1 Test for Interaction In Section 14.4.1.1, we express the interaction hypothesis in terms of estimable para- meters, and in Sections 14.4.1.2 and 14.4.1.3, we discuss two approaches to the full–reduced-model test.",
    "rs, and in Sections 14.4.1.2 and 14.4.1.3, we discuss two approaches to the full–reduced-model test. 14.4.1.1 The Interaction Hypothesis By (14.8), estimable contrasts in the gij’s have the form gij/C0gij0/C0gi0jþgi0j0,i=i0,j=j0: (14 :29) We now show that the interaction hypothesis can be expressed in terms of these esti- mable functions. For the illustrative model in (14.3) with a¼3 and b¼2, the cell means in (14.12) are given in Figure 14.1.",
    "e illustrative model in (14.3) with a¼3 and b¼2, the cell means in (14.12) are given in Figure 14.1. The Beffect at the ﬁrst level of Aism11/C0m12, the Beffect at the second level of Aism21/C0m22, and the Beffect at the third level of Aism31/C0m32.14.4 TESTING HYPOTHESES 385 --- Page 396 --- If these three Beffects are equal, we have no interaction. If at least one effect differs from the other two, we have an interaction.",
    ", we have no interaction. If at least one effect differs from the other two, we have an interaction. The hypothesis of no interaction can therefore be expressed as H0:m11/C0m12¼m21/C0m22¼m31/C0m32: (14 :30) To show that this hypothesis is testable, we ﬁrst write the three differences in terms of thegij’s by using (14.12).",
    "s hypothesis is testable, we ﬁrst write the three differences in terms of thegij’s by using (14.12). For the ﬁrst two differences in (14.30), we obtain m11/C0m12¼mþa1þb1þg11/C0(mþa1þb2þg12) ¼b1/C0b2þg11/C0g12, m21/C0m22¼mþa2þb1þg21/C0(mþa2þb2þg22) ¼b1/C0b2þg21/C0g22: Then the equality m11/C0m12¼m21/C0m22in (14.30) becomes b1/C0b2þg11/C0g12¼b1/C0b2þg21/C0g22 or g11/C0g12/C0g21þg22¼0: (14 :31) The function g11/C0g12/C0g21þg22on the left side of (14.31) is an estimable contrast [see (14.29)].",
    "The function g11/C0g12/C0g21þg22on the left side of (14.31) is an estimable contrast [see (14.29)]. Similarly, the third difference in (14.30) becomes m31/C0m32¼b1/C0b2þg31/C0g32, and when this is set equal to m21/C0m22¼b1/C0b2þg21/C0g22, we obtain g21/C0g22/C0g31þg32¼0: (14 :32)Figure 14.1 Cell means for the model in (14.2) and (14.12).386 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 397 --- By (14.29), the function g21/C0g22/C0g31þg32on the left side of (14.32) is esti- mable.",
    "Page 397 --- By (14.29), the function g21/C0g22/C0g31þg32on the left side of (14.32) is esti- mable. Thus the two expressions in (14.31) and (14.32) are equivalent to the inter- action hypothesis in (14.30), and the hypothesis is therefore testable. Since the interaction hypothesis can be expressed in terms of estimable functions ofgij’s that do not involve ai’s orbj’s, we can proceed with a full–reduced-model approach.",
    "unctions ofgij’s that do not involve ai’s orbj’s, we can proceed with a full–reduced-model approach. On the other hand, by (14.7), the a’s andb’s are not estimable without theg’s. We therefore have to redeﬁne the main effects in order to get a test in the pre- sence of interaction; see Section 14.4.2.",
    "edeﬁne the main effects in order to get a test in the pre- sence of interaction; see Section 14.4.2. To get a reduced model from (14.1) or (14.3), we work with g/C3 ij¼ mij/C0/C22mi:/C0/C22m:jþ/C22m::in (14.14), which is estimable [it can be estimated unbiasedly by^gij¼/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...in (14.21)].",
    "estimable [it can be estimated unbiasedly by^gij¼/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...in (14.21)]. Using (14.13), the model can be expressed in terms of parameters subject to the side conditions in (14.15): yijk¼m/C3þa/C3 iþb/C3jþg/C3ijþ1ijk, ð14:33Þ We can get a reduced model from (14.33) by setting g/C3 ij¼0. In the following theorem, we show that H0:g/C3ij¼0 for all i, jis equivalent to the interaction hypothesis expressed as (14.30) or as (14.31) and (14.32).",
    "all i, jis equivalent to the interaction hypothesis expressed as (14.30) or as (14.31) and (14.32). Since all three of these expressions involve a¼3 and b¼2, we continue with this illustrative special case. Theorem 14.4a. Consider the model (14.33) for a¼3 and b¼2.",
    "inue with this illustrative special case. Theorem 14.4a. Consider the model (14.33) for a¼3 and b¼2. The hypothesis H0:g/C3 ij¼0,i¼1,2,3,j¼1,2, is equivalent to (14.30) H0:m11/C0m12¼m21/C0m22¼m31/C0m32, (14 :34) and to the equivalent form H0:g11/C0g12/C0g21þg22 g21/C0g22/C0g31þg32/C18/C19 ¼0 0/C18/C19 (14 :35) obtained from (14.31) and (14.32). PROOF. To establish the equivalence of g/C3 ij¼0 and the ﬁrst equality in (14.35), we ﬁnd an expression for each gijby setting g/C3ij¼0.",
    "of g/C3 ij¼0 and the ﬁrst equality in (14.35), we ﬁnd an expression for each gijby setting g/C3ij¼0. Forg12andg/C312, for example, we use (14.18) to obtain g/C3 12¼g12/C0/C22g1:/C0/C22g:2þ/C22g::: (14 :36) Theng/C3 12¼0 gives g12¼/C22g1:þ/C22g:2/C0/C22g:::14.4 TESTING HYPOTHESES 387 --- Page 398 --- Similarly, from (14.18) and the equalities g/C3 11¼0,g/C321¼0, andg/C322¼0, we obtain g11¼/C22g1:þ/C22g:1/C0/C22g::,g21¼/C22g2:þ/C22g:1/C0/C22g::,g22¼/C22g2:þ/C22g:2/C0/C22g::: When these are substituted into g11/C0g12/C0g21þg22, we obtain g11/C0g12/C0g21þg22¼/C22g1:þ/C22g:1/C0/C22g::/C0(/C22g1:þ/C22g:2/C0/C22g::) /C0(/C22g2:þ/C22g:1/C0/C22g::)þ/C22g2:þ/C22g:2/C0/C22g:: ¼0, which is the ﬁrst equality in (14.35).",
    "/C0(/C22g2:þ/C22g:1/C0/C22g::)þ/C22g2:þ/C22g:2/C0/C22g:: ¼0, which is the ﬁrst equality in (14.35). The second equality in (14.35) is obtained similarly. To show that the ﬁrst equality in (14.34) is equivalent to the ﬁrst equality in (14.35), we substitute mij¼mþaiþbjþgijintom11/C0m12¼m21/C0m22: 0¼m11/C0m12/C0m21þm22 ¼mþa1þb1þg11/C0(mþa1þb2þg12) /C0(mþa2þb1þg21)þmþa2þb2þg22 ¼g11/C0g12/C0g21þg22: Similarly, the second equality in (14.34) is equivalent to the second equality in (14.35).",
    "0g21þg22: Similarly, the second equality in (14.34) is equivalent to the second equality in (14.35). A In Section 14.4.1.2, we obtain the test for interaction based on the normal equations, and in Section 14.4.1.3, we give the test based on a generalized inverse. 14.4.1.2 Full–Reduced-Model Test Based on the Normal Equations In this section, we develop the full–reduced-model test for interaction using the normal equations.",
    "In this section, we develop the full–reduced-model test for interaction using the normal equations. We express the full model in terms of parameters subject to side conditions, as in (14.33) yijk¼m/C3þa/C3 iþb/C3jþg/C3ijþ1ijk; ð14:37Þ wherem/C3¼/C22m::,a/C3 i¼/C22mi:/C0/C22m::,b/C3 j¼/C22m:j/C0/C22m::, andg/C3 ij¼mij/C0/C22mi:/C0/C22m:jþ/C22m::are as given in (14.14).",
    "/C22m::,b/C3 j¼/C22m:j/C0/C22m::, andg/C3 ij¼mij/C0/C22mi:/C0/C22m:jþ/C22m::are as given in (14.14). The reduced model under H0:g/C3ij¼0 for all iandjis yijk¼m/C3þa/C3 iþb/C3jþ1ijk: (14 :38) Since we are considering a balanced model, the parameters m/C3,a/C3 i, andb/C3 j(subject to side conditions) in the reduced model (14.38) are the same as those in the full388 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 399 --- model (14.37) [in (14.44), the estimates in the two models are also shown to be the same].",
    "e 399 --- model (14.37) [in (14.44), the estimates in the two models are also shown to be the same]. Using the notation of Chapter 13, the sum of squares for testing H0:g/C3 ij¼0i s given by SS(gjm,a,b)¼SS(m,a,b,g)/C0SS(m,a,b): (14 :39) The estimators ^m,^ai,^bj,^gijin (14.21) are unbiased estimators of m/C3,a/C3i,b/C3 j,g/C3 ij.",
    ":39) The estimators ^m,^ai,^bj,^gijin (14.21) are unbiased estimators of m/C3,a/C3i,b/C3 j,g/C3 ij. Extending X0yin (14.19) from a¼3 and b¼2 to general aandb, we obtain SS(m,a,b,g)¼^b0X0y ¼^my...þXa i¼1^aiyi::þXb j¼1^bjy:j:þXa i¼1Xb j¼1^gijyij: ¼/C22y...y...þX i(/C22yi::/C0/C22y...)yi...þX j(/C22y:j:/C0/C22y...)y:j: þX ij(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...)yij: ¼y2... abnþX iy2i:: bn/C0y2... abn ! þX jy2:j: an/C0y2... abn ! þX ijy2ij: n/C0X iy2i:: bn/C0X jy2:j: anþy2... abn !",
    "i:: bn/C0y2... abn ! þX jy2:j: an/C0y2... abn ! þX ijy2ij: n/C0X iy2i:: bn/C0X jy2:j: anþy2... abn ! (14 :40) ¼X ijy2ij: n: ð14:41Þ Note that we would obtain the same result using ^bin (14.24) (extended to general a andb). For the reduced model in (14.38), the X1matrix and X0 1yvector for a¼3 and b¼ 2 consist of the ﬁrst six columns of Xin (14.4‘) and the ﬁrst six elements of X0yin (14.19).",
    "and b¼ 2 consist of the ﬁrst six columns of Xin (14.4‘) and the ﬁrst six elements of X0yin (14.19). We thus obtain X0 1X1¼12 4 4 4 6 6 44 0 0 2 2 40 4 0 2 2 40 0 4 2 262 2 2 6 0 62 2 2 0 60 BBBBBB@1 CCCCCCA,X 0 1y¼y... y1:: y2:: y3:: y:1: y:2:0 BBBBBB@1 CCCCCCA: (14 :42)14.4 TESTING HYPOTHESES 389 --- Page 400 --- From the pattern in (14.42), we see that for general aandbthe normal equations for the reduced model become abn ^mþbnXa i¼1^aiþanXb j¼1^bj¼y..., bn^mþbn^aiþnXb j¼1^bj¼yi::,i¼1,2,...",
    "r the reduced model become abn ^mþbnXa i¼1^aiþanXb j¼1^bj¼y..., bn^mþbn^aiþnXb j¼1^bj¼yi::,i¼1,2,... ,a, (14 :43) an^mþnXa i¼1^aiþan^bj¼y:j:,j¼1,2,... ,b: Using the side conditionsP i^ai¼0 andP j^bj¼0, we obtain the solutions ^m¼y... abn¼/C22y...,^ai¼yi:: bn/C0^m¼/C22yi::/C0/C22y...,^bj¼y:j: an/C0^m¼/C22y:j:/C0/C22y...:(14 :44) These solutions are the same as those for the full model in (14.21), as expected in the case of a balanced model.",
    "ns are the same as those for the full model in (14.21), as expected in the case of a balanced model. The sum of squares for the reduced model is therefore SS(m,a,b)¼^b0 1X01y ¼y2 ... abnþX iy2i:: bn/C0y2... abn ! þX jy2:j: an/C0y2... abn ! , and the difference in (14.39) is SS(gjm,a,b)¼SS(m,a,b,g)/C0SS(m,a,b) ¼X ijy2ij: n/C0X iy2i:: bn/C0X jy2:j: anþy2... abn: (14 :45) The error sum of squares is given by SSE ¼y0y/C0^b0X0y ¼X ijky2 ijk/C0X ijy2 ij: n(14 :46) (see Problem 14.13b).",
    "m of squares is given by SSE ¼y0y/C0^b0X0y ¼X ijky2 ijk/C0X ijy2 ij: n(14 :46) (see Problem 14.13b). In terms of means rather than totals, (14.45) and (14.46) become SS(gjm,a,b)¼nX ij(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...)2, (14 :47) SSE ¼X ijk(yijk/C0/C22yij:)2: (14 :48)390 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 401 --- There are abparameters involved in the hypothesis H0:g/C3 ij¼0,i¼1,2,... ,a, j¼1,2,... ,b. However, the aþbside conditionsP ig/C3ij¼0 for j¼1,2,...",
    "is H0:g/C3 ij¼0,i¼1,2,... ,a, j¼1,2,... ,b. However, the aþbside conditionsP ig/C3ij¼0 for j¼1,2,... ,b andP jg/C3ij¼0 for i¼1,2,... ,aimpose a/C01+ b/C01 restrictions. With the additional conditionPa i¼1Pb j¼1g/C3 ij¼0, we have a total of aþb/C02þ1¼ aþb/C01 restrictions. Therefore the degrees of freedom for SS( gjm,a,b)a r e ab/C0(aþb/C01)¼(a/C01)(b/C01) (see Problem 14.14).",
    "fore the degrees of freedom for SS( gjm,a,b)a r e ab/C0(aþb/C01)¼(a/C01)(b/C01) (see Problem 14.14). To test H0:g/C3ij¼0 for all i,j, we therefore use the test statistic F¼SS(gjm,a,b)=(a/C01)(b/C01) SSE =ab(n/C01), (14 :49) which is distributed as F[(a/C01)(b/C01),ab(n/C01)] if H0is true (see Section 12.7.2). 14.4.1.3 Full–Reduced-Model Test Based on a Generalized Inverse We now consider a matrix development of SSE and SS( gjm,a,b) based on a gener- alized inverse.",
    "verse We now consider a matrix development of SSE and SS( gjm,a,b) based on a gener- alized inverse. By (12.21), SSE ¼y0[I/C0X(X0X)/C0X0]y. For our illustrative model with a¼3,b¼2, and n¼2, the matrix X0Xis given in (14.5) and a generalized inverse (X0X)/C0is provided in (14.23). The 12 /C212 matrix X(X0X)/C0X0is then given by X(X0X)/C0X0¼1 2JO /C1/C1/C1O OJ /C1/C1/C1O ......... OO /C1/C1/C1J0 BBB@1 CCCA¼1 2jj0O /C1/C1/C1O Oj j0/C1/C1/C1O .........",
    "1/C1O OJ /C1/C1/C1O ......... OO /C1/C1/C1J0 BBB@1 CCCA¼1 2jj0O /C1/C1/C1O Oj j0/C1/C1/C1O ......... OO /C1/C1/C1jj00 BBB@1 CCCA, (14 :50) where JandOare 2 /C22 and jis 2/C21 (see Problem 14.17). The vector yin (14.4) can be written as y¼y11 y12 y21 y22 y31 y320 BBBBBB@1 CCCCCCA, (14 :51) where y ij¼yij1 yij2/C18/C19 ,i¼1,2,3,j¼1,2.",
    "s y¼y11 y12 y21 y22 y31 y320 BBBBBB@1 CCCCCCA, (14 :51) where y ij¼yij1 yij2/C18/C19 ,i¼1,2,3,j¼1,2. By (12.21), (14.50), and (14.51), SSE becomes SSE ¼y0I/C0X(X0X)/C0X0½/C138 y¼y0y/C0y0X(X0X)/C0X0y ¼X ijky2 ijk/C01 2P ijy0 ijjj0yij¼P ijky2ijk/C01 2P ijy2 ij:, which is the same as (14.46) with n¼2.14.4 TESTING HYPOTHESES 391 --- Page 402 --- For SS(gjm,a,b), we obtain SS(gjm,a,b)¼SS(m,a,b,g)/C0SS(m,a,b) ¼^b0X0y/C0^b0 1X0 1y ¼y0[X(X0X)/C0X0/C0X1(X01X1)/C0X01]y, (14 :52) where X(X0X)/C0X0is as found in (14.50) and X1consists of the ﬁrst six columns of X in (14.4).",
    ":52) where X(X0X)/C0X0is as found in (14.50) and X1consists of the ﬁrst six columns of X in (14.4). The matrix X0 1X1is given in (14.42), and a generalized inverse of X01X1is given by (X0 1X1)/C0¼1 12/C0100000 030000 003000000300 000020 0000020 BBBBBB@1 CCCCCCA: (14 :53) Then X 1(X0 1X1)/C0X01¼1 124J2JJ /C0JJ /C0J 2J4J/C0JJ /C0JJ J/C0J4J2JJ /C0J /C0JJ 2J4J/C0JJ J/C0JJ /C0J4J2J /C0JJ /C0JJ 2J4J0 BBBBBB@1 CCCCCCA, (14 :54) where Jis 2/C22.",
    "J /C0J /C0JJ 2J4J/C0JJ J/C0JJ /C0J4J2J /C0JJ /C0JJ 2J4J0 BBBBBB@1 CCCCCCA, (14 :54) where Jis 2/C22. For the difference between (14.50) and (14.54), we obtain X(X 0X)/C0X0/C0X1(X0 1X1)/C0X01¼1 122J/C02J /C0JJ /C0JJ /C02J 2JJ /C0JJ /C0J /C0JJ 2J/C02J /C0JJ J /C0J/C02J 2JJ /C0J /C0JJ /C0JJ 2J/C02J J /C0JJ /C0J/C02J 2J0 BBBBBBBB@1 CCCCCCCCA, (14 :55) where Jis 2/C22.",
    "J 2JJ /C0J /C0JJ /C0JJ 2J/C02J J /C0JJ /C0J/C02J 2J0 BBBBBBBB@1 CCCCCCCCA, (14 :55) where Jis 2/C22. To show that SS( gjm,a,b)¼y0[X(X0X)/C0X0/C0X1(X0 1X1)/C0X0]yin (14.52) is equal to the formulation of SS( gjm,a,b) shown in (14.45), we ﬁrst write (14.45)392 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 403 --- in matrix notation: X3 i¼1X2 j¼1y2 ij: 2/C0X3 i¼1y2i:: 4/C0X2 j¼1y2:j: 6þy2...",
    "CASE --- Page 403 --- in matrix notation: X3 i¼1X2 j¼1y2 ij: 2/C0X3 i¼1y2i:: 4/C0X2 j¼1y2:j: 6þy2... 12¼y01 2A/C014B/C016Cþ1 12D/C0/C1 y: (14 :56) We now ﬁnd A,B,C, and D.F o r12P ijy2 ij:¼1 2y0Ay, we have by (14.50) and (14.51), 12X ijy2 ij:¼1 2X3 i¼1X2 j¼1y0 ijjj0yij, where jis 2/C21. This can be written as 1 2X ijy2 ij:¼1 2(y0 11,y012,... ,y032)jj0O /C1/C1/C1O Oj j0/C1/C1/C1O ......... OO /C1/C1/C1jj00 BBBB@1 CCCCAy 11 y12 ...",
    "1,y012,... ,y032)jj0O /C1/C1/C1O Oj j0/C1/C1/C1O ......... OO /C1/C1/C1jj00 BBBB@1 CCCCAy 11 y12 ... y320 BBBB@1 CCCCA(14 :57) ¼ 1 2y0Ay , where A¼JO /C1/C1/C1O OJ /C1/C1/C1O ......... OO /C1/C1/C1J0 BBB@1 CCCA, andJis 2/C22. Note that by (14.50), we also have1 2A¼X(X0X)/C0X0.",
    "....... OO /C1/C1/C1J0 BBB@1 CCCA, andJis 2/C22. Note that by (14.50), we also have1 2A¼X(X0X)/C0X0. For the second term in (14.56),1 4P iy2 i::, we ﬁrst use (14.51) to write yi::and y2i::as yi::¼X jkyijk¼X kyi1kþX kyi2k¼y0 i1jþy0i2j¼(y0i1,y0i2)j j/C18/C19 , y2 i::¼(y0i1,y0i2)j j/C18/C19 (j0,j0)yi1 yi2/C18/C19 ¼(y0 i1,y0i2)jj0jj0 jj0jj0/C18/C19yi1 yi2/C18/C19 :14.4 TESTING HYPOTHESES 393 --- Page 404 --- Thus1 4P3 i¼1y2 i::can be written as 1 4X3 i¼1y2 i::¼1 4(y0 11,y012,...",
    "OTHESES 393 --- Page 404 --- Thus1 4P3 i¼1y2 i::can be written as 1 4X3 i¼1y2 i::¼1 4(y0 11,y012,... ,y032)J JOOOO J JOOOO OOJ JOO OOJ JOO OOOOJ J OOOOJ J0 BBBBBBBB@1 CCCCCCCCAy 11 y12 ...",
    "11,y012,... ,y032)J JOOOO J JOOOO OOJ JOO OOJ JOO OOOOJ J OOOOJ J0 BBBBBBBB@1 CCCCCCCCAy 11 y12 ... y310 BBBB@1 CCCCA(14 :58) ¼1 4y0By: Similarly, the third term of (14.56),1 6P2 j¼1y2 :j:, can be written as 1 6X2 j¼1y2 :j:¼1 6y0JOJOJO OJOJOJ JOJOJO OJOJOJ JOJOJO OJOJOJ0 BBBBBB@1 CCCCCCAy¼1 6y0Cy: (14 :59) For the fourth term of (14.56), y2 ...=12, we have y...¼X ijkyijk¼y0j12, 1 12y2...¼1 12y0j12j0 12y¼1 12y0J12y¼1 12y0Dy , (14 :60) where j12is 12 /C21 and J12is 12 /C212.",
    "12, 1 12y2...¼1 12y0j12j0 12y¼1 12y0J12y¼1 12y0Dy , (14 :60) where j12is 12 /C21 and J12is 12 /C212. To conform with A,B, and Cin (14.57), (14.58), and (14.59), we write D¼J12as D¼J12¼JJJJJJ JJJJJJ JJJJJJ JJJJJJJJJJJJ JJJJJJ0 BBBBBB@1 CCCCCCA, where Jis 2/C22.394 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 405 --- Now, combining (14.57)–(14.60), we obtain the matrix of the quadratic form in (14.56): 1 2A/C014B/C016Cþ1 12D¼1 122J/C02J /C0JJ /C0JJ /C02J 2JJ /C0JJ /C0J /C0JJ 2J/C02J /C0JJ J /C0J/C02J 2JJ /C0J /C0JJ /C0JJ 2J/C02J J /C0JJ /C0J/C02J 2J0 BBBBBB@1 CCCCCCA,(14 :61) which is the same as (14.55).",
    "0J /C0JJ /C0JJ 2J/C02J J /C0JJ /C0J/C02J 2J0 BBBBBB@1 CCCCCCA,(14 :61) which is the same as (14.55). Thus the matrix version of SS( gjm,a,b) in (14.52) is equal to SS( gjm,a,b) in (14.45): y0[X(X0X)/C0X0/C0X1(X0 1X1)/C0X01]y¼X ijy2 ij n/C0X iy2i:: bn/C0X jy2:j: anþy2... abn: 14.4.2 Tests for Main Effects In Section 14.4.2.1, we develop a test for main effects using the full–reduced–model approach.",
    "fects In Section 14.4.2.1, we develop a test for main effects using the full–reduced–model approach. In Section 14.4.2.2, a test for main effects is obtained using the general linear hypothesis approach. Throughout much of this section, we use a¼3 and b¼2, where ais the number of levels of factor Aandbis the number of levels of factor B.",
    "use a¼3 and b¼2, where ais the number of levels of factor Aandbis the number of levels of factor B. 14.4.2.1 Full–Reduced-Model Approach If interaction is present in the two-way model, then by (14.9) and (14.10), we cannottestH 0:a1¼a2¼a3(fora¼3) because a1/C0a2anda1/C0a3are not estimable. In fact, there are no estimable contrasts in the a’s alone or the b’s alone (see Problem 14.2). Thus, if there is interaction, the effect of factor Ais different for each level of factor Band vice versa.",
    "f there is interaction, the effect of factor Ais different for each level of factor Band vice versa. To examine the main effect of factor A, we consider a/C3 i¼/C22mi:/C0/C22m::, as deﬁned in (14.14). This can be written as a/C3 i¼/C22mi:/C0/C22m::¼Xb j¼1mij b/C0Xa i¼1Xb j¼1mij ab ¼1 bX jmij/C0X imij a ! ¼1bX j(mij/C0/C22m:j): (14 :62) The expression in parentheses, mij/C0/C22m:j, is the effect of the ith level of factor Aat the jth level of factor B.",
    "parentheses, mij/C0/C22m:j, is the effect of the ith level of factor Aat the jth level of factor B. Thus in (14.62), a/C3 i¼/C22mi:/C0/C22m::is expressed as the average effect14.4 TESTING HYPOTHESES 395 --- Page 406 --- of the ith level of factor A(averaged over the levels of B). This deﬁnition leads to the side conditionP ia/C3 i¼0.",
    "l of factor A(averaged over the levels of B). This deﬁnition leads to the side conditionP ia/C3 i¼0. Since the a/C3i’s are estimable [see (14.21) and the comment following], we can use them to express the hypothesis for factor A.F o r a¼3, this becomes H0:a/C3 1¼a/C32¼a/C33, (14 :63) which is equivalent to H0:a/C31¼a/C32¼a/C33¼0 (14 :64) becauseP ia/C3 i¼0. The hypothesis H0:a/C31¼a/C32¼a/C33in (14.63) states that there is no effect of factor Awhen averaged over the levels of B.",
    "¼a/C32¼a/C33in (14.63) states that there is no effect of factor Awhen averaged over the levels of B. Usinga/C3i¼/C22mi:/C0/C22m::, we can express H0:a/C31¼a/C32¼a/C33in terms of means: H0:/C22m1:/C0/C22m::¼/C22m2:/C0/C22m::¼/C22m3:/C0/C22m::, which can be written as H0:/C22m1:¼/C22m2:¼/C22m3:: The values for the cell means in Figure 14.2 illustrate a situation in which H0holds in the presence of interaction.",
    "he cell means in Figure 14.2 illustrate a situation in which H0holds in the presence of interaction. Because H0in (14.63) or (14.64) is based on an average effect, many texts recommend that the interaction ABbe tested ﬁrst, and if it is found to be signiﬁcant, then the main effects should not be tested. However, with the main effect of Adeﬁned as the average effect over the levels of Band similarly for the effect of B, the tests for Aand Bcan be carried out even if ABis signiﬁcant.",
    "Band similarly for the effect of B, the tests for Aand Bcan be carried out even if ABis signiﬁcant. Admittedly, interpretation requires more care, and the effect of a factor may change if the number of levels of the other factor is altered. But in many cases useful information can be gained about the main effects in the presence of interaction.",
    "n many cases useful information can be gained about the main effects in the presence of interaction. Figure 14.2 Cell means illustrating /C22m1:¼/C22m2:¼/C22m3:in the presence of interaction.396 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 407 --- Under H0:a/C3 1¼a/C32¼a/C33¼0, the full model in (14.33) reduces to yijk¼m/C3þb/C3 jþg/C3ijþ1ijk: (14 :65) Because of the orthogonality of the balanced model, the estimators of m/C3,b/C3 j,andg/C3 ij in (14.65) are the same as in the full model.",
    "lanced model, the estimators of m/C3,b/C3 j,andg/C3 ij in (14.65) are the same as in the full model. If we use ^m,^bj,and ^gijin (14.21) and elements of X0yin (14.19) extended to general a,b, and n, we obtain SS(m,b,g)¼^my...þXb j¼1^bjy:j:þXa i¼1Xb j¼1^gijyij:, which, by (14.40), becomes SS(m,b,g)¼y2... abnþX jy2:j: an/C0y2... abn ! þX ijy2 ij: n/C0X iy2 i:: bn/C0X jy2 :j: anþy2 ... abn ! : (14 :66) From (14.40) and (14.66), we have SS(ajm,b,g)¼SS(m,a,b,g)/C0SS(m,b,g) ¼Xa i¼1y2i:: bn/C0y2...",
    "4 :66) From (14.40) and (14.66), we have SS(ajm,b,g)¼SS(m,a,b,g)/C0SS(m,b,g) ¼Xa i¼1y2i:: bn/C0y2... abn: (14 :67) For the special case of a¼3, we see by (14.7) that there are two linearly independent estimable functions involving the three a’s. Therefore, SS( ajm,b,g) has 2 degrees of freedom. In general, SS( ajm,b,g) has a/C01 degrees of freedom. In an analogous manner, for factor Bwe obtain SS(bjm,a,g)¼SS(m,a,b,g)/C0SS(m,a,g) ¼Xb j¼1y2:j: an/C0y2...",
    "analogous manner, for factor Bwe obtain SS(bjm,a,g)¼SS(m,a,b,g)/C0SS(m,a,g) ¼Xb j¼1y2:j: an/C0y2... abn, (14 :68) which has b/C01 degrees of freedom.",
    "g)¼SS(m,a,b,g)/C0SS(m,a,g) ¼Xb j¼1y2:j: an/C0y2... abn, (14 :68) which has b/C01 degrees of freedom. In terms of means, we can express (14.67) and (14.68) as SS(ajm,b,g)¼bnXa i¼1(/C22yi::/C0/C22y...)2, (14 :69)14.4 TESTING HYPOTHESES 397 --- Page 408 --- SS(bjm,a,g)¼anXb j¼1(/C22y:j:/C0/C22y...)2: (14 :70) It is important to note that the full–reduced-model approach leading to SS(ajm,b,g) in (14.67) cannot be expressed in terms of matrices in a manner analogous to that in (14.52) for the interaction, namely, SS( gjm,a,b)¼ y0[X(X0X)/C0X0/C0X1(X0 1X1)/C0X01]y:The matrix approach is appropriate for the inter- action because there are estimable functions of the gij’s that do not involve mor theaiorbjterms.",
    "r- action because there are estimable functions of the gij’s that do not involve mor theaiorbjterms. In the case of the Amain effect, however, we cannot obtain a matrix X1by deleting the three columns of Xcorresponding to a1,a2,anda3 because contrasts of the form a1/C0a2are not estimable without involving the gij’s [see (14.9) and (14.10)].",
    "contrasts of the form a1/C0a2are not estimable without involving the gij’s [see (14.9) and (14.10)]. If we add the sums of squares for factor A,B, and the interaction in (14.67), (14.68), and (14.45), we obtainP ijy2 ij:=n/C0y2...=abn, which is the overall sum of squares for “treatments,” SS( a,b,gjm). This can also be seen in (14.40). In the fol- lowing theorem, the three sums of squares are shown to be independent. Theorem 14.4b.",
    "). In the fol- lowing theorem, the three sums of squares are shown to be independent. Theorem 14.4b. IfyisNabn(Xb,s2I), then SS( ajm,b,g),SS(bjm,a,g), and SS(gjm,a,b) are independent. PROOF. This follows from Theorem 5.6c; see Problem 14.23. A Using (14.45), (14.46), (14.67), and (14.68), we obtain the analysis-of-variance (ANOVA) table given in Table 14.1. TABLE 14.1 ANOVA Table for a Two-Way Model with Interaction Source of Variation df Sum of Squares Factor Aa 21 P iy2 i:: bn/C0y2...",
    "o-Way Model with Interaction Source of Variation df Sum of Squares Factor Aa 21 P iy2 i:: bn/C0y2... abn Factor Bb 21 P jy2:j: an/C0y2... abn Interaction ( a21)(b21)P ijy2 ij: n/C0X iy2i:: bn/C0X jy2:j: anþy2... abn Error ab(n21)P ijky2 ijk/C0P ijy2ij: n Total abn 21P ijky2ijk/C0y2...",
    "i:: bn/C0X jy2:j: anþy2... abn Error ab(n21)P ijky2 ijk/C0P ijy2ij: n Total abn 21P ijky2ijk/C0y2... abn398 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 409 --- The test statistic for factor Ais F¼SS(ajm,b,g)=(a/C01) SSE =ab(n/C01), (14 :71) which is distributed as F[a/C01,ab(n/C01)] if H0:a/C3 1¼a/C32¼/C1/C1/C1¼a/C3a¼0 is true.",
    "), (14 :71) which is distributed as F[a/C01,ab(n/C01)] if H0:a/C3 1¼a/C32¼/C1/C1/C1¼a/C3a¼0 is true. For factor B, we use SS( bjm,a,g) in (14.68), and the Fstatistic is given by F¼SS(bjm,a,g)=(b/C01) SSE =ab(n/C01), which is distributed as F[b/C01,ab(n/C01)] if H0:b/C3 1¼b/C32¼/C1/C1/C1¼b/C3b¼0 is true. In Section 14.4.2.2, these Fstatistics are obtained by the general linear hypothesis approach. The Fdistributions can thereby be justiﬁed by Theorem 12.7c. Example 14.4.",
    "near hypothesis approach. The Fdistributions can thereby be justiﬁed by Theorem 12.7c. Example 14.4. The moisture content of three types of cheese made by two methods was recorded by Marcuse (1949) (format altered). Two cheeses were measured for each type and each method. If method is designated as factor Aandtype is factor B, then a¼2,b¼3, and n¼2. The data are given in Table 14.2, and the totals are shown in Table 14.3.",
    "B, then a¼2,b¼3, and n¼2. The data are given in Table 14.2, and the totals are shown in Table 14.3. The sum of squares for factor Ais given by (14.67) as SS(ajm,b,g)¼X2 i¼1y2 i:: (3)(2)/C0y2...",
    "14.3. The sum of squares for factor Ais given by (14.67) as SS(ajm,b,g)¼X2 i¼1y2 i:: (3)(2)/C0y2... (2)(3)(2) ¼1 6[(221 :98)2þ(220 :81)2]/C01 12(442 :79)2 ¼:114075 : TABLE 14.2 Moisture Content of Two Cheeses from Each of Three Different Types Made by Two Methods MethodType of Cheese 12 3 1 39.02 35.74 37.02 38.79 35.41 36.00 2 38.96 35.58 35.70 39.01 35.52 36.0414.4 TESTING HYPOTHESES 399 --- Page 410 --- Similarly, for factor Bwe use (14.68): SS(bjm,a,g)¼X3 j¼1y2 :j: (2)(2)/C0y2...",
    "399 --- Page 410 --- Similarly, for factor Bwe use (14.68): SS(bjm,a,g)¼X3 j¼1y2 :j: (2)(2)/C0y2... 12 ¼1 4[(155 :78)2þ(142 :25)2þ(144 :76)2]/C01 12(442 :79)2 ¼25:900117 : For error, we use (14.46) to obtain SSE ¼X ijky2 ijk/C01 2P ijy2 ij: ¼(39 :02)2þ(38 :79)2þ/C1/C1/C1þ (36 :04)2/C01 2[(77 :81)2þ/C1/C1/C1þ (71 :74)2] ¼16 ,365 :56070 /C016364 :89875 ¼:661950 : The total sum of squares is given by SST ¼X ijky2 ijk/C0y2 ...",
    "365 :56070 /C016364 :89875 ¼:661950 : The total sum of squares is given by SST ¼X ijky2 ijk/C0y2 ... 12¼26:978692 : The sum of squares for interaction can be found by (14.45) or by subtracting all other terms from the total sum of squares: SS(gjm,a,b)¼26:978692 /C0:114075 /C025:900117 /C0:661950 ¼:302550 : With these sums of squares, we can compute mean squares and Fstatistics as shown in Table 14.4. Only the Ftest for type is signiﬁcant, since F:05 ,1,6¼5:99 and F:05 ,2,6¼5:14.",
    "shown in Table 14.4. Only the Ftest for type is signiﬁcant, since F:05 ,1,6¼5:99 and F:05 ,2,6¼5:14. The pvalue for type is .0000155.",
    "st for type is signiﬁcant, since F:05 ,1,6¼5:99 and F:05 ,2,6¼5:14. The pvalue for type is .0000155. The pvalues for method and the interaction are .3485 and .3233, respectively.TABLE 14.3 Totals for Data in Table 14.2 B A 1 2 3 Totals 1 y11.¼77.81 y12.¼71.15 y13.¼73.02 y1..¼221.98 2 y21.¼77.97 y22.¼71.10 y23.¼71.74 y2..¼220.81 Totals y .1.¼155.78 y.2.¼142.25 y.3.¼144.76 y...¼442.79400 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 411 --- Note that in Table 14.2, the difference between the two replicates in each cell is very small except for the cell with method 1 and type 3.",
    "between the two replicates in each cell is very small except for the cell with method 1 and type 3. This suggests that the repli- cates may be repeat measurements rather than true replications; that is, the exper- imenter may have measured the same piece of cheese twice rather than measuring two different cheeses. A 14.4.2.2 General Linear Hypothesis Approach We now obtain SS( ajm,b,g) for a¼3 and b¼2 by an approach based on the general linear hypothesis.",
    "ch We now obtain SS( ajm,b,g) for a¼3 and b¼2 by an approach based on the general linear hypothesis. Using a/C3 i¼ai/C0/C22a:þ/C22gi:/C0/C22g::in (14.16), the hypothesis H0:a/C31¼a/C32¼a/C33in (14.63) can be expressed as H0:a1þ/C22g1:¼a2þ/C22g2:¼ a3þ/C22g3:or H0:a1þ1 2(g11þg12)¼a2þ12(g21þg22)¼a3þ12(g31þg32) (14 :72) [see also (14.9) and (14.10)]. The two equalities in (14.72) can be expressed in the form H0:a1þ1 2g11þ12g12/C0a3/C012g31/C012g32 a2þ1 2g21þ12g22/C0a3/C012g31/C012g32 !",
    "expressed in the form H0:a1þ1 2g11þ12g12/C0a3/C012g31/C012g32 a2þ1 2g21þ12g22/C0a3/C012g31/C012g32 ! ¼0 0/C18/C19 : Rearranging the order of the parameters to correspond to the order in b¼(m,a1,a2,a3,b1,b2,g11,g12,g21,g22,g31,g32)0in (14.4), we have H0:a1/C0a3þ1 2g11þ12g12/C012g31/C012g32 a2/C0a3þ1 2g21þ12g22/C012g31/C012g32 !",
    "2)0in (14.4), we have H0:a1/C0a3þ1 2g11þ12g12/C012g31/C012g32 a2/C0a3þ1 2g21þ12g22/C012g31/C012g32 ! ¼0 0/C18/C19 , (14 :73)TABLE 14.4 ANOVA for the Cheese Data in Table 14.2 Source of VariationSum of Squares dfMean Square F Method 0.114075 1 0.114075 1.034 Type 25.900117 2 12.950058 117.381 Interaction 0.302550 2 0.151275 1.371 Error 0.661950 6 0.110325 Total 26.978692 1114.4 TESTING HYPOTHESES 401 --- Page 412 --- which can now be written in the form H0:Cb¼0with C¼010 /C01001 21200 /C012/C012 001 /C0100001 212/C012/C012 !",
    "now be written in the form H0:Cb¼0with C¼010 /C01001 21200 /C012/C012 001 /C0100001 212/C012/C012 ! : (14 :74) By Theorem 12.7b(iii), the sum of squares corresponding to H0:Cb¼0is SSH ¼(C^b)0[C(X0X)/C0C0]/C01C^b: (14 :75) Substituting ^b¼(X0X)/C0X0yfrom (12.13), SSH in (14.75) becomes SSH ¼y0X(X0X)/C0C0[C(X0X)/C0C0]/C01C(X0X)/C0X0y¼y0Ay: (14 :76) Using Cin (14.74), ( X0X)/C0in (14.23), and Xin (14.4), we obtain C(X0X)/C0X0¼1411110000 /C01/C01/C01/C01 00001111 /C01/C01/C01/C01/C18/C19 ,(14 :77) C(X0X)/C0C0¼1421 12/C18/C19 , [C(X0X)/C0C0]/C01¼4 32/C01 /C012/C18/C19 : (14 :78) Then A¼X(X0X)/C0C0[C(X0X)/C0C0]/C01C(X0X)/C0X0in (14.76) becomes A¼1 122J/C0J/C0J /C0J2J/C0J /C0J/C0J2J0 @1A, (14 :79) where Jis 4/C24.",
    "C(X0X)/C0X0in (14.76) becomes A¼1 122J/C0J/C0J /C0J2J/C0J /C0J/C0J2J0 @1A, (14 :79) where Jis 4/C24. This can be expressed as A¼ 1 122J/C0J/C0J /C0J2J/C0J /C0J/C0J2J0 @1A¼ 1 123JOO O 3JO OO 3J0@1A/C0 1 12JJJ JJJ JJJ0 @1A:(14 :80) To evaluate y 0Ay, we redeﬁne yin (14.51) as y¼y11 y12 y21 y22 y31 y320BBBBBB@1 CCCCCCA¼y 1 y2 y30 @1A,wherey i1 yi2/C18/C19 ¼yi: (14 :81)402 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 413 --- Then (14.76) becomes SSH ¼y0Ay¼1 12(y0 1,y02,y03)3J4OO O 3J4O OO 3J40 B@1 CAy1 y2 y30 B@1 CA/C01 12y0J12y ¼3 12X3 i¼1y0 iJ4yi/C01 12y0J12y ¼1 4X iy0 ij4j04yi/C01 12y0j12j0 12y ¼X iy2 i:: 4/C0y2...",
    "12y0J12y ¼3 12X3 i¼1y0 iJ4yi/C01 12y0J12y ¼1 4X iy0 ij4j04yi/C01 12y0j12j0 12y ¼X iy2 i:: 4/C0y2... 12, which is the same as SS( ajm,b,g) in (14.67) with a¼3 and b¼n¼2. The sum of squares for testing the Bmain effect can be obtained similarly using a general linear hypothesis approach (see Problem 14.25). 14.5 EXPECTED MEAN SQUARES We ﬁnd expected mean squares by direct evaluation of the expected value of sums of squares and also by a matrix method based on the expected value of quadratic forms.",
    "value of sums of squares and also by a matrix method based on the expected value of quadratic forms. 14.5.1 Sums-of-Squares Approach The expected mean squares for the tests in Table 14.1 are given in Table 14.5.",
    "s-of-Squares Approach The expected mean squares for the tests in Table 14.1 are given in Table 14.5. Note that these are expressed in terms of a/C3 i,b/C3 j, andg/C3 ijsubject to the side TABLE 14.5 Expected Mean Squares for a Two-Way ANOVA SourceSum of Squares Mean Square Expected Mean Square A SS(ajm,b,g) SSðajm,b,gÞ a/C01 s2þbnP ia/C32 i a/C01 B SS(bjm,a,g) SSðbjm,a,gÞ b/C01s2þanP jb/C32 j b/C01 AB SS(gjm,a,b) SSðgjm,a,bÞ ða/C01Þðb/C01Þs2þnP ijg/C32 ij (a/C01)(b/C01) Error SSE SSE ab(n/C01)s214.5 EXPECTED MEAN SQUARES 403 --- Page 414 --- conditionsP ia/C3 i¼0,P jb/C3 j¼0, andP ig/C3 ij¼P jg/C3ij¼0.",
    "CTED MEAN SQUARES 403 --- Page 414 --- conditionsP ia/C3 i¼0,P jb/C3 j¼0, andP ig/C3 ij¼P jg/C3ij¼0. These expected mean squares can be derived by inserting the model yijk¼m/C3þa/C3iþb/C3 jþg/C3 ijþ1ijkin (14.33) into the sums of squares and then ﬁnding expected values. We illustrate this approach for the ﬁrst expected mean square in Table 14.5.",
    "ﬁnding expected values. We illustrate this approach for the ﬁrst expected mean square in Table 14.5. To ﬁnd the expected value of SS( ajm,b,g)¼P iy2 i::=bn/C0y2...=abnin (14.67), we ﬁrst note that by using assumption 1 in Section 14.1, we can write assumptions 2 and 3 in the form E(12 ijk)¼s2for all i,j,k, (14 :82) E(1ijk1rst)¼0 for all ( i,j,k)=(r,s,t): (14 :83) Using these results, along with assumption 1 and the side conditions in (14.15), we can show that E(y2 ...)¼a2b2n2m/C32þabns2as follows: E(y2 ...)¼EX ijkyijk !2 ¼EX ijk(m/C3þa/C3iþb/C3jþg/C3ijþ1ijk)\"#2 ¼E abnm/C3þbnX ia/C3 iþanX jb/C3jþnX ijg/C3ijþX ijk1ijk !2 ¼Ea2b2n2m/C32þ2abnm/C3X ijk1ijkþX ijk1ijk !22 435 ¼a 2b2n2m/C32þEX ijk12 ijk !",
    "ijg/C3ijþX ijk1ijk !2 ¼Ea2b2n2m/C32þ2abnm/C3X ijk1ijkþX ijk1ijk !22 435 ¼a 2b2n2m/C32þEX ijk12 ijk ! þEX ijk=rst1ijk1rst ! ¼a2b2n2m/C32þabns2: It can likewise be shown that EXa i¼1y2i:: ! ¼ab2n2m/C32þb2n2Xa i¼1a/C32 iþabns2(14 :84) (see Problem 14.27). Thus ESS(ajm,b,g) a/C01/C20/C21 ¼1 a/C01EX iy2 i:: bn/C0y2... abn !",
    "ns2(14 :84) (see Problem 14.27). Thus ESS(ajm,b,g) a/C01/C20/C21 ¼1 a/C01EX iy2 i:: bn/C0y2... abn ! ¼1 a/C01ab2n2m/C32 bnþb2n2P ia/C32 i bnþabns2 bn/C0a2b2n2m/C32 abn/C0abns2 abn/C20/C21 ¼1 a/C01(a/C01)s2þbnX ia/C32 i\"# :404 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 415 --- The other expected mean squares in Table 14.5 can be obtained similarly (see Problem 14.28). 14.5.2 Quadratic Form Approach We now obtain the ﬁrst expected mean square in Table 14.2 using a matrix approach.",
    "tic Form Approach We now obtain the ﬁrst expected mean square in Table 14.2 using a matrix approach. We illustrate with a¼3,b¼2, and n¼2. By (14.75), we obtain E[SS(ajm,b,g)]¼E{(C^b)0[C(X0X)/C0C0]/C01C^b}: (14 :85) The matrix Ccontains estimable functions, and therefore by (12.44) and (12.45), we have E(C^b)¼Cband cov( C^b)¼s2C(X0X)/C0C0.",
    "mable functions, and therefore by (12.44) and (12.45), we have E(C^b)¼Cband cov( C^b)¼s2C(X0X)/C0C0. If we deﬁne Gto be the 2 /C22 matrix [ C(X0X)/C0C0]/C01, then by Theorem 5.2a, (14.85) becomes E[SS(ajm,b,g)]¼E[(C^b)0G(C^b)] ¼tr[Gcov(C^b)]þ[E(C^b)]0G[E(C^b)] ¼tr(Gs2G/C01)þ(Cb)0G(Cb) ¼2s2þb0C0[C(X0X)/C0C0]/C01Cb (14 :86) ¼2s2þb0Lb, (14 :87) where L¼C0[C(X0X)/C0C0]/C01C.",
    ")þ(Cb)0G(Cb) ¼2s2þb0C0[C(X0X)/C0C0]/C01Cb (14 :86) ¼2s2þb0Lb, (14 :87) where L¼C0[C(X0X)/C0C0]/C01C. Using Cin (14.74) and [ C(X0X)/C0C0]/C01in (14.78), L becomes L¼1 30000 0 0000000 08 /C04/C04 0 044 /C02/C02/C02/C02 0/C048 /C0400 /C02/C0244 /C02/C02 0/C04/C048 0 0 /C02/C02/C02/C0244 0000 0 0000000 0000 0 0000000 04 /C02/C02 0 022 /C01/C01/C01/C01 04 /C02/C02 0 022 /C01/C01/C01/C01 0/C024 /C0200 /C01/C0122 /C01/C01 0/C024 /C0200 /C01/C0122 /C01/C01 0/C02/C024 0 0 /C01/C01/C01/C0122 0/C02/C024 0 0 /C01/C01/C0112 20 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCA:(14 :88)14.5 EXPECTED MEAN SQUARES 405 --- Page 416 --- This can be written as the difference L¼1 30 0 0 0 00000000 01 2 0 0 00660000 0 0 1 2 0 00006600 00 01 2 0 0 0 0 0 0 6 6 0 0 0 0 00000000 0 0 0 0 00000000 0 6 0 0 00330000 0 6 0 0 00330000 0 0 6 0 00003300 0 0 6 0 00003300 0 0 0 6 00000033 0 0 0 6 000000330 BBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCA /C0 1 3000000000000 044400222222044400222222 044400222222 000000000000 000000000000 022200111111 022200111111 022200111111022200111111 022200111111 0222001111110 BBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCA ¼ 1 30000000 0A 11OA 12 0OOO 0A 21OA 220 BBB@1 CCCA/C01 30000000 0B 11OB 12 0OOO 0B 21OB 220 BBB@1 CCCA, (14 :89) where A11¼12I3,B11¼4j3j0 3,B12¼2j3j06,B21¼2j6j03,B22¼j6j06, A12¼6j020000 006j0200 00006j020 B@1 CA,A21¼6j200 06j20 00 6j20 B@1 CA,406 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 417 --- A22¼3j2j0 2OO O 3j2j02O OO 3j2j020 B@1 CA: If we write bin (14.4) in the form b¼(m,a0,b1,b2,g0)0, wherea0¼(a1,a2,a3) andg0¼(g11,g12,g21,g22,g31,g32), thenb0Lbin (14.87) becomes b0Lb¼1 3a0A11aþ13a0A12gþ13g0A21aþ13g0A22g/C013a0B11a /C01 3a0B12g/C013g0B21a/C013g0B22g: Since A0 21¼A12andB0 21¼B12, this reduces to b0Lb¼13a0A11aþ23a0A12gþ13g0A22g/C013a0B11a /C023a0B12g/C013g0B22g: If we partition gasg0¼(g0 1,g0 2,g0 3),whereg0 i¼(gi1,gi2), then 23a0A12g¼12 3a0j0 20000 00j0 200 0000j0 20 B@1 CAg1 g2 g30 B@1 CA ¼4a0j0 2g1 j0 2g2 j0 2g30 B@1 CA¼4X3 i¼1aigi:: Now, using the deﬁnitions of A11,A22,B11,B12,andB22following (14.89), we obtain b0Lb¼4a0aþ4X3 i¼1aigi:þX3 i¼1g0 ij2j0 2gi/C04 3a0j3j0 3a /C043a0j3j0 6g/C013g0j6j0 6g ¼4X3 i¼1a2 iþ4X3 i¼1aigi:þX3 i¼1g2i:/C04 3a2 :/C04 3a:g::/C013g2 ::: (14 :90)14.5 EXPECTED MEAN SQUARES 407 --- Page 418 --- By expressing gi:,a:,andg::in terms of means, (14.90) can be written in the form b0Lb¼4X3 i¼1(ai/C0/C22a:þ/C22gi:/C0/C22g::)2¼4X3 i¼1a/C32 i [by (14 :16)] : (14 :91) For an alternative approach leading to (14.91), note that since E(C^b)¼Cb, (14.86) can be written as E[SS(ajm,b,g)]¼2s2þ[E(C^b)]0[C(X0X)/C0C0]/C01E(C^b): (14 :92) By (14.75), SS( ajm,b,g)¼SSH ¼(C^b)0[C(X0X)/C0C0]/C01C^b.",
    "[E(C^b)]0[C(X0X)/C0C0]/C01E(C^b): (14 :92) By (14.75), SS( ajm,b,g)¼SSH ¼(C^b)0[C(X0X)/C0C0]/C01C^b. Thus, by (14.92), we can obtain E[SS(ajm,b,g)] by replacing C^bin SS(ajm,b,g) with Cband adding 2s2. To illustrate, we replace /C22yi::and /C22y...with E(/C22yi::) and E(/C22y...)i n SS(ajm,b,g)¼4P3 i¼1(/C22yi::/C0/C22y...)2in (14.69).",
    "and /C22y...with E(/C22yi::) and E(/C22y...)i n SS(ajm,b,g)¼4P3 i¼1(/C22yi::/C0/C22y...)2in (14.69). We ﬁrst ﬁnd E(/C22yi::): E(/C22yi::)¼E/C16 1 4X jkyijk/C17 ¼14X jkE(yijk) ¼14X jkE(mþaiþbjþgijþ1ijk) ¼1 4X jk(mþaiþbjþgij) ¼14/C16 4mþ4aiþ2X jbjþ2X jgij/C17 ¼mþaiþ/C22b:þ/C22gi:: (14 :93) Similarly E(/C22y...)¼mþ/C22a:þ/C22b:þ/C22g::: (14 :94) Then, E[SS(ajm,b,g)]¼2s2þ4X3 i¼1[E(/C22yi::)/C0E(/C22y...)]2 ¼2s2þ4X i(mþaiþ/C22b:þ/C22gi:/C0m/C0/C22a:/C0/C22b:þ/C22g::)2 ¼2s2þ4X i(ai/C0/C22a:þ/C22gi:/C0/C22g::)2 ¼2s2þ4X ia/C32 i [by (14 :16)] :408 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 419 --- PROBLEMS 14.1 Obtainu1andu5in (14.7) from (14.6).",
    "ALYSIS-OF-VARIANCE: BALANCED CASE --- Page 419 --- PROBLEMS 14.1 Obtainu1andu5in (14.7) from (14.6). 14.2 In a comment following (14.8), it is noted that there are no estimable con- trasts in the a’s alone or b’s alone. Verify this statement. 14.3 Show that1 3(u3þu0 3þu00 3) has the value shown in (14.11). 14.4 Verify the following results in (14.15) using the deﬁnitions of a/C3 i,b/C3 j, andg/C3 ij in (14.14): (a)P ia/C3i¼0 (b)P jb/C3 j¼0 (c)P ig/C3 ij¼0,j¼1,2,... ,b (d)P jg/C3ij¼0,i¼1,2,...",
    "C3 ij in (14.14): (a)P ia/C3i¼0 (b)P jb/C3 j¼0 (c)P ig/C3 ij¼0,j¼1,2,... ,b (d)P jg/C3ij¼0,i¼1,2,... ,a 14.5 Verify the following results from (14.15) using the deﬁnitions of a/C3i,b/C3 j, and g/C3 ijin (14.16), (14.17), and (14.18): (a)P ia/C3i¼0 (b)P jb/C3 j¼0 (c)P ig/C3 ij¼0,j¼1,2,... ,b (d)P jg/C3ij¼0,i¼1,2,... ,a 14.6 (a)Show that b/C3 j¼bj/C0/C22b:þ/C22g:j/C0/C22g::as in (14.17). (b)Show that g/C3 ij¼gij/C0/C22gi:/C0/C22g:jþ/C22g::as in (14.18).",
    ":þ/C22g:j/C0/C22g::as in (14.17). (b)Show that g/C3 ij¼gij/C0/C22gi:/C0/C22g:jþ/C22g::as in (14.18). 14.7 Show that ^aiand ^gijin (14.21) are unbiased estimators of a/C3iandg/C3ijas noted following (14.21). 14.8 (a)Show that /C22y11:þ/C22y12:¼2/C22y1::and that /C22y21:þ/C22y22:¼2/C22y2::, as used to obtain (14.22). (b)Show that ^a1/C0^a2þ1 2(^g11þ^g12)/C012(^g21þ^g22)¼/C22y1::/C0/C22y2::as in (14.22). 14.9 Show that ( X0X)/C0in (14.23) is a generalized inverse of X0Xin (14.5).",
    "0/C22y2::as in (14.22). 14.9 Show that ( X0X)/C0in (14.23) is a generalized inverse of X0Xin (14.5). 14.10 Show that SSE in (14.26) is equal to SSE in (14.25). 14.11 Show that the second equality in (14.34) is equivalent to the second equality in (14.35); that is, m21/C0m22¼m31/C0m32 implies g21/C0g22/C0 g31þg32¼0.",
    "lent to the second equality in (14.35); that is, m21/C0m22¼m31/C0m32 implies g21/C0g22/C0 g31þg32¼0. 14.12 Show thatP i(/C22yi::/C0/C22y...)yi::¼P iy2 i::=bn/C0y2...=abn and thatP ij(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...)yij:¼P ijy2ij:=n/C0P iy2i::=bn/C0P jy2:j:=anþ y2...=abn, as in (14.40). 14.13 (a)In a comment following (14.41), it was noted that the use of ^bfrom (14.24) would produce the same result as in (14.41), namely, ^b0X0y¼P ijy2ij:=n.",
    "t the use of ^bfrom (14.24) would produce the same result as in (14.41), namely, ^b0X0y¼P ijy2ij:=n. Verify this.PROBLEMS 409 --- Page 420 --- (b)Show that SSE ¼P ijky2 ijk/C0nP ij/C22y2ij:in (14.25) is equal to SSE ¼P ijky2ijk/C0P ijy2ij:=nin (14.46). 14.14 Show that ( a/C01)(b/C01) is the number of independent g/C3ijterms in H0:g/C3ij¼ 0 for i¼1,2,... ,aandj¼1,2,... ,b, as noted near the end of Section 14.4.1.2.",
    "3ijterms in H0:g/C3ij¼ 0 for i¼1,2,... ,aandj¼1,2,... ,b, as noted near the end of Section 14.4.1.2. 14.15 Show that SS( gjm,a,b)¼nP ijk(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...)2in (14.47) is the same as SS( gjm,a,b) in (14.45). 14.16 Show that SSE ¼P ijk(yijk/C0/C22yij:)2in (14.48) is equal to SSE ¼P ijky2ijk/C0P ijy2ij:=nin (14.46). 14.17 Using X0Xin (14.5) and ( X0X)/C0in (14.23), show that X(X0X)/C0X0has the form given in (14.50).",
    "4.17 Using X0Xin (14.5) and ( X0X)/C0in (14.23), show that X(X0X)/C0X0has the form given in (14.50). 14.18 (a)Show that ( X0 1X1)/C0in (14.53) is a generalized inverse of X0 1X1in (14.42). (b)Show that X1(X0 1X1)/C0X0 1has the form given by (14.54). 14.19 Show that1 6P2 j¼1y2 :j:can be written in the matrix form given in (14.59). 14.20 Show that1 2A/C014B/C016Cþ1 12Dhas the value shown in (14.61). 14.21 Show that H0:a/C3 1¼a/C32¼a/C33in (14.63) is equivalent to H0:a/C31¼a/C32¼a/C33¼0 in (14.64).",
    "14.21 Show that H0:a/C3 1¼a/C32¼a/C33in (14.63) is equivalent to H0:a/C31¼a/C32¼a/C33¼0 in (14.64). 14.22 Obtain SS( m,a,g) and show that SS( bjm,a,g)¼Pb j¼1y2:j:=bn/C0y2...=abnas in (14.68). 14.23 Prove Theorem 14.4b for the special case a¼3,b¼2, and n¼2. 14.24 (a)Using Cin (14.74), ( X0X)/C0in (14.23), and Xin (14.4), show that C(X0X)/C0X0is the 2 /C212 matrix given in (14.77). (b)Using Cin (14.74) and ( X0X)/C0in (14.23), show that C(X0X)/C0C0is the 2/C22 matrix shown in (14.78).",
    "sing Cin (14.74) and ( X0X)/C0in (14.23), show that C(X0X)/C0C0is the 2/C22 matrix shown in (14.78). (c)Show that the matrix A¼X(X0X)/C0C0[C(X0X)/C0C0]/C01C(X0X)/C0X0has the form shown in (14.79). 14.25 For the Bmain effect, formulate a hypothesis H0:Cb¼0and obtain SS(bjm,a,g) using SSH in (14.75). 14.26 Using assumptions 1, 2, and 3 in Section 14.1, show that E(12ijk)¼s2for all i,j,kandE(1ijk1rst)¼0 for ( i,j,k)=(r,s,t), as in (14.82) and (14.82).",
    "show that E(12ijk)¼s2for all i,j,kandE(1ijk1rst)¼0 for ( i,j,k)=(r,s,t), as in (14.82) and (14.82). 14.27 Show that E(Pa i¼1y2 i::)¼ab2n2m/C32þb2n2Pa i¼1a/C32 iþabns2as in (14.84). 14.28(a)Show that E(Pb j¼1y2 :j:)¼a2bn2m/C32þa2n2Pb j¼1b/C32 jþabns2. (b)Show that E(P ijy2ij:)¼abn2m/C32þbn2P ia/C32 iþan2P jb/C32 jþ n2P ijg/C32 ijþabns2. (c)Show that E[SS(bjm,a,g)=(b/C01)]¼s2þanP jb/C32 j=(b/C01).",
    "iþan2P jb/C32 jþ n2P ijg/C32 ijþabns2. (c)Show that E[SS(bjm,a,g)=(b/C01)]¼s2þanP jb/C32 j=(b/C01). (d)Show that E[SS(gjm,a,b)=(a/C01)(b/C01)]¼s2þ nP ijg/C32 ij=(a/C01)(b/C01).410 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 421 --- 14.29 Using C in (14.74) and ( X0X)/C0in (14.23), show that L¼C0[C(X0X)/C0C0]/C01Chas the form shown in (14.88). 14.30 ExpandP3 i¼1(ai/C0/C22a:þ/C22gi:/C0/C22g::)2in (14.91) to obtain (14.90).",
    "rm shown in (14.88). 14.30 ExpandP3 i¼1(ai/C0/C22a:þ/C22gi:/C0/C22g::)2in (14.91) to obtain (14.90). 14.31 (a)Show that E(/C22y...)¼mþ/C22a:þ/C22b:þ/C22g::as in (14.94). (b)Show that E(/C22y:j:)¼mþ/C22a:þbjþ/C22g:j:. (c)Show that E(/C22yij:)¼mþaiþbjþgij. 14.32 Obtain the following expected values using the method suggested by (14.92) and illustrated at the end of Section 14.5.2. Use the results of Problem 14.31b, c.",
    "ested by (14.92) and illustrated at the end of Section 14.5.2. Use the results of Problem 14.31b, c. (a)E[SS(bjm,a,g)]¼s2þ6P jb/C32 j (b)E[SS(gjm,a,b)]¼2s2þ2P ijg/C32 ijTABLE 14.6 Lactic Acidaat Five Successive Time Periods for Fresh and Wilted Alfalfa Silage Period Condition 1 2 3 4 5 Fresh 13.4 37.5 65.2 60.8 37.7 16.0 42.7 54.9 57.1 49.2 Wilted 14.4 29.3 36.4 39.1 39.4 20.0 34.5 39.7 38.7 39.7 aIn mg /g of silage.",
    ".0 42.7 54.9 57.1 49.2 Wilted 14.4 29.3 36.4 39.1 39.4 20.0 34.5 39.7 38.7 39.7 aIn mg /g of silage. TABLE 14.7 Hemoglobin Concentration (g /mL) in Blood of Brown Trouta Rate: 1 2 3 4 Method: A B A B A B A B 6.7 7.0 9.9 9.9 10.4 9.9 9.3 11.0 7.8 7.8 8.4 9.6 8.1 9.6 9.3 9.3 5.5 6.8 10.4 10.2 10.6 10.4 7.8 11.08.4 7.0 9.3 10.4 8.7 10.4 7.8 9.07.0 7.5 10.7 11.3 10.7 11.3 9.3 8.47.8 6.5 11.9 9.1 9.1 10.9 10.2 8.48.6 5.8 7.1 9.0 8.8 8.0 8.7 6.87.4 7.1 6.4 10.6 8.1 10.2 8.6 7.25.8 6.5 8.6 11.7 7.8 6.1 9.3 8.17.0 5.5 10.6 9.6 8.0 10.7 7.2 11.0 aAfter 35 days of treatment at the daily rates of 0, 5, 10, and 15g of sulfamerazine per 100 lb of ﬁsh employing two methods for each rate.PROBLEMS 411 --- Page 422 --- 14.33 A preservative was added to fresh and wilted alfalfa silage (Snedecor 1948).",
    "--- Page 422 --- 14.33 A preservative was added to fresh and wilted alfalfa silage (Snedecor 1948). The lactic acid concentration was measured at ﬁve periods after ensiling began. There were two replications. The results are given in Table 14.6.Let factor Abe condition (fresh or wilted) and factor Bbe period. Test for main effects and interactions. 14.34 Gutsell (1951) measured hemoglobin in the blood of brown trout after treat- ment with four rates of sulfamerazine.",
    "measured hemoglobin in the blood of brown trout after treat- ment with four rates of sulfamerazine. Two methods of administering the sul- famerazine were used. Ten ﬁsh were measured for each rate and each method. The data are given in Table 14.7.",
    "ne were used. Ten ﬁsh were measured for each rate and each method. The data are given in Table 14.7. Test for effect of rate and method and interaction.412 TWO-WAY ANALYSIS-OF-VARIANCE: BALANCED CASE --- Page 423 --- 15 Analysis-of-Variance: The Cell Means Model forUnbalanced Data 15.1 INTRODUCTION The theory of linear models for ANOVA applications was developed in Chapter 12.",
    "ta 15.1 INTRODUCTION The theory of linear models for ANOVA applications was developed in Chapter 12. Although all the examples used in that and the following chapters have involved balanced data (where the number of observations is equal from one cell to another), the theory also applies to unbalanced data. Chapters 13 and 14 show that simple and intuitive results are obtained when the theory is applied to balanced ANOVA situations.",
    "t simple and intuitive results are obtained when the theory is applied to balanced ANOVA situations. Intuitive marginal means are informative in analysis of the data [e.g., see (14.69) and (14.70)]. When applied to unbalanced data, however, the general results of Chapter 12 do not simplify to intui-tive formulas. Even worse, the intuitive marginal means one is tempted to use can be misleading and sometimes paradoxical. This is especially true for two-way or higher- way data.",
    "an be misleading and sometimes paradoxical. This is especially true for two-way or higher- way data. As an example, consider the unbalanced two-way data in Figure 15.1. Thedata follow the two-way additive model (Section 12.1.2) with no error y ij¼mþaiþbj,i¼1,2,j¼1,2, wherem¼25,a1¼0,a2¼220,b1¼0,b2¼5. Simple marginal means of the data are given to the right and below the box. The true effects of factors AandBare, respectively, a22a1¼220 andb22 b1¼5.",
    "right and below the box. The true effects of factors AandBare, respectively, a22a1¼220 andb22 b1¼5. Even for error-free unbalanced data, however, naive estimates of these effects based on the simple marginal means are highly misleading. The effect of factor Aappears to be 8.75 225.125 ¼216.375, and even more surprisingly the effect of factor Bappears to be 15 220¼25. Still other complications arise in the analysis of unbalanced data.",
    "factor Bappears to be 15 220¼25. Still other complications arise in the analysis of unbalanced data. For example, it was mentioned in Section 14.4.2.1 that many texts discourage testing for main effects in the presence of interactions. But little harm or controversy results from doing so when the data are balanced. The numerators for the main effect Ftests are exactly Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc.",
    ",Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 413 --- Page 424 --- the same whether the model with or without interactions is being entertained as the full model. Such is not the case for unbalanced data. The numerator sums of squares in these Ftests depend greatly on which model is used as the full model, and, obviously, conclusions can be affected.",
    "epend greatly on which model is used as the full model, and, obviously, conclusions can be affected. Several types of sums of squares [usually types I, II, and III; see Milliken and Johnson (1984, pp. 138–158)] have been suggested to help clarify this issue. The issues involved in choosing the appropriate full model for a test are subtle and often confusing. The use of different full models results in different weightings in the sums of squares calculations and expected mean squares.",
    "odels results in different weightings in the sums of squares calculations and expected mean squares. But some of the sameweightings also arise for other reasons. For example, the weights might arise because the data are based on “probability proportional to size” (pps) sampling of populations (Cochran 1977, pp. 250–251). Looking at this complex issue from different points of view has led to completely contradictory conclusions. For example, Milliken and Johnson (1984, p.",
    "of view has led to completely contradictory conclusions. For example, Milliken and Johnson (1984, p. 158) wrote that “in almost all cases, type III sums of squares will be preferred,” whereas Nelderand Lane (1995) saw “no place for types III and IV sums of squares in making infer- ences from the use of linear models.” Further confusion regarding the analysis of unbalanced data has arisen from the interaction of computing advances with statistical practice.",
    "of unbalanced data has arisen from the interaction of computing advances with statistical practice. Historically, severaldifferent methods for unbalanced data analysis were developed as approximate methods, suitable for the computing resources available at the time. Looking back,however, we simply see a confusing array of alternative methods. Some such methods include weighted squares of means (Yates 1934; Morrison 1983, pp.",
    "rnative methods. Some such methods include weighted squares of means (Yates 1934; Morrison 1983, pp. 407– 412), the method of unweighted means (Searle 1971; Winer 1971), the method ofﬁtting constants (Rao 1965, pp. 211–214; Searle 1971, p. 139; Snedecor and Cochran 1967), and various methods of imputing data to make the dataset balanced (Hartley 1956; Healy and Westmacott 1969; Little and Rubin 2002, pp. 28–30).",
    "ke the dataset balanced (Hartley 1956; Healy and Westmacott 1969; Little and Rubin 2002, pp. 28–30). The overparameterized (non-full rank) model (Sections 12.2, 12.5, 13.1, and 14.1) has some advantages in the analysis of unbalanced data, while the cell means approach (Section 12.1.1) has other advantages.",
    "he analysis of unbalanced data, while the cell means approach (Section 12.1.1) has other advantages. The non-full rank approach buildsthe structure (additive two-way, full two-way, etc.) of the dataset into the model from the start, but relies on the subtle concepts of estimability, testability, andFigure 15.1 Hypotetical error-free data from an unbalanced two-way model.414 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 425 --- generalized inverses.",
    "NALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 425 --- generalized inverses. The cell means model has the advantages of being a full-rank model, but the structure of the dataset is not an explicit part of the model. Whichever model is used, hard questions about the exact hypotheses of interesthave to be faced. Many of the complexities are a matter of statistical practice rather than mathematical statistics.",
    ". Many of the complexities are a matter of statistical practice rather than mathematical statistics. The most extreme form of imbalance is that in which one or more of the cells have no observations. In this “empty cells” situation, even the cell means model is an over- parameterized model. Nonetheless, the cell means approach allows one to deal speciﬁcally with nonestimability problems arising from the empty cells. Such anapproach is almost impossible using the overparameterized approach.",
    "ing from the empty cells. Such anapproach is almost impossible using the overparameterized approach. In the remainder of this chapter we discuss the analysis of unbalanced data using the cell means model. Unbalanced one-way and two-way models are covered inSections 15.2 and 15.3. In Section 15.4 we discuss the empty-cell situation.",
    "ay models are covered inSections 15.2 and 15.3. In Section 15.4 we discuss the empty-cell situation. 15.2 ONE-WAY MODEL The non-full-rank and cell means versions of the one-way unbalanced model are y ij¼mþaiþ1ij (15:1) ¼miþ1ij, (15:2) i¼1,2,...,k,j¼1,2,...,ni: For making inferences, we assume the 1ij’s are independently distributed as N(0,s2).",
    "k,j¼1,2,...,ni: For making inferences, we assume the 1ij’s are independently distributed as N(0,s2). 15.2.1 Estimation and Testing To estimate the mi’s, we begin by writing the N¼P iniobservations for the model (15.2) in the form y¼Wmþ1, (15:3) where W¼10 /C1/C1/C1 0 ......... 10 /C1/C1/C1 0 01 /C1/C1/C1 0 ......... 01 /C1/C1/C1 0 ......... 00 1 ......... 00 /C1/C1/C1 10 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCA, m¼m1 m2 ...",
    "/C1 0 ......... 00 1 ......... 00 /C1/C1/C1 10 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCA, m¼m1 m2 ... mk0 BBB@1 CCCA:15.2 ONE-WAY MODEL 415 --- Page 426 --- The normal equations are given by W0W^m¼W0y, where W0W¼diag( n1,n2,...,nk) and W0y¼(y1:,y2:,...,yk:)0, with yi:¼Pni j¼1yij:. Since the matrix Wis full rank, we have, by (7.6) ^m¼ðW0WÞ/C01W0y ð15:4Þ ¼/C22y¼/C22y1: /C22y2: ... /C22yk:0 BBBB@1 CCCCA, (15:5) where /C22y i:¼Pni j¼1yij=ni.",
    "C01W0y ð15:4Þ ¼/C22y¼/C22y1: /C22y2: ... /C22yk:0 BBBB@1 CCCCA, (15:5) where /C22y i:¼Pni j¼1yij=ni. To test H0:m1¼m2¼/C1/C1/C1¼mk, we compare the full model in (15.2) and (15.3) with the reduced model yij¼mþ1/C3 ij, wheremis the common value of m1,m2,...,mk under H0. (We do not use the notation m/C3in the reduced model because there is no m in the full model yij¼miþ1ij.) In matrix form, the Nobservations in the reduced model become y¼mjþ1/C3,where jisN/C21.",
    "yij¼miþ1ij.) In matrix form, the Nobservations in the reduced model become y¼mjþ1/C3,where jisN/C21. For the full model, we have SS( m1, m2,...,mk)¼^m0W0y, and for the reduced model, we have SS( m)¼^mj0y¼N/C22y2::, where N¼P iniand /C22y::¼P ijyij=N.",
    "0W0y, and for the reduced model, we have SS( m)¼^mj0y¼N/C22y2::, where N¼P iniand /C22y::¼P ijyij=N. The difference SS( m1,m2,...,mk)/C0SS(m) is equal to the regression sum of squares SSR in (8.6), which we denote by SSB for “between” sum of squares SSB¼^m0W0y/C0N/C22y2 ::¼Xk i¼1/C22yi:yi:/C0N/C22y2::(15:6) ¼Xk i¼1y2 i: ni/C0y2:: N, (15:7) where y::¼P ijyijand /C22y::¼y::=N. From (15.7), we see that SSB has k21 degrees of freedom.",
    "(15:7) where y::¼P ijyijand /C22y::¼y::=N. From (15.7), we see that SSB has k21 degrees of freedom. The error sum of squares is given by (7.24) or (8.6) as SSE¼y0y/C0^m0W0y ¼Xk i¼1Xni j¼1y2 ij/C0Xk i¼1y2 i: ni, (15:8) which has N2kdegrees of freedom.",
    "6) as SSE¼y0y/C0^m0W0y ¼Xk i¼1Xni j¼1y2 ij/C0Xk i¼1y2 i: ni, (15:8) which has N2kdegrees of freedom. These sums of squares are summarized in Table 15.1.416 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 427 --- The sums of squares SSB and SSE in Table 15.1 can also be written in the form SSB¼Xk i¼1ni(/C22yi:/C0/C22y::)2, (15:9) SSE¼Xk i¼1Xni j¼1(yij/C0/C22yi:)2: (15:10) If we assume that the yij’s are independently distributed as N(mi,s2), then by Theorem 8.1d, an Fstatistic for testing H0:m1¼m2¼/C1/C1/C1¼mkis given by F¼SSB =(k/C01) SSE =(N/C0k): (15:11) IfH0is true, Fis distributed as F(k21,N2k).",
    "C1/C1¼mkis given by F¼SSB =(k/C01) SSE =(N/C0k): (15:11) IfH0is true, Fis distributed as F(k21,N2k). Example 15.2.1. A sample from the output of ﬁve ﬁlling machines is given in Table 15.2 (Ostle and Mensing 1975, p. 359). The analysis of variance is given in Table 15.3. The Fis calculated by (15.11). There is no signiﬁcant difference in the average weights ﬁlled by the ﬁve machines. A 15.2.2 Contrasts A contrast in the population means is deﬁned as d¼c1m1þc2m2þ/C1/C1/C1þ ckmk, wherePk i¼1ci¼0.",
    "trasts A contrast in the population means is deﬁned as d¼c1m1þc2m2þ/C1/C1/C1þ ckmk, wherePk i¼1ci¼0. The contrast can be expressed as d¼c0m, whereTABLE 15.1 One-Way Unbalanced ANOVA Source Sum of Squares df Between SSB¼P iy2 i:=ni/C0y2::=N k21 Error SSE¼P ijy2ij/C0P iy2i:=ni N2k Total SST¼P ijy2ij/C0y2::=N N21 TABLE 15.2 Net Weight of Cans Filled by Five Machines (A–E) AB C D E 11.95 12.18 12.16 12.25 12.10 12.00 12.11 12.15 12.30 12.0412.25 12.08 12.10 12.0212.10 12.0215.2 ONE-WAY MODEL 417 --- Page 428 --- SST¼P ijy2 ij/C0y2::=Nandm¼(m1,m2,...,mk)0.",
    "12.0212.10 12.0215.2 ONE-WAY MODEL 417 --- Page 428 --- SST¼P ijy2 ij/C0y2::=Nandm¼(m1,m2,...,mk)0. The best linear unbiased estimator ofdis given by ^d¼c1/C22y1:þc2/C22y2:þ/C1/C1/C1þ ck/C22yk:¼c0ˆm[see (15.5) and Corollary 1 to Theorem 7.3d]. By (3.42), var( ^d)¼s2c0(W0W)/C01c, which can be written as var( ^d)¼s2Pk i¼1c2 i=ni, since W0W¼diag( n1,n2,...,nk).",
    "^d)¼s2c0(W0W)/C01c, which can be written as var( ^d)¼s2Pk i¼1c2 i=ni, since W0W¼diag( n1,n2,...,nk). By (8.38), the Fstatistic for testing H0:d¼0i s F¼(c0^m)0c0(W0W)/C01c/C2/C3/C01c0^m s2, (15:12) ¼Pk i¼1ci/C22yi:/C16/C172 =/C16Pki¼1c2 i=ni/C17 s2, (15:13) where s2¼SSE /(N2k) with SSE given by (15.8) or (15.10). We refer to the numera- tor of (15.13) as the sum of squares for the contrast.",
    "by (15.8) or (15.10). We refer to the numera- tor of (15.13) as the sum of squares for the contrast. If H0is true, the Fstatistic in (15.12) or (15.13) is distributed as F(1,N2k), and we reject H0:d¼0i f F/C21Fa,1,N/C0kor if p/C20a, where pis the pvalue.",
    "distributed as F(1,N2k), and we reject H0:d¼0i f F/C21Fa,1,N/C0kor if p/C20a, where pis the pvalue. Two contrasts, say, ^d¼Pk i¼1ai/C22yi:and ^g¼Pki¼1bi/C22yi:,are said to be orthogonal ifPk i¼1aibi¼0:However, in the case of unbalanced data, two orthogonal contrasts of this type are not independent, as they were in the balanced case (Theorem 13.6a). Theorem 15.2.",
    "s of this type are not independent, as they were in the balanced case (Theorem 13.6a). Theorem 15.2. If the yij’s are independently distributed as N(mi,s2) in the unba- lanced model (15.2), then two contrasts ^d¼Pk i¼1ai/C22yi:and ^g¼Pki¼1bi/C22yi:are inde- pendent if and only ifPki¼1aibi=ni¼0: PROOF. We express the two contrasts in vector notation as ^d¼a0/C22yand ^g¼b0/C22y, where /C22y¼(/C22y1:,/C22y2:,...,/C22yk:)0. By (7.14), we obtain cov( /C22y)¼s2(W0W)/C01¼s21=n1 0 ... 0 01 =n2...",
    "/C22y1:,/C22y2:,...,/C22yk:)0. By (7.14), we obtain cov( /C22y)¼s2(W0W)/C01¼s21=n1 0 ... 0 01 =n2... 0 ......... 00 ...",
    ",/C22yk:)0. By (7.14), we obtain cov( /C22y)¼s2(W0W)/C01¼s21=n1 0 ... 0 01 =n2... 0 ......... 00 ... 1=nk0 BBB@1 CCCA¼s2D:TABLE 15.3 ANOVA for the Fill Data in Table 15.2 Source dfSum of SquaresMean Square Fp Value Between 4 .05943 .01486 1.9291 .176 Error 11 .08472 .00770 Total 15 .14414418 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 429 --- Then by (3.43), we have cov( ^d,^g)¼cov(a0/C22y,b0/C22y)¼a0cov( /C22y)b¼s2a0Db ¼s2Xk i¼1aibi ni: (15:14) Hence, by Theorem 4.4c, dˆandgˆare independent if and only ifP iaibi/ni¼0.A We refer to contrasts whose coefﬁcients satisfyP iaibi/ni¼0a sweighted orthog- onal contrasts .",
    "¼0.A We refer to contrasts whose coefﬁcients satisfyP iaibi/ni¼0a sweighted orthog- onal contrasts . If we deﬁne k21 contrasts of this type, they partition the treatment sum of squares SSB into k21 independent sums of squares, each with 1 degree of freedom. Unweighted orthogonal contrasts that satisfy onlyP iaibi¼0 are not inde- pendent (see Theorem 15.2), and their sums of squares do not add up to the treatment sum of squares (as they do for balanced data; see Theorem 13.6a).",
    "res do not add up to the treatment sum of squares (as they do for balanced data; see Theorem 13.6a). In practice, weighted orthogonal contrasts are often of less interest than unweighted orthogonal contrasts because we may not wish to choose the ai’s and bi’s on the basis of the ni’s in the sample. The ni’s seldom reﬂect population charac- teristics that we wish to take into account. However, it is not necessary that the sums of squares be independent in order to proceed with the tests.",
    "ver, it is not necessary that the sums of squares be independent in order to proceed with the tests. If we use unweighted orthogonal contrasts withP iaibi¼0, the general linear hypothesis test based on (15.12) or (15.13) tests each contrast adjusted for the other contrasts (see Theorem 8.4d). Example 15.2.2a.",
    "or (15.13) tests each contrast adjusted for the other contrasts (see Theorem 8.4d). Example 15.2.2a. Suppose that we wish to compare the means of three treatments and that the coefﬁcients of the orthogonal contrasts d¼a0mandg¼b0mare given by a0¼(2/C01/C01) and b0¼(0 1/C01) with corresponding hypotheses H01:m1¼1 2(m2þm3),H02:m2¼m3: If the sample sizes for the three treatments are, for example, n1¼10,n2¼20, and n3¼5, then the two estimated contrasts ^d¼2/C22y1:/C0/C22y2:/C0/C22y3:and ^g¼/C22y2:/C0/C22y3: are not independent, and the corresponding sums of squares do not add to the treat- ment sum of squares.",
    "not independent, and the corresponding sums of squares do not add to the treat- ment sum of squares. The following two vectors provide an example of contrasts whose coefﬁcients satisfyP iaibi=ni¼0 for n1¼10,n2¼20, and n3¼5: a0¼(25/C020/C05) and b0¼(0 1/C01): (15:15)15.2 ONE-WAY MODEL 419 --- Page 430 --- However, a0leads to the comparison H03:25m1¼20m2þ5m3orH03:m1¼4 5m2þ15m3, which is not the same as the hypothesis H01:m1¼1 2(m2þm3) that we were initially interested in. A Example 15.2.2b.",
    "he same as the hypothesis H01:m1¼1 2(m2þm3) that we were initially interested in. A Example 15.2.2b. We illustrate both weighted and unweighted contrasts for the ﬁll data in Table 15.2.",
    "xample 15.2.2b. We illustrate both weighted and unweighted contrasts for the ﬁll data in Table 15.2. Suppose that we wish to make the following comparisons of the ﬁve machines: A, D versus B, C, E B, E versus D A versus D B versus E Orthogonal (unweighted) contrast coefﬁcients that provide these comparisons are given as rows of the following matrix: 3/C02/C023 /C02 01 /C0201 100 /C010 0100 /C010 BB@1 CCA: We give the sums of squares for these four contrasts and the Fvalues [see (15.13)] in Table 15.4.",
    "A: We give the sums of squares for these four contrasts and the Fvalues [see (15.13)] in Table 15.4. Since these are unweighted contrasts, the contrast sums of squares do not add up to the between sum of squares in Table 15.3. None of the pvalues is less than .05, so we do not reject H 0:P icimi¼0 for any of the four contrasts.",
    "ne of the pvalues is less than .05, so we do not reject H 0:P icimi¼0 for any of the four contrasts. In fact, the p values should be less than .05 /4 for familywise signiﬁcance (see the Bonferroni approach in Section 8.5.2), since the overall test in Table 15.3 did not reject H0:m1¼m2/C1/C1/C1¼m5.",
    "pproach in Section 8.5.2), since the overall test in Table 15.3 did not reject H0:m1¼m2/C1/C1/C1¼m5. TABLE 15.4 Sums of Squares and FValues for Contrasts for the Fill Data in Table 15.2 Contrast dfContrast SS Fp Value A, D versus B, C, E 1 .005763 0.75 .406 B, E versus C 1 .002352 0.31 .592A versus D 1 .034405 4.47 .0582 B versus E 1 .013333 1.73 .215420 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 431 --- As an example of two weighted orthogonal contrasts that satisfyP iaibi/ni,w e keep the ﬁrst contrast above and replace the second contrast with (0 2 /C06 0 4).",
    "isfyP iaibi/ni,w e keep the ﬁrst contrast above and replace the second contrast with (0 2 /C06 0 4). Then, for these two contrasts, we have X iaibi ni¼3(0) 4/C02(2) 2/C02(/C06) 3þ3(0) 3/C02(4) 4¼0: The sums of squares and Fvalues [using (15.13)] for the two contrasts are as follows: Contrast df Contrast SSF pValue A, D versus B, C, E 1 .005763 .75 .406 B, E versus C 1 .005339 .69 .423A 15.3 TWO-WAY MODEL The unbalanced two-way model can be expressed as yijk¼mþaiþbjþgijþ1ijk (15:16) ¼mijþ1ijk, (15:17) i¼1,2,...,a,j¼1,2,...,b,k¼1,2,...,nij: The1ijk’s are assumed to be independently distributed as N(0,s2).",
    "2,...,a,j¼1,2,...,b,k¼1,2,...,nij: The1ijk’s are assumed to be independently distributed as N(0,s2). In this section we consider the case in which all nij.0. The cell means model for analyzing unbalanced two-way data was ﬁrst proposed by Yates (1934). The cell means model has been advocated by Speed (1969), Urquhart et al. (1973), Nelder (1974), Hocking and Speed (1975), Bryce (1975), Bryce et al. (1976, 1980b), Searle (1977), Speed et al. (1978), Searle et al.",
    "(1975), Bryce (1975), Bryce et al. (1976, 1980b), Searle (1977), Speed et al. (1978), Searle et al. (1981), Milliken and Johnson (1984, Chapter 11), and Hocking (1985, 1996). Turner (1990) discusses the relationship between (15.16) and (15.17). In our development we follow Bryce et al. (1980b) and Hocking (1985, 1996). 15.3.1 Unconstrained Model We ﬁrst consider the unconstrained model in which the mij’s are unrestricted.",
    ".1 Unconstrained Model We ﬁrst consider the unconstrained model in which the mij’s are unrestricted. To accommodate a no-interaction model, for example, we must place constraints on themij’s. The constrained model is discussed in Section. To illustrate the cell means model (15.17), we use a¼2 and b¼3 with the cell counts nijgiven in Figure 15.2.",
    "trate the cell means model (15.17), we use a¼2 and b¼3 with the cell counts nijgiven in Figure 15.2. This example with N¼P ijnij¼11 will be referred to throughout the present section and Section 15.3.2.15.3 TWO-WAY MODEL 421 --- Page 432 --- For each of the 11 observations in Figure 15.2, the model yijk¼mijþ1ijkis y111¼m11þ1111 y112¼m11þ1112 y121¼m12þ1121 ... y231¼m23þ1231 y232¼m23þ1232, or in matrix form y¼Wmþ1, (15:18) where y¼y111 y112 ...",
    "21¼m12þ1121 ... y231¼m23þ1231 y232¼m23þ1232, or in matrix form y¼Wmþ1, (15:18) where y¼y111 y112 ... y2320 BBB@1 CCCA,w¼100000 100000010000 001000 001000 .................. 000001 0000010 BBBBBBBBBBB@1 CCCCCCCCCCCA, m¼m11 m12 m13 m21 m22 m230 BBBBBB@1 CCCCCCA,1¼1111 1112 ... 12320 BBB@1 CCCA: Each row of Wcontains a single 1 that corresponds to the appropriate mijinm.F o r example, the fourth row gives y131¼(001000)mþ1131¼m13þ1131. In this illus- tration, yand1are 11 /C21, and Wis 11/C26.",
    "th row gives y131¼(001000)mþ1131¼m13þ1131. In this illus- tration, yand1are 11 /C21, and Wis 11/C26. In general, yand1areN/C21, and WisN/C2ab,w h e r e N¼P ijnij.Figure 15.2 Cell counts for unbalanced data illustration.422 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 433 --- Since Wis full-rank, we can use the results in Chapters 7 and 8. The analysis is further simpliﬁed because W0W¼diag( n11,n12,n13,n21,n22,n23).",
    "s in Chapters 7 and 8. The analysis is further simpliﬁed because W0W¼diag( n11,n12,n13,n21,n22,n23). By (7.6), the least-squares estimator of mis given by ^m¼(W0W)/C01W0y¼/C22y, (15:19) where /C22y¼(/C22y12:,/C22y13:,/C22y14:,/C22y21:,/C22y22:,/C22y23:)0contains the sample means of the cells, /C22yij:¼P kyijk=nij.",
    ":,/C22y14:,/C22y21:,/C22y22:,/C22y23:)0contains the sample means of the cells, /C22yij:¼P kyijk=nij. By (7.14), the covariance matrix for mˆis cov( ^m)¼s2(W0W)/C01¼s2diag1 n11,1 n12,/C1/C1/C1,1 n23/C18/C19 (15:20) ¼diags2 n11,s2 n12,/C1/C1/C1,s2 n23/C18/C19 : For general a,b, and N, an unbiased estimator of s2[see (7.23)] is given by s2¼SSE nE¼(y/C0W^m)0(y/C0W^m) N/C0ab, (15:21) wherenE¼Pa i¼1Pb j¼1(nij/C01)¼N/C0ab, with N¼P ijnij. In our illustration with a¼2 and b¼3, we have N2ab¼1126¼5.",
    "¼1Pb j¼1(nij/C01)¼N/C0ab, with N¼P ijnij. In our illustration with a¼2 and b¼3, we have N2ab¼1126¼5. Two alternative forms of SSE are SSE¼y0[I/C0W(W0W)/C01W0]y[see (7 :26)], (15:22) SSE¼Xa i¼1Xb j¼1Xnij k¼1(yijk/C0/C22yij:)2[see (14 :48)] : (15:23) Using (15.23), we can express s2as the pooled estimator s2¼Pai¼1Pb j¼1(nij/C01)s2 ij N/C0ab, (15:24) where s2 ijis the variance estimator in the ( ij)th cell, s2 ij¼Pnij k¼1(yijk/C0/C22yij:)2=(nij/C01).",
    "here s2 ijis the variance estimator in the ( ij)th cell, s2 ij¼Pnij k¼1(yijk/C0/C22yij:)2=(nij/C01). The overparameterized model (15.16) includes parameters representing main effects and interactions, but the cell means model (15.17) does not have such par- ameters. To carry out tests in the cell means model, we use contrasts to express the main effects and the interaction as functions of the mij’s inm. We begin with the main effect of A.",
    "main effects and the interaction as functions of the mij’s inm. We begin with the main effect of A. In the vector m¼(m11,m12,m13,m21,m22,m23)0, the ﬁrst three elements corre- spond to the ﬁrst level of Aand the last three to the second level, as seen in Figure 15.3. Thus, for the main effect of A, we could compare the average of m11,15.3 TWO-WAY MODEL 423 --- Page 434 --- m12, andm13with the average of m21,m22, andm23.",
    "erage of m11,15.3 TWO-WAY MODEL 423 --- Page 434 --- m12, andm13with the average of m21,m22, andm23. The difference between these averages (sums) can be conveniently expressed by the contrast a0m¼m11þm12þm13/C0m21/C0m22/C0m23, ¼(1,1,1,/C01,/C01,/C01)m: To compare the two levels of A, we can test the hypothesis H0:a0m¼0, which can be written as H0:(m11/C0m21)þ(m12/C0m22)þ(m13/C0m23)¼0. In this form, H0 states that the effect of Aaveraged (summed) over the levels of Bis 0.",
    "13/C0m23)¼0. In this form, H0 states that the effect of Aaveraged (summed) over the levels of Bis 0. This corre- sponds to a common main effect deﬁnition in the presence of interaction; see com- ments following (14.62). Note that this test is not useful in model selection. Itsimply tests whether the interaction is “symmetric” such that the effect of A, averaged over the levels of B, is zero. Factor Bhas three levels corresponding to the three columns of Figure 15.3.",
    "he levels of B, is zero. Factor Bhas three levels corresponding to the three columns of Figure 15.3. In a comparison of three levels, there are 2 degrees of freedom, which will require two con- trasts. Suppose that we wish to compare the ﬁrst level of Bwith the other two levels and then compare the second level of Bwith the third.",
    "e the ﬁrst level of Bwith the other two levels and then compare the second level of Bwith the third. To do this, we compare the average of the means in the ﬁrst column of Figure 15.3 with the average in the second and thirdcolumns and similarly compare the second and third columns.",
    "with the average in the second and thirdcolumns and similarly compare the second and third columns. We can make these com- parisons using H 0:b0 1m¼0a n d b02m¼0, where b01mandb02mare the following two orthogonal contrasts: b0 1m¼2(m11þm21)/C0(m12þm22)/C0(m13þm23)( 1 5 :25) ¼2m11/C0m12/C0m13þ2m21/C0m22/C0m23 ¼(2,/C01,/C01,2,/C01,/C01)m, b02m¼(m12þm22)/C0(m13þm23)( 1 5 :26) ¼m12/C0m13þm22/C0m23 ¼(0,1,/C01,0,1,/C01)m: We can combine b10andb20into the matrix B¼b0 1 b02/C18/C19 ¼2/C01/C012 /C01/C01 01 /C010 1 /C01/C18/C19 , (15:27)Figure 15.3 Cell means corresponding to Figure 15.1.424 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 435 --- and the hypothesis becomes H0:Bm¼0, which, by (15.25) and (15.26), is equivalent to H0:m11þm21¼m12þm22¼m13þm23 (15:28) (see Problem15.9).",
    "hich, by (15.25) and (15.26), is equivalent to H0:m11þm21¼m12þm22¼m13þm23 (15:28) (see Problem15.9). In this form, H0states that the interaction is symmetric such that the three levels of Bdo not differ when averaged over the two levels of A. Note that other orthogonal or linearly independent contrasts besides those in b10andb20would lead to (15.28) and to the same Fstatistic in (15.33) below.",
    "ntrasts besides those in b10andb20would lead to (15.28) and to the same Fstatistic in (15.33) below. By analogy to (14.30), the interaction hypothesis can be written as H0:m11/C0m21¼m12/C0m22¼m13/C0m23, which is a comparison of the “ Aeffects” across the levels of B. If these Aeffects differ, we have an interaction.",
    "parison of the “ Aeffects” across the levels of B. If these Aeffects differ, we have an interaction. We can express the two equalities in H0in terms of orthogonal contrasts similar to those in (15.25) and (15.26): c0 1m¼2(m11/C0m21)/C0(m12/C0m22)/C0(m13/C0m23)¼0, c02m¼(m12/C0m22)/C0(m13/C0m23)¼0: Thus H0can be written as H0:Cm¼0, where C¼c0 1 c0 2/C18/C19 ¼2/C01/C01/C021 1 01 /C010 /C011/C18/C19 : Note that c1can be found by taking products of corresponding elements of aandb1 andc2can be obtained similarly from aandb2, where a,b1, and b2are the coefﬁcient vectors in a0m,b10mandb20m.",
    "be obtained similarly from aandb2, where a,b1, and b2are the coefﬁcient vectors in a0m,b10mandb20m. Thus c0 1¼[(1)(2) ,(1)(/C01),(1)(/C01),(/C01)(2),(/C01)(/C01),(/C01)(/C01)] ¼(2,/C01,/C01,/C02,1,1), c02¼[(1)(0) ,(1)(1) ,(1)(/C01),(/C01)(0),(/C01)(1),(/C01)(/C01)] ¼(0,1,/C01,0,/C01,1): The elementwise multiplication of these two vectors (the Hadamard product — see Section 2.2.4) produces interaction contrasts that are orthogonal to each other and to the main effect contrasts.",
    ") produces interaction contrasts that are orthogonal to each other and to the main effect contrasts. We now construct tests for the general linear hypotheses H0:a0m¼0,H0:Bm¼ 0, and H0:Cm¼0for the main effects and interaction. The hypothesis H0:a0m¼0 for the main effect of A, is easily tested using an Fstatistic similar to (8.38) or (15.12): F¼(a0^m)0[a0(W0W)/C01a]/C01(a0^m) s2¼SSA SSE =nE, (15:29)15.3 TWO-WAY MODEL 425 --- Page 436 --- where s2is given by (15.21) and nE¼N2ab.",
    "SSA SSE =nE, (15:29)15.3 TWO-WAY MODEL 425 --- Page 436 --- where s2is given by (15.21) and nE¼N2ab. [For our illustration, N2ab¼112 (2)(3)¼5.] IfH0is true, Fin (15.29) is distributed as F(1,N2ab). TheFstatistic in (15.29) can be written as F¼(a0^m)2 s2a0(W0W)/C01a(15:30) ¼P ijaij/C22yij:/C16/C172 s2P ija2 ij=nij, (15:31) which is analogous to (15.13).",
    "0(W0W)/C01a(15:30) ¼P ijaij/C22yij:/C16/C172 s2P ija2 ij=nij, (15:31) which is analogous to (15.13). Since t2(nE)¼F(1,nE) (see Problem 5.16), a tstatistic for testing H0:a0m¼0 is given by the square root of (15.30) t¼a0^m sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ a0(W0W)/C01ap ¼a0^m/C00ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ cvar(a0^m)p , (15:32) which is distributed as t(N2ab) when H0is true.",
    "1ap ¼a0^m/C00ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ cvar(a0^m)p , (15:32) which is distributed as t(N2ab) when H0is true. Note that the test based on either of (15.29) or (15.32) is a full–reduced-model test (see Theorem 8.4d) and therefore tests for factor A“above and beyond” (adjusted for) factor Band the interaction.",
    "8.4d) and therefore tests for factor A“above and beyond” (adjusted for) factor Band the interaction. By Theorem 8.4b, a test statistic for the factor Bmain effect hypothesis H0:Bm¼0 is given by F¼(B^m)0[B(W0W)/C01B0]/C01B^m=nB SSE =nE¼SSB =nB SSE =nE, (15:33) wherenE¼N2abandnBis the number of rows of B. (For our illustration, nE¼5 andnB¼2.) When H0is true, Fin (15.33) is distributed as F(nB,nE).",
    "of B. (For our illustration, nE¼5 andnB¼2.) When H0is true, Fin (15.33) is distributed as F(nB,nE). A test statistic for the interaction hypothesis H0:Cm¼0is obtained similarly: F¼(C^m)0[C(W0W)/C01C0]/C01C^m=nAB SSE =nE¼SSAB =nAB SSE =nE, (15:34) which is distributed as F(nAB,nE), wherenAB, the degrees of freedom for interaction, is the number of rows of C.",
    "tributed as F(nAB,nE), wherenAB, the degrees of freedom for interaction, is the number of rows of C. (In our illustration, nAB¼2.) Because of the unequal nij’S, the three sums of squares SSA, SSB, and SSAB do not add to the overall sum of squares for treatments and are not statistically indepen-dent, as in the balanced case [see (14.40) and Theorem 14.4b].",
    "nts and are not statistically indepen-dent, as in the balanced case [see (14.40) and Theorem 14.4b]. Each of SSA, SSB, and SSAB is adjusted for the other effects; that is, the given effect is tested “above and beyond” the others (see Theorem 8.4d). Example 15.3a. Table 15.5 contains dressing percentages of pigs in a two-way classiﬁcation (Snedecor and Cochran 1967, p. 480).",
    "ontains dressing percentages of pigs in a two-way classiﬁcation (Snedecor and Cochran 1967, p. 480). Let factor Abe gender and factor Bbe breed.426 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 437 --- We arrange the elements of the vector mto correspond to a row of Table 15.5, that is m¼(m11,m12,m21,m22,...,m52)0, where the ﬁrst subscript represents breed and the second subscript is associated with gender.",
    "m52)0, where the ﬁrst subscript represents breed and the second subscript is associated with gender. The vector mis 10 /C21, the matrix Wis 75 /C210, the vector ais 10 /C21, and the matrices BandCare each 4 /C210.",
    "is 10 /C21, the matrix Wis 75 /C210, the vector ais 10 /C21, and the matrices BandCare each 4 /C210. We show a,B, and C: a0¼(1,/C01,1,/C01,1,/C01,1,/C01,1,/C01), B¼33 3 3 /C02/C02/C02/C02/C02/C02 11 /C01/C01000000 0 00011 /C02/C0211 0 0001100 /C01/C010 BBB@1 CCCA, C¼3/C033 /C03/C022 /C022 /C022 1/C01/C011000 000 00001 /C01/C022 1 /C01 00001 /C010 0 /C0110 BBB@1 CCCA:TABLE 15.5 Dressing Percentages (Less 70%) of 75 Swine Classiﬁed by Breed and Gender Breed 12345 Male Female Male Female Male Female Male Female Male Female 13.3 18.2 10.9 14.3 13.6 12.9 11.6 13.8 10.3 12.8 12.6 11.3 3.3 15.3 13.1 14.4 13.2 14.4 10.3 8.411.5 14.2 10.5 11.8 4.1 12.6 4.9 10.1 10.6 15.4 15.9 11.6 11.0 10.8 15.2 6.9 13.9 12.7 12.9 15.4 10.9 14.7 13.2 10.015.7 15.1 14.4 10.5 12.4 11.0 13.2 11.6 12.9 12.215.0 14.4 12.5 13.314.3 7.5 13.0 12.916.5 10.8 7.6 9.915.0 10.5 12.913.7 14.5 10.913.015.912.815.3 TWO-WAY MODEL 427 --- Page 438 --- (Note that other sets of othogonal contrasts could be used in B, and the value of FB below would be the same.) By (15.19), we obtain ^m¼/C22y¼(14:08,14:60,11:75,12:06,10:40,13:65,13:28,11:03,11:01,11:14)0: By (15.22) or (15.23) we obtain SSE¼425.08895, with nE¼65.",
    "06,10:40,13:65,13:28,11:03,11:01,11:14)0: By (15.22) or (15.23) we obtain SSE¼425.08895, with nE¼65. Using (15.29), (15.33), and (15.34), we obtain FA¼:30337 ,FB¼3:47318 ,FC¼:95095 : The sums of squares leading to these Fs are given in Table 15.6. Note that the sums of squares for A,B,AB, and error do not add up to the total sum of squares because the data are unbalanced.",
    "res for A,B,AB, and error do not add up to the total sum of squares because the data are unbalanced. (These are the type III sums of squares referred to in Section 15.1.) A 15.3.2 Constrained Model To allow for additivity or other restrictions, constraints on the mij’s must be added to the cell means model (15.17) or (15.18). For example, the model yijk¼mijþ1ijk cannot represent the no-interaction model yijk¼mþaiþbjþ1ijk (15:35) unless we specify some relationships among the mij’s.",
    "no-interaction model yijk¼mþaiþbjþ1ijk (15:35) unless we specify some relationships among the mij’s. In our 2 /C23 illustration in Section 15.3.1, the two interaction contrasts are expres- sible as Cm¼2/C01/C01/C021 1 01 /C010 /C011/C18/C19 m: If we wish to use a model without interaction, then Cm¼0is not a hypothesis to be tested but an assumption to be included in the statement of the model.TABLE 15.6 ANOVA for Unconstrained Model Source dfSum of SquaresMean Square Fp Value A(gender) 1 1.984 1.984 0.303 .584 B(breed) 4 90.856 22.714 3.473 .0124 AB 4 24.876 6.219 0.951 .440 Error 65 425.089 6.540 Total 74 552.095428 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 439 --- In general, for constraints Gm¼0, the model can be expressed as y¼Wmþ1subject to Gm¼0: (15:36) We now consider estimation and testing in this constrained model.",
    "as y¼Wmþ1subject to Gm¼0: (15:36) We now consider estimation and testing in this constrained model. [For the case Gm¼h, where h=0, see Bryce et al. (1980b).] To incorporate the constraints Gm¼0into y¼Wmþ1, we can use the Lagrange multiplier method (Section 2.14.3). Alternatively, we can reparameterize the model using the matrix A¼K G/C18/C19 , (15:37) where Kspeciﬁes parameters of interest in the constrained model.",
    "the matrix A¼K G/C18/C19 , (15:37) where Kspeciﬁes parameters of interest in the constrained model. For the no-inter- action model (15.35), for example, Gwould equal C, the ﬁrst row of Kcould corre- spond to a multiple of the overall mean, and the remaining rows of Kcould include the contrasts for the AandBmain effects.",
    "the overall mean, and the remaining rows of Kcould include the contrasts for the AandBmain effects. Thus, we would have K¼111111 111 /C01/C01/C01 2/C01/C012 /C01/C01 01 /C0101 /C010 BBB@1 CCCA, G¼C¼2/C01/C01/C021 1 01 /C010 /C011/C18/C19 : The second row of Kisa0and corresponds to the average effect of A. The third and fourth rows are from Band represent the average Beffect. If the rows of Gare linearly independent of the rows of K, then the matrix Ain (15.37) is of full rank and has an inverse.",
    "nearly independent of the rows of K, then the matrix Ain (15.37) is of full rank and has an inverse. This holds true in our example, in which we have G¼C. In our example, in fact, the rows of Gare orthogonal to the rows ofK. We can therefore insert A21A¼Iinto (15.36) to obtain the reparameterized model y¼WA/C01Amþ1subject to Gm¼0 ¼Zdþ1 subject to Gm¼0,(15:38) where Z¼WA21andd¼Am.",
    "reparameterized model y¼WA/C01Amþ1subject to Gm¼0 ¼Zdþ1 subject to Gm¼0,(15:38) where Z¼WA21andd¼Am. In the balanced two-way model, we obtained a no-interaction model by simply inserting gij/C3¼0 into yijk¼mþa/C3 iþb/C3 jþg/C3 ijþ1ij[(see 14.37) and (14.38)].",
    "n model by simply inserting gij/C3¼0 into yijk¼mþa/C3 iþb/C3 jþg/C3 ijþ1ij[(see 14.37) and (14.38)]. To analogously incorporate the constraint Gm¼0directly into the model in the15.3 TWO-WAY MODEL 429 --- Page 440 --- unbalanced case, we partition dinto d¼Am¼K G/C18/C19 m¼Km Gm/C18/C19 ¼d1 d2/C18/C19 : With a corresponding partitioning on the columns of Z, the model can be written as y¼Zdþ1¼(Z1,Z2)d1 d2/C18/C19 þ1 ¼Z1d1þZ2d2þ1subject to Gm¼0: (15:39) Sinced2¼Gm, the constraint Gm¼0givesd2¼0and the constrained model in (15.39) simpliﬁes to y¼Z1d1þ1: (15:40) An estimator of d1[see (7.6)] is given by ^d1¼(Z0 1Z1)/C01Z01y: To obtain an expression for msubject to the constraints, we multiply Am¼d1 d2/C18/C19 ¼d1 0/C18/C19 by A/C01¼(K/C3,G/C3): If the rows of Gare orthogonal to the rows of K, then (K/C3,G/C3)¼[K0(KK0)/C01,G0(GG0)/C01] (15 :41) (see Problem15.13).",
    "orthogonal to the rows of K, then (K/C3,G/C3)¼[K0(KK0)/C01,G0(GG0)/C01] (15 :41) (see Problem15.13). If the rows of Gare linearly independent of (but not necessarily orthogonal to) the rows of K, we obtain K/C3¼HGK0(KH GK0)/C01, (15:42) where HG¼I/C0G0(GG0)/C01G, andG/C3is similarly deﬁned (see Problem15.14).",
    "¼HGK0(KH GK0)/C01, (15:42) where HG¼I/C0G0(GG0)/C01G, andG/C3is similarly deﬁned (see Problem15.14). In any case, we denote the product of K/C3andd1bymc: mc¼K/C3d1: We estimate mcby ^mc¼K/C3^d1¼K/C3(Z01Z1)/C01Z01y, (15:43)430 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 441 --- which has covariance matrix cov( ^mc)¼s2K/C3(Z0 1Z1)/C01K/C30: (15:44) To test for factor Bin the constrained model, the hypothesis is H0:Bmc¼0.",
    "(Z0 1Z1)/C01K/C30: (15:44) To test for factor Bin the constrained model, the hypothesis is H0:Bmc¼0. The covariance matrix of Bmˆcis obtained from (3.44) and (15.44) as cov(B^mc)¼s2BK/C3(Z01Z1)/C01K/C30B0: Then, by Theorem 8.4b, the test statistic for H0:Bmc¼0in the constrained model becomes F¼(B^mc)0[BK/C3(Z0 1Z1)/C01K/C30B0]/C01B^mc=nB SSE c=nEc, (15:45) where SSE c(subject to Gm¼0) is obtained using mˆc[from (15.43)] in (15.21).",
    "c=nB SSE c=nEc, (15:45) where SSE c(subject to Gm¼0) is obtained using mˆc[from (15.43)] in (15.21). (In our example, where G¼Cfor interaction, SSE ceffectively pools SSE and SSAB from the unconstrained model.) The degrees of freedom nEcis obtained as nEc¼nEþrank(G), where nE¼N/C0abis for the unconstrained model, as deﬁned following (15.21).",
    "ained as nEc¼nEþrank(G), where nE¼N/C0abis for the unconstrained model, as deﬁned following (15.21). [In our example, rank( G)¼2 since there are 2 degrees of freedom for SSAB.] We reject H0:Bmc¼0ifF/C21Fa,nB,nEc, where Fais the upperapercentage point of the central Fdistribution. ForH0:a0mc¼0, the Fstatistic becomes F¼(a0^mc)0[a0K/C3(Z01Z1)/C01K/C30a]/C01(a0^mc) SSE c=nEc, (15:46) which is distributed as F(1,nEc)i fH0is true. Example 15.3b.",
    "01K/C30a]/C01(a0^mc) SSE c=nEc, (15:46) which is distributed as F(1,nEc)i fH0is true. Example 15.3b. For the pigs data in Table 15.5, we test for factors AandBin a no- interaction model, where factor Ais gender and factor Bis breed. The matrix Gis the same as Cin Example 15.3a.",
    "n model, where factor Ais gender and factor Bis breed. The matrix Gis the same as Cin Example 15.3a. For Kwe have K¼j0 a0 B0 @1A¼1111111111 1/C011 /C011 /C011 /C011 /C01 3333 /C02/C02/C02/C02/C02/C02 11 /C01/C01000000 000011 /C02/C0211 00001100 /C01/C010 BBBBBB@1 CCCCCCA By (15.43), we obtain ^ mc¼(14:16,14:42,11:77,12:03,11:40,11:65,12:45,12:70,10:97,11:22):015.3 TWO-WAY MODEL 431 --- Page 442 --- For SSE c, we usemˆcin place of mˆin (15.21) to obtain SSE c¼449.96508.",
    "Y MODEL 431 --- Page 442 --- For SSE c, we usemˆcin place of mˆin (15.21) to obtain SSE c¼449.96508. For nEc, we have nEc¼nEþrank(G)¼65þ4¼69: Then by (15.45), we obtain FBc¼3:8880003. The sums of squares leading to FBcand FAcare given in Table 15.7.",
    "(15.45), we obtain FBc¼3:8880003. The sums of squares leading to FBcand FAcare given in Table 15.7. A 15.4 TWO-WAY MODEL WITH EMPTY CELLS Possibly the greatest advantage of the cell means model in the analysis of unbalanced data is that extreme situations such as empty cells can be dealt with relatively easily.The cell means approach allows one to deal speciﬁcally with nonestimability pro- blems arising from the empty cells (as contrasted with nonestimability arising from overparameterization of the model).",
    "the empty cells (as contrasted with nonestimability arising from overparameterization of the model). Much of our discussion here follows that ofBryce et al. (1980a). Consider the unbalanced two-way model in (15.17), but allow n ijto be equal to 0 for one or more (say m)isolated cells; that is, the empty cells do not constitute a whole row or whole column. Assume also that the empty cells are missing at random (Little and Rubin 2002, p.",
    "w or whole column. Assume also that the empty cells are missing at random (Little and Rubin 2002, p. 12); that is, the emptiness of the cells is indepen- dent of the values that would be observed in those cells. In the empty cells model, Wis non-full-rank in that it has mcolumns equal to 0. To simplify notation, assume that the columns of Whave been rearranged with the columns of 0occurring last. Hence W¼(W1,O), where W1is an n/C2(ab2m) matrix and Oisn/C2m.",
    "d with the columns of 0occurring last. Hence W¼(W1,O), where W1is an n/C2(ab2m) matrix and Oisn/C2m. Correspondingly m¼mo me/C18/C19 , wheremois the vector of cell means for the occupied cells while meis the vector of cell means for the empty cells.",
    "vector of cell means for the occupied cells while meis the vector of cell means for the empty cells. The model is thus the non-full-rank model y¼(W1,O)mo me/C18/C19 þ1: (15:47)TABLE 15.7 ANOVA for Constrained Model Source dfSum of SquaresMean Square Fp Value A(gender) 1 1.132 1.132 0.17 .678 B(breed) 4 101.418 25.355 3.89 .00660 Error 69 449.965 6.521 Total 74 552.0955432 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 443 --- The ﬁrst task in the analysis of two-way data with empty cells is to test for the interaction between the factors AandB.",
    "analysis of two-way data with empty cells is to test for the interaction between the factors AandB. To test for the interaction when there are iso- lated empty cells, care must be exercised to ensure that a testable hypothesis is being tested (Section 12.6). The full–reduced-model approach [see (8.31)] is useful here. A sensible full model is the unconstrained cell means model in (15.47).",
    "[see (8.31)] is useful here. A sensible full model is the unconstrained cell means model in (15.47). Even though W is not full-rank SSE u¼y0[I/C0W(W0W)/C0W0]y (15:48) is invariant to the choice of a generalized inverse (Theorem 12.3e).",
    "u¼y0[I/C0W(W0W)/C0W0]y (15:48) is invariant to the choice of a generalized inverse (Theorem 12.3e). The reduced model is the additive model, given by y¼WA/C01Amþ1subject to Gm¼0, where A¼K G/C18/C19 , in which Kis a matrix specifying the overall mean and linearly independent main effect contrasts for factors AandB, and the rows of Gare linearly independent inter- action contrasts (see Section 15.3.2) such that Ais full-rank. We deﬁne Z1asWK/C3 [see (15.41)].",
    "er- action contrasts (see Section 15.3.2) such that Ais full-rank. We deﬁne Z1asWK/C3 [see (15.41)]. Because the empty cells are isolated, Z1is full-rank even though some of the constraints in Gm¼0are nonestimable.",
    "mpty cells are isolated, Z1is full-rank even though some of the constraints in Gm¼0are nonestimable. The error sum of squares for the addi- tive model is then SSE a¼y0[I/C0Z1(Z0 1Z1)/C01Z01]y, (15:49) and the test statistic for the interaction is F¼(SSE a/C0SSE u)=[(a/C01)(b/C01)/C0m] SSE u=(n/C0abþm): (15:50) Equivalently the interaction could be tested by the general linear hypothesis approach in (8.27).",
    "0) Equivalently the interaction could be tested by the general linear hypothesis approach in (8.27). However, a maximal set of nonestimable interaction side con- ditions involving memust ﬁrst be imposed on the model. For example, the side con- ditions could be speciﬁed as Tm¼0, (15:51) where Tis an m/C2abmatrix with rows corresponding to the contrasts mij/C0mi:/C0 m:jþm::for all mempty cells (Henderson and McAllister 1978).",
    "esponding to the contrasts mij/C0mi:/C0 m:jþm::for all mempty cells (Henderson and McAllister 1978). Using (12.37), we obtain ^m¼(W0WþT0T)/C01W0y (15:52)15.4 TWO-WAY MODEL WITH EMPTY CELLS 433 --- Page 444 --- and cov( ^m)¼s2(W0WþT0T)/C01W0W(W0WþT0T)/C01: (15:53) The interaction can then be tested using the general linear hypothesis test of H0:Cm¼0where Cis the full matrix of ( a21)(b21) interaction contrasts.",
    "ral linear hypothesis test of H0:Cm¼0where Cis the full matrix of ( a21)(b21) interaction contrasts. Even though some of the rows of Cmare not estimable, the test statistic can be com- puted using a generalized inverse in the numerator as F¼(C^m)0{C[cov( ^m)=s2]C0}/C0(C^m)=[(a/C01)(b/C01)/C0m] SSE =(n/C0abþm): (15:54) The error sum of squares for this model, SSE, turns out to be the same as SSE uin (15.48).",
    ": (15:54) The error sum of squares for this model, SSE, turns out to be the same as SSE uin (15.48). By Theorem 2.8c(v), the numerator of this Fstatistic is invariant to the choice of a generalized inverse (Problem 15.16). Both versions of this additivity test involve the unveriﬁable assumption that the means of the empty cells follow the additive pattern displayed by the means of the occupied cells. If there are relatively few empty cells, this is usually a reasonable assumption.",
    "he occupied cells. If there are relatively few empty cells, this is usually a reasonable assumption. If the interaction is not signiﬁcant and is deemed to be negligible, the additive model can be used as in Section 15.3.2 without any modiﬁcations. The isolated empty cells present no problems for the use of the additive model. If the interaction is signiﬁcant, it may be possible to partially constrain the inter- action in an attempt to render all cell means (including those in me) estimable.",
    "onstrain the inter- action in an attempt to render all cell means (including those in me) estimable. This is not always possible, because it requires a set of constraints that are both a priori reasonable and such that they render mestimable. Nonetheless, it is often advisable to make this attempt because no new theoretical results are needed. The greatest chal- lenges are practical, in that sensible constraints must be used.",
    "ults are needed. The greatest chal- lenges are practical, in that sensible constraints must be used. Many constraints will do the job mathematically, but the results are meaningless unless the constraints are reasonable. Unlike many other methods associated with linear models, the validity of this procedure depends on the parameterization of the model and the speciﬁc con- straints that are chosen.",
    "rocedure depends on the parameterization of the model and the speciﬁc con- straints that are chosen. We proceed in this attempt by proposing partial interaction constraints Gm¼0 for the empty cells model in (15.47). We choose Ksuch that its rows are linearly independent of the rows of Gso that A¼K G/C18/C19 is nonsingular. Thus A21¼(K/C3G/C3) as in the comments following (15.41).",
    "of Gso that A¼K G/C18/C19 is nonsingular. Thus A21¼(K/C3G/C3) as in the comments following (15.41). Suppose that the constraints are realistic, and that they are such that the constrained model is not the additive model; that is, at least a portion of the interaction is unconstrained.",
    "ed model is not the additive model; that is, at least a portion of the interaction is unconstrained. Then, if Z1¼WK/C3is full-rank, all the cell means (including me) can be estimated as ^m¼K/C3(Z0 1Z1)/C01Z01y, (15:55)434 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 445 --- and cov(mˆ) is given by (15.44). Further inferences about linear combinations of the cell means can then be readily carried out.",
    "44). Further inferences about linear combinations of the cell means can then be readily carried out. If Z1is not full-rank, care must be exer- cised to ensure that only estimable functions of mare estimated and that testable hypotheses involving mare tested (see Section 12.2). A simple way to quickly check whether Z1is full-rank (and thus all cell means are estimable) is given in the following theorem. Theorem 15.4. Consider the constrained empty cells model in (15.47) with mempty cells.",
    "wing theorem. Theorem 15.4. Consider the constrained empty cells model in (15.47) with mempty cells. Partition Aas A¼K G/C18/C19 ¼K1K2 G1G2/C18/C19 conformal with the partitioned vector m¼mo me/C18/C19 : The elements of mare estimable (equivalently Z1is full-rank) if and only if rank(G2)¼m. PROOF. We prove this theorem for the special case in which Ghasmrows so that G2 ism/C2m. We partition A21as K/C3 1G/C3 1 K/C3 2G2/C3/C18/C19 , with submatrices conforming to the partitioning of A.",
    "ition A21as K/C3 1G/C3 1 K/C3 2G2/C3/C18/C19 , with submatrices conforming to the partitioning of A. Then Z1¼(W1,O)K/C31 K/C32/C18/C19 ¼W1K/C3 1: Since W1is full-rank and each of its rows consists of one 1 with several 0s, W1K/C3 1 contains one or more copies of all of the rows of W1. Thus rank( Z1)¼rank(K/C31). Since A/C01is nonsingular, K/C3/C01 1exists if K/C31is full rank.",
    "of W1. Thus rank( Z1)¼rank(K/C31). Since A/C01is nonsingular, K/C3/C01 1exists if K/C31is full rank. If so, the product IO /C0K/C32K1/C3/C01I/C18/C19K/C31G/C3 1 K/C3 2G/C3 2/C18/C19 ¼K/C3 1 G/C3 1 OG/C32/C0K/C3 2K/C3/C01 1G/C3 1/C18/C19 is deﬁned and is nonsingular by Theorem 2.4(ii). By Corollary 1 to Theorem 2.9b, G/C3 2/C0K/C3 2K/C3/C01 1G/C3 1is also nonsingular. But by equation (2.50), ( G/C32/C0K/C3 2K/C3/C01 1 G/C3 1)/C01¼G2.",
    "3/C01 1G/C3 1is also nonsingular. But by equation (2.50), ( G/C32/C0K/C3 2K/C3/C01 1 G/C3 1)/C01¼G2. Thus, if A/C01is nonsingular, nonsingularity of K/C3 1implies nonsingular- ity ofG2. Analogous reasoning leads to the converse. Thus K/C31is full-rank if and only ifG2is full-rank. Furthermore, Z1is full-rank if and only if rank( G2)¼m.A Example 15.4a. For the second-language data of Table 15.8, we test for the inter- action of native language and gender.",
    "the second-language data of Table 15.8, we test for the inter- action of native language and gender. There are two empty cells, and thus Wis a15.4 TWO-WAY MODEL WITH EMPTY CELLS 435 --- Page 446 --- 281/C216 matrix with two columns of 0.",
    "Wis a15.4 TWO-WAY MODEL WITH EMPTY CELLS 435 --- Page 446 --- 281/C216 matrix with two columns of 0. For the unconstrained model we use (15.48) to obtain SSE u¼113 :235 : Numbering the cells of Table 15.8 from 1 to 8 for the ﬁrst column and from 9 to 16 for the second column, we now deﬁne A¼K G/C18/C19 (15:56)TABLE 15.8 Comfort in Using English as a Second Language for Students at BYU-Hawaiia NativeGender Language Male Female Samoan 24 28 3.20 3.38 0.66 0.68 Tongan 25 39 3.03 3.10 0.69 0.61 Hawaiian 4 2 3.47 3.13 0.68 0.47 Fijian 1 — 3.79 — —— Paciﬁc Islands English 26 49 3.71 3.13 0.58 0.73 Maori 3 1 4.07 3.040.061 — Mandarin 15 43 3.33 3.140.74 0.61 Cantonese — 21 — 3.00— 0.54 aBrigham Young University–Hawaii; data classiﬁed by gender and native language.",
    "se — 21 — 3.00— 0.54 aBrigham Young University–Hawaii; data classiﬁed by gender and native language. Key to table entries : number of observations, mean, and standard deviation.436 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 447 --- where K¼1111111111111111 11111111 /C01/C01/C01/C01/C01/C01/C01/C01 1/C010000001 /C01000000 0010 /C010000010 /C01000 0001 /C010000001 /C01100 0000 /C011000000 /C01/C0100 22 /C01/C01/C01/C010022 /C01/C01/C01/C0100 0000001 /C01000000 /C011 111111 /C03/C03111111 /C03/C030 BBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCC CCCA and G¼1/C010000001 /C0100000 0 0010 /C0100000 /C010100 0 0001 /C01000000 /C01100 0 0000 /C0110000001 /C010 0 22 /C01/C01/C01/C0100 /C02/C0211110 0 0000001 /C01000000 /C011 111111 /C03/C03/C01/C01/C01/C01/C01/C013 30 BBBBBBBBBBB@1 CCCCCCCCCCCA The overall mean and main effect contrasts are speciﬁed by Kwhile interaction con- trasts are speciﬁed by G.",
    "all mean and main effect contrasts are speciﬁed by Kwhile interaction con- trasts are speciﬁed by G. Using (15.49), SSE a¼119 :213. The full–reduced Ftest for additivity (15.50) yields the test statistic F¼(119 :213/C0113 :235) =5 119 :213 =267¼2:82, which is larger than the critical value of F:05,5,267¼2:25.",
    ":213/C0113 :235) =5 119 :213 =267¼2:82, which is larger than the critical value of F:05,5,267¼2:25. As an alternative approach to testing additivity, we impose the nonestimable side conditions m8,1/C0m8:/C0m:1þm::¼0 andm4,2/C0m4:/C0m:2þm::¼0 on the model by setting T¼/C01/C01/C01/C01/C01/C01/C01 7111 1111 /C07 /C01/C01/C017 /C01/C01/C01/C01111 /C07111 1/C18/C1915.4 TWO-WAY MODEL WITH EMPTY CELLS 437 --- Page 448 --- in (15.51) and C¼1/C01000000 /C011000000 10 /C0100000 /C010100000 100 /C010000 /C010010000 1000 /C01000 /C010001000 10000 /C0100 /C010000100 100000 /C010 /C010000010 1000000 /C01/C0100000010 BBBBBBBB@1 CCCCCCCCA in (15.54).",
    "C0100 /C010000100 100000 /C010 /C010000010 1000000 /C01/C0100000010 BBBBBBBB@1 CCCCCCCCA in (15.54). The Fstatistic for the general linear hypothesis test of additivity (15.54) is again equal to 2.82. Since the interaction is signiﬁcant for this dataset, we partially constrain the inter- action with contextually sensible estimable constraints in an effort to make all of the cell means estimable.",
    "th contextually sensible estimable constraints in an effort to make all of the cell means estimable. We use Aas deﬁned in (15.56), but repartition it so that K¼1111111111111111 11111111 /C01/C01/C01/C01/C01/C01/C01/C01 1/C010000001 /C01000000 0010 /C010000010 /C01000 0001 /C010000001 /C01000 0000 /C011000000 /C01100 22 /C01/C01/C01/C010022 /C01/C01/C01/C0100 0000001 /C010000001 /C01 111111 /C03/C03111111 /C03/C03 0010 /C0100000 /C0101000 0000 /C0110000001 /C0100 22 /C01/C01/C01/C0100 /C02/C02111100 111111 /C03/C03/C01/C01/C01/C01/C01/C01330 BBBBBBBBBBBBBBBBBBBBBBBBBB@1 CC CCCCCCCCCCCCCCCCCCCCCCCCA and G¼1/C0100 000 0 /C0110 000 00 00 0 1 /C0100 0 000 /C0110 00 0 000 001 /C01 000 000 /C0110 @1A: The partial interaction constraints speciﬁed by G m¼0seem sensible in that they specify that the male–female difference is the same for Samoan and Tongan speak- ers, for Fijian and Hawaiian speakers, and for Mandarin and Cantonese speakers.",
    "an and Tongan speak- ers, for Fijian and Hawaiian speakers, and for Mandarin and Cantonese speakers. Because the empty cells are the eighth and twelfth cells, we have G2¼00 0/C01 /C0100 @1A438 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 449 --- which obviously has rank ¼2. Thus, by Theorem 15.4, all the cell means are estimable. Using (15.55) to compute the constrained estimates and (15.44) to compute their standard errors, we obtain the results in Table 15.9.",
    "trained estimates and (15.44) to compute their standard errors, we obtain the results in Table 15.9. A PROBLEMS 15.1 For the model y¼Wmþ1in (15.2.1), ﬁnd W0WandW0yand show that (W0W)21W0y¼y¯as in (15.5). 15.2 (a) Show that for the reduced model yij¼mþ1/C3 ijin Section 15.3, SS(m)¼N/C22y2 ::as used in (15.6). (b)Show that SSB ¼Pk i¼1/C22yi:yi:/C0N/C22y2 ::as in (15.6).",
    "5.3, SS(m)¼N/C22y2 ::as used in (15.6). (b)Show that SSB ¼Pk i¼1/C22yi:yi:/C0N/C22y2 ::as in (15.6). (c)Show that (15.6) is equal to (15.7), that is, SSB ¼P i/C22yi:yi:/C0 N/C22y2::¼P iy2i:=ni/C0y2::=N: 15.3 (a)Show that SSB in (15.9) is equal to SSB in (15.7), that is,Pk i¼1ni(/C22yi:/C0 /C22y::)2¼Pk i¼1y2 i:=ni/C0y2::=N. (b)Show that SSE in (15.10) is equal to SSE in (15.8), that is,Pk i¼1Pni j¼1(yij/C0/C22yi:)2¼Pki¼1Pni j¼1y2 ij/C0Pk i¼1y2 i:=ni.",
    "is equal to SSE in (15.8), that is,Pk i¼1Pni j¼1(yij/C0/C22yi:)2¼Pki¼1Pni j¼1y2 ij/C0Pk i¼1y2 i:=ni. 15.4 Show that F¼ðSici/C22yi:)2=(s2P ic2i=niÞin (15.13) follows from (15.12). 15.5 Show that a0andb0in (15.15) provide contrast coefﬁcients that satisfy the propertyP iaibi=ni¼0. 15.6 Show that mˆ¼y¯as in (15.19).",
    "ovide contrast coefﬁcients that satisfy the propertyP iaibi=ni¼0. 15.6 Show that mˆ¼y¯as in (15.19). 15.7 Obtain (15.23) from (15.21); that is, show that ( y/C0W^m)0(y/C0W^m)¼Pa i¼1Pb j¼1Pnij k¼1(yijk/C0/C22yij:)2.TABLE 15.9 Estimated Mean Comfort in Using English as Second Language (with Standard Error) for Students at BYU-Hawaiia NativeGender Language Male Female Samoan 3.23 (.11) 3.35 (.11) Tongan 3.00 (.11) 3.12 (.09)Hawaiian 3.47 (.33) 3.13 (.46)Fijian 3.79 (.65) 3.20 (.67)Paciﬁc Islands English 3.71 (.03) 3.13 (.09)Maori 4.07 (.38) 3.04 (.65)Mandarin 3.33 (.17) 3.14 (.10)Cantonese 3.19 (.24) 3.00 (.14) aOn the basis of a constrained empty-cells model.PROBLEMS 439 --- Page 450 --- 15.8 Obtain (15.24) from (15.23); that is, show thatPnij k¼1(yijk/C0/C22yij:)2¼ (nij/C01)s2 ij.",
    "--- 15.8 Obtain (15.24) from (15.23); that is, show thatPnij k¼1(yijk/C0/C22yij:)2¼ (nij/C01)s2 ij. 15.9 Show that H0:Bm¼0, where Bis given in (15.27), is equivalent to H0:m11þm21¼m12þm22¼m13þm23in (15.28). 15.10 Obtain F¼P ijaij/C22yij:/C16/C172 =s2P ija2ij=nij/C16/C17 in (15.31) from F¼(a0^m)2= [s2a0(W0W)/C01a] in (15.30). 15.11 Evaluate a0(W0W)/C01ain (15.29) or (15.30) for a0¼(1,1,1,/C01,/C01,/C01): Use the Wmatrix for the 11 observations in the illustration in Section 15.3.1.",
    ",1,1,/C01,/C01,/C01): Use the Wmatrix for the 11 observations in the illustration in Section 15.3.1. 15.12 Evaluate B(W0W)/C01B0in (15.33) for the matrices BandWused in the illus- tration in Section 15.3.1. 15.13 Show that AA21¼I, where A¼K G/C18/C19 as in (15.37) and A21¼ [K0(KK0)21,G0(GG0)21] as in (15.41). 15.14 Obtain G/C3analogous to K/C3in (15.42). 15.15 Show that cov( ^mc)¼s2K0(KK0)/C01(Z0 1Z1)/C01(KK0)/C01K, thus verifying (15.44).",
    "C3in (15.42). 15.15 Show that cov( ^mc)¼s2K0(KK0)/C01(Z0 1Z1)/C01(KK0)/C01K, thus verifying (15.44). 15.16 Show that the numerator of the Fstatistic in (15.54) is invariant to the choice of a generalized inverse. 15.17 In a feeding trial, chicks were given ﬁve protein supplements. Their ﬁnal weights at 6 weeks are given in Table 15.10 (Snedecor 1948, p. 214). (a)Calculate the sums of squares in Table 15.1 and the Fstatistic in (15.11).",
    "nedecor 1948, p. 214). (a)Calculate the sums of squares in Table 15.1 and the Fstatistic in (15.11). (b)Compare the protein supplements using (unweighted) orthogonal con- trasts whose coefﬁcients are the rows in the matrix 3/C02/C02/C023 01 /C0210 010 /C010 1000 /C010 BB@1 CCA: Thus we are making the following comparisons: L, C versus So, Su, M So, M versus Su So versus M L versus C (c)Replace the second contrast with a weighted contrast whose coefﬁcients satisfyP iaibi=ni¼0 when paired with the ﬁrst contrast.",
    "t with a weighted contrast whose coefﬁcients satisfyP iaibi=ni¼0 when paired with the ﬁrst contrast. Find sums of squares and Fstatistics for these two contrasts.440 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 451 --- 15.18 (a)Carry out the computations to obtain mˆ,SSE,FA,FB, and FCin Example 15.3a. (b)Carry out the computations to obtain ^mc,SSE c,FAc, and FBcin Example 15.3b. (c)Carry out the tests in parts (a) and (b) using a software package such as SAS GLM.",
    "Example 15.3b. (c)Carry out the tests in parts (a) and (b) using a software package such as SAS GLM. 15.19 Table 15.11 lists weight gains of male rats under three types of feed and twolevels of protein. (a)Let factor Abe level of protein and factor Bbe type of feed. Deﬁne a vector acorresponding to factor Aand matrices BandCfor factor B and interaction AB, respectively, as in Section 15.3.1.",
    "ng to factor Aand matrices BandCfor factor B and interaction AB, respectively, as in Section 15.3.1. Use these to con- struct general linear hypothesis tests for main effects and interaction as in (15.29), (15.33), and (15.34). (b)Test the main effects in the no-interaction model (15.35) using the con- strained model (15.36). Deﬁne KandGand ﬁnd ^ mcin (15.43), SSE c, andFforH0:a0mc¼0 and H0:Bmc¼0in (15.45). (c)Carry out the tests in parts (a) and (b) using a software package such as SAS GLM.",
    "c¼0in (15.45). (c)Carry out the tests in parts (a) and (b) using a software package such as SAS GLM. 15.20 Table 15.12 lists yields when ﬁve varieties of plants and four fertilizers were tested.",
    "S GLM. 15.20 Table 15.12 lists yields when ﬁve varieties of plants and four fertilizers were tested. Test for main effects and interaction.TABLE 15.10 Final Weights (g) of Chicks at 6 Weeks Protein Supplement Linseed Soybean Sunﬂower Meat Casein 309 243 423 325 368 229 230 340 257 390181 248 392 303 379141 327 339 315 260260 329 341 380 404203 250 226 153 318 148 193 320 263 352 169 271 295 242 359213 316 334 206 216 257 267 322 344 222244 199 297 258 283271 177 318 332 158248PROBLEMS 441 --- Page 452 --- TABLE 15.11 Weight Gains (g) of Rats under Six Diet Combinations High Protein Low Protein Beef Cereal Pork Beef Cereal Pork 73 98 94 90 107 49 102 74 79 76 95 82 118 56 96 90 97 73104 111 98 64 80 8681 95 102 86 98 81 107 88 102 51 74 97 100 82 72 10687 77 90 86 95 92 78 Source : Snedecor and Cochran (1967, p.",
    "98 81 107 88 102 51 74 97 100 82 72 10687 77 90 86 95 92 78 Source : Snedecor and Cochran (1967, p. 347). TABLE 15.12 Yield from Five Varieties of Plants Treated with Four Fertilizers Variety Fertilizer 12345 1 5 72 63 92 34 8 4 63 8—3 63 5 —2 0—1 8— 2 6 74 45 77 46 1 72 68 61 47 —6 66 4—6 9— 3 9 59 29 19 87 8 90 89 82 85 89 8 9———9 5 4 9 29 69 89 99 9 88 95 93 90 ———9 89 8— Source : Ostle and Mensing (1975, p.",
    "89 82 85 89 8 9———9 5 4 9 29 69 89 99 9 88 95 93 90 ———9 89 8— Source : Ostle and Mensing (1975, p. 368).442 ANALYSIS-OF-VARIANCE: THE CELL MEANS MODEL FOR UNBALANCED DATA --- Page 453 --- 16 Analysis-of-Covariance 16.1 INTRODUCTION In addition to the dependent variable y, there may be one or more quantitative variables that can also be measured on each experimental unit (or subject) in an ANOVA situation.",
    "ve variables that can also be measured on each experimental unit (or subject) in an ANOVA situation. If it appears that these extra variables may affect the outcome of the experiment, they can be included in the model as independent variables ( x’s ) and are then known as covariates orconcomitant variables .Analysis of covariance is sometimes described as a blend of ANOVA and regression.",
    "omitant variables .Analysis of covariance is sometimes described as a blend of ANOVA and regression. The primary motivation for the use of covariates in an experiment is to gain precision by reducing the error variance. In some situations, analysis of covariance can be used to lessen the effect of factors that the experimenter cannot effectively control, because an attempt to include various levels of a quantitative variable as a full factor may cause the design to become unwieldy.",
    "various levels of a quantitative variable as a full factor may cause the design to become unwieldy. In such cases, the variablecan be included as a covariate, with a resulting adjustment to the dependent variable before comparing means of groups.",
    "a covariate, with a resulting adjustment to the dependent variable before comparing means of groups. Variables of this type may also occur in exper- imental situations in which the subjects cannot be randomly assigned to treatments.In such cases, we forfeit the causality implication of a designed experiment, and analysis of covariance is closer in spirit to descriptive model building.",
    "a designed experiment, and analysis of covariance is closer in spirit to descriptive model building. In terms of a one-way model with one covariate, analysis of covariance will be successful if the following three assumptions hold. 1.The dependent variable is linearly related to the covariate . If this assumption holds, part of the error in the model is predictable and can be removed to reducethe error variance.",
    "holds, part of the error in the model is predictable and can be removed to reducethe error variance. This assumption can be checked by testing H 0:b¼0, where bis the slope from the regression of the dependent variable on the covariate. Since the estimated slope ^bwill never be exactly 0, analysis of covariance will always give a smaller sum of squares for error than the corresponding ANOVA.",
    "ysis of covariance will always give a smaller sum of squares for error than the corresponding ANOVA. However, if ^bis close to 0, the small reduction in error sum of squares may not offset the loss of a degree of freedom [see (16.27) and a comment following]. This problem is more likely to arise with multiple covari- ates, especially if they are highly correlated. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc.",
    ",Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 443 --- Page 454 --- 2.The groups (treatments )have the same slope . In assumption 1 above, a common slope bfor all kgroups is implied (assuming a one-way model with kgroups). We can check this assumption by testing H0:b1¼b2¼ /C1/C1/C1¼bk, wherebiis the slope in the ith group. 3.The covariate does not affect the differences among the means of the groups (treatments ).",
    "group. 3.The covariate does not affect the differences among the means of the groups (treatments ). If differences among the group means were reduced when the dependent variable is adjusted for the covariate, the test for equality of group means would be less powerful. Assumption 3 can be checked by performing an ANOVA on the covariate. Covariates can be either ﬁxed constants (values chosen by the researcher) or random variables.",
    "iate. Covariates can be either ﬁxed constants (values chosen by the researcher) or random variables. The models we consider in this chapter involve ﬁxed covariates, but in practice, the majority are random. However, the estimation and testing pro-cedures are the same in both cases, although the properties of estimators and tests are somewhat different for ﬁxed and random covariates.",
    "though the properties of estimators and tests are somewhat different for ﬁxed and random covariates. For example, in the ﬁxed- covariate case, the power of the test depends on the actual values chosen for the cov-ariates, whereas in the random-covariate case, the power of the test depends on the population covariance matrix of the covariates. As an illustration of the use of analysis of covariance, suppose that we wish to compare three methods of teaching language.",
    "e use of analysis of covariance, suppose that we wish to compare three methods of teaching language. Three classes are available, and weassign a class to each of the teaching methods. The students are free to sign up for any one of the three classes and are therefore not randomly assigned. One of theclasses may end up with a disproportionate share of the best students, in which case we cannot claim that teaching methods have produced a signiﬁcant difference in ﬁnal grades.",
    "ich case we cannot claim that teaching methods have produced a signiﬁcant difference in ﬁnal grades. However, we can use previous grades or other measures of performanceas covariates and then compare the students’ adjusted scores for the three methods. We give a general approach to estimation and testing in Section 16.2 and then cover speciﬁc balanced models in Sections 16.3–16.5. Unbalanced models are dis-cussed brieﬂy in Section 16.6.",
    "ciﬁc balanced models in Sections 16.3–16.5. Unbalanced models are dis-cussed brieﬂy in Section 16.6. We use overparameterized models for the balanced case in Sections 16.2–16.5. and use the cell means model in Section 16.6. 16.2 ESTIMATION AND TESTING We introduce and illustrate the analysis of covariance model in Section 16.2.1 and discuss estimation and testing for this model in Sections 16.2.2 and 16.2.3.",
    "l in Section 16.2.1 and discuss estimation and testing for this model in Sections 16.2.2 and 16.2.3. 16.2.1 The Analysis-of-Covariance Model In general, an analysis of covariance model can be written as y¼Z aþXbþ1, (16:1) where Zcontains 0s and 1s, acontainsmand parameters such as ai,bi, andgijrepre- senting factors and interactions (or other effects); Xcontains the covariate values; and bcontains coefﬁcients of the covariates.",
    "ons (or other effects); Xcontains the covariate values; and bcontains coefﬁcients of the covariates. Thus the covariates appear on the right444 ANALYSIS-OF-COVARIANCE --- Page 455 --- side of (16.1) as independent variables. Note that Zais the same as Xbin the ANOVA models in Chapters 12–14, whereas in this chapter, we use Xbto represent the covariates in the model. We now illustrate (16.1) for some of the models that will be considered in this chapter.",
    "the model. We now illustrate (16.1) for some of the models that will be considered in this chapter. A one-way (balanced) model with one covariate can be expressed as yij¼mþaiþbxijþ1ij,i¼1,2,... ,k,j¼1,2,... ,n, (16:2) whereaiis the treatment effect, xijis a covariate observed on the same sampling unit asyij, andbis a slope relating xijtoyij. [If (16.2) is viewed as a regression model, then the parameters mþaii¼1,2,...",
    "ope relating xijtoyij. [If (16.2) is viewed as a regression model, then the parameters mþaii¼1,2,... ,k, serve as regression intercepts for the k groups.] The kn observations for (16.2) can be written in the form y¼ZaþXbþ1as in (16.1), where Z¼110 /C1/C1/C1 0 ............ 110 /C1/C1/C1 0 101 /C1/C1/C1 0 ............ 100 /C1/C1/C1 10 BBBBBBB@1 CCCCCCCA, a¼m a1 ... ak0 BBB@1 CCCA,X¼x¼x11 ... x1n x2n ... xkn0 BBBBBBB@1 CCCCCCCA, (16:3) and b¼b. In this case, Zis the same as Xin (13.6).",
    "1 ... x1n x2n ... xkn0 BBBBBBB@1 CCCCCCCA, (16:3) and b¼b. In this case, Zis the same as Xin (13.6). For a one-way (balanced) model with qcovariates, the model is yij¼mþaiþb1xij1þ/C1/C1/C1þbqxijqþ1ij,i¼1,2,... ,k,j¼1,2,... ,n: (16:4) In this case, Zandaare as given in (16.3), and Xbhas the form Xb¼x111 x112 /C1/C1/C1 x11q x121 x122 /C1/C1/C1 x12q ......... xkn1xkn2/C1/C1/C1 xknq0 BBB@1 CCCAb1 b2 ...",
    "1 x112 /C1/C1/C1 x11q x121 x122 /C1/C1/C1 x12q ......... xkn1xkn2/C1/C1/C1 xknq0 BBB@1 CCCAb1 b2 ... bq0 BBB@1 CCCA: (16:5) For a two-way model with one covariate yijk¼mþaiþdjþgijþbxijkþ1ijk, (16:6) Zahas the form given in (14.4), and Xbis Xb¼xb¼x111 x112 ... xabn0 BBB@1 CCCAb: The two-way model in (16.6) could be extended to include several covariates.16.2 ESTIMATION AND TESTING 445 --- Page 456 --- 16.2.2 Estimation We now develop estimators of aandbfor the general case in (16.1), y¼ZaþXbþ1.",
    "6 --- 16.2.2 Estimation We now develop estimators of aandbfor the general case in (16.1), y¼ZaþXbþ1. We assume that Zis less than full rank as in overparameterized ANOVA models and that Xis full-rank as in regression models. We also assume that E(1)¼0and cov( 1)¼s2I: The model can be expressed as y¼ZaþXbþ1 ¼(Z,X)a b/C18/C19 þ1 ¼Uuþ1, (16:7) where U¼(Z,X) andu¼a b/C18/C19 .",
    "el can be expressed as y¼ZaþXbþ1 ¼(Z,X)a b/C18/C19 þ1 ¼Uuþ1, (16:7) where U¼(Z,X) andu¼a b/C18/C19 . The normal equations for (16.7) are U0U^u¼U0y, which can be written in partitioned form as Z0 X0/C18/C19 (Z,X)^a ^b/C18/C19 ¼Z0 X0/C18/C19 y, Z0ZZ0X X0ZX0X/C18/C19^a ^b/C18/C19 ¼Z0y X0y/C18/C19 : (16:8) We can express (16.8) as two sets of equations in ^aand ^b: Z0Z^aþZ0X^b¼Z0y, (16:9) X0Z^aþX0X^b¼X0y: (16:10) Using a generalized inverse of Z0Z, we can solve for ^ain (16.9): ^a¼(Z0Z)/C0Z0y/C0(Z0Z)/C0Z0X^b ¼^a0/C0(Z0Z)/C0Z0X^b, (16:11) where ^a0¼(Z0Z)/C0Z0yis a solution for the normal equations for the model y¼Zaþ1without the covariates [see (12.13)].",
    "0Z0yis a solution for the normal equations for the model y¼Zaþ1without the covariates [see (12.13)]. To solve for ^b, we substitute (16.11) into (16.10) to obtain X0Z[(Z0Z)/C0Z0y/C0(Z0Z)/C0Z0X^b]þX0X^b¼X0y446 ANALYSIS-OF-COVARIANCE --- Page 457 --- or X0Z(Z0Z)/C0Z0yþX0[I/C0Z(Z0Z)/C0Z0]X^b¼X0y: (16:12) Deﬁning P¼Z(Z0Z)/C0Z0, (16:13) we see that (16.12) becomes X0(I/C0P)X^b¼X0y/C0X0Py¼X0(I/C0P)y: Since the elements of Xtypically exhibit a pattern unrelated to the 0s and 1s in Z,w e can assume that the columns of Xare linearly independent of the columns of Z.",
    "the 0s and 1s in Z,w e can assume that the columns of Xare linearly independent of the columns of Z. Then X0(I/C0P)Xis nonsingular (see Problem 16.1), and a solution for ^bis given by ^b¼[X0(I/C0P)X]/C01X0(I/C0P)y (16:14) ¼E/C01 xxexy, (16:15)where Exx¼X0(I/C0P)Xand exy¼X0(I/C0P)y: (16:16) For the analysis-of-covariance model (16.1) or (16.7), we denote SSE as SSE y/C1x.",
    "(I/C0P)y: (16:16) For the analysis-of-covariance model (16.1) or (16.7), we denote SSE as SSE y/C1x. By (12.20), SSE y/C1xcan be expressed as SSE y/C1x¼y0y/C0^u0U0y¼y0y/C0(^a0,^b0)Z0y X0y/C18/C19 ¼y0y/C0^a0Z0y/C0^b0X0y ¼y0y/C0[^a0 0/C0^b0X0Z(Z0Z)/C0]Z0y/C0^b0X0y[by (16 :11)] ¼y0y/C0^a00Z0y/C0^b0X0[I/C0Z(Z0Z)/C0Z0]y ¼SSE y/C0^b0X0(I/C0P)y, (16:17) where ^a0is as deﬁned in (16.11), Pis deﬁned as in (16.13), and SSE y¼y0y/C0^a0 0Z0y is the same as the SSE for the ANOVA model y¼Zaþ1without the covariates.",
    "), and SSE y¼y0y/C0^a0 0Z0y is the same as the SSE for the ANOVA model y¼Zaþ1without the covariates. Using (16.16), we can write (16.17) in the form SSE y/C1x¼eyy/C0e0 xyE/C01 xxexy, (16:18) where eyy¼SSE y¼y0(I/C0P)y: (16:19)16.2 ESTIMATION AND TESTING 447 --- Page 458 --- In (16.18), we see the reduction in SSE that was noted in the second paragraph of Section 16.1. The proof that Exx¼X0(I/C0P)Xis nonsingular (see Problem 16.1) can be extended to show that Exxis positive deﬁnite.",
    "Exx¼X0(I/C0P)Xis nonsingular (see Problem 16.1) can be extended to show that Exxis positive deﬁnite. Therefore, e0 xyE/C01 xxexy.0, and SSE y/C1x,SSE y. 16.2.3 Testing Hypotheses In order to test hypotheses, we assume that 1in (16.1) is distributed as Nn(0,s2I), where nis the number of rows of ZorX. Using the model (16.7), we can express a hypothesis about ain the form H0:Cu¼0, where C¼(C1,O), so that H0becomes H0:(C1,O)a b/C18/C19 ¼0orH0:C1a¼0: We can then use a general linear hypothesis test.",
    "that H0becomes H0:(C1,O)a b/C18/C19 ¼0orH0:C1a¼0: We can then use a general linear hypothesis test. Alternatively, we can incorporate the hypothesis into the model and use a full–reduced-model approach. Hypotheses about bcan also be expressed in the form H0:Cu¼0: H0:Cu¼(O,C2)a b/C18/C19 ¼0orH0:C2b¼0: A basic hypothesis of interest is H0:b¼0, that is, that the covariate(s) do not belong in the model (16.1).",
    "hypothesis of interest is H0:b¼0, that is, that the covariate(s) do not belong in the model (16.1). In order to make a general linear hypothesis test of H0:b¼0,w e need cov( ^b), where ^bis given by (16.14) as ^b¼[X0(I/C0P)X]/C01X0(I/C0P)y.",
    "esis test of H0:b¼0,w e need cov( ^b), where ^bis given by (16.14) as ^b¼[X0(I/C0P)X]/C01X0(I/C0P)y. Since I/C0Pis idempotent (see Theorems 2.13e and 2.13f), cov( ^b) can readily be found from (3.44) as cov( ^b)¼[X0(I/C0P)X]/C01X0(I/C0P)s2I(I/C0P)X[X0(I/C0P)X]/C01 ¼s2[X0(I/C0P)X]/C01: (16:20) Then SSH for testing H0:b¼0is given by Theorem 8.4a(ii) as SSH ¼^b0X0(I/C0P)X^b: (16:21) Using (16.16), we can express this as SSH ¼e0 xyE/C01 xxexy: (16:22) Note that SSH in (16.22) is equal to the reduction in SSE due to the covariates; see (16.17), (16.18), and (16.19).448 ANALYSIS-OF-COVARIANCE --- Page 459 --- We now discuss some speciﬁc models, beginning with the one-way model in Section 16.3.",
    "- Page 459 --- We now discuss some speciﬁc models, beginning with the one-way model in Section 16.3. 16.3 ONE-WAY MODEL WITH ONE COVARIATE We review the one-way model in Section 16.3.1, consider estimators of parameters in Section 16.3.2, and discuss tests of hypotheses in Section 16.3.3. 16.3.1 The Model The one-way (balanced) model was introduced in (16.2): yij¼mþaiþbxijþ1ij,i¼1,2,... ,k,j¼1,2,...",
    "odel The one-way (balanced) model was introduced in (16.2): yij¼mþaiþbxijþ1ij,i¼1,2,... ,k,j¼1,2,... ,n: (16:23) Allknobservations can be written in the form of (16.1) y¼ZaþXbþ1¼Zaþxbþ1, where Z,a, and xare as given in (16.3). 16.3.2 Estimation By (16.11), (13.11), and (13.12), an estimator of ais obtained as ^a¼^a0/C0(Z0Z)/C0Z0X^b¼^a0/C0(Z0Z)/C0Z0x^b ¼0 /C22y1: /C22y2: ... /C22yk:0 BBBBBBB@1 CCCCCCCA/C00 ^ b/C22x1: ^b/C22x2: ...",
    "¼^a0/C0(Z0Z)/C0Z0x^b ¼0 /C22y1: /C22y2: ... /C22yk:0 BBBBBBB@1 CCCCCCCA/C00 ^ b/C22x1: ^b/C22x2: ... ^b/C22xk:0 BBBBBBB@1 CCCCCCCA¼0 /C22y 1:/C0^b/C22x1: /C22y2:/C0^b/C22x2: ... /C22yk:/C0^b/C22x2:0 BBBBBBB@1 CCCCCCCA(16:24) (see Problem 16.4).",
    "^b/C22x1: /C22y2:/C0^b/C22x2: ... /C22yk:/C0^b/C22x2:0 BBBBBBB@1 CCCCCCCA(16:24) (see Problem 16.4). In this case, with a single x,E xxandexyreduce to scalars, along with eyy: Exx¼exx¼Xk i¼1Xn j¼1(xij/C0/C22xi:)2, exy¼exy¼X ij(xij/C0/C22xi:)(yij/C0/C22yi:), eyy¼X ij(yij/C0/C22yi:)2:(16:25)16.3 ONE-WAY MODEL WITH ONE COVARIATE 449 --- Page 460 --- Now, by (16.15), the estimator of bis ^b¼exy exx¼P ij(xij/C0/C22xi:)(yij/C0/C22yi:) P ij(xij/C0/C22xi:)2: (16:26) By (16.18), (16.19), and the three results in (16.25), SSE y/C1xis given by SSE y/C1x¼eyy/C0e0 xyE/C01 xxexy¼eyy/C0e2 xy exx ¼X ij(yij/C0/C22yi:)2/C0/C2P ij(xij/C0/C22xi:)(yij/C0/C22yi:)/C32 P ij(xij/C0/C22xi:)2, (16:27) which has k(n/C01)/C01 degrees of freedom.",
    "/C22xi:)(yij/C0/C22yi:)/C32 P ij(xij/C0/C22xi:)2, (16:27) which has k(n/C01)/C01 degrees of freedom. Note that the degrees of freedom of SSE y/C1xare reduced by 1 for estimation of b, since SSE y¼eyyhask(n/C01) degrees of freedom and e2xy=exxhas 1 degree of freedom. In using analysis of covariance, the researcher expects the reduction from SSE yto SSE y/C1xto at least offset the loss of a degree of freedom.",
    "cher expects the reduction from SSE yto SSE y/C1xto at least offset the loss of a degree of freedom. 16.3.3 Testing Hypotheses For testing hypotheses, we assume that the 1ij’s in (16.23) are independently distrib- uted as N(0,s2). We begin with a test for equality of treatment effects. 16.3.3.1 Treatments To test H01:a1¼a2¼/C1/C1/C1¼ak adjusted for the covariate, we use a full–reduced-model approach.",
    "nts To test H01:a1¼a2¼/C1/C1/C1¼ak adjusted for the covariate, we use a full–reduced-model approach. The full model is (16.23), and the reduced model (with a1¼a2¼/C1/C1/C1¼ak¼a)i s yij¼mþaþbxijþ1ij ¼m/C3þbxijþ1ij,i¼1,2,... ,k,j¼1,2,... ,n: 16:28) This is essentially the same as the simple linear regression model (6.1).",
    ".. ,k,j¼1,2,... ,n: 16:28) This is essentially the same as the simple linear regression model (6.1). By (6.13),SSE for this reduced model (denoted by SSE rd) is given by SSE rd¼Xk i¼1Xn j¼1(yij/C0/C22y::)2/C0/C2P ij(xij/C0/C22x::)(yij/C0/C22y::)/C32 P ij(xij/C0/C22x::)2, (16:29) which has kn/C01/C01¼kn/C02 degrees of freedom.450 ANALYSIS-OF-COVARIANCE --- Page 461 --- Using a notation adapted from Sections 8.2, 13.4, and 14.4, we express the sum of squares for testing H01as SS(ajm,b)¼SS(m,a,b)/C0SS(m,b): In (16.27), SSE y/C1xis for the full model, and in (16.29), SSE rdis for the reduced model.",
    "SS(m,b): In (16.27), SSE y/C1xis for the full model, and in (16.29), SSE rdis for the reduced model. They can therefore be written as SSE y/C1x¼y0y/C0SS(m,a,b) and SSE rd¼y0y/C0SS(m,b). Hence SS(ajm,b)¼SSE rd/C0SSE y/C1x, (16:30) which has kn/C02/C0[k(n/C01)/C01]¼k/C01 degrees of freedom. The test statistic for H01:a1¼a2¼/C1/C1/C1¼akis therefore given by F¼SS(ajm,b)=(k/C01) SSE y/C1x=[k(n/C01)/C01], (16:31) which is distributed as F[k/C01,k(n/C01)/C01] when H01is true.",
    "01) SSE y/C1x=[k(n/C01)/C01], (16:31) which is distributed as F[k/C01,k(n/C01)/C01] when H01is true. By (16.30), we have SSE rd¼SS(ajm,b)þSSE y/C1x: Hence, SSE rdfunctions as the “total sum of squares” for the test of treatment effects adjusted for the covariate.",
    "unctions as the “total sum of squares” for the test of treatment effects adjusted for the covariate. We can therefore denote SSE rdby SST y/C1x, so that the expression above becomes SST y/C1x¼SS(ajm,b)þSSE y/C1x: (16:32) To complete the analogy with SSE y/C1x¼eyy/C0e2 xy=exxin (16.27), we write (16.29) as SST y/C1x¼tyy/C0t2 xy txx, (16:33) where SST y/C1x¼SSE rd,tyy¼X ij(yij/C0/C22y::)2,txy¼X ij(xij/C0/C22x::)(yij/C0/C22y::), txx¼X ij(xij/C0/C22x::)2: (16:34) Note that the procedure used to obtain (16.30) is fundamentally different from that used to obtain SSE y/C1xand SSE rdin (16.27) and (16.29).",
    "6.30) is fundamentally different from that used to obtain SSE y/C1xand SSE rdin (16.27) and (16.29). The sum of squares SS(ajm,b) in (16.30) is obtained as the difference between the sums of squares16.3 ONE-WAY MODEL WITH ONE COVARIATE 451 --- Page 462 --- for full and reduced models, not as an adjustment to SS( ajm)¼nP i(/C22yi:/C0/C22y::)2in (13.24) analogous to the adjustment used in SSE y/C1xand SST y/C1xin (16.27) and (16.33).",
    "C22y::)2in (13.24) analogous to the adjustment used in SSE y/C1xand SST y/C1xin (16.27) and (16.33). We must use the full–reduced-model approach to compute SS( ajm,b), because we do not have the same covariate values for each treatment and the design is therefore unbalanced (even though the nvalues are equal). If SS( ajm,b) were computed in an “adjusted” manner as in (16.27) or (16.33), then SS(ajm,b)þSSE y/C1xwould not equal SST y/C1xas in (16.32).",
    "ed” manner as in (16.27) or (16.33), then SS(ajm,b)þSSE y/C1xwould not equal SST y/C1xas in (16.32). In Section 16.4, we will follow a computational scheme similar to that of (16.30) and (16.32) for each term in the two-way (balanced) model. We display the various sums of squares for testing H0:a1¼a2¼/C1/C1/C1¼akin Table 16.1. 16.3.3.2 Slope We now consider a test for H02:b¼0: By (16.22), the general linear hypothesis approach leads to SSH ¼e0 xyE/C01 xxexyfor testing H0:b¼0.",
    "By (16.22), the general linear hypothesis approach leads to SSH ¼e0 xyE/C01 xxexyfor testing H0:b¼0. For the case of a single covariate, this reduces to SSH ¼e2xy exx, (16:35) where exyandexxare as found in (16.25). The Fstatistic is therefore given by F¼e2xy=exx SSE y/C1x=[k(n/C01)/C01], (16:36) which is distributed as F[1,k(n/C01)/C01] when H02is true. 16.3.3.3 Homogeneity of Slopes The tests of H01:a1¼a2¼/C1/C1/C1¼akandH02:b¼0 assume a common slope for all kgroups.",
    "neity of Slopes The tests of H01:a1¼a2¼/C1/C1/C1¼akandH02:b¼0 assume a common slope for all kgroups. To check this assumption, we can test the hypothesis of equal slopes in the groups H03:b1¼b2¼/C1/C1/C1¼bk, (16:37) wherebiis the slope in the ith group. In effect, H03states that the kregression lines are parallel.",
    "wherebiis the slope in the ith group. In effect, H03states that the kregression lines are parallel. TABLE 16.1 Analysis of Covariance for Testing H0:a1¼a2¼/C1/C1/C1¼akin the One-Way Model with One Covariate Source SS Adjusted for Covariate Adjusted df Treatments SS( ajm,b)¼SST y/C1x/C0SSE y/C1x k21 Error SSE y/C1x¼eyy/C0e2 xy=exx k(n21)21 Total SST y/C1x¼tyy/C0t2 xy=txx kn22452 ANALYSIS-OF-COVARIANCE --- Page 463 --- The full model allowing for different slopes becomes yij¼mþaiþbixijþ1ij,i¼1,2,...",
    "E --- Page 463 --- The full model allowing for different slopes becomes yij¼mþaiþbixijþ1ij,i¼1,2,... ,k,j¼1,2,... ,n: (16:38) The reduced model with a single slope is (16.23). In matrix form, the nkobservations in (16.38) can be expressed as y¼ZaþXbþ1, where Zandaare as given in (16.3) and Xb¼x10/C1/C1/C1 0 0x 2/C1/C1/C1 0 ......... 00 /C1/C1/C1xk0 BBB@1 CCCAb1 b2 ... bk0 BBB@1 CCCA, (16:39) withxi¼(xi1,xi2,... ,xin)0.",
    "0 ......... 00 /C1/C1/C1xk0 BBB@1 CCCAb1 b2 ... bk0 BBB@1 CCCA, (16:39) withxi¼(xi1,xi2,... ,xin)0. By (16.14) and (16.15), we obtain ^b¼E/C01 xxexy¼[X0(I/C0P)X]/C01X0(I/C0P)y: To evaluate Exxandexy, we ﬁrst note that by (13.11), (13.25), and (13.26) I/C0P¼I/C0Z(Z0Z)/C0Z0 ¼I/C01 nJO /C1/C1/C1 O OI /C01nJ/C1/C1/C1 O ......... OO /C1/C1/C1I/C01 nJ0 BBBBBBBBBB@1 CCCCCCCCCCA, (16:40) where IinI/C0Piskn/C2knandIinI/C0ð1=nÞJisn/C2n.",
    "/C1/C1/C1I/C01 nJ0 BBBBBBBBBB@1 CCCCCCCCCCA, (16:40) where IinI/C0Piskn/C2knandIinI/C0ð1=nÞJisn/C2n. Thus E xx¼X0(I/C0P)X¼x0 1/C16 I/C01 nJ/C17 x1 0 /C1/C1/C1 0 0 x0 2/C16 I/C01 nJ/C17 x2/C1/C1/C1 0 ......... 00 /C1/C1/C1x0 k/C16 I/C01 nJ/C17 xk0 BBBBBBBBBB@1 CCCCCCCCCCA ¼P j(x1j/C0/C22x1:)20 /C1/C1/C1 0 0P j(x2j/C0/C22x2:)2/C1/C1/C1 0 .........",
    "BBBBBBBBBB@1 CCCCCCCCCCA ¼P j(x1j/C0/C22x1:)20 /C1/C1/C1 0 0P j(x2j/C0/C22x2:)2/C1/C1/C1 0 ......... 00 /C1/C1/C1P j(xkj/C0/C22xk:)20 BBBBBB@1 CCCCCCA(16:41)16.3 ONE-WAY MODEL WITH ONE COVARIATE 453 --- Page 464 --- ¼exx,1 0 /C1/C1/C1 0 0 exx,2/C1/C1/C1 0 ......... 00 /C1/C1/C1 exx,k0 BBB@1 CCCA, (16:42) where e xx,i¼P j(xij/C0/C22xi:)2. To ﬁnd exy, we partition yasy¼(y0 1,y02,... ,y0k)0, where y0i¼(yi1,yi2,... ,yin). Then exy¼X0(I/C0P)y ¼x0100/C1/C1/C100 00x02/C1/C1/C100 .........",
    "0k)0, where y0i¼(yi1,yi2,... ,yin). Then exy¼X0(I/C0P)y ¼x0100/C1/C1/C100 00x02/C1/C1/C100 ......... 0000/C1/C1/C1x0 k0 BBBBB@1 CCCCCAI/C01 nJO /C1/C1/C1 O OI /C01nJ/C1/C1/C1 O ......... OO /C1/C1/C1I/C01 nJ0 BBBBBBBBBB@1 CCCCCCCCCCAy 1 y2 ... yk0 BBBBB@1 CCCCCA ¼x 10/C16 I/C01 nJ/C17 y1 x20/C16 I/C01nJ/C17 y 2 ... x0 k/C16 I/C01 nJ/C17 yk0 BBBBBBBBBB@1 CCCCCCCCCCA ¼P j(x1j/C0/C22x1:)(y1j/C0/C22y1:)P j(x2j/C0/C22x2:)(y2j/C0/C22y2:) ...",
    "yk0 BBBBBBBBBB@1 CCCCCCCCCCA ¼P j(x1j/C0/C22x1:)(y1j/C0/C22y1:)P j(x2j/C0/C22x2:)(y2j/C0/C22y2:) ... P j(xkj/C0/C22xk:)(ykj/C0/C22yk:)0 BBBBB@1 CCCCCA(16:43) ¼e xy,1 exy,2 ... exy,k0 BBB@1 CCCA, (16:44)454 ANALYSIS-OF-COVARIANCE --- Page 465 --- where exy,i¼P j(xij/C0/C22xi:)(yij/C0/C22yi:). Then, by (16.15), we obtain ^b¼E/C01 xxexy¼exy,1=exx,1 exy,2=exx,2 ...",
    "ij/C0/C22xi:)(yij/C0/C22yi:). Then, by (16.15), we obtain ^b¼E/C01 xxexy¼exy,1=exx,1 exy,2=exx,2 ... exy,k=exx,k0 BBB@1 CCCA: (16:45) By analogy with (16.30), we obtain the sum of squares for the test of H 03in (16.37) by subtracting SSE y/C1xfor the full model from SSE y/C1xfor the reduced model, that is, SSE( R)y/C1x/C0SSE( F)y/C1x. For the full model in (16.38), SSE( F)y/C1xis given by (16.18), (16.44), and (16.45) as SSE( F)y/C1x¼eyy/C0e0 xyE/C01 xxexy¼eyy/C0e0xy^b ¼eyy/C0(exy,1,exy,2,...",
    "8), (16.44), and (16.45) as SSE( F)y/C1x¼eyy/C0e0 xyE/C01 xxexy¼eyy/C0e0xy^b ¼eyy/C0(exy,1,exy,2,... ,exy,k)exy,1=exx,1 exy,2=exx,2 ... exy,k=exx,k0 BBBBB@1 CCCCCA ¼eyy/C0Xk i¼1e2 xy,i exx,i, (16:46) which has k(n/C01)/C0k¼k(n/C02) degrees of freedom. The reduced model in which H03:b1¼b2¼/C1/C1/C1¼bk¼bis true is given by (16.23), for which SSE( R)y/C1xis found in (16.27) as SSE( R)y/C1x¼eyy/C0e2xy exx, (16:47) which has k(n/C01)/C01 degrees of freedom.",
    "found in (16.27) as SSE( R)y/C1x¼eyy/C0e2xy exx, (16:47) which has k(n/C01)/C01 degrees of freedom. Thus, the sum of squares for testing H03is SSE( R)y/C1x/C0SSE( F)y/C1x¼Xk i¼1e2xy,i exx,i/C0e2xy exx, (16:48) which has k(n/C01)/C01/C0k(n/C02)¼k/C01 degrees of freedom.",
    "x¼Xk i¼1e2xy,i exx,i/C0e2xy exx, (16:48) which has k(n/C01)/C01/C0k(n/C02)¼k/C01 degrees of freedom. The test statistic is F¼Pk i¼1e2 xy,i=exx,i/C0e2xy=exxhi =(k/C01) SSE( F)y/C1x=k(n/C02), (16:49) which is distributed as F[k/C01,k(n/C02)] when H03:b1¼b2¼/C1/C1/C1¼bkis true.16.3 ONE-WAY MODEL WITH ONE COVARIATE 455 --- Page 466 --- If the hypothesis of equal slopes is rejected, the hypothesis of equal treatment effects can still be tested, but interpretation is more difﬁcult.",
    "the hypothesis of equal treatment effects can still be tested, but interpretation is more difﬁcult. The problem is some- what analogous to that of interpretation of a main effect in a two-way ANOVA in the presence of interaction. In a sense, the term bixijin (16.38) is an interaction. For further discussion of analysis of covariance with heterogeneity of slopes, see Reader (1973) and Hendrix et al. (1982). Example 16.3.",
    "covariance with heterogeneity of slopes, see Reader (1973) and Hendrix et al. (1982). Example 16.3. To investigate the effect of diet on maturation weight of guppy ﬁsh (Poecilia reticulata ), three groups of ﬁsh were fed different diets. The resulting weights yare given in Table 16.2 (Morrison 1983, p. 475) along with the initial weights x. We ﬁrst estimate b, using xas a covariate.",
    "(Morrison 1983, p. 475) along with the initial weights x. We ﬁrst estimate b, using xas a covariate. By the three results in (16.25), we have exx¼350 :2857 ,exy¼412 :71429 ,eyy¼1465 :7143 : Then by (16.26), we obtain ^b¼exy exx¼412 :7143 350 :2857¼1:1782 : We now test for equality of treatment means adjusted for the covariate, H0:a1¼a2¼a3. By (16.27), we have SSE y/C1x¼eyy/C0e2 xy exx¼1465 :7143 /C0(412 :7143)2 350 :2857 ¼979 :4453 with 17 degrees of freedom.",
    "SE y/C1x¼eyy/C0e2 xy exx¼1465 :7143 /C0(412 :7143)2 350 :2857 ¼979 :4453 with 17 degrees of freedom. By (16.29) and (16.33), we have SST y/C1x¼1141 :4709TABLE 16.2 Maturation Weight and Initial Weight (mg) of Guppy Fish Feeding Group 123 y xyxyx 49 35 68 33 59 33 61 26 70 35 53 3655 29 60 28 54 2669 32 53 29 48 3051 23 59 32 54 3338 26 48 23 53 2564 31 46 26 37 23456 ANALYSIS-OF-COVARIANCE --- Page 467 --- with 19 degrees of freedom.",
    "48 23 53 2564 31 46 26 37 23456 ANALYSIS-OF-COVARIANCE --- Page 467 --- with 19 degrees of freedom. Thus by (16.30), we have SS(ajm,b)¼SST y/C1x/C0SSE y/C1x¼1141 :4709 /C0979 :4453 ¼162 :0256 with 2 degrees of freedom. The Fstatistic is given in (16.31) as F¼SS(ajm,b)=(k/C01) SSE y/C1x=[k(n/C01)/C01]¼162 :0256 =2 979 :4453 =17¼1:4061 : Thepvalue is .272, and we do not reject H0:a1¼a2¼a3.",
    "n/C01)/C01]¼162 :0256 =2 979 :4453 =17¼1:4061 : Thepvalue is .272, and we do not reject H0:a1¼a2¼a3. To test H0:b¼0, we use (16.36): F¼e2 xy=exx SSE y/C1x=[k(n/C01)/C01]¼(412 :7143)2=350 :2857 979 :4453 =17 ¼8:4401 : Thep-value is .0099, and we reject H0:b¼0.",
    "/C01)/C01]¼(412 :7143)2=350 :2857 979 :4453 =17 ¼8:4401 : Thep-value is .0099, and we reject H0:b¼0. To test the hypothesis of equal slopes in the groups, H0:b1¼b2¼b3, we ﬁrst estimateb1,b2, andb3using (16.45): ^b1¼:7903 ,^b2¼1:9851 ,^b3¼:8579 : Then by (16.46) and (16.47), SSE( F)y/C1x¼880 :5896 ,SSE( R)y/C1x¼979 :4453 : The difference SSE( R)y/C1x/C0SSE( F)y/C1xis used in the numerator of the Fstatistic in (16.49): F¼(979 :4453 /C0880 :5896) =2 880 :5896 =(3)(5)¼:8420 : Thepvalue is .450, and we do not reject H0:b1¼b2¼b3.",
    "4453 /C0880 :5896) =2 880 :5896 =(3)(5)¼:8420 : Thepvalue is .450, and we do not reject H0:b1¼b2¼b3. A 16.4 TWO-WAY MODEL WITH ONE COVARIATE In this section, we discuss the two-way (balanced) ﬁxed-effects model with one covariate. The model was introduced in (16.6) as yijk¼mþaiþgjþdijþbxijkþ1ijk, (16:50) i¼1,2,... ,a,j¼1,2,... ,c,k¼1,2,...",
    "was introduced in (16.6) as yijk¼mþaiþgjþdijþbxijkþ1ijk, (16:50) i¼1,2,... ,a,j¼1,2,... ,c,k¼1,2,... ,n,16.4 TWO-WAY MODEL WITH ONE COVARIATE 457 --- Page 468 --- whereaiis the effect of factor A,gjis the effect of factor C,dijis the ACinteraction effect, and xijkis a covariate measured on the same experimental unit as yijk.",
    "jis the ACinteraction effect, and xijkis a covariate measured on the same experimental unit as yijk. 16.4.1 Tests for Main Effects and Interactions In order to ﬁnd SSE y/C1x, we consider the hypothesis of no overall treatment effect, that is, no Aeffect, no Ceffect, and no interaction (see a comment preceding Theorem 14.4b).",
    "effect, that is, no Aeffect, no Ceffect, and no interaction (see a comment preceding Theorem 14.4b). By analogy to (16.28), the reduced model is yijk¼m/C3þbxijkþ1ijk: (16:51) By analogy to (16.29), SSE for the reduced model is given by SSE rd¼Xa i¼1Xc j¼1Xn k¼1(yijk/C0/C22y...)2/C0P ijk(xijk/C0/C22x...)(yijk/C0/C22y...)hi2 P ijk(xijk/C0/C22x...)2 ¼X ijky2 ijk/C0y2 ...",
    "C22y...)2/C0P ijk(xijk/C0/C22x...)(yijk/C0/C22y...)hi2 P ijk(xijk/C0/C22x...)2 ¼X ijky2 ijk/C0y2 ... acn/C0P ijk(xijk/C0/C22x...)(yijk/C0/C22y...)hi2 P ijk(xijk/C0/C22x...)2: (16:52) By analogy to (16.27), SSE for the full model in (16.50) is SSE y/C1x¼X ijk(yijk/C0/C22yij:)2/C0P ijk(xijk/C0/C22xij:)(yijk/C0/C22yij:)hi2 P ijk(xijk/C0/C22xij:)2 ¼X ijky2 ijk/C0X ijy2 ij: n/C0P ijk(xijk/C0/C22xij:)(yijk/C0/C22yij:)hi2 P ijk(xijk/C0/C22xij:)2, (16:53) which has ac(n/C01)/C01 degrees of freedom.",
    "j:)(yijk/C0/C22yij:)hi2 P ijk(xijk/C0/C22xij:)2, (16:53) which has ac(n/C01)/C01 degrees of freedom. Note that the degrees of freedom for SSE y/C1xhave been reduced by 1 for the covariate adjustment. Now by analogy to (16.30), the overall sum of squares for treatments is SS(a,g,djm,b)¼SSE rd/C0SSE y/C1x ¼X ijy2ij: n/C0y2...",
    ", the overall sum of squares for treatments is SS(a,g,djm,b)¼SSE rd/C0SSE y/C1x ¼X ijy2ij: n/C0y2... acnþP ijk(xijk/C0/C22xij:)(yijk/C0/C22yij:)hi2 P ijk(xijk/C0/C22xij:)2 /C0P ijk(xijk/C0/C22x...)(yijk/C0/C22y...)hi2 P ijk(xijk/C0/C22x...)2, (16:54) which has ac/C01 degrees of freedom.458 ANALYSIS-OF-COVARIANCE --- Page 469 --- Using (14.47), (14.69), and (14.70), we can partition the termP ijy2 ij:=n/C0y2...=acn in (16.54), representing overall treatment sum of squares, as in (14.40): X ijy2ij: n/C0y2...",
    "..=acn in (16.54), representing overall treatment sum of squares, as in (14.40): X ijy2ij: n/C0y2... acn¼cnX i(/C22yi::/C0/C22y...)2þanX j(/C22y:j:/C0/C22y...)2 þnX ij(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...)2 ¼SSA yþSSC yþSSAC y: (16:55) To conform with this notation, we deﬁne SSE y¼X ijk(yijk/C0/C22yij:)2: We have an analogous partitioning of the overall treatment sum of squares for x: X ijx2ij: n/C0x2...",
    "We have an analogous partitioning of the overall treatment sum of squares for x: X ijx2ij: n/C0x2... acn¼SSA xþSSC xþSSAC x, (16:56) where, for example SSA x¼cnXa i¼1(/C22xi::/C0/C22x...)2: We also deﬁne SSE x¼X ijk(xijk/C0/C22xij:)2: The “overall treatment sum of products”P ijxij:yij:=n/C0x...y...=acncan be parti- tioned in a manner analogous to that in (16.55) and (16.56) (see Problem 16.8): X ijxij:yij: n/C0x...y...",
    "d in a manner analogous to that in (16.55) and (16.56) (see Problem 16.8): X ijxij:yij: n/C0x...y... acn¼cnX i(/C22xi::/C0/C22x...)(/C22yi::/C0/C22y...)þanX j(/C22x:j:/C0/C22x...)(/C22y:j:/C0/C22y...) þnX ij(/C22xij:/C0/C22xi::/C0/C22x:j:þ/C22x...)(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...) ¼SPA þSPC þSPAC : (16:57)16.4 TWO-WAY MODEL WITH ONE COVARIATE 459 --- Page 470 --- We also deﬁne SPE ¼X ijk(xijk/C0/C22xij:)(yijk/C0/C22yij:): We can now write SSE y/C1xin (16.53) in the simpliﬁed form SSE y/C1x¼SSE y/C0(SPE)2 SSE x: We display these sums of squares and products in Table 16.3.",
    "ed form SSE y/C1x¼SSE y/C0(SPE)2 SSE x: We display these sums of squares and products in Table 16.3. We now proceed to develop hypothesis tests for factor A, factor C, and the inter- action AC. The orthogonality of the balanced design is lost when adjustments are made for the covariate [see comments following (16.34); see also Bingham and Feinberg (1982)].",
    "s are made for the covariate [see comments following (16.34); see also Bingham and Feinberg (1982)]. We therefore obtain a “total” for each term ( A,C,o r AC)b y adding the error SS or SP to the term SS or SP for each of x,yandxy(see the entries for AþE,CþE, and ACþEin Table 16.3). These totals are analogous to SST y/C1x¼SS(ajm,b)þSSE y/C1xin (16.32) for the one-way model.",
    "16.3). These totals are analogous to SST y/C1x¼SS(ajm,b)þSSE y/C1xin (16.32) for the one-way model. The totals are used to obtain sums of squares adjusted for the covariate in a manner analogous to that employed in the one-way model [see (16.30) or the “treatments” line in Table 16.1].",
    "nalogous to that employed in the one-way model [see (16.30) or the “treatments” line in Table 16.1]. For example, the adjusted sum of squares SSA y/C1xfor factor Ais obtained as follows: SS(AþE)y/C1x¼SSA yþSSE y/C0(SPA þSPE)2 SSA xþSSE x, (16:58) SSE y/C1x¼SSE y/C0(SPE)2 SSE x, (16:59) SSA y/C1x¼SS(AþE)y/C1x/C0SSE y/C1x: (16:60) From inspection of (16.58), (16.59), and (16.60), we see that SSA y/C1xhasa21 degrees of freedom.",
    "0) From inspection of (16.58), (16.59), and (16.60), we see that SSA y/C1xhasa21 degrees of freedom. The statistic for testing H01:a1¼a2¼/C1/C1/C1¼aa, corresponding to the TABLE 16.3 Sums of Squares and Products for xand yin a Two-Way Model SS and SP Corrected for the Mean Source yx x y A SSA y SSA x SPA C SSC y SSC x SPC AC SSAC y SSAC x SPAC Error SSE y SSE x SPE AþE SSA yþSSE y SSA xþSSE x SPA þSPE CþE SSC yþSSE y SSC xþSSE x SPC þSPE ACþE SSAC yþSSE y SSAC xþSSE x SPAC þSPE460 ANALYSIS-OF-COVARIANCE --- Page 471 --- main effect of A, is then given by F¼SSA y/C1x=(a/C01) SSE y/C1x=[ac(n/C01)/C01], (16:61) which is distributed as F[a/C01,ac(n/C01)/C01] if H01is true.",
    "01) SSE y/C1x=[ac(n/C01)/C01], (16:61) which is distributed as F[a/C01,ac(n/C01)/C01] if H01is true. Tests for factor Cand the interaction ACare developed in an analogous fashion. Example 16.4a. In each of three counties in Iowa, a sample of farms was taken from farms for which landlord and tenant are related and also from farms for which landlord and tenant are not related. Table 16.4 gives the data for y¼value of crops produced and x¼size of farm.",
    "tenant are not related. Table 16.4 gives the data for y¼value of crops produced and x¼size of farm. We ﬁrst obtain the sums of squares and products listed in Table 16.3, where factor Ais relationship status and factor Cis county.",
    "uares and products listed in Table 16.3, where factor Ais relationship status and factor Cis county. These are given in Table 16.5, where,TABLE 16.4 Value of Crops yand Size xof Farms in Three Iowa Counties County Landlord– Tenant12 3 y xyxyx Related 6399 160 2490 90 4489 120 8456 320 5349 154 10026 245 8453 200 5518 160 5659 1604891 160 10417 234 5475 160 3491 120 4278 120 11382 320 Not related 6944 160 4936 160 5731 160 6971 160 7376 200 6787 173 4053 120 6216 160 5814 134 8767 280 10313 240 9607 2396765 160 5124 120 9817 320 Source : Ostle and Mensing (1975, p.",
    "5814 134 8767 280 10313 240 9607 2396765 160 5124 120 9817 320 Source : Ostle and Mensing (1975, p. 480).",
    "134 8767 280 10313 240 9607 2396765 160 5124 120 9817 320 Source : Ostle and Mensing (1975, p. 480). TABLE 16.5 Sums of Squares and Products for xand y SS and SP Corrected for the Mean Source yx x y A 2,378,956.8 132.30 17,740.8 C 8,841,441.3 7724.47 249,752.8 AC 1,497,572.6 2040.20 41,440.3 Error 138,805,865 106,870 3,427,608.6 AþE 141,184,822 107,002.3 3,445,349.4 CþE 147,647,306 114,594.5 3,677,361.4 ACþE 140,303,437 108,910.2 3,469,048.916.4 TWO-WAY MODEL WITH ONE COVARIATE 461 --- Page 472 --- for example, SSA y¼2378956.8, SSA yþSSE y¼141,184,822, and SPAC þ SPE¼3,469,048.9.",
    "--- Page 472 --- for example, SSA y¼2378956.8, SSA yþSSE y¼141,184,822, and SPAC þ SPE¼3,469,048.9. By (16.58), (16.59), and (16.60), we have SS(AþE)y/C1x¼30,248 ,585 ,SSE y/C1x¼28,873 ,230 , SSA y/C1x¼1,375 ,355 :1: Then by (16.61), we have F¼SSA y/C1x=(a/C01) SSE y/C1x=[ac(n/C01)/C01] ¼1,375 ,355 :1=1 28,873 ,230 =23¼1,375 ,355 :1 1,255 ,357 :8¼1:0956 : Thepvalue is .306, and we do not reject H0:a1¼a2. Similarly, for factor C,w eh a v e F¼766 ,750 :1=2 1,255 ,357 :8¼:3054 with p¼.740.",
    "reject H0:a1¼a2. Similarly, for factor C,w eh a v e F¼766 ,750 :1=2 1,255 ,357 :8¼:3054 with p¼.740. For the interaction AC, we obtain F¼932 ,749 :5=2 1,255 ,357 :8¼:3715 with p¼.694. A 16.4.2 Test for Slope To test the hypothesis H02:b¼0, the sum of squares due to bis (SPE)2=SSE x, and theFstatistic is given by F¼(SPE)2=SSE x SSE y/C1x=[ac(n/C01)/C01], (16:62) which (under H02and also H03below) is distributed as F[1,ac(n/C01)/C01]. Eample 16.4b.",
    "C01], (16:62) which (under H02and also H03below) is distributed as F[1,ac(n/C01)/C01]. Eample 16.4b. To test H0:b¼0 for the farms data in Table 16.4, we use SPE and SSE xfrom Table 16.5 and SSE y/C1xin Example 16.4a. Then by (16.62), we obtain F¼(SPE)2=SSE x SSE y/C1x=[ac(n/C01)/C01] ¼(3,427 ,608 :6)2=106 ,870 1,255 ,357 :8¼87:5708 : Thepvalue is 2 :63/C210/C09, and H0:b¼0 is rejected.",
    ",427 ,608 :6)2=106 ,870 1,255 ,357 :8¼87:5708 : Thepvalue is 2 :63/C210/C09, and H0:b¼0 is rejected. A462 ANALYSIS-OF-COVARIANCE --- Page 473 --- 16.4.3 Test for Homogeneity of Slopes The test for homogeneity of slopes can be carried out separately for factor A, factor C, and the interaction AC. We describe the test for homogeneity of slopes among the levels of A. The hypothesis is H03:b1¼b2¼/C1/C1/C1¼ba; that is, the regression lines for the alevels of Aare parallel.",
    "ypothesis is H03:b1¼b2¼/C1/C1/C1¼ba; that is, the regression lines for the alevels of Aare parallel. The intercepts, of course, may be different. To obtain a slope estimator ^bifor the ith level of A, we deﬁne SSE x and SPE for the ith level of A: SSE x,i¼Xc j¼1Xn k¼1(xijk/C0/C22xij:)2,SPE i¼X jk(xijk/C0/C22xij:)(yijk/C0/C22yij:): (16:63) Then ^biis obtained as ^bi¼SPE i SSE x,i, and the sum of squares due to biis (SPE i)2=SSE x,i.",
    "6:63) Then ^biis obtained as ^bi¼SPE i SSE x,i, and the sum of squares due to biis (SPE i)2=SSE x,i. By analogy to (16.46), the sum of squares for the full model in which the bi’s are different is given by SS(F)¼SSE y/C0Xa i¼1(SPE i)2 SSE x,i, and by analogy to (16.47), the sum of squares in the reduced model with a common slope is SS(R)¼SSE y/C0(SPE)2 SSE x: Our test statistic for H03:b1¼b2¼/C1/C1/C1¼bais then similar to (16.49): F¼[SS(R)/C0SS(F)]=(a/C01) SS(F)=[ac(n/C01)/C01] ¼Pa i¼1(SPE i)2=SSE x,i/C0(SPE)2=SSE x/C2/C3 =(a/C01) [SSE y/C0Pai¼1(SPE i)2=SSE x,i]=[ac(n/C01)/C0a], (16:64) which (under H03) is distributed as F[a/C01,ac(n/C01)/C0a].",
    "PE i)2=SSE x,i]=[ac(n/C01)/C0a], (16:64) which (under H03) is distributed as F[a/C01,ac(n/C01)/C0a]. The tests for homo- geneity of slopes for CandACare constructed in a similar fashion.16.4 TWO-WAY MODEL WITH ONE COVARIATE 463 --- Page 474 --- Example 16.4c.",
    "ucted in a similar fashion.16.4 TWO-WAY MODEL WITH ONE COVARIATE 463 --- Page 474 --- Example 16.4c. To test homogeneity of slopes for factor A, we ﬁrst ﬁnd ^b1and ^b2 for the two levels of A: ^b1¼SPE 1 SSE x,1¼2,141 ,839 :8 61,359 :2¼34:9066 , ^b2¼SPE 2 SSE x,2¼1,285 ,768 :8 45,510 :8¼28:2519 : Then SS(F)¼SSE y/C0X2 i¼1(SPE i)2 SSE x,i¼27,716 ,088 :7, SS(R)¼SSE y/C0(SPE)2 SSE x¼28,873 ,230 : The difference is SS( R)2SS(F)¼1,157,140.94.",
    "7,716 ,088 :7, SS(R)¼SSE y/C0(SPE)2 SSE x¼28,873 ,230 : The difference is SS( R)2SS(F)¼1,157,140.94. Then by (16.64), we obtain F¼1,157 ,140 :94=1 27,716 ,088 :7=22¼:9185 : Thepvalue is .348, and we do not reject H0:b1¼b2. For homogeneity of slopes for factor C,w eh a v e ^b1¼23:2104 ,^b2¼50:0851 ,^b3¼31:6693 , F¼9,506 ,034 :16=2 19,367 ,195 :5=21¼5:1537 with p¼.0151.",
    "e ^b1¼23:2104 ,^b2¼50:0851 ,^b3¼31:6693 , F¼9,506 ,034 :16=2 19,367 ,195 :5=21¼5:1537 with p¼.0151. A 16.5 ONE-WAY MODEL WITH MULTIPLE COVARIATES 16.5.1 The Model In some cases, the researcher has more than one covariate available. Note, however, that each covariate decreases the error degrees of freedom by 1, and therefore theinclusion of too many covariates may lead to loss of power.",
    "egrees of freedom by 1, and therefore theinclusion of too many covariates may lead to loss of power. For the one-way model with qcovariates, we use (16.4): y ij¼mþaiþb1xij1þb2xij2þ/C1/C1/C1þbqxijqþ1ij ¼mþaiþb0xijþ1ij, (16:65) i¼1,2,... ,k,j¼1,2,... ,n,464 ANALYSIS-OF-COVARIANCE --- Page 475 --- whereb0¼(b1,b2,... ,bq) andxij¼(xij1,xij2,... ,xijq)0. For this model, we wish to testH01:a1¼a2¼/C1/C1/C1¼akandH02:b¼0.",
    ",bq) andxij¼(xij1,xij2,... ,xijq)0. For this model, we wish to testH01:a1¼a2¼/C1/C1/C1¼akandH02:b¼0. We will also extend the model to allow for a different bvector in each of the kgroups and test equality of these bvectors. The model in (16.65) can be written in matrix notation as y¼ZaþXbþ1, where Zandaare given following (16.3) and Xbis as given by (16.5): Xb¼x111 x112 /C1/C1/C1 x11q x121 x122 /C1/C1/C1 x12q ......... xkn1xkn2/C1/C1/C1 xknq0 BBB@1 CCCAb1 b2 ...",
    "1 x112 /C1/C1/C1 x11q x121 x122 /C1/C1/C1 x12q ......... xkn1xkn2/C1/C1/C1 xknq0 BBB@1 CCCAb1 b2 ... bq0 BBB@1 CCCA: The vector yiskn/C21 and the matrix Xiskn/C2q. We can write yandXbin parti- tioned form corresponding to the kgroups: y¼y1 y2 ... yk0 BBB@1 CCCA,Xb¼X1 X2 ... Xk0 BBB@1 CCCAb, (16:66) where yi¼yi1 yi2 ... yin0 BBB@1 CCCAand Xi¼xi11xi12/C1/C1/C1 xi1q xi21xi22/C1/C1/C1 xi2q .........",
    ") where yi¼yi1 yi2 ... yin0 BBB@1 CCCAand Xi¼xi11xi12/C1/C1/C1 xi1q xi21xi22/C1/C1/C1 xi2q ......... xin1xin2/C1/C1/C1 xinq0 BBB@1 CCCA: 16.5.2 Estimation We ﬁrst obtain Exx,exy, and eyyfor use in ^band SSE y/C1x. By (16.16), Exxcan be expressed as Exx¼X0I/C0P ðÞ X: Using Xpartitioned as in (16.66) and I2Pin the form given in (16.40), Exxbecomes Exx¼Xk i¼1X0 iI/C01 nJ/C18/C19 Xi (16:67)16.5 ONE-WAY MODEL WITH MULTIPLE COVARIATES 465 --- Page 476 --- (see Problem 16.10).",
    "/C19 Xi (16:67)16.5 ONE-WAY MODEL WITH MULTIPLE COVARIATES 465 --- Page 476 --- (see Problem 16.10). Similarly, using ypartitioned as in (16.66), exyis given by (16.16) as exy¼X0(I/C0P)y¼Xk i¼1X0 iI/C01 nJ/C18/C19 yi: (16:68) By (16.19) and (16.40), we have eyy¼y0(I/C0P)y¼Xk i¼1y0 iI/C01 nJ/C18/C19 yi: (16:69) The elements of Exx,exy, and eyyare extensions of the sums of squares and products found in the three expressions in (16.25).",
    "and eyyare extensions of the sums of squares and products found in the three expressions in (16.25). To examine the elements of the matrix Exx, we ﬁrst note that I2(1/n)Jis symmetric and idempotent and therefore X0 i[I/C0(1=n)J)]Xiin (16.67) can be written as X0 i(I/C0(1=n)J)Xi¼X0i(I/C0(1=n)J)0(I/C0(1=n)J)Xi ¼X0 ciXci,(16:70) where Xci¼[I/C0(1=n)J]Xiis the centered matrix Xci¼xi11/C0/C22xi:1xi12/C0/C22xi:2/C1/C1/C1 xi1q/C0/C22xi:q xi21/C0/C22xi:1xi22/C0/C22xi:2/C1/C1/C1 xi2q/C0/C22xi:q .........",
    "0/C22xi:2/C1/C1/C1 xi1q/C0/C22xi:q xi21/C0/C22xi:1xi22/C0/C22xi:2/C1/C1/C1 xi2q/C0/C22xi:q ......... xin1/C0/C22xi:1xin2/C0/C22xi:2/C1/C1/C1 xinq/C0/C22xi:q0 BBBBB@1 CCCCCA(16:71) [see (7.33) and Problem 7.15], where /C22x i:2, for example, is the mean of the second column of Xi, that is, /C22xi:2¼Pn j¼1xij2=n. By Theorem 2.2c(i), the diagonal elements ofX0 ciXciare Xn j¼1(xijr/C0/C22xi:r)2,r¼1,2,...",
    "¼1xij2=n. By Theorem 2.2c(i), the diagonal elements ofX0 ciXciare Xn j¼1(xijr/C0/C22xi:r)2,r¼1,2,... ,q, (16:72) and the off-diagonal elements are Xn j¼1(xijr/C0/C22xi:r)(xijs/C0/C22xi:s),r=s: (16:73) By (16.67) and (16.72), the diagonal elements of Exxare Xk i¼1Xn j¼1(xijr/C0/C22xi:r)2,r¼1,2,...",
    "73) By (16.67) and (16.72), the diagonal elements of Exxare Xk i¼1Xn j¼1(xijr/C0/C22xi:r)2,r¼1,2,... ,q, (16:74)466 ANALYSIS-OF-COVARIANCE --- Page 477 --- and by (16.67) and (16.73), the off-diagonal elements are Xk i¼1Xn j¼1(xijr/C0/C22xi:r)(xijs/C0/C22xi:s),r=s: (16:75) These are analogous to exx¼P ij(xij/C0/C22xi:)2in (16.25).",
    "C0/C22xi:r)(xijs/C0/C22xi:s),r=s: (16:75) These are analogous to exx¼P ij(xij/C0/C22xi:)2in (16.25). To examine the elements of the vector exy, we note that by an argument similar to that used to obtain (16.70), X0 i[I/C0(1=n)J]yiin (16.68) can be written as X0 i[I/C0(1=n)J)yi¼X0i[I/C0(1=n)J]0[I/C0(1=n)J]yi¼X0ciyci, where Xciis as given in (16.71) and yci¼yi1/C0/C22yi: yi2/C0/C22yi: ... yin/C0/C22yi:0 BBB@1 CCCA with /C22y i:¼Pn j¼1yij=n.",
    "(16.71) and yci¼yi1/C0/C22yi: yi2/C0/C22yi: ... yin/C0/C22yi:0 BBB@1 CCCA with /C22y i:¼Pn j¼1yij=n. Thus the elements of X0 ciyciare of the form Xn j¼1(xijr/C0/C22xi:r)(yij/C0/C22yi:)r¼1,2,... ,q, and by (16.68), the elements of exyare Xk i¼1Xn j¼1(xijr/C0/C22xi:r)(yij/C0/C22yi:)r¼1,2,...",
    "... ,q, and by (16.68), the elements of exyare Xk i¼1Xn j¼1(xijr/C0/C22xi:r)(yij/C0/C22yi:)r¼1,2,... ,q: Similarly, eyyin (16.69) can be written as eyy¼Xk i¼1y0 iI/C01 nJ/C18/C190 I/C01nJ/C18/C19 y i¼Xk i¼1y0 ciyci ¼Xk i¼1Xn j¼1(yij/C0/C22yi:)2:(16:76) By (16.15), we obtain ^b¼E/C01 xxexy,16.5 ONE-WAY MODEL WITH MULTIPLE COVARIATES 467 --- Page 478 --- where Exxis as given by (16.67) and exyis as given by (16.68).",
    "TIPLE COVARIATES 467 --- Page 478 --- where Exxis as given by (16.67) and exyis as given by (16.68). Likewise, by (16.18), we have SSE y/C1x¼eyy/C0e0 xyE/C01 xxexy, (16:77) where eyyis as given in (16.69) or (16.76). The degrees of freedom of SSE y/C1xare k(n/C01)/C0q. By (16.11) and (13.12), we obtain ^a¼^a0/C0(Z0Z)/C0Z0X^b ¼0 /C22y1: /C22y2: ... /C22yk:0 BBBBBBB@1 CCCCCCCA/C00 ^ b0/C22x1: ^b0/C22x2: ... ^b0/C22xk:0 BBBBBBB@1 CCCCCCCA¼0 /C22y 1:/C0^b0/C22x1: /C22y2:/C0^b0/C22x2: ...",
    "22x1: ^b0/C22x2: ... ^b0/C22xk:0 BBBBBBB@1 CCCCCCCA¼0 /C22y 1:/C0^b0/C22x1: /C22y2:/C0^b0/C22x2: ... /C22yk:/C0^b0/C22xk:0 BBBBBBB@1 CCCCCCCA(16:78) ¼/C22y 1:/C0(^b1/C22x1:1þ^b2/C22x1:2þ/C1/C1/C1þ ^bq/C22x1:q) /C22y2:/C0(^b1/C22x2:1þ^b2/C22x2:2þ/C1/C1/C1þ ^bq/C22x2:q) ...",
    "1þ^b2/C22x1:2þ/C1/C1/C1þ ^bq/C22x1:q) /C22y2:/C0(^b1/C22x2:1þ^b2/C22x2:2þ/C1/C1/C1þ ^bq/C22x2:q) ... /C22yk:/C0(^b1/C22xk:1þ^b2/C22xk:2þ/C1/C1/C1þ ^bq/C22xk:q)0 BBBB@1 CCCCA: (16:79) 16.5.3 Testing Hypotheses 16.5.3.1 Treatments To test H01:a1¼a2¼/C1/C1/C1¼ak adjusted for the qcovariates, we use the full–reduced-model approach as in Section 16.3.3.1.",
    "1/C1¼ak adjusted for the qcovariates, we use the full–reduced-model approach as in Section 16.3.3.1. The full model is given by (16.65), and the reduced model (with a1¼a2¼/C1/C1/C1¼ak¼a)i s yij¼mþaþb0xijþ1ij ¼m/C3þb0xijþ1ij, (16:80) which is essentially the same as the multiple regression model (7.3).",
    "jþ1ij ¼m/C3þb0xijþ1ij, (16:80) which is essentially the same as the multiple regression model (7.3). By (7.37) and (7.39) and by analogy with (16.33), SSE rd¼SST y/C1x¼tyy/C0t0 xyT/C01 xxtxy, (16:81)468 ANALYSIS-OF-COVARIANCE --- Page 479 --- where tyyis tyy¼X ij(yij/C0/C22y::)2, the elements of txyare X ij(xijr/C0/C22x::r)(yij/C0/C22y::),r¼1,2,... ,q, and the elements of Txxare X ij(xijr/C0/C22x::r)(xijs/C0/C22x::s),r¼1,2,... ,q,s¼1,2,...",
    "1,2,... ,q, and the elements of Txxare X ij(xijr/C0/C22x::r)(xijs/C0/C22x::s),r¼1,2,... ,q,s¼1,2,... ,q: Thus, by analogy with (16.30), we use (16.81) and (16.77) to obtain SS(ajm,b)¼SST y/C1x/C0SSE y/C1x ¼tyy/C0t0 xyT/C01 xxtxy/C0eyyþe0xyE/C01 xxexy ¼X ij(yij/C0/C22y::)2/C0X ij(yij/C0/C22yi:)2/C0t0xyT/C01 xxtxyþe0xyE/C01 xxexy ¼nX i(/C22yi:/C0/C22y::)2/C0t0xyT/C01 xxtxyþe0xyE/C01 xxexy, (16:82) which has k21 degrees of freedom (see Problem 16.13).",
    ":)2/C0t0xyT/C01 xxtxyþe0xyE/C01 xxexy, (16:82) which has k21 degrees of freedom (see Problem 16.13). We display these sums of squares and products in Table 16.6. The test statistic for H01:a1¼a2¼/C1/C1/C1¼akis F¼SS(ajm,b)=(k/C01) SSE y/C1x=[k(n/C01)/C0q], (16:83) which (under H01) is distributed as F[k/C01,k(n/C01)/C0q].",
    "(k/C01) SSE y/C1x=[k(n/C01)/C0q], (16:83) which (under H01) is distributed as F[k/C01,k(n/C01)/C0q]. TABLE 16.6 Analysis-of-Covariance Table for Testing H01:a1¼a2¼/C1/C1/C1¼akin the One-Way Model with qCovariates Source SS Adjusted for the Covariate Adjusted df Treatments SS( ajm,b)¼SST y/C1x/C0SSE y/C1x k21 Error SSE y/C1x¼eyy/C0e0 xyE/C01 xxexy k(n21)2q Total SST y/C1x¼tyy/C0t0 xyT/C01 xxtxy kn/C0q/C0116.5 ONE-WAY MODEL WITH MULTIPLE COVARIATES 469 --- Page 480 --- 16.5.3.2 Slope Vector To test H02:b¼0, the sum of squares is given by (16.22) as SSH ¼e0 xyE/C01 xxexy, where Exxis as given by (16.67) and exyis the same as in (16.68).",
    "(16.22) as SSH ¼e0 xyE/C01 xxexy, where Exxis as given by (16.67) and exyis the same as in (16.68). The Fstatistic is then F¼e0 xyE/C01 xxexy=q SSE y/C1x=[k(n/C01)/C0q], (16:84) which is distributed as F[q,k(n/C01)/C0q]i fH02:b¼0is true. 16.5.3.3 Homogeneity of Slope Vectors The tests of H01:a1¼a2¼/C1/C1/C1¼akandH02:b¼0assume a common coefﬁcient vectorbfor all kgroups.",
    "tors The tests of H01:a1¼a2¼/C1/C1/C1¼akandH02:b¼0assume a common coefﬁcient vectorbfor all kgroups. To check this assumption, we can extend the model (16.65) to obtain a full model allowing for different slope vectors: yij¼mþaiþb0 ixijþ1ij,i¼1,2,... ,k,j¼1,2,... ,n: (16:85) The reduced model with a single slope vector is given by (16.65). We now develop a test for the hypothesis H03:b1¼b2¼/C1/C1/C1¼bk, that is, that the kregression planes (for the ktreatments) are parallel.",
    "sis H03:b1¼b2¼/C1/C1/C1¼bk, that is, that the kregression planes (for the ktreatments) are parallel. By extension of (16.46) and (16.47), we have SSE( F)y/C1x¼eyy/C0Xk i¼1e0 xy,iE/C01 xx,iexy,i, (16:86) SSE( R)y/C1x¼eyy/C0e0xyE/C01 xxexy, (16:87) where Exx,i¼X0 i[I/C0(1=n)J]Xiand exy,i¼X0i[I/C0(1=n)J]yi are terms in the summations in (16.67) and (16.68).",
    ",i¼X0 i[I/C0(1=n)J]Xiand exy,i¼X0i[I/C0(1=n)J]yi are terms in the summations in (16.67) and (16.68). The degrees of freedom for SSE( F)y/C1xand SSE( R)y/C1xarek(n/C01)/C0kq¼k(n/C0q/C01) and k(n/C01)/C0q,470 ANALYSIS-OF-COVARIANCE --- Page 481 --- respectively. Note that SSE( R)y/C1xin (16.87) is the same as SSE y/C1xin (16.77).",
    "--- Page 481 --- respectively. Note that SSE( R)y/C1xin (16.87) is the same as SSE y/C1xin (16.77). The estimator of bifor the ith group is ^bi¼E/C01 xx,iexy,i: (16:88) By analogy to (16.48), the sum of squares for testing H03:b1¼b2¼/C1/C1/C1¼bkis SSE( R)y/C1x/C0SSE( F)y/C1x¼Pk i¼1exy,iE/C01 xx,iexy,i/C0e0 xyE/C01 xxexy, which has k(n/C01)/C0 q/C0[k(n/C01)/C0kq]¼q(k/C01) degrees of freedom.",
    "xx,iexy,i/C0e0 xyE/C01 xxexy, which has k(n/C01)/C0 q/C0[k(n/C01)/C0kq]¼q(k/C01) degrees of freedom. The test statistic for H03:b1¼ b2¼/C1/C1/C1¼bkis F¼[SSE( R)y/C1x/C0SSE( F)y/C1x]=q(k/C01) SSE( F)y/C1x=k(n/C0q/C01), (16:89) which is distributed as F[q(k/C01),k(n/C0q/C01)] if H03is true. Note that if nis not large, n2q21 may be small, and the test will have low power. Example 16.5.",
    "rue. Note that if nis not large, n2q21 may be small, and the test will have low power. Example 16.5. In Table 16.7, we have instructor rating yand two course ratings x1 andx2for ﬁve instructors in each of three courses (Morrison 1983, p. 470). We ﬁrst ﬁnd ^band SSE y/C1x.",
    "x2for ﬁve instructors in each of three courses (Morrison 1983, p. 470). We ﬁrst ﬁnd ^band SSE y/C1x. Using (16.67), (16.68), and (16.69), we obtain Exx¼1:0619 0 :6791 0:6791 1 :2363/C18/C19 ,exy¼1:0229 1:9394/C18/C19 ,exy¼3:6036 : Then by (16.15), we obtain ^b¼E/C01 xxexy¼/C00:0617 1:6026/C18/C19 : By (16.77) and (16.81), we have SSE y/C1x¼:5585 ,SST y/C1x¼:7840 : TABLE 16.7 Instructor Rating yand Two Course Ratings x1 and x2in Three Courses Course 12 3 yx 1 x2 yx 1 x2 yx 1 x2 2.14 2.71 2.50 2.77 2.29 2.45 1.11 1.74 1.82 1.34 2.00 1.95 1.23 1.83 1.64 2.41 2.19 2.542.50 2.66 2.69 1.37 1.78 1.83 1.74 1.40 2.23 1.40 2.80 2.00 1.52 2.18 2.24 1.15 1.80 1.82 1.90 2.38 2.30 1.81 2.14 2.11 1.66 2.17 2.3516.5 ONE-WAY MODEL WITH MULTIPLE COVARIATES 471 --- Page 482 --- Then by (16.82), we see that SS(ajm,b)¼SST y/C1x/C0SSE y/C1x¼:2254 : TheFstatistic for testing H0:a1¼a2¼a3is given by (16.83) as F¼SS(ajm,b)=(k/C01) SSE y/C1x=[k(n/C01)/C0q]¼:2254 =2 :5585 =10¼2:0182 ,p¼:184 : To test H02:b¼0, we use (16.84) to obtain F¼e0 xyE/C01 xxexy=q SSE y/C1x=[k(n/C01)/C0q]¼27:2591 ,p¼8:95/C210/C05: Before testing homogeneity of slope vectors, H0:b1¼b2¼b3, we ﬁrst obtain estimates of b1,b2, andb3using (16.88): ^b1¼E/C01 xx,1exy,1¼:4236 :1900 :1900 :4039/C18/C19 /C01:2786 :6254/C18/C19 ¼/C00:0467 1:5703/C18/C19 , ^b2¼:2037 :2758 :2758 :4161/C18/C19 /C01:4370 :6649/C18/C19 ¼/C00:1781 1:7159/C18/C19 , ^b3¼:4346 :2133 :2133 :4163/C18/C19 /C01:3073 :6492/C18/C19 ¼/C00:0779 1:5993/C18/C19 : Then by (16.86) and (16.87), we obtain SSE( F)y/C1x¼eyy/C0X3 i¼1e0 xy,iE/C01 xx,iexy,i¼:55725 , SSE( R)y/C1x¼eyy/C0e0xyE/C01 xxexy¼:55855 : TheFstatistic for testing H0:b1¼b2¼b3is then given by (16.89) as F¼[SSE( R)y/C1x/C0SSE( F)y/C1x]=q(k/C01) SSE( F)y/C1x=k(n/C0q/C01) ¼:0012993 =4 :55725 =6¼:003498 : A472 ANALYSIS-OF-COVARIANCE --- Page 483 --- 16.6 ANALYSIS OF COVARIANCE WITH UNBALANCED MODELS The results in previous sections are for balanced ANOVA models to which covariates have been added.",
    "The results in previous sections are for balanced ANOVA models to which covariates have been added. The case in which the ANOVA model is itself unbalanced before the addition of a covariate was treated by Hendrix et al. (1982), who also discussed heterogeneity of slopes. The following approach, based on the cell means model of Chapter 15, was suggested by Bryce (1998).",
    "The following approach, based on the cell means model of Chapter 15, was suggested by Bryce (1998). For an analysis-of-covariance model with a single covariate and a common slope b, we extend the cell means model (15.3) or (15.18) as y¼(W,x)m b/C18/C19 þ1¼Wmþbxþ1: (16:90) This model allows for imbalance in the nij’s as well as the inherent imbalance in analysis of covariance models [see Bingham and Feinberg (1982) and a comment following (16.34)].",
    "in analysis of covariance models [see Bingham and Feinberg (1982) and a comment following (16.34)]. The vector mcontains the means for a one-way model as in (15.2), a two-way model as in (15.17), or some other model. Hypotheses about main effects, interactions, the covariate, or other effects can be tested by using contrasts onm b/C18/C19 as in Section 15.3. The hypothesis H02:b¼0 can be expressed in the form H02:(0,... ,0,1)m b/C18/C19 ¼0.",
    "in Section 15.3. The hypothesis H02:b¼0 can be expressed in the form H02:(0,... ,0,1)m b/C18/C19 ¼0. To test H02, we use a statistic analogous to (15.29) or (15.32). To test homogeneity of slopes, H03:b1¼b2¼/C1/C1/C1¼bkfor a one-way model (or H03:b1¼b2¼/C1/C1/C1¼bafor the slopes of the alevels of factor Ain a two-way model, and so on), we expand the model (16.90) to include the bi’s y¼(W,Wx)m b/C18/C19 þ1¼WmþWxbþ1, (16:91) whereb¼(b1,b2,...",
    "and the model (16.90) to include the bi’s y¼(W,Wx)m b/C18/C19 þ1¼WmþWxbþ1, (16:91) whereb¼(b1,b2,... ,bk)0andWxhas a single value of xijin each row and all other elements are 0s. (The xijinWxis in the same position as the corresponding 1 in W.) Then H03:b1¼b2¼/C1/C1/C1¼bkcan be expressed as H03:(O,C)m b/C18/C19 ¼Cb¼0, where Cis a ( k/C01)/C2kmatrix of rank k/C01 such that Cj¼0. We can test H03:Cb¼0using a statistic analogous to (15.33).",
    "/C2kmatrix of rank k/C01 such that Cj¼0. We can test H03:Cb¼0using a statistic analogous to (15.33). Constraints on the m’s and the b’s can be introduced by inserting nonsingular matrices AandAxinto (16.91): y¼WA/C01AmþWxA/C01 xAxbþ1: (16:92) The matrix Ahas the form illustrated in (15.37) for constraints on the m’s. The matrix Axprovides constraints on the b’s.",
    "orm illustrated in (15.37) for constraints on the m’s. The matrix Axprovides constraints on the b’s. For example, if Ax¼j0 C/C18/C19 ,16.6 ANALYSIS OF COVARIANCE WITH UNBALANCED MODELS 473 --- Page 484 --- where Cis a ( k/C01)/C2kmatrix of rank k/C01 such that Cj¼0as above, then the model (16.92) has a common slope. In some cases, the matrices AandAxwould be the same.",
    "ove, then the model (16.92) has a common slope. In some cases, the matrices AandAxwould be the same. PROBLEMS 16.1 Show that if the columns of Xare linearly independent of those of Z, then X0(I/C0P)Xis nonsingular, as noted preceding (16.14). 16.2 (a) Show that SSE y/C1x¼eyy/C0e0 xyE/C01 xxexyas in (16.18). (b) Show that eyy¼y0(I/C0P)yas in (16.19). 16.3 Show that for H0:b¼0,w eh a v eS S H ¼^b0X0(I/C0P)X^bas in (16.21). 16.4 Show that ^a¼(0,/C22y1:/C0^b/C22x1:,...",
    "r H0:b¼0,w eh a v eS S H ¼^b0X0(I/C0P)X^bas in (16.21). 16.4 Show that ^a¼(0,/C22y1:/C0^b/C22x1:,... ,/C22yk:/C0^b/C22xk:)0as in (16.24). 16.5 Show that exx¼P ij(xij/C0/C22xi:)2,exy¼P ij(xij/C0/C22xi:)(yij/C0/C22yi:),and eyy¼P ij(yij/C0/C22yi:)2, as in (16.25). 16.6 (a) Show that Exxhas the form shown in (16.41). (b) Show that exyhas the form shown in (16.43).",
    "16.6 (a) Show that Exxhas the form shown in (16.41). (b) Show that exyhas the form shown in (16.43). 16.7 Show that the sums of products in (16.52) and (16.53) can be written asP ijk(xijk/C0/C22xij:)(yijk/C0/C22yij:)¼P ijkxijkyijk/C0nP ij/C22xij:/C22yij:andP ijk(xijk/C0/C22x...) (yijk/C0/C22y...)¼P ijkxijkyijk/C0acn/C22x.../C22y...: 16.8 Show that the “treatment sum of products”P ijxij:yij:=n/C0x...y...=acncan be partitioned into the three sums of products in (16.57).",
    "products”P ijxij:yij:=n/C0x...y...=acncan be partitioned into the three sums of products in (16.57). 16.9 (a) Express the sums of squares and test statistic for factor Cin a form ana- logous to those for factor Ain (16.58), (16.60), and (16.61). (b) Express the sums of squares and test statistic for the interaction ACin a form analogous to those for factor Ain (16.58), (16.60), and (16.61). 16.10 (a) Show that Exx¼Pk i¼1X0 i[I/C0(1=n)J]Xias in (16.67).",
    "or Ain (16.58), (16.60), and (16.61). 16.10 (a) Show that Exx¼Pk i¼1X0 i[I/C0(1=n)J]Xias in (16.67). (b) Show that exy¼Pk i¼1X0 i[I/C0(1=n)J]yias in (16.68). (c) Show that eyy¼Pk i¼1y0 i[I/C0(1=n)J]yias in (16.69). 16.11 Show that the elements of X0 icXicare given by (16.72) and (16.73). 16.12 Show that ^ahas the form given in (16.78). 16.13 Show thatP ij(yij/C0/C22y::)2/C0P ij(yij/C0/C22yi:)2¼nP i(/C22yi:/C0/C22y::)2as in (16.82).",
    "). 16.13 Show thatP ij(yij/C0/C22y::)2/C0P ij(yij/C0/C22yi:)2¼nP i(/C22yi:/C0/C22y::)2as in (16.82). 16.14 In Table 16.8 we have the weight gain yand initial weight xof pigs under four diets (treatments). (a) Estimate b. (b) Test H0:a1¼a2¼a3¼a4using Fin (16.31).474 ANALYSIS-OF-COVARIANCE --- Page 485 --- (c) Test H0:b¼0 using Fin (16.36). (d) Estimate b1,b2,b3, andb4and test homogeneity of slopes H0:b1¼ b2¼b3¼b4using Fin (16.49).",
    "16.36). (d) Estimate b1,b2,b3, andb4and test homogeneity of slopes H0:b1¼ b2¼b3¼b4using Fin (16.49). 16.15 In a study to investigate the effect of income and geographic area of residence on daily calories consumed, three people were chosen at random in each of the 18 income–zone combinations. Their daily caloric intake yand age xare recorded in Table 16.9. (a) Obtain the sums of squares and products listed in Table 16.3, where zone is factor Aand income group is factor C.",
    "ms of squares and products listed in Table 16.3, where zone is factor Aand income group is factor C. (b) Calculate SS( AþE)y/C1x,SSE y/C1x, and SSA y/C1xusing (16.58), (16.59), and (16.60). For factor Acalculate Fby (16.61) for H0:a1¼a2¼a3. Similarly, obtain the Fstatistic for factor Cand the interaction. (c) Using SPE, SSE x, and SSE y/C1xfrom parts (a) and (b), calculate the Fstat- istic to test H0:b¼0.",
    "c) Using SPE, SSE x, and SSE y/C1xfrom parts (a) and (b), calculate the Fstat- istic to test H0:b¼0. (d) Calculate the separate slopes for the three levels of factor A, ﬁnd SS( F) and SS( R), and test for homogeneity of slopes. Repeat for factor C. 16.16 In a study to investigate differences in ability to distinguish aurally between environmental sounds, 10 male subjects and 10 female subjects were assigned randomly to each of two levels of treatment (experimental and control).",
    "emale subjects were assigned randomly to each of two levels of treatment (experimental and control). The variables were x= pretest score and y= posttest score on audi- tory discrimination. The data are given in Table 16.10. We use the posttest score yas the dependent variable and the pretest score x as the covariate. This gives the same result as using the gain score (post–pre) as the dependent variable and the pretest as the covariate (Hendrix et al. 1978).",
    "n score (post–pre) as the dependent variable and the pretest as the covariate (Hendrix et al. 1978). (a) Obtain the sums of squares and products listed in Table 16.3, where treat- ment is factor Aand gender is factor C.TABLE 16.8 Gain in Weight yand Initial Weight x of Pigs Treatment 1234 y xyxyxyx 165 30 180 24 156 34 201 41 170 27 169 31 189 32 173 32130 20 171 20 138 35 200 30156 21 161 26 190 35 193 35167 33 180 20 160 30 142 28151 29 170 25 172 29 189 36 Source : Ostle and Malone (1988, p.",
    "35 193 35167 33 180 20 160 30 142 28151 29 170 25 172 29 189 36 Source : Ostle and Malone (1988, p. 445).PROBLEMS 475 --- Page 486 --- TABLE 16.9 Caloric Intake yand Age xfor People Classiﬁed by Geographic Zone and Income Group Income GroupZone 1 Zone 2 Zone 3 yxyxyx 1 1911 46 1318 80 1127 74 1560 66 1541 67 1509 71 2639 38 1350 73 1756 60 2 1034 50 1559 58 1054 83 2096 33 1260 74 2238 471356 44 1772 44 1599 71 3 2130 35 2027 32 1479 56 1878 45 1414 51 1837 40 1152 59 1526 34 1437 66 4 1297 68 1938 33 2136 31 2093 43 1551 40 1765 562035 59 1450 39 1056 70 5 2189 33 1183 54 1156 47 2078 36 1967 36 2660 431905 38 1452 53 1474 50 6 1156 57 2599 35 1015 63 1809 52 2355 64 2555 341997 44 1932 79 1436 54 Source : Ostle and Mensing (1975, p.",
    "2599 35 1015 63 1809 52 2355 64 2555 341997 44 1932 79 1436 54 Source : Ostle and Mensing (1975, p. 482). TABLE 16.10 Pretest Score xand Posttest Score yon Auditory Discrimination Male Female Exp.aControl Exp. Control x yxyxyxy 58 71 35 49 64 71 68 70 57 69 31 69 39 71 52 6463 71 54 69 69 71 53 67 66 70 65 65 56 76 43 63 45 65 54 63 67 71 54 63 51 69 37 55 39 65 35 5362 69 64 66 32 66 62 6558 66 69 69 62 70 67 6952 61 70 69 64 68 51 6859 63 39 57 66 68 42 61 aExperimental.",
    "4 66 32 66 62 6558 66 69 69 62 70 67 6952 61 70 69 64 68 51 6859 63 39 57 66 68 42 61 aExperimental. Source : Hendrix (1967, pp. 154–157).476 ANALYSIS-OF-COVARIANCE --- Page 487 --- (b) Calculate SS( AþE)y/C1x,SSE y/C1x, and SSA y/C1xusing (16.58), (16.59), and (16.60). For factor Acalculate Fby (16.61) for H0:a1¼a2. Similarly, obtain the Fstatistic for factor Cand the interaction. (c) Using SPE, SSE x, and SSE y/C1xfrom parts (a) and (b), calculate the Fstat- istic to test H0:b¼0.",
    "c) Using SPE, SSE x, and SSE y/C1xfrom parts (a) and (b), calculate the Fstat- istic to test H0:b¼0. (d) Calculate the separate slopes for the two levels of factor A, ﬁnd SS( F) and SS( R), and test for homogeneity of slopes. Repeat for factor C. 16.17 In an experiment comparing four diets (treatments), the weight gain y (pounds per day) of pigs was recorded along with two covariates, initial agex1(days) and initial weight x2(pounds). The data are presented in Table 16.11.",
    "ovariates, initial agex1(days) and initial weight x2(pounds). The data are presented in Table 16.11. (a) Using (16.67), (16.68), and (16.69), ﬁnd Exx,exy, and eyy. Find ^b. (b) Using (16.77), (16.81), and (16.82), ﬁnd SSE y/C1x,SST y/C1x, and SS(ajm,b). Then test H0:a1¼a2¼a3¼a4, adjusted for the covariates, using the Fstatistic in (16.83). (c) Test H0:b¼0using (16.84). (d) Find ^b1,^b2,^b3, and ^b4using (16.88). Find SSE( F)y/C1xand SSE( R)y/C1x using (16.86) and (16.87).",
    "Find ^b1,^b2,^b3, and ^b4using (16.88). Find SSE( F)y/C1xand SSE( R)y/C1x using (16.86) and (16.87). Test H0:b1¼b2¼b3¼b4using (16.89).TABLE 16.11 Initial Age x1, Initial Weight x2, and Rate of Gain yof 40 Pigs Treatment 1 Treatment 2 Treatment 3 Treatment 4 x1 x2 yx 1 x2 yx 1 x2 yx 1 x2 y 78 61 1.40 78 74 1.61 78 80 1.67 77 62 1.40 90 59 1.79 99 75 1.31 83 61 1.41 71 55 1.47 94 76 1.72 80 64 1.12 79 62 1.73 78 62 1.3771 50 1.47 75 48 1.35 70 47 1.23 70 43 1.1599 61 1.26 94 62 1.29 85 59 1.49 95 57 1.2280 54 1.28 91 42 1.24 83 42 1.22 96 51 1.48 83 57 1.34 75 52 1.29 71 47 1.39 71 41 1.31 75 45 1.55 63 43 1.43 66 52 1.39 63 40 1.2762 41 1.57 62 50 1.29 67 40 1.56 62 45 1.2267 40 1.26 67 40 1.26 67 40 1.36 67 39 1.36 Source : Snedecor and Cochran (1967, p.",
    "0 1.56 62 45 1.2267 40 1.26 67 40 1.26 67 40 1.36 67 39 1.36 Source : Snedecor and Cochran (1967, p. 440).PROBLEMS 477 --- Page 488 --- 17 Linear Mixed Models 17.1 INTRODUCTION In Section 7.8 we brieﬂy considered linear models in which the yvariables are correlated or have nonconstant variances (or both). We used the model y¼Xbþ1,E(1)¼0,cov(1)¼S¼s2V, (17:1) where Vis aknown positive deﬁnite matrix, and developed estimators for bin (7.63) ands2in (7.65).",
    "1) where Vis aknown positive deﬁnite matrix, and developed estimators for bin (7.63) ands2in (7.65). Hypothesis tests and conﬁdence intervals were not given, but they could have been developed by adding the assumption of normality and modifying the approaches of Chapter 8 (see Problems 17.1 and 17.2). Correlated data are commonly encountered in practice (Brown and Prescott 2006, pp. 1–3; Fitzmaurice et al. 2004, p. xvi; Mclean et al. 1991).",
    "in practice (Brown and Prescott 2006, pp. 1–3; Fitzmaurice et al. 2004, p. xvi; Mclean et al. 1991). We can use the methodsof Section 7.8 as a starting point in approaching such data, but those methods are actually of limited practical use because we rarely, if ever, know V. On the other hand, the structure ofVis often known and in many cases can be speciﬁed up to rela- tively few unknown parameters.",
    "cture ofVis often known and in many cases can be speciﬁed up to rela- tively few unknown parameters. This chapter is an introduction to linear models for correlated yvariables where the structure of S¼ s2Vcan be speciﬁed. 17.2 THE LINEAR MIXED MODEL Nonindependence of observations may result from serial correlation or clustering of the observations (Diggle et al. 2002).",
    "ervations may result from serial correlation or clustering of the observations (Diggle et al. 2002). Serial correlation, which will not be discussed further in this chapter, is present when a time- (or space-) varying stochastic process is operating on the units and the units are repeatedly measured over time (or space).",
    "hastic process is operating on the units and the units are repeatedly measured over time (or space). Cluster correlation is present when the observations are grouped in various ways.The groupings might be due, for example, to repeated random sampling of subgroups or repeated measuring of the same units. Examples are given in Section 17.3. In many cases the covariance structure of cluster-correlated data can be speciﬁed using an Linear Models in Statistics ,Second Edition , by Alvin C.",
    "r-correlated data can be speciﬁed using an Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 479 --- Page 489 --- extension of the standard linear model (7.4) resembling the partitioned linear model (7.78). If yis an n/C21 vector of responses, the model is y¼XbþZ1a1þZ2a2þ/C1/C1/C1þ Zmamþ1, (17:2) where E(1)¼0and cov(1)¼s2Inas usual.",
    "esponses, the model is y¼XbþZ1a1þZ2a2þ/C1/C1/C1þ Zmamþ1, (17:2) where E(1)¼0and cov(1)¼s2Inas usual. Here Xis an n/C2pknown, possibly non- full-rank matrix of ﬁxed predictors as in Chapters 7, 8, 11, 12, and 16. It could be used to specify a multiple regression model, analysis-of-variance model, or analysisof covariance model. It could be as simple as vector of 1s. As usual, bis an n/C21 vector of unknown ﬁxed parameters.",
    "el. It could be as simple as vector of 1s. As usual, bis an n/C21 vector of unknown ﬁxed parameters. TheZi’s are known n/C2rifull-rank matrices of ﬁxed predictors, usually used to specify membership in the various clusters or subgroups. The major innovation in this model is that the ai’s are ri/C21 vectors of unknown random quantities similar to1. We assume that E(ai)¼0and cov( ai)¼s2 iIrifori¼1,...,m.",
    "rs of unknown random quantities similar to1. We assume that E(ai)¼0and cov( ai)¼s2 iIrifori¼1,...,m. For simplicity we further assume that cov( ai,aj)¼Ofori=j, where Oisri/C2rj, and that cov(ai,1)¼Ofor all i, where Oisri/C2n. These assumptions are often reasonable (McCulloch and Searle 2001, pp. 159–160). Note that this model is very different from the random- xmodel of Chapter 10. In Chapter 10 the predictors in Xwere random while the parameters in bwere ﬁxed.",
    "odel of Chapter 10. In Chapter 10 the predictors in Xwere random while the parameters in bwere ﬁxed. Here the opposite scenario applies; predictors in each Ziare ﬁxed while the elements of aiare random. On the other hand, this model has much in common with the Bayesian linear model of Chapter 11.",
    "ndom. On the other hand, this model has much in common with the Bayesian linear model of Chapter 11. In fact, if the normality assumption is added, the model can be stated in a form reminiscent of the Bayesian linear model as yja1,a2,...,amisNn(XbþZ1a1þZ2a2þ/C1/C1/C1þ Zmam,s2In), aiisNni(0,si2Iri) for i¼1,...,m: The label linear mixed model seems appropriate to describe (17.2) because the model involves a mixture of linear functions of ﬁxed parameters in band linear func- tions of random quantities in the ai’s.",
    "of linear functions of ﬁxed parameters in band linear func- tions of random quantities in the ai’s. The special case in which X¼j(so that there is only one ﬁxed parameter) is sometimes referred to as a random model . Thesi2’s (including s2) are referred to as variance components . We now investigate E(y) and cov( y)¼Sunder the model in (17.2). Theorem 17.2.",
    "variance components . We now investigate E(y) and cov( y)¼Sunder the model in (17.2). Theorem 17.2. Consider the model y¼XbþPm i¼1Ziaiþ1,where Xis a known n/C2pmatrix, the Zi’s are known n/C2rifull-rank matrices, bis ap/C21 vector of unknown parameters, 1is an n/C21 unknown random vector such that E(1)¼0 and cov(1)¼s2In, and the ai0sareri/C21 unknown random vectors such that E(ai)¼0and cov( ai)¼si2Iri.",
    "and cov(1)¼s2In, and the ai0sareri/C21 unknown random vectors such that E(ai)¼0and cov( ai)¼si2Iri. Furthermore, cov( ai,aj)¼Ofori=j, where Ois ri/C2rj, and cov( ai,1)¼Ofor all i, where Oisri/C2n. Then E(y)¼Xband cov(y)¼S¼Pmi¼1s2 iZiZ0 iþs2In:480 LINEAR MIXED MODELS --- Page 490 --- PROOF E(y)¼EXbþXm i¼1Ziaiþ1 ! ¼XbþEXm i¼1Ziaiþ1 ! ¼XbþXm i¼1ZiE(ai)þE(1) [by (3 :21) and (3 :38)] ¼Xb: cov(y)¼covXbþXm i¼1Ziaiþ1 ! ¼covXm i¼1Ziaiþ1 !",
    "¼XbþXm i¼1ZiE(ai)þE(1) [by (3 :21) and (3 :38)] ¼Xb: cov(y)¼covXbþXm i¼1Ziaiþ1 ! ¼covXm i¼1Ziaiþ1 ! ¼Xm i¼1cov(Ziai)þcov(1)þX i=jcov(Ziai,Zjaj) þXm i¼1cov(Ziai,1)þXm i¼1cov(1,Ziai) [see Problem 3 :19] ¼Xm i¼1Zicov(ai)Z0 iþcov(1)þX i=jZicov(ai,aj)Z0j þXm i¼1Zicov(ai,1)þXm i¼1cov(1,ai)Z0i[by Theorem 3.6d and Theorem 3.6e] ¼Xm i¼1si2ZiZ0iþs2In: A Note that the z’s only enter into the covariance structure while the x’s only determine the mean of y.",
    "te that the z’s only enter into the covariance structure while the x’s only determine the mean of y. 17.3 EXAMPLES We illustrate the broad applicability of the model in (17.2) with several simple examples. Example 17.3a (Randomized Blocks). An experiment involving three treatments was carried out by randomly assigning the treatments to experimental units within each of four blocks of size 3.",
    "out by randomly assigning the treatments to experimental units within each of four blocks of size 3. We could use the model yij¼mþtiþajþ1ij,17.3 EXAMPLES 481 --- Page 491 --- where i¼1,...,3,j¼1,...,4,ajisN(0,s2 1),1ijisN(0,s2), and cov( aj,1ij)¼0.",
    "MPLES 481 --- Page 491 --- where i¼1,...,3,j¼1,...,4,ajisN(0,s2 1),1ijisN(0,s2), and cov( aj,1ij)¼0. If we assume that the observations are sorted by blocks and treatments within blocks, we can express this model in the form of (17.2) with m¼1,X¼j3I3 j3I3 j3I3 j3I30 BB@1 CCA,and Z1¼j3030303 03j30303 0303j303 030303j30 BB@1 CCA: Then s¼s12Z1Z0 1þs2I12¼S1OOO OS1OO OO S1O OOO S10 BBB@1 CCCA, whereS 1¼s2 1þs2s2 1 s2 1 s2 1s2 1þs2s2 1 s2 1 s2 1s2 1þs20 B@1 CA: A Example 17.3b (Subsampling).",
    ", whereS 1¼s2 1þs2s2 1 s2 1 s2 1s2 1þs2s2 1 s2 1 s2 1s2 1þs20 B@1 CA: A Example 17.3b (Subsampling). Five batches were produced using each of two pro- cesses. Two samples were obtained and measured from each of the batches. Constraining the process effects to sum to zero, the model is yijk¼mþtiþaijþ1ijk, where i¼1,2;j¼1,...,5;k¼1,2;t2¼/C0t1;aijisN(0,s2 1);1ijkisN(0,s2); and cov(aij,1ijk)¼0.",
    "tiþaijþ1ijk, where i¼1,2;j¼1,...,5;k¼1,2;t2¼/C0t1;aijisN(0,s2 1);1ijkisN(0,s2); and cov(aij,1ijk)¼0. If the observations are sorted by processes, batches within pro- cesses, and samples within batches, we can put this model in the form of (17.2) with m¼1,X¼j10j10 j10/C0j10/C18/C19 and Z1¼j202/C1/C1/C102 02j2/C1/C1/C102 ......... 0202/C1/C1/C1j20 BBB@1 CCCA: Hence S¼ s2 1Z1Z0 1þs2I20¼S1O/C1/C1/C1 O OS1/C1/C1/C1 O ......... OO /C1/C1/C1S10 BBBB@1 CCCCA, whereS 1¼s2 1þs2s2 1 s2 1s2 1þs2 !",
    "C1/C1/C1 O OS1/C1/C1/C1 O ......... OO /C1/C1/C1S10 BBBB@1 CCCCA, whereS 1¼s2 1þs2s2 1 s2 1s2 1þs2 ! : A482 LINEAR MIXED MODELS --- Page 492 --- Example 17.3c (Split-Plot Studies). A3/C22 factorial experiment (with factors A andB, respectively) was carried out using six main units, each of which was subdi- vided into two subunits. The levels of Awere each randomly assigned to two of the main units, and the levels of Bwere randomly assigned to subunits within main units.",
    "d to two of the main units, and the levels of Bwere randomly assigned to subunits within main units. An appropriate model is yijk¼mþtiþdjþuijþaikþ1ijk, where i¼1,...,3;j¼1,2;k¼1,2;aikisN(0,s12);1ijkisN(0,s2) and cov(aik,1ijk)¼0.",
    "k¼mþtiþdjþuijþaikþ1ijk, where i¼1,...,3;j¼1,2;k¼1,2;aikisN(0,s12);1ijkisN(0,s2) and cov(aik,1ijk)¼0. If the observations are sorted by levels of A, main units within levels of A, and levels of Bwithin main units, we can express this model in the form of (17.2) with m¼1,X¼110010100000 110001010000110010100000 110001010000 101010001000 101001000100 101010001000101001000100 100110000010 100101000001 100110000010 1001010000010 BBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCA,and Z 1¼100000 100000 010000 010000 001000 001000 000100 000100 000010 000010000001 0000010 BBBBBBBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCCCCCCA:17.3 EXAMPLES 483 --- Page 493 --- Then S¼s12Z1Z0 1þs2I12¼S1O/C1/C1/C1 O OS1/C1/C1/C1 O .........",
    "CA:17.3 EXAMPLES 483 --- Page 493 --- Then S¼s12Z1Z0 1þs2I12¼S1O/C1/C1/C1 O OS1/C1/C1/C1 O ......... OO /C1/C1/C1S10 BBBBB@1 CCCCCA,where S 1¼s12þs2s2 1 s2 1s12þs2 ! : A Example 17.3d (One-Way Random Effects). A chemical plant produced a large number of batches. Each batch was packaged into a large number of containers. We chose three batches at random, and randomly selected four containers from each batch from which to measure y.",
    "ee batches at random, and randomly selected four containers from each batch from which to measure y. The model is yij¼mþaiþ1ij, where i¼1,...,3;j¼1,...,4;ajisN(0,s12);1ijisN(0,s2); and cov( aj,1ij)¼0.",
    "The model is yij¼mþaiþ1ij, where i¼1,...,3;j¼1,...,4;ajisN(0,s12);1ijisN(0,s2); and cov( aj,1ij)¼0. If the observations are sorted by batches and containers within batches, we can express this model in the form of (17.2) with m¼1,X¼j12,andZ1¼j40404 04j404 0404j40 @1A: Thus S¼ s12Z1Z0 1þs2I12¼S1OO OS1O OO S10 B@1 CA,where S1¼s12þs2s2 1 s2 1 s2 1 s2 1s12þs2s2 1 s2 1 s2 1 s2 1s12þs2s2 1 s2 1 s2 1 s2 1s12þs20 BBB@1 CCCA: A Example 17.3e (Independent Random Coefﬁcients).",
    "1 s2 1s12þs2s2 1 s2 1 s2 1 s2 1s12þs20 BBB@1 CCCA: A Example 17.3e (Independent Random Coefﬁcients). Three pups from each of four litters of mice were used in an experiment. One pup from each litter was exposed to one of three quantitative levels of a carcinogen. The relationshipbetween weight gain ( y) and carcinogen level is a straight line, but slopes and484 LINEAR MIXED MODELS --- Page 494 --- intercepts vary randomly and independently among litters.",
    "nd484 LINEAR MIXED MODELS --- Page 494 --- intercepts vary randomly and independently among litters. The three levels of the carcinogen are denoted by x. The model is yij¼b0þaiþb1xjþbixjþ1ij, where i¼1,...,4;j¼1,...,3;aiisN(0,s12);biisN(0,s22);1ijisN(0,s2), and all the random effects are independent.",
    ",...,4;j¼1,...,3;aiisN(0,s12);biisN(0,s22);1ijisN(0,s2), and all the random effects are independent. If the data are sorted by litter and carcinogen levels within litter, we can express this model in the form of (17.2) with m¼2,X¼j3x j3x j3x j3x0 BB@1 CCA,Z1¼j3030303 03j30303 0303j303 030303j30 BB@1 CCA,and Z2¼x0 30303 03x0 303 0303x0 3 030303x0 BB@1 CCA: Then S¼s12Z1Z0 1þs22Z2Z02þs2I12¼S1OOO OS1OO OO S1O OOO S10 BBB@1 CCCA, whereS 1¼s12J3þs22xx0þs2I3: A Example 17.3f (Heterogeneous Variances).",
    "OO OO S1O OOO S10 BBB@1 CCCA, whereS 1¼s12J3þs22xx0þs2I3: A Example 17.3f (Heterogeneous Variances). Four individuals were randomly sampled from each of four groups. The groups had different means and different variances. We assume here that s2¼0. The model is yij¼miþ1ij, where i¼1,...,4;j¼1,...,4;1ijisN(0,si2).",
    "riances. We assume here that s2¼0. The model is yij¼miþ1ij, where i¼1,...,4;j¼1,...,4;1ijisN(0,si2). If the data are sorted by groups and individuals within groups, we can express this model in the form of (17.2) with m¼4,X¼I4 I4 I4 I40 BBB@1 CCCA,Z 1¼I4 O4 O4 O40 BBB@1 CCCA,Z 2¼O4 I4 O4 O40 BBB@1 CCCA,Z 3¼O4 O4 I4 O40 BBB@1 CCCA, and Z 4¼O4 O4 O4 I40 BBB@1 CCCA:17.3 EXAMPLES 485 --- Page 495 --- Hence S¼s12Z1Z0 1þs22Z2Z02þs32Z3Z03þs42Z4Z04¼s12I4O4O4O4 O4s22I4O4O4 O4O4s32I4O4 O4O4O4s42I40 BB@1 CCA: A These models can be generalized and combined to yield a rich set of models appli- cable to a broad spectrum of situations (see Problem 17.3).",
    "ned to yield a rich set of models appli- cable to a broad spectrum of situations (see Problem 17.3). All the examples involved balanced data for convenience of description, but model (17.2) applies equally well to unbalanced situations. Allowing the covariance matrices of the ai’s and1to be non- diagonal (providing for such things as serial correlation) increases the scope of appli- cation of these models even more, with only moderate increases in complexity (see Problem 17.4).",
    "li- cation of these models even more, with only moderate increases in complexity (see Problem 17.4). 17.4 ESTIMATION OF VARIANCE COMPONENTS After specifying the appropriate model, the next task in using the linear mixed model (17.2) in the analysis of data is to estimate the variance components.",
    "using the linear mixed model (17.2) in the analysis of data is to estimate the variance components. Once the var- iance components have been estimated, Scan be estimated and the estimate used in the approximate generalized least-squares estimation of band other inferences as suggested by the results of Section 7.8. Several methods for estimation of the variance components have been proposed (Searle et al. 1992, pp. 168–257).",
    "hods for estimation of the variance components have been proposed (Searle et al. 1992, pp. 168–257). We discuss one of these approaches, that of restricted (or residual) maximum likelihood (REML) (Patterson and Thompson 1971). One reason for our emphasis of REML is that in standard linear models, the usual estimate s2in (7.22) is the REML estimate. Also, REML is general; for example, it can be applied regardless of balance. In certain balanced situations the REML estimator has closed form.",
    "be applied regardless of balance. In certain balanced situations the REML estimator has closed form. It is often the best (minimum variance) quadratic unbiased estimator (see Theorem 7.3g). To develop the REML estimator, we add the normality assumption. Thus the model is yisNn(Xb,S),whereS¼Xm i¼1si2ZiZ0 iþs2In, (17:3) where Xisn/C2pof rank r/C20p, andSis a positive deﬁnite n/C2nmatrix.",
    "eS¼Xm i¼1si2ZiZ0 iþs2In, (17:3) where Xisn/C2pof rank r/C20p, andSis a positive deﬁnite n/C2nmatrix. To simplify the notation, we let s2 0¼s2andZ0¼Inso that (17.3) becomes yisNn(Xb,S),whereS¼Xm i¼0si2ZiZ0i: (17:4)486 LINEAR MIXED MODELS --- Page 496 --- The idea of REML is to carry out maximum likelihood estimation for data Ky rather than y, where Kis chosen so that the distribution of Kyinvolves only the var- iance components, not b. In order for this to occur, we seek a matrix Ksuch that KX¼O.",
    "only the var- iance components, not b. In order for this to occur, we seek a matrix Ksuch that KX¼O. Hence E(Ky)¼KX¼0. For simplicity we require that Kbe of full- rank. We also want Kyto contain as much information as possible about the variance components, so Kmust have the maximal number of rows for such a matrix. Theorem 17.4a. LetXbe as in (17.3). A full-rank matrix Kwith maximal number of rows such that KX¼O,i sa n( n/C0r)/C2nmatrix.",
    "in (17.3). A full-rank matrix Kwith maximal number of rows such that KX¼O,i sa n( n/C0r)/C2nmatrix. Furthermore, Kmust be of the form K¼C(I/C0H)¼C[I/C0X(X0X)/C0X0] where Cspeciﬁes a full-rank transformation of the rows of I/C0H. PROOF. The rows k0 iofKmust satisfy the equations k0iX¼00or equivalently X0ki¼0. Using Theorem 2.8e, solutions to this system of equations are given by ki¼(I/C0X/C0X)cfor all possible p/C21 vectors c.",
    "solutions to this system of equations are given by ki¼(I/C0X/C0X)cfor all possible p/C21 vectors c. In other words, the solutions include all possible linear combinations of the columns of I/C0X/C0X. By Theorem 2.8c(i), rank( X/C0X)¼rank(X)¼r. Also, by Theorem 2.13e, I/C0X/C0Xis idempotent. Because of this idempotency, rank( I/C0X/C0X)¼ tr(I/C0X/C0X)¼tr(I)/C0tr(X/C0X)¼n/C0r.",
    "is idempotent. Because of this idempotency, rank( I/C0X/C0X)¼ tr(I/C0X/C0X)¼tr(I)/C0tr(X/C0X)¼n/C0r. Hence by the deﬁnition of rank (see Section 2.4), there are n/C0rlinearly independent vectors kithat satisfy X0ki¼0 and thus the maximal number of rows in Kisn/C0r. Since ki¼(I/C0X/C0X)c,K¼C(I/C0X/C0X) for some full-rank ( n/C0r)/C2nmatrix Cthat speciﬁes n/C0rlinearly independent linear combinations of the rows of the symmetric matrix I/C0X/C0X.",
    "peciﬁes n/C0rlinearly independent linear combinations of the rows of the symmetric matrix I/C0X/C0X. By Theorem 2.8c(iv)–(v), Kcan also be written as C(I/C0H)¼C[I/C0X(X0X)/C0X0]. A There are an inﬁnite number of such Ks, and it does not matter which is used. Also, note that ( I/C0H)ygives the ordinary residual vector ^1in (9.5), so that Ky¼C(I/C0H)yis a vector of linear combinations of these residuals. Thus the designation residual maximum likelihood is appropriate.",
    "ar combinations of these residuals. Thus the designation residual maximum likelihood is appropriate. The distribution of Kyfor any Kdeﬁned as in Theorem 17.4a is given in the following theorem. Theorem 17.4b. Consider the model in which yisNn(Xb,S), where S¼Pm i¼0s2 iZiZ0 i, and let Kbe speciﬁed as in Theorem 17.4a. Then Ky isNn/C0r(0,KSK0)o r Nn/C0r0,KXm i¼0s2 iZiZ0 i ! K0\"# : (17:5) PROOF. Since KX¼O, the theorem follows directly from Theorem 4.4a(ii).",
    "i¼0s2 iZiZ0 i ! K0\"# : (17:5) PROOF. Since KX¼O, the theorem follows directly from Theorem 4.4a(ii). A Thus the distribution of the transformed data Kyinvolves only the mþ1 variance components as unknown parameters. In order to estimate the variance components, the next step in REML is to maximize the likelihood of Kywith respect to these17.4 ESTIMATION OF VARIANCE COMPONENTS 487 --- Page 497 --- variance components.",
    "ith respect to these17.4 ESTIMATION OF VARIANCE COMPONENTS 487 --- Page 497 --- variance components. We now develop a set of estimating equations by taking partial derivatives of the log likelihood with respect to the variance components, and setting them to zero. Theorem 17.4c. Consider the model in which yisNn(Xb,S), where S¼Pm i¼0si2ZiZ0 i, and let Kbe speciﬁed as in Theorem 17.4a.",
    "r the model in which yisNn(Xb,S), where S¼Pm i¼0si2ZiZ0 i, and let Kbe speciﬁed as in Theorem 17.4a. Then a set of mþ1 estimating equations for s2 0,...,s2 mis given by tr[K0(KSK0)/C01KZ iZ0 i]¼y0K0(KSK0)/C01KZ iZ0iK0(KSK0)/C01Ky (17:6) fori¼0,...,m. PROOF. Since E(Ky)¼0, the log likelihood of Kyis lnL(s2 0,...,s2 m)¼n/C0r 2ln(2p)/C01 2lnjKSK0j/C012y 0K0(KSK0)/C01Ky ¼n/C0r 2ln(2p)/C012lnKX m i¼0si2ZiZ0 i ! K0/C12/C12/C12/C12/C12/C12/C12/C12/C12/C12 /C01 2y0K0KXm i¼0si2ZiZ0 i !",
    "p)/C012lnKX m i¼0si2ZiZ0 i ! K0/C12/C12/C12/C12/C12/C12/C12/C12/C12/C12 /C01 2y0K0KXm i¼0si2ZiZ0 i ! K0\"#/C01 Ky Using (2.117) and (2.118) to take the partial derivative of lnL(s2 0,...,s2 m) with respect to each of the si2’s, we obtain @ @si2lnL(s2 0,...,s2 m)¼/C01 2tr (KSK0)/C01 @ @si2(KSK0)/C20/C21/C18/C19 þ12y 0K0(KSK0)/C01 @ @si2(KSK0)/C20/C21 (KSK0)/C01Ky ¼/C012tr[(KSK 0)/C01KZ iZ0 iK0] þ1 2y0K0(KSK0)/C01KZ iZ0 iK0(KSK0)/C01Ky ¼/C01 2tr[K0(KSK0)/C01KZ iZ0 i] þ1 2y0K0(KSK0)/C01KZ iZ0 iK0(KSK0)/C01Ky Setting these equations to zero, the result follows.",
    "iZ0 i] þ1 2y0K0(KSK0)/C01KZ iZ0 iK0(KSK0)/C01Ky Setting these equations to zero, the result follows. A488 LINEAR MIXED MODELS --- Page 498 --- It is interesting to note that using Theorem 5.2a, the expected value of the quad- ratic form on the right side of (17.6) is given by the left side of (17.6). Applying Theorem 17.4c, we obtain mþ1 equations in mþ1 unknown si2’s. In some cases these equations can be simpliﬁed to yield closed-form estimating equations.",
    "own si2’s. In some cases these equations can be simpliﬁed to yield closed-form estimating equations. In most cases, numerical methods have to be used to solve the equations (McCulloch and Searle 2001, pp. 263–269). If the solutions to the equations are nonnegative, the solutions are REML estimates of the variance components.",
    "tions to the equations are nonnegative, the solutions are REML estimates of the variance components. If any of the solutions are negative, the log likelihood must be examined to ﬁnd values of the variance components within the parameter space (i.e., nonnegative values) that maximize the function. Example 17.4 (One-Way Random Effects). This is an extension of Example 17.3(d). Four containers are randomly selected from each of three batches produced by a chemical plant.",
    "3(d). Four containers are randomly selected from each of three batches produced by a chemical plant. Hence X¼j12,Z0¼I12,Z1¼j40404 04j404 0404j40 B@1 CA andS¼s2 0I12þs2 1Z1Z0 1: ThenI/C0H¼I12/C01 12J12, a suitable Cwould be C¼(I12,012), and K¼C(I/C0H).",
    "andS¼s2 0I12þs2 1Z1Z0 1: ThenI/C0H¼I12/C01 12J12, a suitable Cwould be C¼(I12,012), and K¼C(I/C0H). Inserting these matrices into (17.6), it can be shown that we obtain the two estimating equations 9s2 0¼y0(I12/C01 4Z1Z0 1)y, 2(4s2 1þs2 0)¼y0(1 4Z1Z0 1/C01 12J12)y: From these we obtain the closed-form solutions ^s2 0¼y0I12/C01 4Z1Z0 1/C0/C1 y 9, ^s2 1¼y01 4Z1Z0 1/C01 12J12/C0/C1 y=2/C0^s2 0 4: If both ^s2 0and ^s2 1are positive, they are the REML estimates of s2 0ands2 1.",
    "C0/C1 y=2/C0^s2 0 4: If both ^s2 0and ^s2 1are positive, they are the REML estimates of s2 0ands2 1. Because (I12/C01 4Z1Z0 1) is positive deﬁnite, ^s2 0will always be positive. However, ^s2 1could be negative. In such a case, the REML estimates become ^s2 0¼y0I12/C01 12J12/C0/C1 y 11, ^s2 1¼0: A17.4 ESTIMATION OF VARIANCE COMPONENTS 489 --- Page 499 --- In practice, the equations in (17.6) are seldom used directly to obtain solutions.",
    "--- Page 499 --- In practice, the equations in (17.6) are seldom used directly to obtain solutions. The usual procedure involves any of a number of iterative methods (Rao 1997 pp. 104–105, McCulloch and Searle 2001, pp.",
    "involves any of a number of iterative methods (Rao 1997 pp. 104–105, McCulloch and Searle 2001, pp. 265–269) To motivate one ofthese methods, note that the system of mþ1 equations generated by (17.6) can be written as M s¼q, (17:7) wheres¼(s2 0s2 1/C1/C1/C1s2 m)0,Mis a nonsingular ( mþ1)/C2(mþ1) matrix with ( ij)th element tr[ K0(KSK0)/C01KZ iZ0 iK0(KSK0)/C01KZ jZ0j], and qis an ( mþ1)/C21 vector with ith element y0K0(KSK0)/C01KZ iZ0iK0(KSK0)/C01Ky(Problem 17.6).",
    "j], and qis an ( mþ1)/C21 vector with ith element y0K0(KSK0)/C01KZ iZ0iK0(KSK0)/C01Ky(Problem 17.6). Equation (17.7) is more complicated than it looks because both Mandqare themselves func- tions ofs. Nonetheless, the equation is useful for stepwise improvement of an initial guesss(1). The method proceeds by computing M(t)andq(t)usings(t)at step t. Then lets(tþ1)¼M/C01 (t)q(t). The procedure continues until s(t)converges.",
    "ndq(t)usings(t)at step t. Then lets(tþ1)¼M/C01 (t)q(t). The procedure continues until s(t)converges. 17.5 INFERENCE FOR b 17.5.1 An Estimator for b Estimates of the variance components can be inserted into Sto obtain ^S¼Pm i¼0^s2 iZiZ0 i. A sensible estimator for bis then obtained by replacing s2V in equation (7.64) by its estimate,^S.",
    ". A sensible estimator for bis then obtained by replacing s2V in equation (7.64) by its estimate,^S. Generalizing the model to accommodate non- full-rank Xmatrices, we obtain ^b¼(X0^S/C01X)/C0X0^S/C01y: (17:8) This estimator, sometimes called the estimated generalized least-squares (EGLS) estimator, is a nonlinear function of y(since^Sis a nonlinear function of y). Even ifXis full-rank, ^bis not in general a (minimum variance) unbiased estimator (MVUE) or normally distributed.",
    "ll-rank, ^bis not in general a (minimum variance) unbiased estimator (MVUE) or normally distributed. However, it is always asymptotically MVUE and normally distributed (Fuller and Battese 1973).",
    "buted. However, it is always asymptotically MVUE and normally distributed (Fuller and Battese 1973). Similarly, a sensible approximate covariance matrix for ^bis, by extension of (12.18), as follows: cov( ^b)¼(X0^S/C01X)/C0X0^S/C01X(X0^S/C01X)/C0: (17:9) Of course, if Xis full-rank, the expression in (17.9) simpliﬁes to cov( ^b)¼(X0^S/C01X)/C01:490 LINEAR MIXED MODELS --- Page 500 --- 17.5.2 Large-Sample Inference for Estimable Functions of b Carrying the procedure of replacing s2Vby its estimate^Sa bit further, it seems reasonable to extend Theorem 12.7c(ii) and conclude that for a known full-rank g/C2pmatrix Lwhose rows deﬁne estimable functions of b L^bis approximately Ng[Lb,L(X0^S/C01X)/C0L0] (17 :10) and therefore by (5.35) (L^b/C0Lb)0[L(X0^S/C01X)/C0L0]/C01(L^b/C0Lb) is approximately x2(g): (17:11) If so, an approximate general linear hypothesis test for the testable hypothesis H0:Lb¼tis carried out using the test statistic G¼(L^b/C0t)0[L(X0^S/C01X)/C0L0]/C01(L^b/C0t): (17:12) IfH0is true, Gis approximately distributed as x2(g).",
    "/C0t)0[L(X0^S/C01X)/C0L0]/C01(L^b/C0t): (17:12) IfH0is true, Gis approximately distributed as x2(g). If H0is false, Gis approxi- mately distributed as x2(g,l) where l¼(Lb/C0t)0[L(X0S/C01X)/C0L0]/C01(Lb/C0t). The test is carried out by rejecting H0ifG/C21x2 a,g.",
    "ere l¼(Lb/C0t)0[L(X0S/C01X)/C0L0]/C01(Lb/C0t). The test is carried out by rejecting H0ifG/C21x2 a,g. Similarly, an approximate 100(1 /C0a)% conﬁdence interval for a single estimable function c0bis given by c0^b+za=2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ c0(X0^S/C01X)/C0cq : (17:13) Approximate joint conﬁdence regions for b, approximate conﬁdence intervals for individual bj’s, and approximate conﬁdence intervals for E(y) can be similarly pro- posed using (17.10) and (17.11).",
    "and approximate conﬁdence intervals for E(y) can be similarly pro- posed using (17.10) and (17.11). 17.5.3 Small-Sample Inference for Estimable Functions of b The inferences of Section 17.5.2 are not satisfactory for small samples. Exact small- sample inferences based on the tdistribution and Fdistribution are available in rare cases, but are not generally available for mixed models. However, much work has been done on approximate inference for small sample mixed models.",
    "xed models. However, much work has been done on approximate inference for small sample mixed models. First we discuss the exact small-sample inferences that are available in rare cases, usually involving balanced designs, nonnegative solutions to the REML equations, and certain estimable functions.",
    "ving balanced designs, nonnegative solutions to the REML equations, and certain estimable functions. In order for this to occur, [ L(X0^S/C01X)/C0L0]/C01must be of the form ( d=w)Q, where wis a central chi-square random variable with d degrees of freedom, and independently ( L^b/C0t)0Q(L^b/C0t) must be distributed as a (possibly noncentral) chi-square random variable with gdegrees of freedom.17.5 INFERENCE FOR b 491 --- Page 501 --- Under these conditions, by (5.30), the statistic (L^b/C0t)0Q(L^b/C0t) gw d¼(L^b/C0t)0[L(X0^S/C01X)/C0L0]/C01(L^b/C0t) g isF-distributed.",
    "he statistic (L^b/C0t)0Q(L^b/C0t) gw d¼(L^b/C0t)0[L(X0^S/C01X)/C0L0]/C01(L^b/C0t) g isF-distributed. We demonstrate this with an example. Example 17.5 (Balanced Split-Plot Study). Similarly to Example 17.3c, consider a 3/C22 balanced factorial experiment carried out using six main units, each of which is subdivided into two subunits. The levels of Aare each randomly assigned to two of the main units, and the levels of Bare randomly assigned to subunits within main units.",
    "ed to two of the main units, and the levels of Bare randomly assigned to subunits within main units. We assume that the data are sorted by replicates (with two complete replicates in the study), levels of A, and then levels of B. We use the cell means parameterization as in Section 14.3.1. The means in bare sorted by levels of Aand then levels of B. Hence X¼I6 I6/C18/C19 andS¼S1OOOOO OS1OOOO OO S1OOO OOO S1OO OOOO S1O OOOOO S10 BBBBBBBB@1 CCCCCCCCA;where S 1¼s2 1þs2s2 1 s2 1s2 1þs2 !",
    "S1OOOO OO S1OOO OOO S1OO OOOO S1O OOOOO S10 BBBBBBBB@1 CCCCCCCCA;where S 1¼s2 1þs2s2 1 s2 1s2 1þs2 ! : We test the no-interaction hypothesis H0:Lb¼0,where L¼1/C01/C011 00 1/C010 0 /C011/C18/C19 : Assuming that the REML estimating equations yield nonnegative solutions, ^s2is given by ^s2¼1 12y0ROO /C0ROO OROO /C0RO OOROO /C0R /C0ROOROO O/C0ROORO OO /C0ROOR0 BBBBBB@1 CCCCCCAywhere R¼1/C01 /C011/C18/C19 :492 LINEAR MIXED MODELS --- Page 502 --- Multiplying and simplifying, we obtain X0^S/C01X¼2^S/C01 1 OO O^S/C01 1 O OO^S/C01 10 B@1 CA: By (2.52), we have (X0^S/C01X)/C01¼1 2^S/C01 1 OO O^S/C01 1 O OO^S/C01 10 B@1 CA: Thus [L(X0^S/C01X)/C01L0]/C01 ¼1 21/C01/C011 00 1/C010 0 /C011/C18/C19 ^S1OO O^S1O OO^S10 B@1 CA11 /C01/C01 /C010 100/C01 010 BBBBBBBB@1 CCCCCCCCA2 6666666643 777777775/C01 ¼/C03 3^s2=^s21 3s22/C01 /C012/C18/C19/C20/C21 ¼3 wQ, where w¼3^s2 s2and Q¼1 3s22/C01 /C012/C18/C19 : (17:14) Also note that in this particular case, the EGLS estimator is equal to the ordinary least-squares estimator for bsince ^b¼(X0^S/C01X)/C01X0^S/C01y ¼1 2^S1OO O^S1O OO^S10 B@1 CA^S/C01 1 OO^S/C01 1 OO O^S/C01 1 OO^S/C01 1 O OO^S/C01 1 OO^S/C01 10 BBB@1 CCCAy ¼ 1 2I6I6 ðÞ y ¼(X0X)/C01X0y:17.5 INFERENCE FOR b 493 --- Page 503 --- Hence (L^b)0Q(L^b)¼y0X(X0X)/C01L0QL(X0X)/C01X0y: It can be shown that X(X0X)/C01L0QL(X0X)/C01X0Sis idempotent, and thus (L^b)0Q(L^b) is distributed as a chi-square with 2 degrees of freedom.",
    "C01X0Sis idempotent, and thus (L^b)0Q(L^b) is distributed as a chi-square with 2 degrees of freedom. It can similarly be shown that wis a chi-square with 3 degrees of freedom. Furthermore, wand (L^b)0Q(L^b) are independent chi-squares because of Theorem 5.6b. Thus we can testH0:Lb¼0using the test statistic ( L^b)0[L(X0^S/C01X)/C01L0]/C01(L^b)=2 because its distribution is exactly an Fdistribution.",
    "atistic ( L^b)0[L(X0^S/C01X)/C01L0]/C01(L^b)=2 because its distribution is exactly an Fdistribution. If even one observation of this design is missing, exact small-sample inferences are not available for Lb. Exact inferences are not available even when the design is balanced for estimable functions such as c0b where c0¼100 /C0100 ðÞ . A In most cases, approximate small-sample methods must be used.",
    "such as c0b where c0¼100 /C0100 ðÞ . A In most cases, approximate small-sample methods must be used. The exact distri- bution of t¼c0^bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ c0(X0^S/C01X)/C0cq (17:15) is unknown in general (McCulloch and Searle 2001, p. 167). However, a satisfactory small-sample test of H0:c0b¼0or conﬁdence interval for c0bis available by assuming that tapproximately follows a tdistribution with unknown degrees of freedom d(Giesbrecht and Burns 1985).",
    "tapproximately follows a tdistribution with unknown degrees of freedom d(Giesbrecht and Burns 1985). To calculate d, we follow the premise of Satterthwaite (1941) to assume, analogously to Theorem 8.4aiii, that d[c0(X0^S/C01X)/C0c] c0(X0S/C01X)/C0c(17:16) approximately follows the central chi-square distribution.",
    "0(X0^S/C01X)/C0c] c0(X0S/C01X)/C0c(17:16) approximately follows the central chi-square distribution. Equating the variance of the expression in (17.16) vard[c0(X0^S/C01X)/C0c] c0(X0S/C01X)/C0c\"# ¼d c0(X0S/C01X)/C0c/C20/C212 var[c0(X0^S/C01X)/C0c], to the variance of a central chi-square distribution, 2 d(Theorem 5.3a), we obtain the approximation d82[c0(X0^S/C01X)/C0c]2 var[c0(X0^S/C01X)/C0c]: (17:17)494 LINEAR MIXED MODELS --- Page 504 --- This approximation cannot be used, of course, unless var[ c0(X0^S/C01X)/C0c] is known or can be estimated.",
    "pproximation cannot be used, of course, unless var[ c0(X0^S/C01X)/C0c] is known or can be estimated. We obtain an estimate of var[ c0(X0^S/C01X)/C0c] using the multi- variate delta method (Lehmann 1999, p. 315). This method uses the ﬁrst-order multi- variate Taylor series (Harville 1997, p. 288) to approximate the variance of any scalar-valued function of a random vector, say, f(u).",
    "97, p. 288) to approximate the variance of any scalar-valued function of a random vector, say, f(u). By this method var[ f(u)] is approximated as var [ f(u)]8@f(u) @u/C12/C12/C12/C120 u¼^u^S^u@f(u) @u/C12/C12/C12/C12 u¼^u where @f(u) @u/C12/C12/C12/C12 u¼^u is the vector of partial derivatives of f(u) with respect to uevaluated at ˆuand^S^u denotes an estimate of the covariance matrix of ^u. In the case of inference for c0b in the mixed linear model (17.4), let u¼sandf(s)¼[c0(X0S/C01X)/C0c].",
    "n the case of inference for c0b in the mixed linear model (17.4), let u¼sandf(s)¼[c0(X0S/C01X)/C0c]. Then @f(s) @s/C12/C12/C12/C12 s¼^s¼/C0c0(X0^S/C01X)/C0X0^S/C01Z0Z0 0^S/C01X(X0^S/C01X)/C0c c0(X0^S/C01X)/C0X0^S/C01Z1Z01^S/C01X(X0^S/C01X)/C0c ... c0(X0^S/C01X)/C0X0^S/C01ZmZ0 m^S/C01X(X0^S/C01X)/C0c0 BBBB@1 CCCCA: Also^S^s, an estimate of the covariance matrix of ^s, can be obtained as the inverse of the negative Hessian [the matrix of second derivatives — see Harville (1997, p.",
    "ed as the inverse of the negative Hessian [the matrix of second derivatives — see Harville (1997, p. 288)] of the restricted log-likelihood function (Theorem 17.4c) evaluated at ^s(Pawitan 2001, pp. 226, 258).",
    "the restricted log-likelihood function (Theorem 17.4c) evaluated at ^s(Pawitan 2001, pp. 226, 258). We now generalize this idea obtain the approximate small-sample distribution of F¼(L^b/C0Lb)0[L(X0^S/C01X)/C0L0]/C01(L^b/C0Lb) g(17:18) in order to develop tests for H0:Lb¼tand joint conﬁdence regions for Lb.W e obtain these inferences by assuming that the distribution of Fis approximately an Fdistribution with numerator degrees of freedom g, and unknown denominator degrees of freedom n(Fai and Cornelius 1996).",
    "umerator degrees of freedom g, and unknown denominator degrees of freedom n(Fai and Cornelius 1996). The method involves the spectral decomposition (see Theorem 2.12b) of [ L(X0^S/C01X)/C0L0]/C01to yield P0[L(X0^S/C01X/C0L0]/C01P¼D,17.5 INFERENCE FOR b 495 --- Page 505 --- where D¼diag(l1,l2,...,lm) is the diagonal matrix of eigenvalues and P¼(p1,p2,...,pm) is the orthogonal matrix of normalized eigenvectors of [L(X0^S/C01X)/C0L0]/C01.",
    "and P¼(p1,p2,...,pm) is the orthogonal matrix of normalized eigenvectors of [L(X0^S/C01X)/C0L0]/C01. Using this decomposition, G¼gFcan be written as G¼Xg i¼1(p0 iL^b)2 li¼Xg i¼1t2 i (17:19) where the ti’s are approximate independent t-variables with respective degrees of freedomni. We compute the nivalues by repeatedly applying equation (17.16). Then we ﬁnd n such that F¼g/C01Gis distributed approximately as Fg,n.",
    "dly applying equation (17.16). Then we ﬁnd n such that F¼g/C01Gis distributed approximately as Fg,n. Since the square of a t-dis- tributed random variable with nidegrees of freedom is an F-distributed random vari- able with 1 and nidegrees of freedom: EðG)¼EXg i¼1t2 i ! ¼Xg i¼1ni ni/C02[by (5 :34)] : Now, since E(F)¼1=gE(G)¼n=(n/C02), n¼2E(G) E(G)/C0g¼2Xg i¼1ni ni/C02 !,Xg i¼1ni ni/C02 !",
    "(5 :34)] : Now, since E(F)¼1=gE(G)¼n=(n/C02), n¼2E(G) E(G)/C0g¼2Xg i¼1ni ni/C02 !,Xg i¼1ni ni/C02 ! /C0g\"# : (17:20) A method due to Kenward and Roger (1997) provides further improvements for small-sample inferences in mixed models. 1.",
    "nward and Roger (1997) provides further improvements for small-sample inferences in mixed models. 1. The method adjusts for two sources of bias in L(X0^S/C01X)/C0L0as an estimator of the covariance matrix of L^bin small-sample situations, namely, that L(X0S/C01X)/C0L0does not account for the variability in ^s, and that L(X0^S/C01X)/C0L0is a biased estimator of L(X0S/C01X)/C0L0.",
    "ount for the variability in ^s, and that L(X0^S/C01X)/C0L0is a biased estimator of L(X0S/C01X)/C0L0. Kackar and Harville (1984) give an approximation to the ﬁrst source of bias, and Kenward and Roger (1997) propose an adjustment for the second source of bias. Both adjust-ments are based on a Taylor series expansion around s(Kenward and Roger 1997, McCulloch and Searle 2001, pp. 164–167).",
    "a Taylor series expansion around s(Kenward and Roger 1997, McCulloch and Searle 2001, pp. 164–167). The adjusted approximate covariance matrix of L^bis ^S/C3 L^b¼L[X0S/C01X)/C0þ2(X0S/C01X)/C0Xm i¼0Xm j¼0sij(Qij/C0Pi^S^bPj)() /C2(X0S/C01X)/C0]L0(17:21)496 LINEAR MIXED MODELS --- Page 506 --- where sijis the ( i,j)th element of^S^s, Qij¼X0@^S/C01 @si2^S@^S @si2X,and Pi¼X0@^S/C01 @si2X: 2.",
    "-- where sijis the ( i,j)th element of^S^s, Qij¼X0@^S/C01 @si2^S@^S @si2X,and Pi¼X0@^S/C01 @si2X: 2. Kenward and Roger (1997) assume that F/C3¼dFKR¼d g(L^b)0^S/C3 L^b(L^b) (17 :22) is approximately F-distributed with two (rather than one) adjustable constants, a scale factor d, and the denominator degrees of freedom n. They use a second- order Taylor series expansion (Harville 1997, p.",
    "enominator degrees of freedom n. They use a second- order Taylor series expansion (Harville 1997, p. 289) of^S/C3 L^b/C01aroundsand conditional expectation relationships to yield E(FKR) and var( FKR) approxi- mately.",
    "b/C01aroundsand conditional expectation relationships to yield E(FKR) and var( FKR) approxi- mately. After equating these to the mean (5.29) and variance of the Fdistri- bution to solve for dandn, they obtain n¼4þgþ2 gg/C01 and d¼n E(FKR)(n/C02) where g¼var(FKR) 2E(FKR)2: These small-sample methods result in conﬁdence coefﬁcients and type I error rates closer to target values than do the large-sample methods.",
    "nﬁdence coefﬁcients and type I error rates closer to target values than do the large-sample methods. However, they involve many approximations, and it is therefore not surprising that simulation studies have shown that their statistical properties are not universally satisfactory (Schaalje et al. 2002, Gomez et al. 2005, Keselman et al. 1999). Another approach to small-sample inferences in mixed linear models is the Bayesian approach (Chapter 11).",
    "er approach to small-sample inferences in mixed linear models is the Bayesian approach (Chapter 11). Bayesian linear mixed models are not much harder to specify than Bayesian linear models, and Markov chain Monte Carlo methods can be used to draw samples from exact small-sample posterior distributions(Gilks et al. 1998, pp. 275–320).",
    "sed to draw samples from exact small-sample posterior distributions(Gilks et al. 1998, pp. 275–320). 17.6 INFERENCE FOR THE a i A new kind of estimation problem sometimes arises for the linear mixed model in (17.2) y¼XbþXm i¼1Ziaiþ1, (17:23)17.6 INFERENCE FOR THE a i 497 --- Page 507 --- namely, the problem of estimation of realized values of the random components (the ai’s) or linear functions of them.",
    "em of estimation of realized values of the random components (the ai’s) or linear functions of them. For simplicity, and without loss of generality, we rewrite (17.22) as y¼XbþZaþ1, (17:24) where Z¼(Z1Z2...Zm),a¼(a0 1a02...a0m)0,1isN(0,s2In),aisN(0,G) where G¼s2 1In1O /C1/C1/C1 O Os2 2In2/C1/C1/C1 O ............ OO /C1/C1/C1s2 mInm0 BBB@1 CCCA, and cov( 1,a)¼0. Then the problem can be expressed as that of estimating aor a linear function Ua.",
    ", and cov( 1,a)¼0. Then the problem can be expressed as that of estimating aor a linear function Ua. To differentiate this problem from inference for an estimable func- tion ofb, the current problem is often referred to as prediction of a random effect . Prediction of random effects dates back at least to the pioneering work of Henderson (1950) on prediction of the “value” of a genetic line of animals or plants, where the line is viewed as a random selection from a population of such lines.",
    "f animals or plants, where the line is viewed as a random selection from a population of such lines. In education the speciﬁc effects of randomly chosen schools might be of inter- est, in medical research the effect of a randomly chosen clinic may be desired, and in agriculture the effect of a speciﬁc year on crop yields may be of interest. The phenom- enon of regression to the mean (Stigler 2000) for repeated measurements is closely related to prediction of random effects.",
    "he mean (Stigler 2000) for repeated measurements is closely related to prediction of random effects. The general problem is that of predicting afor a given value of the observation vector y. Note that because of the model in (17.23), aandyare jointly multivariate normal, and cov(a,y)¼cov(a,XbþZaþ1) ¼cov(a,Zaþ1) ¼cov(a,Za)þcov(a,1) (see Problem 3.19) ¼GZ0þO ¼GZ0: By extension of Theorem 10.6 to the case of a random vector a, the predictor based onythat minimizes the mean squared error is E(ajy).",
    "e case of a random vector a, the predictor based onythat minimizes the mean squared error is E(ajy). To be more precise, the vector function t(y) that minimizes E[a/C0t(y)]0[a/C0t(y)] is given by t(y)¼E(ajy).",
    "ore precise, the vector function t(y) that minimizes E[a/C0t(y)]0[a/C0t(y)] is given by t(y)¼E(ajy). Since aandyare jointly multivariate normal, we have, by (4.26) E(ajy)¼E(a)þcov(a,y)[cov( y)]/C01[y/C0E(y)] ¼0þGZ0S/C01(y/C0Xb) ¼GZ0S/C01(y/C0Xb):(17:25)498 LINEAR MIXED MODELS --- Page 508 --- IfbandSwere known, this predictor would be a linear function of y.I ti s therefore sometimes called the best linear predictor (BLP) of a.",
    "ld be a linear function of y.I ti s therefore sometimes called the best linear predictor (BLP) of a. More generally, the BLP of Uais E(Uajy)¼UGZ0S/C01(y/C0Xb): (17:26) Because the BLP is a linear function of y, the covariance matrix of E(Uajy)i s cov[E(Uajy)]¼UGZ0S/C01ZGU0: (17:27) Replacing bby^bin (17.8), and replacing GandSby^Gand^S(based on the REML estimates of the variance components), we obtain ^E(Uajy)¼UˆGZ0^S/C01(y/C0X^b): (17:28) This predictor is neither unbiased nor a linear function of y.",
    "E(Uajy)¼UˆGZ0^S/C01(y/C0X^b): (17:28) This predictor is neither unbiased nor a linear function of y. Nonetheless, it is an approximately unbiased estimate of a linear predictor, so it is often referred to as the estimated best linear unbiased predictor (EBLUP).",
    "inear predictor, so it is often referred to as the estimated best linear unbiased predictor (EBLUP). Ignoring the randomness in ^G and^S, we obtain cov[ ^E(Uajy)]8cov[UGZ0S/C01(y/C0X^b)] ¼cov{UGZ0S/C01[I/C0X(X0S/C01X)/C0X0S/C01]y} ¼UGZ0S/C01[I/C0X(X0S/C01X)/C0X0S/C01]S[I/C0S/C01X(X0S/C01X)/C0X0] /C2S/C01ZGU0 ¼UGZ0[S/C01/C0^S/C01X(X0S/C01X)/C0X0S/C01]ZGU0 8UˆGZ0[S/C01/C0^S/C01X(X0^S/C01X)/C0X0^S/C01]Z^GU0: (17:29) Small-sample improvements to (17.28) have been suggested by Kackar and Harville (1984), and approximate degrees of freedom for inferences based on EBLUPs have been investigated by Jeske and Harville (1988).",
    "grees of freedom for inferences based on EBLUPs have been investigated by Jeske and Harville (1988). Example 17.6 (One-Way Random Effects). To illustrate EBLUP, we continue with the one-way random effects model of Examples 17.3d and 17.4 involving four con- tainers randomly selected from each of three batches produced by a chemical plant.",
    "volving four con- tainers randomly selected from each of three batches produced by a chemical plant. In terms of the linear mixed model in (17.23), we obtain X¼j12,b¼m,Z¼j40404 04j404 0404j40 B@1 CA,G¼s2 1I3,and S¼s2I12þs2 1ZZ0¼s2I4þs2 1J4 O4 O4 O4s2I4þs2 1J4 O4 O4 O4s2I4þs2 1J40 B@1 CA:17.6 INFERENCE FOR THE a i 499 --- Page 509 --- By (2.52) and (2.53), S/C01¼1 s2I4/C0s2 1 s2þ4s2 1J4 O4 O4 O4 I4/C0s2 1 s2þ4s2 1J4 O4 O4 O4 I4/C0s2 1 s2þ4s2 1J40 BBBBBBB@1 CCCCCCCA: To predict a, which in this case is the vector of random effects associated with the three batches, by (17.27) and using the REML estimates of the variance components, we obtain EBLUP( a)¼^GZ 0^S/C01(y/C0X^b)¼^s2 1I3j0 4004004 004j04004 004004j040 B@1 CA^S/C01(y/C0^mj12) ¼^s2 1 ^s2j0 4/C04^s2 1 ^s2þ4^s2 1j0 4 00 4 004 004j04/C04^s2 1 ^s2þ4^s2 1j0 4 00 4 004004j04/C04^s2 1 ^s2þ4^s2 1j0 40 BBBBBBB@1 CCCCCCCA(y/C0^ mj12) ¼^s2 1 ^s2þ4^s2 1j0 4004004 004j04004 004004j040 B@1 CA(y/C0^mj12) ¼^s2 1 ^s2þ4^s2 1y1:/C04^m y2:/C04^m y3:/C04^m0 B@1 CA¼4^s2 1 ^s2þ4^s2 1/C22y1:/C0/C22y:: /C22y2:/C0/C22y:: /C22y3:/C0/C22y::0 B@1 CA: Thus EBLUP( ai)¼4^s2 1 ^s2þ4^s2 1(/C22yi:/C0/C22y::): (17:30) If batch had been considered a ﬁxed factor, and the one-way ANOVA model in (13.1) had been used with the constraintP iai¼0, we showed in (13.9) that ^ai¼(/C22yi:/C0/C22y::): Thus EBLUP( ai)¼c^aiwhere 0 /C20c/C201.",
    "ntP iai¼0, we showed in (13.9) that ^ai¼(/C22yi:/C0/C22y::): Thus EBLUP( ai)¼c^aiwhere 0 /C20c/C201. For this reason, EBLUPs are sometimes referred to as shrinkage estimators .500 LINEAR MIXED MODELS --- Page 510 --- The approximate covariance matrix of the EBLUPs in (17.29) can be derived using (17.28), and conﬁdence intervals can then be computed or hypothesis tests carried out. A An extensive development and discussion of EBLUPs is given by Searle et al. (1992, pp. 258–289).",
    ". A An extensive development and discussion of EBLUPs is given by Searle et al. (1992, pp. 258–289). 17.7 RESIDUAL DIAGNOSTICS The assumptions of the linear mixed model in (17.2) and (17.3) are independence, normality, and constant variance of the elements of each of the aivectors, as well as independence, normality, and constant variance of the elements of 1.",
    "ch of the aivectors, as well as independence, normality, and constant variance of the elements of 1. These assumptions are harder to check than for the standard linear model, and the usefulness of various types of residual plots for mixed model diagnosis is presently not fullyunderstood (Brown and Prescott 1999, p. 77). As a ﬁrst step, we can examine each of the EBLUP ( a i) vectors as in (17.27) for normality, constant variance and independence (see Section 9.1).",
    "UP ( a i) vectors as in (17.27) for normality, constant variance and independence (see Section 9.1). This makes sensebecause, using (4.25) and assuming for simplicity that S(and therefore G)a r e known, we have cov(ajy)¼G/C0GZ 0S/C01ZG: Thus if U¼(O...OIniO...O), cov(Uajy)¼cov(aijy) ¼UGU0/C0UGZ0S/C01ZGU0 ¼si2Ini/C0si4Z0 iS/C01Zi ¼si2(Ini/C0si2Z0 iS/C01Zi): (17:31) As was the case for the hat matrix in Section 9.1, the off-diagonal elements of the second term in (17.31) are often small in absolute value.",
    "tion 9.1, the off-diagonal elements of the second term in (17.31) are often small in absolute value. Hence the elements of EBLUP( ai) should display normality, constant variance, and approximate indepen- dence if the model assumptions are met. It turns out, however, that constant variance and normality of the EBLUP( ai) vectors is a necessary rather than a sufﬁcient con- dition for the model assumptions to hold. Simulation studies (Verbeke and Molenberghs 2000, pp.",
    "con- dition for the model assumptions to hold. Simulation studies (Verbeke and Molenberghs 2000, pp. 83–87) have shown that EBLUPs tend to reﬂect the distribu- tional assumptions of the model rather than the actual distribution of random effects in some situations. The next step is to consider the assumptions of independence, normality, and con- stant variance for the elements of 1.",
    "consider the assumptions of independence, normality, and con- stant variance for the elements of 1. The simple residual vector y/C0X^bis seldom useful for this purpose because, assuming that Sis known, we have cov(y/C0X^b)¼cov{[I/C0X(X0S/C01X)/C0X0S/C01]y} ¼[I/C0X(X0S/C01X)/C0X0S/C01]S[I/C0S/C01X(X0S/C01X)/C0X0],17.7 RESIDUAL DIAGNOSTICS 501 --- Page 511 --- which may not exhibit constant variance or independence.",
    "7 RESIDUAL DIAGNOSTICS 501 --- Page 511 --- which may not exhibit constant variance or independence. However, the vector ^S/C01=2(y/C0X^b), where^S/C01=2is the inverse of the square root matrix of^S(2.109), does have the desired properties. Theorem 17.7. Consider the model in which yisNn(Xb,S), where S¼s2IþPm i¼1si2ZiZ0 i. Assume that Sis known, and let ^b¼(X0S/C01X)/C0X0S/C01y. Then cov[S/C01=2(y/C0X^b)]¼I/C0H/C3 (17:32) where H/C3¼S/C01=2X(X0S/C01X)/C0X0S/C01=2.",
    ")/C0X0S/C01y. Then cov[S/C01=2(y/C0X^b)]¼I/C0H/C3 (17:32) where H/C3¼S/C01=2X(X0S/C01X)/C0X0S/C01=2. PROOF cov[S/C01=2(y/C0X^b)]¼cov{S/C01=2[I/C0X(X0S/C01X)/C0X0S/C01]y} ¼S/C01=2[I/C0X(X0S/C01X)/C0X0S/C01] /C2S[I/C0S/C01X(X0S/C01X)/C0X0]S/C01=2 ¼S/C01=2SS/C01=2/C0S/C01=2X(X0S/C01X)/C0X0S/C01=2: Now, since S/C01=2¼(CD1=2C0)/C01where Cis orthogonal as in Theorem 2.12d, and D1/2is a diagonal matrix as in (2.109), we obtain S/C01=2SS/C01=2¼(CD1 2C0)/C01CDC0(CD12C 0)/C01 ¼CD/C01=2C0CDC0CD/C01=2C0 ¼CD/C01=2DD/C01=2C0 ¼CC0¼I and the result follows.",
    "0)/C01CDC0(CD12C 0)/C01 ¼CD/C01=2C0CDC0CD/C01=2C0 ¼CD/C01=2DD/C01=2C0 ¼CC0¼I and the result follows. A Thus the vector^S/C01=2(y/C0X^b) can be examined for constant variance, normality and approximate independence to verify the assumptions regarding 1. A more common approach (Verbeke and Molenberghs 2000, p. 132; Brown and Prescott 1999, p. 77) to verifying the assumptions regarding 1is to compute and examine y/C0X^b/C0Z^a. To see why this makes sense, assume that Sandbare known.",
    "g 1is to compute and examine y/C0X^b/C0Z^a. To see why this makes sense, assume that Sandbare known. Then cov(y/C0Xb/C0Za)¼cov(y)/C0cov(y,Za)/C0cov(Za,y)þcov(Za) ¼S/C0ZGZ0/C0ZGZ0þZGZ0 ¼S/C0ZGZ0 ¼(ZGZ0þs2I)/C0ZGZ0 ¼s2I:502 LINEAR MIXED MODELS --- Page 512 --- PROBLEMS 17.1 Consider the model y¼Xbþ1, where1isNn(0,s2V),Vis a known posi- tive deﬁnite n/C2nmatrix, and Xis a known n/C2(kþ1) matrix of rank kþ1.",
    "sNn(0,s2V),Vis a known posi- tive deﬁnite n/C2nmatrix, and Xis a known n/C2(kþ1) matrix of rank kþ1. Also assume that Cis a known q/C2(kþ1) matrix and tis a known q/C21 vector such that Cb¼tis consistent. Let ^b¼(X0V/C01X)/C01X0V/C01y. Find the distribution of F¼(C^b/C0t)0[C(X0V/C01X)/C01C0]/C01(C^b/C0t)=q y0[V/C01/C0V/C01(X0V/C01X)/C01X0V/C01]y=(n/C0k/C01) (a)Assuming that H0:Cb¼tis false. (b)Assuming that H0:Cb¼tis true.",
    "0V/C01X)/C01X0V/C01]y=(n/C0k/C01) (a)Assuming that H0:Cb¼tis false. (b)Assuming that H0:Cb¼tis true. (Hint: Consider the model for P21y, where Pis a nonsingular matrix such thatPP0¼V.) 17.2 For the model described in Problem 17.1, ﬁnd a 100(1 2a)% conﬁdence interval for a0b.",
    "atPP0¼V.) 17.2 For the model described in Problem 17.1, ﬁnd a 100(1 2a)% conﬁdence interval for a0b. 17.3 An exercise science experiment was conducted to investigate how ankle roll (y) is affected by the combination of four casting treatments (control, tape cast, air cast, and tape and brace) and two exercise levels (preexercise and postexercise). Each of the 16 subjects used in the experiment was assigned to each of the four casting treatments in random order.",
    "subjects used in the experiment was assigned to each of the four casting treatments in random order. Five ankle roll measurements were made preexercise and ﬁve measurements were made post exercise for each casting treatment. Thus a total of 40 observations were obtained for each subject. This study can be regarded as a randomized block split-plot study with subsampling.",
    "or each subject. This study can be regarded as a randomized block split-plot study with subsampling. A sensible model is yijkl¼mþtiþdjþuijþakþbikþcijkþ1ijkl, where i¼1,...,4;j¼1,2;k¼1,...,16;l¼1,...,5;akisN(0,s2 1);bijk isN(0,s2 2);cijkisN(0,s2 3);1ijklisN(0,s2), and all of the random effects are independent. If the data are sorted by subject, casting treatment, and exercise level, sketch out the XandZimatrices for the matrix form of this model as in (17.2).",
    "t, and exercise level, sketch out the XandZimatrices for the matrix form of this model as in (17.2). 17.4 (a) Consider the model y¼XbþPm i¼1Ziaiþ1where Xis a known n/C2p matrix, the Zi’s are known n/C2rifull-rank matrices, bis ap/C21 vector of unknown parameters, 1is an n/C21 unknown random vector such that E(1)¼0and cov(1)¼R=s2In, and the ai’s are ri/C21 unknown random vectors such that E(ai)¼0and cov( ai)¼Gi=si2Iri.A s usual, cov( ai,aj)¼Ofori=j, where Oisri/C2rj, and cov( ai,1)¼O for all i, where Oisri/C2n.",
    "i2Iri.A s usual, cov( ai,aj)¼Ofori=j, where Oisri/C2rj, and cov( ai,1)¼O for all i, where Oisri/C2n. Find cov( y).PROBLEMS 503 --- Page 513 --- (b)For the model in part (a), let Z¼(Z1Z2...Zm) anda¼(a0 1a02...a0m)0so that the model can be written as y¼XbþZaþ1and cov(a)¼G¼G1O ... OO OG 2... OO ............... OO ...Gm/C01O OO ... OG m0 BBBBB@1 CCCCCA: Express covðyÞin terms of Z,G, and R.",
    "OO ............... OO ...Gm/C01O OO ... OG m0 BBBBB@1 CCCCCA: Express covðyÞin terms of Z,G, and R. 17.5 Consider the model in which yisN n(Xb,S), whereS¼Pm i¼0s2 iZiZ0 i, and letKbe a full-rank matrix of appropriate dimensions as in Theorem 17.4c.",
    "hereS¼Pm i¼0s2 iZiZ0 i, and letKbe a full-rank matrix of appropriate dimensions as in Theorem 17.4c. Show that for any i, E[y0K0(KSK0)/C01KZ iZ0 iK0(KSK0)/C01Ky]¼tr[K0(KSK0)/C01KZ iZ0i]: 17.6 Show that that the system of mþ1 equations generated by (17.6) can be written as Ms¼q, wheres¼(s2 0s2 1...s2m)0,Mis an ( mþ1)/C2(mþ1) matrix with ijth element tr[ K0(KSK0)/C01KZ iZ0 iK0(KSK0)/C01KZ jZ0j], and q is an ( mþ1)/C21 vector with ith element y0K0(KSK0)/C01KZ iZ0iK0(KSK0)/C01Ky.",
    "K0)/C01KZ jZ0j], and q is an ( mþ1)/C21 vector with ith element y0K0(KSK0)/C01KZ iZ0iK0(KSK0)/C01Ky. 17.7 Consider the model in which yisNn(Xb,S), and let Lbe a known full-rank g/C2pmatrix whose rows deﬁne estimable functions of b. (a)Show that L(X0S/C01X)/C0L0is nonsingular. (b)Show that ( L^b/C0Lb)0[L(X0S/C01X)/C0L0]/C01(L^b/C0Lb)i sx2(g). 17.8 For the model described in Problem 17.7, develop a 100(1 2)% conﬁdence interval for E(y0)¼x0 0b. 17.9 Refer to Example 17.5.",
    "in Problem 17.7, develop a 100(1 2)% conﬁdence interval for E(y0)¼x0 0b. 17.9 Refer to Example 17.5. Show that (X0^S/C01X)/C01¼1 2^S1OO O^S1O OO^S10 B@1 CA: 17.10 Refer to Example 17.5. Show that the solution to the REML estimating equations is given by ^s2¼1 12y0ROO /C0ROO OROO /C0RO OOROO /C0R /C0ROOROO O/C0ROORO OO /C0ROOR0 BBBBBBBB@1 CCCCCCCCAy,where R¼1/C01 /C011/C18/C19 :504 LINEAR MIXED MODELS --- Page 514 --- 17.11 Refer to Example 17.5. Show that X(X0X)/C01L0QL(X0X)/C01X0Sis idempotent.",
    "ELS --- Page 514 --- 17.11 Refer to Example 17.5. Show that X(X0X)/C01L0QL(X0X)/C01X0Sis idempotent. 17.12 Refer to Example 17.5. Show that wand (L^b)0Q(L^b) are independent chi-square variables. 17.13 Refer to Example 17.5. Let c0¼100 /C0100 ðÞ ,l e t wbe as in (17.14), and let d¼3. Show that if nis such that v(w=d)¼ [c0(X0^S/C01X)/C0c]/C01thennisnotdistributed as a central chi-square random variable.",
    "h that v(w=d)¼ [c0(X0^S/C01X)/C0c]/C01thennisnotdistributed as a central chi-square random variable. 17.14 To motivate Satterthwaite’s approximation in expression (17.16), consider the model in which yisNn(Xb,S), where Xisn/C2pof rank k,S¼s2I and^S¼s2I.I f c0b is an estimable function, show that (n/C0k)[c0(X0^S/C01X)/C0c]=[c0(X0S/C01X)/C0c], is distributed as x2(n/C0k).",
    "able function, show that (n/C0k)[c0(X0^S/C01X)/C0c]=[c0(X0S/C01X)/C0c], is distributed as x2(n/C0k). 17.15 Given f(s)¼[c0(X0S/C01X)/C0c], where s¼(s2 0s2 1/C1/C1/C1s2 m)0and S¼Pm i¼0si2ZiZ0 i, show that @f(s) @s¼/C0c0(X0S/C01X)/C0X0S/C01Z0Z00S/C01X(X0S/C01X)/C0c c0(X0S/C01X)/C0X0S/C01Z1Z01S/C01X(X0S/C01X)/C0c ...",
    "0c0(X0S/C01X)/C0X0S/C01Z0Z00S/C01X(X0S/C01X)/C0c c0(X0S/C01X)/C0X0S/C01Z1Z01S/C01X(X0S/C01X)/C0c ... c0(X0S/C01X)/C0X0S/C01ZmZ0 mS/C01X(X0S/C01X)/C0c0 BBB@1 CCCA: 17.16 Consider the model in which yisNn(Xb,S), let Lbe a known full-rank g/C2pmatrix whose rows deﬁne estimable functions of b, and let^Sbe the REML estimate of S.",
    "full-rank g/C2pmatrix whose rows deﬁne estimable functions of b, and let^Sbe the REML estimate of S. As in (17.19), let D¼diag(l1,l2,...,lm) be the diagonal matrix of eigenvalues and P¼(p1,p2,...,pm) be the orthogonal matrix of normalized eigenvectors of [ L(X0^S/C01X)/C0L0]/C01. (a)Show that ( L^b/C0Lb)0[L(X0^S/C01X)/C0L0]/C01(L^b/C0Lb)¼ Pg i¼1p0 i(L^b/C0Lb)hi2 =li. (b)Show that ( p0iL^b)2=liis of the form c0^b=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ c0(X0^S/C01X)/C0cq as in (17.14).",
    "that ( p0iL^b)2=liis of the form c0^b=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ c0(X0^S/C01X)/C0cq as in (17.14). (c)Show that cov( p0 iL^b,p0i0L^b)¼0fori=i0. 17.17 Consider the model in which y¼XbþZaþ1, where1isN(0,s2In) and a isN(0,G) as in (17.24). (a)Show that the linear function B(y2X) that minimizes E[a/C0B(y/C0X)]0[a/C0B(y/C0X)] isGZ0S/C01(y/C0Xb).",
    "ow that the linear function B(y2X) that minimizes E[a/C0B(y/C0X)]0[a/C0B(y/C0X)] isGZ0S/C01(y/C0Xb). (b)Show that B¼GZ0S/C01(y/C0Xb) also “minimizes” E[a/C0B(y/C0X)][a/C0B(y/C0X)]0:By “minimize,” we mean that any other choice for Badds a positive deﬁnite matrix to the result.PROBLEMS 505 --- Page 515 --- 17.18 Show that [ I/C0X(X0S/C01X)/C0X0S/C01]S[I/C0X(X0S/C01X)/C0X0S/C01]0¼S/C0 X(X0S/C01X)/C0X0as in (17.29). 17.19 Consider the model described in Problem 17.17.",
    "C0X0S/C01]0¼S/C0 X(X0S/C01X)/C0X0as in (17.29). 17.19 Consider the model described in Problem 17.17. (a)Show that the best linear predictor of Ua is E(Uajy)¼UGZ0S/C01(y/C0Xb). (b)Show that cov[ E(Uajy)]¼UGZ0S/C01ZGU0. (c)Given ^b¼(X0S/C01X)/C0X0S/C01y, show that cov[UGZ0S/C01(y/C0X^b)]¼UGZ0[S/C01/C0S/C01X(X0S/C01X)/C0X0S/C01]ZGU0: 17.20 Consider the one-way random effects model of Example 17.6. Use (2.52) and (2.53) to derive the expression for S21.",
    "ne-way random effects model of Example 17.6. Use (2.52) and (2.53) to derive the expression for S21. 17.21 Using (17.29), derive the covariance matrix for EBLUP( a) where aiis deﬁned as in (17.30). 17.22 Consider the model described in Problem 17.17. Use (4.27) and assume that SandGare known to show that cov(ajy)¼G/C0GZ0S/C01ZG: 17.23 Use the model of Example 17.3b ( subsampling ). Find the covariance matrix of the predicted batch effects using (17.31).",
    "mple 17.3b ( subsampling ). Find the covariance matrix of the predicted batch effects using (17.31). Comment on the magnitudes of the off-diagonal elements of this matrix. 17.24 Use the model of Example 17.3b. Find the covariance matrix of the trans- formed residuals^S/C01=2(y/C0x^b) using (17.32).",
    "ple 17.3b. Find the covariance matrix of the trans- formed residuals^S/C01=2(y/C0x^b) using (17.32). Comment on the off-diagonal elements of this matrix.506 LINEAR MIXED MODELS --- Page 516 --- 18 Additional Models In this chapter we brieﬂy discuss some models that are not linear in the parameters or that have an error structure different from that assumed in previous chapters.",
    "in the parameters or that have an error structure different from that assumed in previous chapters. 18.1 NONLINEAR REGRESSION Anonlinear regression model can be expressed as yi¼f(xi,b)þ1i,i¼1,2,...,n, (18:1) where f(xi,b) is a nonlinear function of the parameter vector b. The error term 1iis sometimes assumed to be distributed as N(0,s2).",
    "ction of the parameter vector b. The error term 1iis sometimes assumed to be distributed as N(0,s2). An example of a nonlinear model is the exponential model yi¼b0þb1eb2xiþ1i: Estimators of the parameters in (18.1) can be obtained using the method of least squares. We seek the value of bˆthat minimizes Q(^b)¼Xn i¼1[yi/C0f(xi,^b)]2: (18:2) A simple analytical solution for bˆthat minimizes (18.2) is not available for nonlinear f(xi,bˆ). An iterative approach is therefore used to obtain a solution.",
    "not available for nonlinear f(xi,bˆ). An iterative approach is therefore used to obtain a solution. In general, the resulting estimators in bˆare not unbiased, do not have minimum variance, and are not normally distributed. However, according to large-sample theory, the estimators are almost unbiased, have near-minimum variance, and are approximately normally distributed. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G.",
    "ately normally distributed. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 507 --- Page 517 --- Inferential procedures, including conﬁdence intervals and hypothesis tests, are available for the least-squares estimator bˆobtained by minimizing (18.2). Diagnostic procedures are available for checking on the model and on the suitability of the large-sample inferential procedures.",
    "ailable for checking on the model and on the suitability of the large-sample inferential procedures. For details of the above procedures, see Gallant (1975), Bates and Watts (1988), Seber and Wild (1989), Ratkowsky (1983, 1990), Kutner et al. (2005, Chapter 13), Hocking (1996, Section 11.2), Fox (1997, Section 14.2), and Ryan (1997, Chapter 13).",
    "5, Chapter 13), Hocking (1996, Section 11.2), Fox (1997, Section 14.2), and Ryan (1997, Chapter 13). 18.2 LOGISTIC REGRESSION In some regression situations, the response variable yhas only two possible out- comes, for example, high blood pressure or low blood pressure, developing cancer of the esophagus or not developing it, whether a crime will be solved or not solved, and whether a bee specimen is a “killer” (africanized) bee or a domestic honey bee.",
    "d or not solved, and whether a bee specimen is a “killer” (africanized) bee or a domestic honey bee. In such cases, the outcome ycan be coded as 0 or 1 and we wish to predict the outcome (or the probability of the outcome) on the basis of one or more x’s. To illustrate a linear model in which yis binary, consider the model with one x: yi¼b0þb1xiþ1i;yi¼0,1;i¼1,2,...,n: (18:3) Since yiis 0 or 1, the mean E(yi) for each xibecomes the proportion of observations at xifor which yi¼1.",
    "e yiis 0 or 1, the mean E(yi) for each xibecomes the proportion of observations at xifor which yi¼1. This can be expressed as E(yi)¼P(yi¼1)¼pi, 1/C0E(yi)¼P(yi¼0)¼1/C0pi:(18:4) The distribution P(yi¼0)¼12piand P( yi¼1)¼piin (18.4) is known as the Bernoulli distribution .",
    "18:4) The distribution P(yi¼0)¼12piand P( yi¼1)¼piin (18.4) is known as the Bernoulli distribution . By (18.3) and (18.4), we have E(yi)¼pi¼b0þb1xi: (18:5) For the variance of yi, we obtain var(yi)¼E[yi/C0E(yi)]2 ¼pi(1/C0pi): (18:6) By (18.5) and (18.6), we obtain var(yi)¼(b0þb1xi)(1/C0b0/C0b1xi),508 ADDITIONAL MODELS --- Page 518 --- and the variance of each yidepends on the value of xi.",
    "0b1xi),508 ADDITIONAL MODELS --- Page 518 --- and the variance of each yidepends on the value of xi. Thus the fundamental assump- tion of constant variance is violated, and the usual least-squares estimators bˆ0andbˆ1 computed as in (6.5) and (6.6) will not be optimal (see Theorem 7.3d). To obtain optimal estimators of b0andb1, we could use generalized least-squares estimators ^b¼(X0V/C01X)/C01X0V/C01y as in Theorem 7.8a, but there is an additional challenge in ﬁtting the linear model (18.5).",
    "X0V/C01y as in Theorem 7.8a, but there is an additional challenge in ﬁtting the linear model (18.5). Since E(yi)¼piis a probability, it is limited by 0 /C20pi/C201. If we ﬁt (18.5) by generalized least squares to obtain ^pi¼^b0þ^b1xi, then pˆimay be less than 0 or greater than 1 for some values of xi. A model for E(yi) that is bounded between 0 and 1 and reaches 0 and 1 asymptotically (instead oflinearly) would be more suitable. A popular choice is the logistic regression model .",
    "lly (instead oflinearly) would be more suitable. A popular choice is the logistic regression model . p i¼E(yi)¼eb0þb1xi 1þeb0þb1xi¼1 1þe/C0b0/C0b1xi: (18:7) This model is illustrated in Figure 18.1. The model in (18.7) can be linearized by thesimple transformation lnpi 1/C0pi/C18/C19 ¼b0þb1xi, (18:8) sometimes called the logit transformation .",
    "mple transformation lnpi 1/C0pi/C18/C19 ¼b0þb1xi, (18:8) sometimes called the logit transformation . Figure 18.1 Logistic regression function.18.2 LOGISTIC REGRESSION 509 --- Page 519 --- The parameters b0andb1in (18.7) and (18.8) are typically estimated by the method of maximum likelihood (see Section 7.2).",
    "1in (18.7) and (18.8) are typically estimated by the method of maximum likelihood (see Section 7.2). For a random sample y1,y2, ...,ynfrom the Bernoulli distribution with P(yi¼0)¼12piandP(yi¼1)¼pi, the likelihood function becomes L(b0,b1)¼f(y1,y2,...,yn;b0,b1)¼Yn i¼1fi(yi;b0,b1) ¼Yn i¼1pyi i(1/C0pi)1/C0yi: (18:9) Taking the logarithm of both sides of (18.9) and using (18.8), we obtain lnL(b0,b1)¼Xn i¼1yi(b0þb1xi)/C0Xn i¼1ln(1þeb0þb1xi): (18:10) Differentiating (18.10) with respect to b0andb1and setting the results equal to zero gives Xn i¼1yi¼Xn i¼11 1þe/C0^b0/C0^b1xi(18:11) Xn i¼1xiyi¼Xn i¼1xi 1þe/C0^b0/C0^b1xi: (18:12) These equations can be solved iteratively for bˆ0andbˆ1.",
    "i¼1xiyi¼Xn i¼1xi 1þe/C0^b0/C0^b1xi: (18:12) These equations can be solved iteratively for bˆ0andbˆ1. The logistic regression model in (18.7) can be readily extended to include more than one x.",
    "0andbˆ1. The logistic regression model in (18.7) can be readily extended to include more than one x. Using the notation b¼(b0,b1,...,bk)0andxi¼(1,xi1,xi2,...,xik)0, the model in (18.7) becomes pi¼E(yi)¼ex0 ib 1þex0ib¼1 1þe/C0x0ib, and (18.8) takes the form lnpi 1/C0pi/C18/C19 ¼x0 ib, (18:13) where x0 ib¼b0þb1xi1þb2xi2þ/C1/C1/C1þbkxik:For binary yi(yi¼0, 1;i¼1, 2, ...,n), the mean and variance are given by (18.4) and (18.6).",
    "C1þbkxik:For binary yi(yi¼0, 1;i¼1, 2, ...,n), the mean and variance are given by (18.4) and (18.6). The likelihood function and510 ADDITIONAL MODELS --- Page 520 --- the value of bthat maximize it are found in a manner analogous to the approach used to ﬁnd b0andb1. Conﬁdence intervals, tests of signiﬁcance, measures of ﬁt, subset selection procedures, diagnostic techniques, and other procedures are available.",
    "sures of ﬁt, subset selection procedures, diagnostic techniques, and other procedures are available. Logistic regression has been extended from binary to a polytomous logistic regression model in which yhas several possible outcomes. These may be ordinal such as large, medium, and small, or categorical such as Republicans, Democrats, and Independents. The analysis differs for the ordinal and categorical cases. For details of these procedures, see Hosmer and Lemeshow (1989), Hosmer et al.",
    "nd categorical cases. For details of these procedures, see Hosmer and Lemeshow (1989), Hosmer et al. (1989), McCullagh and Nelder (1989), Myers (1990, Section 7.4), Kleinbaum (1994), Stapleton (1995, Section 8.8), Stokes et al. (1995, Chapters 8 and 9), Kutner et al. (2005), Chapter 14, Hocking (1996, Section 11.4), Ryan (1997,Chapter 9), Fox (1997, Chapter 15), Christensen (1997), and McCulloch and Searle (2001, Chapter 5).",
    ",Chapter 9), Fox (1997, Chapter 15), Christensen (1997), and McCulloch and Searle (2001, Chapter 5). 18.3 LOGLINEAR MODELS In the analysis of categorical data, we often use loglinear models. To illustrate a log- linear model for categorical data, consider a two-way contingency table with frequen- cies (counts) designated as y ijas in Table 18.1, with yi:¼Ps j¼1yijandy:j¼Pr i¼1yij: The corresponding cell probabilities pijare given in Table 18.2, with pi:¼Ps j¼1pij andpj¼Pri¼1pij.",
    "The corresponding cell probabilities pijare given in Table 18.2, with pi:¼Ps j¼1pij andpj¼Pri¼1pij. The hypothesis that AandBare independent can be expressed as H0:pij¼pi:p:j for all i,j. Under H0, the expected frequencies are E(yij)¼npi:p:j: This becomes linear if we take the logarithm of both sides: lnE(yij)¼lnnþlnpi:þlnp:j: TABLE 18.1 Contingency Table Showing Frequencies yij(Cell Counts) for anr3sClassiﬁcation of Two Categorical Variables Aand B Variable B1 B2 ... Bs Total A1 y11 y12 ...",
    "or anr3sClassiﬁcation of Two Categorical Variables Aand B Variable B1 B2 ... Bs Total A1 y11 y12 ... y1s y1. A2 y21 y22 ... y2s y2. ............... Ar yr1 yr2 ... yrs yr. Total y .1 y.2 ... y.s y..¼n18.3 LOGLINEAR MODELS 511 --- Page 521 --- To test H0:pij¼pi:p:j, we can use the likelihood ratio test. The likelihood function is given by the multinomial density L(p11,p12,...,prs)¼n!",
    "lihood ratio test. The likelihood function is given by the multinomial density L(p11,p12,...,prs)¼n! y11!y12!/C1/C1/C1yrs!py11 11py12 12/C1/C1/C1pyrs rs: The unrestricted maximum likelihood estimators of pij(subject toP ijpij¼1) are ^pij¼yij=n, and the estimators under H0are ^pij¼yi:y:j=n2(Christensen 1997, pp. 42–46).",
    "ijpij¼1) are ^pij¼yij=n, and the estimators under H0are ^pij¼yi:y:j=n2(Christensen 1997, pp. 42–46). The likelihood ratio is then given by LR¼Yr i¼1Ys j¼1yi:y:j nyij/C18/C19yij : The test statistic is /C02lnLR¼2X ijyijlnnyij yi:y:j/C18/C19 , which is approximately distributed as x2[(r/C01)(s/C01)]. For further details of loglinear models, see Ku and Kullback (1974), Bishop et al.",
    "[(r/C01)(s/C01)]. For further details of loglinear models, see Ku and Kullback (1974), Bishop et al. (1975), Plackett (1981), Read and Cressie (1988), Santner and Duffy (1989), Agresti (1984, 1990) Dobson (1990, Chapter 9), Anderson (1991), and Christensen (1997). 18.4 POISSON REGRESSION If the response yiin a regression model is a count, the Poisson regression model may be useful.",
    "SION If the response yiin a regression model is a count, the Poisson regression model may be useful. The Poisson probability distribution is given by f(y)¼mye/C0m y!,y¼0,1,2,...:TABLE 18.2 Cell Probabilities for an r3sContingency Table Variable B1 B2 ... Bs Total A1 p11 p12 ... p1s p1. A2 p21 p22 ... p2s p2. ............... Ar pr1 pr2 ... prs pr. Total p. 1 p.2 ...",
    "p11 p12 ... p1s p1. A2 p21 p22 ... p2s p2. ............... Ar pr1 pr2 ... prs pr. Total p. 1 p.2 ... p.s p..¼1512 ADDITIONAL MODELS --- Page 522 --- ThePoisson regression model is yi¼E(yi)þ1i,i¼1,2,...,n, where the yi’s are independently distributed as Poisson random variables and mi¼ E(yi) is a function of x0 ib¼b0þb1xi1þ/C1/C1/C1þbkxik. Some commonly used func- tions of x0ibare mi¼x0 ib,mi¼ex0 ib,mi¼ln(x0 ib): (18:14) In each of the three cases in (18.14), the values of mimust be positive.",
    "x0 ib,mi¼ln(x0 ib): (18:14) In each of the three cases in (18.14), the values of mimust be positive. To estimate b, we can use the method of maximum likelihood. Since yihas a Poisson distribution, the likelihood function is given by L(b)¼Yn i¼1f(yi)¼Yn i¼1myi ie/C0mi yi!, wheremiis typically one of the three forms in (18.14). Iterative methods can be used to ﬁnd the value of ^bthat maximizes L(b). Conﬁdence intervals, tests of hypotheses, measures of ﬁt, and other procedures are available.",
    "L(b). Conﬁdence intervals, tests of hypotheses, measures of ﬁt, and other procedures are available. For details, see Myers (1990, Section 7.5) Stokes et al. (1995, pp. 471–475), Lindsey (1997), and Kutner et al.(2005, Chapter 14). 18.5 GENERALIZED LINEAR MODELS Generalized linear models include the classical linear regression and ANOVA models covered in earlier chapters as well as logistic regression in Section 18.2 and some forms of nonlinear regression in Section 18.1.",
    "well as logistic regression in Section 18.2 and some forms of nonlinear regression in Section 18.1. Also included in this broad familyof models are loglinear models for categorical data in Section 18.3 and Poisson regression models for count data in Section 18.4. This expansion of traditional linear models was introduced by Wedderburn (1972). Ageneralized linear model can be brieﬂy characterized by the following three components. 1.",
    "(1972). Ageneralized linear model can be brieﬂy characterized by the following three components. 1. Independent random variables y 1,y2,...,ynwith expected value E(yi)¼mi and density function from the exponential family [described below in (18.15)]. 2. A linear predictor x0 ib¼b0þb1xi1þ/C1/C1/C1þbkxik:18.5 GENERALIZED LINEAR MODELS 513 --- Page 523 --- 3. A link function that describes how E(yi)¼mirelates to x0 ib: g(mi)¼x0 ib: 4. The link function g(mi) is often nonlinear.",
    "describes how E(yi)¼mirelates to x0 ib: g(mi)¼x0 ib: 4. The link function g(mi) is often nonlinear. A density f(yi,ui) belongs to the exponential family of density functions if f(yi,ui) can be expressed in the form f(yi,ui)¼exp[yiuiþb(ui)þc(yi)]: (18:15) A scale parameter such as s2in the normal distribution can be incorporated into (18.15) by considering it to be known and treating it as part of ui. Alternatively, an additional parameter can be inserted into (18.15).",
    "and treating it as part of ui. Alternatively, an additional parameter can be inserted into (18.15). The exponential family of density functions provides a uniﬁed approach to estimation of the parameters in gen-eralized linear models. Some common statistical distributions that are members of the exponential family are the binomial, Poisson, normal, and gamma [see (11.7)]. We illustrate three of these in Example 18.5. Example 18.5.",
    "Poisson, normal, and gamma [see (11.7)]. We illustrate three of these in Example 18.5. Example 18.5. The binomial probability distribution can be written in the form of (18.15) as follows: f(y i,pi)¼ni yi/C18/C19 pyi i(1/C0pi)ni/C0yi ¼expyilnpi/C0yiln(1/C0pi)þniln(1/C0pi)þlnni yi/C18/C19 /C20/C21 ¼expyilnpi 1/C0pi/C18/C19 þniln(1/C0pi)þlnni yi/C18/C19 /C20/C21 ¼exp yiuiþb(ui)þc(yi) ½/C138 , (18:16) whereui¼ln [pi/(12pi)],b(ui)¼niln(1/C0pi)¼/C0niln(1þeui), and c(yi)¼lnni yi/C18/C19 .",
    "C138 , (18:16) whereui¼ln [pi/(12pi)],b(ui)¼niln(1/C0pi)¼/C0niln(1þeui), and c(yi)¼lnni yi/C18/C19 . The Poisson distribution can be expressed in exponential form as follows: f(yi,mi)¼myi ie/C0mi yi!¼exp[yilnmi/C0mi/C0ln(yi!)] ¼exp[yiuiþb(ui)þc(yi)], whereui¼lnmi,b(ui)¼/C0mi¼/C0eui, and c(yi)¼/C0 ln(yi!).514 ADDITIONAL MODELS --- Page 524 --- The normal distribution N(mi,s2) can be written in the form of (18.15 ) as follows: f(yi,mi)¼1 (2ps2)1=2e/C0(yi/C0mi)2=2s2 ¼1 (2ps2)1=2e/C0(y2 i/C02yimiþm2i)=2s2 ¼exp/C0y2 i 2s2þyimi s2/C0m2i 2s2/C01 2ln(2ps2)/C20/C21 ¼exp[yiuiþb(ui)þc(yi)], whereui¼mi=s2,b(ui)¼s2u2 i=2, and c(yi)¼/C0y2 i=2s2/C01 2ln(2ps2).",
    ")/C20/C21 ¼exp[yiuiþb(ui)þc(yi)], whereui¼mi=s2,b(ui)¼s2u2 i=2, and c(yi)¼/C0y2 i=2s2/C01 2ln(2ps2). A To obtain an estimator of bin a generalized linear model, we use the method of maximum likelihood.",
    "A To obtain an estimator of bin a generalized linear model, we use the method of maximum likelihood. From (18.15), the likelihood function is given by L(b)¼Yn i¼1exp[yiuiþb(ui)þc(yi)]: The logarithm of the likelihood is lnL(b)¼Xn i¼1yiuiþXn i¼1b(ui)þXn i¼1c(yi): (18:17) For the exponential family in (18.15), it can be shown that E(yi)¼mi¼/C0b0(ui), where b0(ui) is the derivative with respect to ui.",
    "(18.15), it can be shown that E(yi)¼mi¼/C0b0(ui), where b0(ui) is the derivative with respect to ui. This relates uito the link function g(mi)¼x0 ib: Differentiating (18.17) with respect to each bi, setting the results equal to zero, and solving the resulting (nonlinear) equations iteratively (iteratively reweighted least squares) gives the estimators ^bi. Conﬁdence intervals, tests of hypotheses, measures of ﬁt, subset selection techniques, and other procedures are available.",
    "ests of hypotheses, measures of ﬁt, subset selection techniques, and other procedures are available. For details, see McCullagh and Nelder (1989), Dobson (1990), Myers (1990, Section 7.6), Hilbe (1994), Lindsey (1997), Christensen (1997, Chapter 9), and McCulloch and Searle (2001, Chapter 5).18.5 GENERALIZED LINEAR MODELS 515 --- Page 525 --- PROBLEMS 18.1 For the Bernoulli distribution, P(yi¼0)¼1/C0piand P(yi¼1)¼piin (18.4), show that E(yi)¼piand var( yi)¼pi(1/C0pi) as in (18.5) and (18.6).",
    "0)¼1/C0piand P(yi¼1)¼piin (18.4), show that E(yi)¼piand var( yi)¼pi(1/C0pi) as in (18.5) and (18.6). 18.2 Show that ln [ pi=(1/C0pi)]¼b0þb1xiin (18.8) can be obtained from (18.7). 18.3 Verify that lnL(b0,b1) has the form shown in (18.10), where L(b0,b1)i sa s given by (18.9). 18.4 Differentiate lnL(b0,b1) in (18.10) to obtain (18.11) and (18.12).",
    "0,b1)i sa s given by (18.9). 18.4 Differentiate lnL(b0,b1) in (18.10) to obtain (18.11) and (18.12). 18.5 Show that b(ui)¼/C0nln(1þeui), as noted following (18.16).516 ADDITIONAL MODELS --- Page 526 --- APPENDIX A Answers and Hints to the Problems Chapter 2 2.1 Part (i) follows from the commutativity of real numbers, aijþbij¼bijþaij: For part (ii), let C¼AþB.",
    "Part (i) follows from the commutativity of real numbers, aijþbij¼bijþaij: For part (ii), let C¼AþB. Then, by (2.3), C0¼(cij)0¼(cij)¼ (ajiþbji)¼(aji)þ(bji)¼A0þB0: 2.2 (a) A0¼74 /C039 250 @1A: (b)(A 0)0¼74 /C039 250 @1A0 ¼7/C032 49 5/C18/C19 ¼A: (c)A0A¼65 15 34 15 90 3934 39 290 @1A,AA 0¼62 11 11 122/C18/C19 : 2.3 (a) AB¼10 2 5/C06/C18/C19 ,BA¼/C011 3 55/C18/C19 : (b)jAj¼10,jBj¼/C0 7,jABj¼/C0 70¼(10)(/C07): (c)jBAj¼/C0 70¼jABj: (d)(AB)0¼10 5 2/C06/C18/C19 ,B0A0¼10 5 2/C06/C18/C19 : (e)tr(AB)¼4,tr(BA) ¼4: (f)For AB,l1¼10:6023 ,l2¼/C06:6023 :For BA,l1¼10:6023 , l2¼6:6023 : Linear Models in Statistics ,Second Edition , by Alvin C.",
    "/C06:6023 :For BA,l1¼10:6023 , l2¼6:6023 : Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc.",
    ",Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 517 --- Page 527 --- 2.4 (a) AþB¼411 11 2 9/C18/C19 ,A/C0B¼/C025 /C09 /C01/C016 5/C18/C19 : (b)A0¼15 3/C07 /C0420 @1A,B 0¼36 /C029 570 @1A: (c)(AþB) 0¼41 1 12 190 @1A,A 0þB0¼41 1 12 190 @1A: 2.5 The ( ij)th element of E¼BþCise ij¼bijþcij:The ( ij)th element ofAEisP kaikekj¼P kaik(bkjþckj)¼P k(aikbkjþaikckj)¼P kaikbkjþP kaikckj, which is the ( ij)th element of ABþAC.",
    "ikekj¼P kaik(bkjþckj)¼P k(aikbkjþaikckj)¼P kaikbkjþP kaikckj, which is the ( ij)th element of ABþAC. 2.6 (a) AB¼35 33 13 7/C18/C19 ,BA¼/C026 19 /C029 10 44 0 56/C025 40 @1 A: (b)BþC¼/C017 08800 @1A,AC13 47 /C023/C011/C18/C19 ,A(BþC)¼48 80 /C022 26/C18/C19 , ABþAC¼48 80 /C022 26/C18/C19 : (c)(AB) 0¼35 1 33 37/C18/C19 ,B0A0¼35 133 37/C18/C19 : (d)tr(AB)¼72,tr(BA)¼72: (e)(a 0 1B)¼(35 33) ,(a02B)¼(1 37) ,AB¼35 33 13 7/C18/C19 : (f)(Ab1)¼35 1/C18/C19 ,Ab2¼33 37/C18/C19 ,AB¼35 33 13 7/C18/C19 : 2.7 (a) AB¼000 000 0000 @1A¼O: (b)x¼any multiple of1 /C01 /C010@1A: (c)rank(A)¼1,rank(B)¼1: 2.8 (a) By (2.17), a 0j¼a1/C11þa2/C11þ/C1/C1/C1þ an/C11¼Pn i¼1ai: (b)IfA¼a0 1 a02 ...",
    ")¼1,rank(B)¼1: 2.8 (a) By (2.17), a 0j¼a1/C11þa2/C11þ/C1/C1/C1þ an/C11¼Pn i¼1ai: (b)IfA¼a0 1 a02 ... a0 n0 BBB@1 CCCA,thenAj¼a0 1j a02j ... a0 nj0 BBB@1 CCCA¼P ja1jP ja2j ... P janj0 BBB@1 CCCA:518 ANSWERS AND HINTS TO THE PROBLEMS --- Page 528 --- 2.9 By (2.16), ( ABC )0¼[(AB)C]0¼C0(AB)0¼C0B0A0. 2.10 (iii) (A0A0)¼A0(A0)0¼A0A: (iv) Theith diagonal element of A0Aisai0ai, where aiis the ith column of A.",
    "iii) (A0A0)¼A0(A0)0¼A0A: (iv) Theith diagonal element of A0Aisai0ai, where aiis the ith column of A. Since a0 iai¼P ja2ij¼0, we have ai¼0: 2.11 D1A¼24 9 21 4/C010 6/C18/C19 ,AD 2¼40 9 42 /C010 5 /C018/C18/C19 : 2.12 DA¼a2b3c 4a5b6c 7a8b9c0 @1A,DAD ¼a 22ab 3ac 4ab 5b26cb 7ac 8bc 9c20@1A: 2.13 y 0Ay¼a11y2 1þa22y22þa33y23þ2a12y1y2þ2a13y1y3þ2a23y2y3: 2.14 (a) Bx¼26 20 190 @1A: (h)xy 0¼96 1 2 /C03/C02/C04 6480@1A: (b)y 0B¼(40,/C016,29): (i)B0B¼89/C011 28 /C011 14 /C021 28/C021 340 @1A: (c) x 0Ax¼108.",
    "C04 6480@1A: (b)y 0B¼(40,/C016,29): (i)B0B¼89/C011 28 /C011 14 /C021 28/C021 340 @1A: (c) x 0Ax¼108. (j)yz0¼61 5 41 082 00 @1A: (d) x 0Cz¼229. (k)zy0¼648 15 10 20/C18/C19 : (e) x0x¼14. (l)ﬃﬃﬃﬃﬃﬃy0yp¼ﬃﬃﬃﬃﬃ 29p : (f) x0y¼15.",
    "@1A: (d) x 0Cz¼229. (k)zy0¼648 15 10 20/C18/C19 : (e) x0x¼14. (l)ﬃﬃﬃﬃﬃﬃy0yp¼ﬃﬃﬃﬃﬃ 29p : (f) x0y¼15. (m)C0C¼14 /C07 /C072 6/C18/C19 : (g)xx0¼9/C036 /C031 /C02 6/C0240 @1A: 2.15 (a) xþy¼6 1 60 @1A,x/C0y¼0 /C03 20 @1A: (b)tr(A)¼13,tr(B)¼12,AþB11/C036 62 2 5/C011 20 @1A,tr(AþB)¼25: (c)AB¼29/C020 30 5/C037 46/C025 440 @1A,BA¼41/C023 5 34 /C062 3 28 5 350 @1A: (d)tr(AB)¼70,tr(BA)¼70: (e)jABj¼/C0 403,jBAj¼/C0 403 :ANSWERS AND HINTS TO THE PROBLEMS 519 --- Page 529 --- (f)(AB)0¼29 5 46 /C020/C03/C025 30 7 440 @1A,B 0A0¼29 5 46 /C020/C03/C025 30 7 440@1A: 2.16 Bx¼36 7 20 @1 A/C01/C02 1 /C030 @1 Aþ23 0 50 @1 A¼18 21 60 @1 Aþ2 /C01 30 @1 Aþ6 0 100 @1 A¼26 20 190 @1 A: 2.17 (a) (AB) 0¼27 16 /C012/C06 19 110 @1A,B 0A0¼27 16 /C012/C06 19 110@1A: (b)AI¼25 13/C18/C191001/C18/C19 ¼2513/C18/C19 ¼A, IB¼10 01/C18/C191/C062 50 3/C18/C19 ¼1/C062 50 3/C18/C19 ¼B: (c)jAj¼1: (d)A /C01¼3/C05 /C012/C18/C19 : (e)(A/C01)/C01¼3/C05 /C012/C18/C19/C01 ¼25 13/C18/C19 ¼A: (f)(A0)/C01¼21 53/C18/C19/C01 ¼3/C01 /C052/C18/C19 ,(A/C01)0¼3/C01 /C052/C18/C19 : 2.18 (a) IfC¼AB, then by (2.35), we obtain C11¼A11B11þA12B21 ¼21 32/C18/C19111211/C18/C19 þ20/C18/C19 (2 3 1) ¼433 755/C18/C19 þ462000/C18/C19 ¼895755/C18/C19 : Continuing in this fashion, we obtain AB¼895 6 755 4 342 20 @1A: (b)AB¼8956 7554 34220 @1Awhen found in the usual way.520 ANSWERS AND HINTS TO THE PROBLEMS --- Page 530 --- 2.19 (a)AB¼a1b0 1þA2B2¼2220 3330 11100 @1Aþ6736 4224 23120 @1A ¼8956 7554 34220 @1A: 2.20 Ab¼25 7/C18/C19 þ4/C023/C18/C19 /C0331/C18/C19 ¼/C07 23/C18/C19 , Ab¼/C07 23/C18/C19 when found in the usual way : 2.21 By (2.26), AB¼(Ab 1,Ab2,...,Abp).",
    "23/C18/C19 , Ab¼/C07 23/C18/C19 when found in the usual way : 2.21 By (2.26), AB¼(Ab 1,Ab2,...,Abp). By (2.37) each Abican be expressed as a linear combination of the columns of A, with coefﬁcients from bi.",
    ".37) each Abican be expressed as a linear combination of the columns of A, with coefﬁcients from bi. 2.22 /C023 1 20 B@1 CAþ30 /C01 10 B@1 CAþ2 1 00 B@1 CA,/C013 1 20 B@1 CAþ0 /C01 /C010 B@1 CA/C02 1 00 B@1 CA2 643 75 ¼/C06 /C02 /C040 B@1 CAþ0 /C03 30 B@1 CAþ2 1 00 B@1 CA,/C03 /C01 /C020 B@1 CAþ0 /C01 10 B@1 CA/C02 1 00 B@1 CA2 643 75 ¼/C04/C05 /C04/C03 /C01/C010 B@1 CA¼AB: 2.23 Suppose ai¼0in the set of vectors a1,a2,...,an.",
    "643 75 ¼/C04/C05 /C04/C03 /C01/C010 B@1 CA¼AB: 2.23 Suppose ai¼0in the set of vectors a1,a2,...,an. Then c1a1þ...þci0þ ...þcnan¼0, where c1¼c2¼...¼ci21¼ciþ1¼...¼cn¼0 and ci=0. Hence, by (2.40), a1,a2,...,anare linearly dependent. 2.24 If one of the two matrices, say, A, is nonsingular, multiply AB¼ObyA21to obtain B¼O. Otherwise, they are both singular. In fact, as noted following Example 2.3, the columns of ABare linear combinations of the columns of A, with coefﬁcients from bj.",
    "Example 2.3, the columns of ABare linear combinations of the columns of A, with coefﬁcients from bj. AB¼(b11a1þ/C1/C1/C1þ bn1an,b12a1þ/C1/C1/C1þ bn2an,...) ¼(0,0,...,0): Since a linear combination of the columns of Ais0,Ais singular [see (2.40)].",
    "n2an,...) ¼(0,0,...,0): Since a linear combination of the columns of Ais0,Ais singular [see (2.40)]. Similarly, by a comment following (2.38), the rows of ABare linear combi- nations of the rows of B, and Bis singular.ANSWERS AND HINTS TO THE PROBLEMS 521 --- Page 531 --- 2.25 AB¼35 14/C18/C19 ,CB¼3514/C18/C19 ,rank(A)¼2,rank(B)¼2,rank(C)¼2: 2.26 (a) AB¼85 11/C18/C19 ,CB¼2c 11þc13c11þ2c12 2c21þc23c21þ2c22/C18/C19 :Cis not unique.",
    "rank(C)¼2: 2.26 (a) AB¼85 11/C18/C19 ,CB¼2c 11þc13c11þ2c12 2c21þc23c21þ2c22/C18/C19 :Cis not unique. An example is C¼126 /C0113/C18/C19 : (b)31 2 10 /C01/C18/C19 x1 x2 x30 @1A¼0 0/C18/C19 gives two equations in three unknowns with solution vector x 11 /C0510 @1A, where x 1is an arbitrary constant. We can’t do the same for Bbecause the columns of Bare linearly independent. 2.27 (a) An example is B¼22 3 14 4 /C01/C0130 @1A. Although AandBcan be non- singular, A2Bmust be singular so that ( A2B)x¼0.",
    "22 3 14 4 /C01/C0130 @1A. Although AandBcan be non- singular, A2Bmust be singular so that ( A2B)x¼0. (b)An example is C¼/C0111 1/C041 21 /C040 @1A. In the expression Cx¼0,w e have a linear combination of the columns of Cthat is equal to 0, which is the deﬁnition of linear dependence. Therefore, Cmust be singular. 2.28 A 0is nonsingular by deﬁnition because its rows are the columns of A.T o show that ( A0)21¼(A21)0, transpose both sides of AA21¼Ito obtain (AA21)0¼I0,(A21)0A0¼I.",
    "ns of A.T o show that ( A0)21¼(A21)0, transpose both sides of AA21¼Ito obtain (AA21)0¼I0,(A21)0A0¼I. Multiply both sides on the right by ( A0)21. 2.29 (AB)21exists by Theorem 2.4(ii). Then AB(AB)/C01¼I, A/C01AB(AB)/C01¼A/C01, B/C01B(AB)/C01¼B/C01A/C01: 2.30 AB¼23 1 13 1/C18/C19 ,B/C01¼1 1012 /C034/C18/C19 ,(AB)/C01¼1 101/C01 /C013 23/C18/C19 , B/C01A/C01¼1 101/C01 /C013 23/C18/C19 . 2.31 Multiply AbyA21in (2.48) to get I. 2.32 Multiply AbyA21in (2.49) to get I.",
    "1 /C013 23/C18/C19 . 2.31 Multiply AbyA21in (2.48) to get I. 2.32 Multiply AbyA21in (2.49) to get I. 2.33 Muliply Bþcc0by (Bþcc0)21in (2.50) to get I.522 ANSWERS AND HINTS TO THE PROBLEMS --- Page 532 --- 2.34 Premultiply both sides of the equation by AþPBQ . The left side obviously equals I.",
    "age 532 --- 2.34 Premultiply both sides of the equation by AþPBQ . The left side obviously equals I. The right side becomes (AþPBQ )[A/C01/C0A/C01PB(BþBQA/C01PB)/C01BQA/C01] ¼AA/C01þPBQA/C01/C0AA/C01PB(BþBQA/C01PB)/C01BQA/C01 /C0PBQA/C01PB(BþBQA/C01PB)/C01BQA/C01 ¼IþP[I/C0B(BþBQA/C01PB)/C01/C0BQA/C01PB(BþBQA/C01PB)/C01]BQA/C01 ¼IþP[I/C0(BþBQA/C01PB)(BþBQA/C01PB)/C01]BQA/C01 ¼IþP[I/C0I]BQA/C01 ¼I: 2.35 Since y0A0yis a scalar and is therefore equal to its transpose, we have y0A0y¼(y0A0y)0¼y0(A)0(y0)0¼y0Ay.",
    "e y0A0yis a scalar and is therefore equal to its transpose, we have y0A0y¼(y0A0y)0¼y0(A)0(y0)0¼y0Ay. Then1 2y0(AþA0)y¼12y0Ayþ 1 2y0A0y¼12y0Ayþ12y0Ay. 2.36 Use the proof of part (i) of Theorem 2.6b, substituting /C210 for.0. 2.37 Corollary 1:y0BAB0y¼(B0y)0A(B0y).0i fB0y=0since Ais positive deﬁ- nite. Then B0y¼y1b1þ...þykbk, where biis the ith column of B0; that is , bi0 is the ith row of B. Since the rows of Bare linearly independent, there is no nonzero vector ysuch that B0y¼0.",
    "row of B. Since the rows of Bare linearly independent, there is no nonzero vector ysuch that B0y¼0. 2.38 We must show that if Ais positive deﬁnite, then A¼P0P, where Pis non singular. By Theorems 2.12d and 2.12f, A¼CDC0,w h e r e Cis orthogonal andD¼diag(l1,l2,...,ln) with all li.0. Then A¼CDC0¼CD1=2 D1=2C0¼(D1=2C0)(D1=2C0)¼P0P, where D1=2¼diag(ﬃﬃﬃﬃﬃl1p,ﬃﬃﬃﬃﬃl2p,...,ﬃﬃﬃﬃﬃlpp). Show that P¼D21/2C0is nonsingular. 2.39 This follows by Theorems 2.6c and 2.4(ii). 2.40 (a) rank( A,c)¼rank( A)¼3.",
    "1/2C0is nonsingular. 2.39 This follows by Theorems 2.6c and 2.4(ii). 2.40 (a) rank( A,c)¼rank( A)¼3. Solution x1¼7 6,x2¼/C05 6,x3¼13 6. (b)rank( A)¼2, rank( A,c)¼3. No solution. (c)rank( A,c)¼rank( A)¼2. Solution x1¼7,x2þx3þx4¼1. 2.41 By deﬁnition, AA2A¼A.I fAisn/C2m, then for conformability of multipli- cation, A2must be m/C2n.",
    "2.41 By deﬁnition, AA2A¼A.I fAisn/C2m, then for conformability of multipli- cation, A2must be m/C2n. 2.42 AA/C0 1110 0101100 @1A,AA /C0 1A¼223 1013240 @1A: 2.43 A 1122 10/C18/C19 ,A/C01 11¼/C01 20/C02 /C012/C18/C19 ¼01 12/C01/C18/C19 .ANSWERS AND HINTS TO THE PROBLEMS 523 --- Page 533 --- 2.44 Let Cbe the lower left 2 /C22 matrix C¼10 32/C18/C19 . Then C/C01¼ 1 220 /C031/C18/C19 ¼10 /C032 12/C18/C19 and (C/C01)0¼1/C032 012/C18/C19 .",
    "32/C18/C19 . Then C/C01¼ 1 220 /C031/C18/C19 ¼10 /C032 12/C18/C19 and (C/C01)0¼1/C032 012/C18/C19 . 2.45 (i) By Theorem 2.4(i), rank( A2A)/C20rank( A) and rank( A)¼rank (AA2A)/C20rank ( A2A). Hence rank ( A2A)¼rank( A). (ii)(AA2A)0¼A0(A2)0A0 (iii) LetW¼A[I2(A0A)2A0A]. Show that W0W¼[I/C0(A0A)/C0A0A][A0A/C0A0A(A0A)/C0A0A] ¼[I/C0(A0A)/C0A0A]O¼O: Then by Theorem 2.2c(ii), W¼O. (iv) A [(A0A)2A0]A¼A(A0A)2A0A¼A, by part (iii). (v) (Searle 1982, p.",
    "Then by Theorem 2.2c(ii), W¼O. (iv) A [(A0A)2A0]A¼A(A0A)2A0A¼A, by part (iii). (v) (Searle 1982, p. 222) To show that A(A0A)2A0is invariant to the choice of (A0A)2, letBandCbe two values of ( A0A)2. Then by part (iii), A¼ ABA0AandA¼ACA0A, so that ABA0A¼ACA0A. To demonstrate that this implies ABA0¼ACA0, show that (ABA0A/C0ACA0A)(B0A0/C0C0A0)¼(ABA0/C0ACA0) /C2(ABA0/C0ACA0)0: The left side is Obecause ABA0A¼ACA0A. The right side is then O, and by Theorem 2.2c(ii), ABA02ACA0¼O.",
    "e left side is Obecause ABA0A¼ACA0A. The right side is then O, and by Theorem 2.2c(ii), ABA02ACA0¼O. To show symmetry, let S be a symmetric generalized inverse of A0A(see Problem 2.46). Then ASA0is symmetric and ASA0¼ABA0since ABA0is invariant to (A0A)2. Thus ABA0is also symmetric. To show that rank[ A(A0A)2A0]¼r, use parts (i) and (iv). 2.46 IfAis symmetric and Bis a generalized inverse of A, show that ABA¼ AB0A. Then show that12(BþB)0) and BAB0are symmetric generalized inverses of A.",
    "of A, show that ABA¼ AB0A. Then show that12(BþB)0) and BAB0are symmetric generalized inverses of A. 2.47 (i) By Corollary 1 to Theorem 2.8b, we obtain A/C0¼000 01 20 001 20 @1A: (ii)Using the ﬁve-step approach following Theorem 2.8b, with C¼22 20/C18/C19 deﬁned as the upper right 2 /C22 matrix, we obtain C /C01¼01 21 2/C012/C18/C19 andA/C0¼00 0 01 20 1 2/C01200 @1A. 2.48 (b) By deﬁnition, AA 2A¼A. Multiplying on the left by A0gives A0AA2A¼A0A.",
    "01 20 1 2/C01200 @1A. 2.48 (b) By deﬁnition, AA 2A¼A. Multiplying on the left by A0gives A0AA2A¼A0A. Show that ( A0A)21exists and multiply on the left by it.524 ANSWERS AND HINTS TO THE PROBLEMS --- Page 534 --- 2.49 (iv) IfAis positive deﬁnite, then by Theorem 2.6d, Acan be expressed as A¼P0P, where Pis nonsingular.",
    "iv) IfAis positive deﬁnite, then by Theorem 2.6d, Acan be expressed as A¼P0P, where Pis nonsingular. By Theorem 2.9c, we obtain jAj¼jP0Pj¼jP0jjPj [by (2 :74)] ¼jPjjPj [by (2 :63)] ¼jPj2.0 [by (2 :61)] (vi) jA/C01Aj¼jIj¼1 jA/C01jjAj¼1 ½by (2 :74)/C138 jA/C01j¼1=jAj: 2.50jAj¼25 13/C12/C12/C12/C12/C12/C12/C12/C12¼1=0,note that Ais nonsingular jA0j¼21 53/C12/C12/C12/C12/C12/C12/C12/C12¼1¼jAj A/C01¼3/C05 /C012/C18/C19 ,jA/C01j¼3/C05 /C012/C12/C12/C12/C12/C12/C12/C12/C12¼1, 1 25 13/C12/C12/C12/C12/C12/C12/C12/C12¼1, 2.51 (a) 1025 13/C18/C19 ¼20 5010 30/C18/C19 ,20 5010 30/C12/C12/C12/C12/C12/C12/C12/C12¼100, 10 225 13/C12/C12/C12/C12/C12/C12/C12/C12¼100(1) ¼100 (b)jcAj¼jcIAj¼jcIjjAj¼c njAj 2.52 Corollary 4.",
    "10 225 13/C12/C12/C12/C12/C12/C12/C12/C12¼100(1) ¼100 (b)jcAj¼jcIAj¼jcIjjAj¼c njAj 2.52 Corollary 4. Let A11¼B,A22¼1,A21¼c0,andA12¼c:Then equate the right sides of (2.68) and (2.69).",
    "2.52 Corollary 4. Let A11¼B,A22¼1,A21¼c0,andA12¼c:Then equate the right sides of (2.68) and (2.69). 2.53 jABj¼jAjjBj¼jBjjAj¼jBAj, jA2j¼jAAj¼jAjjAj¼jA2j 2.54 (a) jAj¼1,jBj¼4/C02 31/C12/C12/C12/C12/C12/C12/C12/C12¼10,AB¼23 1 13 1/C18/C19 ,jABj¼10: (b)jA 2j¼1,A2¼92 551 4/C18/C19 ,jA 2j¼1: 2.55 Deﬁne B¼A/C01 11 O /C0A21A/C01 11I/C18/C19 :Then BA¼IA/C01 11A12 OA 22/C0A21A/C01 11A12/C18/C19 :ANSWERS AND HINTS TO THE PROBLEMS 525 --- Page 535 --- By Corollary 1 to Theorem 2.9b, jBAj¼jA22/C0A21A/C01 11A12j:By Theorem 2.9c.",
    "MS 525 --- Page 535 --- By Corollary 1 to Theorem 2.9b, jBAj¼jA22/C0A21A/C01 11A12j:By Theorem 2.9c. jBAj¼jBjjAj:By Corollary 1 to Theorem 2.9b and (2.64), jBj¼jA/C01 11j¼1=jA11j: 2.56 We ﬁrst show that sine c0 icj¼0 for all i=j, the columns of Care linearly independent. Suppose that there exist a1,a2,...,apsuch that a1c1þ a2c2þ/C1/C1/C1þ apcp¼0:Multiply by c10to obtain a1c01c1þa2c01c2þ/C1/C1/C1þ apc01cp¼c010¼0o r a1c01c1¼0, which implies that a1¼0.",
    "ply by c10to obtain a1c01c1þa2c01c2þ/C1/C1/C1þ apc01cp¼c010¼0o r a1c01c1¼0, which implies that a1¼0. In a similar manner, we can show that a2¼a3¼/C1/C1/C1¼ ap¼0:Thus the columns of C are linearly independent and Cis nonsingular. Multiply C0C¼Ion the left by Cand on the right by C21.",
    "are linearly independent and Cis nonsingular. Multiply C0C¼Ion the left by Cand on the right by C21. 2.57 (a)C¼1=ﬃﬃﬃ 3p /C01=ﬃﬃﬃ 2p 1=ﬃﬃﬃ6p /C01=ﬃﬃﬃ 3p 02 =ﬃﬃﬃ 6p 1=ﬃﬃﬃ 3p 1=ﬃﬃﬃ 2p 1=ﬃﬃﬃ6p0 @1A: 2.58 (i) jIj¼jC 0Cj¼jC0jjCj¼jCjjCj¼jCj2:Thus jCj2¼1 and jCj¼+1: (ii)By (2.75), jC0ACj¼jACC0j¼jAIj¼jAj: (iii) Since c0 ici¼1 for all i,w eh a v e c0ici¼P jc2ij¼1, and the maximum value of any c2ijis 1.",
    "j: (iii) Since c0 ici¼1 for all i,w eh a v e c0ici¼P jc2ij¼1, and the maximum value of any c2ijis 1. 2.59 (i) The ith diagonal element of AþBisaiiþbii:Hence tr( AþB)¼P i(aiiþbii)¼P iaiiþP ibii¼tr(A)þtr(B): (iv) By Theorem 2.2c(ii), the ith diagonal element of AA0isa0iai,where a0i is the ith row of A.",
    "(B): (iv) By Theorem 2.2c(ii), the ith diagonal element of AA0isa0iai,where a0i is the ith row of A. (v)By (iii), tr( A0A)¼P ia0iai¼P iP ja2ij,where a0i¼(ai1,ai2,...,aip): (vii) By (2.84), tr( C0AC)¼tr(CC0A)¼tr(IA)¼tr(A): 2.60 B¼21 02 100 B@1 CA,B0B¼52 25/C18/C19 ,BB0¼522 240 2010 B@1 CA, tr(B0B)¼5þ5¼10,tr(BB0)¼5þ4þ1¼10: (iii) Letbibe the ithcolumn ofB. Then X2 i¼1b0 ibi¼(2,0,1)2 0 10 @1Aþ(1,2,0)1 2 00 @1A¼5þ5¼10: (iv)Letb 0ibe the ithrowofB.",
    "lumn ofB. Then X2 i¼1b0 ibi¼(2,0,1)2 0 10 @1Aþ(1,2,0)1 2 00 @1A¼5þ5¼10: (iv)Letb 0ibe the ithrowofB. Then X3 i¼1b0 ibi¼(2,1)2 1/C18/C19 þ(0,2)02/C18/C19 þ(1,0)10/C18/C19 ¼5þ4þ1¼10:526 ANSWERS AND HINTS TO THE PROBLEMS --- Page 536 --- 2.61 A¼31 2 10 /C01/C18/C19 ,A0A¼10 3 5 312 5250 B@1 CA,AA0¼14 1 12/C18/C19 , tr(A0A)¼16,tr(AA0)¼16,X ija2 ij¼32þ12þ22þ12þ02þ(/C01)2¼16: 2.62 (A/C0A)2¼A/C0AA/C0A¼A/C0Asince AA/C0A¼Aby definition.",
    ")¼16,X ija2 ij¼32þ12þ22þ12þ02þ(/C01)2¼16: 2.62 (A/C0A)2¼A/C0AA/C0A¼A/C0Asince AA/C0A¼Aby definition. Hence A/C0A is idempotent and tr( A/C0A)¼rank(A/C0A)¼r¼rank(A) by Theorem 2.8c(i). Show that tr( AA/C0)¼rby a similar argument.",
    "nd tr( A/C0A)¼rank(A/C0A)¼r¼rank(A) by Theorem 2.8c(i). Show that tr( AA/C0)¼rby a similar argument. 2.63 A¼223 101 3240 B@1 CA,A/C0¼01 0 0/C03 212 00 00 B@1 CA,A/C0A¼101 011 2 0000 B@1 CA, tr(A/C0A)¼2, AA/C0¼0/C011 01 0 00 10 B@1 CA,tr(AA/C0)¼2,rank(A/C0A)¼rank(AA/C0)¼2: 2.64l2¼2,(A/C0l2I)x2¼0,1/C022 /C014 /C02/C18/C19x1 x2/C18/C19 ¼0 0/C18/C19 , /C0x1þ2x2¼0, /C0x1þ2x2¼0, x1¼2x2, x2¼x1 x2/C18/C19 ¼2x2 x2/C18/C19 ¼x22 1/C18/C19 : Usex2¼1=ﬃﬃﬃ 5p to normalize x2: x2¼2=ﬃﬃﬃ 2p 1=ﬃﬃﬃ5p !",
    "x1 x2/C18/C19 ¼2x2 x2/C18/C19 ¼x22 1/C18/C19 : Usex2¼1=ﬃﬃﬃ 5p to normalize x2: x2¼2=ﬃﬃﬃ 2p 1=ﬃﬃﬃ5p ! : 2.65 From A2x¼l2x, we obtain AA2x¼l2Ax¼l2lx¼l3x:By induction AAk/C01x¼lk/C01Ax¼lk/C01lx¼lkx: 2.66 By (2.98) and (2.101), Ak¼CDkC0,where Cis an orthogonal matrix con- taining the normalized eigenvectors of AandDk¼diag(lk 1,lk2,...,lkp): If21,li,1 for all i, then Dk!OandAk!O.",
    "the normalized eigenvectors of AandDk¼diag(lk 1,lk2,...,lkp): If21,li,1 for all i, then Dk!OandAk!O. 2.67 (AB/C0lI)x¼0, (BAB /C0lB)x¼0, (BA/C0lI)Bx¼0:ANSWERS AND HINTS TO THE PROBLEMS 527 --- Page 537 --- 2.68 0¼jP/C01AP/C0lIj¼jP/C01AP/C0lP/C01Pj ¼jP/C01(A/C0lI)Pj¼j(A/C0lI)P/C01Pj ¼jA/C0lIj: Thus P21APandAhave the same characteristic equation, as in (2.93).",
    "lI)Pj¼j(A/C0lI)P/C01Pj ¼jA/C0lIj: Thus P21APandAhave the same characteristic equation, as in (2.93). 2.69 Writing (2.92) for xiandxj,w eh a v e Axi¼lixiandAxj¼ljxj:Multiplying byxjandx0igives x0 jAxi¼lix0 jxi, (1) x0 iAxj¼ljx0 ixj: (2) Since Ais symmetric, we can transpose (1) to obtain ( x0 jAxi)0¼li(x0 jxi)0or x0 iAxj¼lix0 ixj:This has the same left side as (2), and thus lix0 ixj¼ljx0 ixjor (li/C0lj)x0ixj¼0. Sinceli/C0lj=0, we have x0ixj¼0: 2.70 By (2.101), A¼CDC0.",
    "thus lix0 ixj¼ljx0 ixjor (li/C0lj)x0ixj¼0. Sinceli/C0lj=0, we have x0ixj¼0: 2.70 By (2.101), A¼CDC0. Since Cis orthogonal, we multiply on the left by C0 and on the right by Cto obtain C0AC¼C0CDC0C¼D.",
    ". Since Cis orthogonal, we multiply on the left by C0 and on the right by Cto obtain C0AC¼C0CDC0C¼D. 2.71 C¼/C0:5774 :8165 0 :5774 :4082 /C0:7071 :5774 :4082 :70710 @1 A: 2.72 (i) By Theorem 21.2d, jAj¼jCDC0j:By (2.75), jCDC0j¼jC0CDj¼jDj: By (2.59), jDj¼Qn i¼1li: 2.73 (a) Eigenvalues of A:1 ,2 ,21 Eigenvectors :x1¼:8018 :5345 :26730 @1A,x 2¼:3015 :9045 :30150@1A,x 3¼:7071 0 :70710 @1A: (b)tr(A)¼1þ2/C01¼2,jAj¼(1)(2)( /C01)¼/C02: 2.74 In the proof of part (i), if Ais positive semideﬁnite, x 0 iAxi/C210,while x0ixi.0.",
    "/C01)¼/C02: 2.74 In the proof of part (i), if Ais positive semideﬁnite, x 0 iAxi/C210,while x0ixi.0. By Corollary 1 to Theorem 2.12d C0AC¼D, where D¼ diag(l1,l2,...,ln):Since Cis orthogonal and nonsingular, then by Theorem 2.4(ii), the rank of Dis the same as the rank of A. Since Dis diag- onal, the rank is the number of nonzero elements on the diagonal, that is, the number of nonzero eigenvalues.",
    "rank is the number of nonzero elements on the diagonal, that is, the number of nonzero eigenvalues. 2.75 (a) jAj¼1: (b)The eigenvalues of Aare .2679, 1, and 3.7321, all of which are positive.528 ANSWERS AND HINTS TO THE PROBLEMS --- Page 538 --- 2.76 (a)(A1=2)0¼(CD1=2C0)0¼(C0)0(D1=2)0C0¼CD1=2C0¼A1=2: (b)(A1=2)2¼A1=2A1=2¼CD1=2C0CD1=2C0¼CD1=2D1=2C0¼CDC0¼A: 2.77l1¼3,l2¼1,x1¼ﬃﬃﬃ 2p =2 /C0ﬃﬃﬃ 2p =2/C18/C19 ,x2¼ﬃﬃﬃ 2p =2ﬃﬃﬃ 2p =2/C18/C19 : A1=2¼CD1=2C¼ﬃﬃﬃ 2p 2/C18/C19 211 /C011/C18/C19 ﬃﬃﬃ3p 0 01 !",
    "C18/C19 ,x2¼ﬃﬃﬃ 2p =2ﬃﬃﬃ 2p =2/C18/C19 : A1=2¼CD1=2C¼ﬃﬃﬃ 2p 2/C18/C19 211 /C011/C18/C19 ﬃﬃﬃ3p 0 01 ! 1/C01 11/C18/C19 ¼1 21þﬃﬃﬃ 3p 1/C0ﬃﬃﬃ3p 1/C0ﬃﬃﬃ 3p 1þﬃﬃﬃ3p ! : 2.78 (i) (I/C0A) 2¼I/C02AþA2¼I/C02AþA¼I/C0A: (ii)A(I/C0A)¼A/C0A2¼A/C0A¼O: (iii) (P/C01AP)2¼P/C01APP/C01AP¼P/C01A2P¼P/C01AP: (iv) (C0AC)2¼C0ACC0AC¼C0A2C¼C0AC, (C0AC)0¼C0A0(C0)0¼C0ACifA¼A0: 2.79 (A/C0A)2¼A/C0AA/C0A¼A/C0A, since AA/C0A¼A: [A(A0A)/C0A0]2¼A(A0A)/C0A0A(A0A0)/C0A0¼A(A0A)/C0A0, since A¼ A(A0A)/C0A0Aby Theorem 2.8c(iii).",
    "0A¼A: [A(A0A)/C0A0]2¼A(A0A)/C0A0A(A0A0)/C0A0¼A(A0A)/C0A0, since A¼ A(A0A)/C0A0Aby Theorem 2.8c(iii). 2.80 (a)2 ,( e)2 ,( f)1 ,1 ,0 . 2.81 By (2.107), tr( A)¼Pp i¼1li:By case 3 of Section 2.12.2 and (2.107), tr( A2)¼Pp i¼1l2 i:Then [tr( A)]2¼Pp i¼1l2 iþ2P i=jlilj¼tr(A2)þ2P i=jlilj: 2.82@H @x¼@B0(BAB0)/C01B @x, ¼B0@(BAB0)/C01 @xB, ¼B0(BAB0)/C01@BAB0 @x(BAB0)/C01B, ¼/C0B0(BAB0)/C01B@A @xB0(BAB0)/C01B, ¼/C0H@A @xH: 2.83 LetX¼ab bc/C18/C19 such that ac.b2.",
    "B0)/C01B, ¼/C0B0(BAB0)/C01B@A @xB0(BAB0)/C01B, ¼/C0H@A @xH: 2.83 LetX¼ab bc/C18/C19 such that ac.b2. Then, @lnjXj @X¼@ln(ac/C0b2) @X,ANSWERS AND HINTS TO THE PROBLEMS 529 --- Page 539 --- ¼@ln(ac/C0b2) @a@ln(ac/C0b2) @b @ln(ac/C0b2) @b@ln(ac/C0b2) @c0 @1A, ¼ 1 ac/C0b2c/C02b /C02ba/C18/C19 , ¼2 ac/C0b2c/C02b /C02ba/C18/C19 /C01 ac/C0b2c0 0a/C18/C19 , ¼2X/C01/C0diagX/C01: 2.84 The constraints can be expressed as h(x)¼Cx2t where C¼10 11 010 @1Aand t¼2 3/C18/C19 :The Lagrange equations are 2 AxþC 0l¼0 and Cx¼t,o r 2AC0 CO/C18/C19 x l/C18/C19 ¼0 t/C18/C19 : The solution to this system of equations is x l/C18/C19 ¼2AC0 CO/C18/C19/C010 t/C18/C19 : Subsituting and simplifying using (2.50) we obtain x¼1=6 11=6 7=60 @1Aand l¼/C01=3 /C07/C18/C19 : Chapter 3 3.1 By (3.3) we have E(ay)¼ð1 /C01ay f(y)dy¼að1 /C01yf(y)dy¼aE(y): 3.2 E(y/C0m)2¼E(y2/C02myþm2) ¼E(y2)/C02mE(y)þm2[by (3 :4) and (3 :5)] ¼E(y2)/C02m2þm2¼E(y2)/C0m2: 3.3 var(ay)¼E(ay/C0am)2[by (3 :6)] ¼E[a(y/C0m)]2¼E[a2(y/C0m)2] ¼a2E(y/C0m)2[by (3 :4)]:530 ANSWERS AND HINTS TO THE PROBLEMS --- Page 540 --- 3.4 The solution is similar to the answer to Problem 3.2.",
    "AND HINTS TO THE PROBLEMS --- Page 540 --- 3.4 The solution is similar to the answer to Problem 3.2. 3.5E(yiyj)¼ð1 /C01ð1 /C01yiyjf(yiyj)dyidyj ¼ðð yiyjfi(yi)fj(yj)dyidyj½by ( 3 :12)/C138 ¼ð yjfj(yj)ð yifi(yi)dyi/C20/C21 dyj ¼E(yi)ð yjfj(yj)dyj¼E(yi)E(yj): 3.6 cov( yi,yj)¼E(yiyj)/C0mimj [by ( 3 :11)] ¼mimj/C0mimj [by ( 3 :14)] : 3.7 (a) Using the quadratic formula to solve for xiny¼1þ2x2x2and y¼2x2x2, we obtain x¼1+1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ2/C0ypandx¼1+ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0yp,respecti- vely, which become the limits of integration in 3.16 and 3.17.",
    "0ypandx¼1+ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ1/C0yp,respecti- vely, which become the limits of integration in 3.16 and 3.17. 3.8 (a) Area =Ð 2 1Ðx x/C01dy dx þÐ3 2Ð4/C0x 3/C0xdy dx ¼2: (b) f1(x)¼ðx x/C011 2dy¼12,1/C20x/C202, f1(x)¼ð4/C0x 3/C0x1 2dy¼12,2/C20x/C203: Hence, f1(x)¼12,1/C20x/C203: f2(y)¼ðyþ1 112dxþð3 3/C0y12,0/C20dx¼y/C201, f2(y)¼ð4/C0y y1 2dx¼2/C0y,1/C20y/C202, E(x)¼ð3 1x(1 2)dx¼2, E(y)¼ð1 0y(y)dyþð2 1y(2/C0y)dy¼13þ23¼1, E(xy)¼ð2 1ðx x/C01xy(12)dy dx þð3 2ð4/C0x 3/C0xxy(12)dy dx ¼2, sxy¼2/C02( 1)¼0:ANSWERS AND HINTS TO THE PROBLEMS 531 --- Page 541 --- (c) f(yjx)¼f(x,y) f1(x)¼1 2 12¼1, E(yjx)¼ðx x/C01y(1 )dy¼x/C01 2,1/C20x/C202, E(yjx)¼ð4/C0x 3/C0xy(1 )dy¼7 2/C0x,2/C20x/C203: 3.9 E(xþy)¼Ex1þy1 x2þx2 ...",
    ")dy¼x/C01 2,1/C20x/C202, E(yjx)¼ð4/C0x 3/C0xy(1 )dy¼7 2/C0x,2/C20x/C203: 3.9 E(xþy)¼Ex1þy1 x2þx2 ... xpþyp0 BBBBB@1 CCCCCA¼E(x 1þy1) E(x2þy2) ... E(xpþyp)0 BBBBB@1 CCCCCA ¼E(x 1)þE(y1) E(x2)þE(y2) ... E(xp)þE(yp)0 BBBBB@1 CCCCCA¼E(x 1) E(x2) ... E(xp)0 BBBBB@1 CCCCCAþE(y 1) E(y2) ... E(yp)0 BBBBB@1 CCCCCA ¼Ex 1 x2 ... xp0 BBBBB@1 CCCCCAþEy 1 y2 ...",
    ")0 BBBBB@1 CCCCCAþE(y 1) E(y2) ... E(yp)0 BBBBB@1 CCCCCA ¼Ex 1 x2 ... xp0 BBBBB@1 CCCCCAþEy 1 y2 ... yp0 BBBBB@1 CCCCCA: 3.10 E[(y/C0 m)(y/C0m)0]¼E[yy0/C0ym0/C0my0þmm0] ¼E(yy0)/C0E(y)m0/C0mE(y0)þE(mm0) [by ( 3 :21) and ( 3 :36)] ¼E(yy0)/C0mm0/C0mm0þmm0: 3.11 Use the square root matrix S1/2deﬁned in (2.107) to write (3.27) as (y/C0m)0S/C01(y/C0m)¼(y/C0m)0(S1=2S1=2)/C01(y/C0m) ¼[(S1=2)/C01(y/C0m)]0[(S1=2)/C01(y/C0m)]¼z0z,say: Show that cov(z)¼I(see Problem 5.17).532 ANSWERS AND HINTS TO THE PROBLEMS --- Page 542 --- 3.13cov(z)¼covy x/C18/C19 ¼E[(z/C0mz)(z/C0mz)0] [by ( 3 :24)] ¼Ey x/C18/C19 /C0my mx/C18/C19/C20/C21y X/C18/C19 /C0my mx/C18/C19/C20/C21 0 [by ( 3 :32)] ¼Ey/C0my x/C0mx/C18/C19 [(y/C0my)0,(x/C0mx)0] ¼E(y/C0my)(y/C0my)0(y/C0my)(x/C0mx)0 (x/C0mx)(y/C0my)0(x/C0mx)(x/C0mx)0\"# ¼E[(y/C0my)(y/C0my)0]E[(y/C0my)(x/C0mx)0] E[(x/C0mx)(y/C0my)0]E[(x/C0mx)(x/C0mx)0]\"# ¼SyySyx SxySxx/C18/C19 :[by ( 3 :34)] 3.14 (i) If we write Ain terms of its rows, then Ay¼a0 1 a02 ...",
    "yySyx SxySxx/C18/C19 :[by ( 3 :34)] 3.14 (i) If we write Ain terms of its rows, then Ay¼a0 1 a02 ... a0 k0 BBB@1 CCCA,y¼a 0 1y a02y ... ak0y0 BBB@1 CCCA Then, by Theorem 3.6a, E(a 0 iy)¼a0iE(y), and the result follows by (3.20). (ii)Write Xin terms of its columns xiasX¼(x1,x2,...,xp).",
    "¼a0iE(y), and the result follows by (3.20). (ii)Write Xin terms of its columns xiasX¼(x1,x2,...,xp). Since Xbis a random vector, we have, by Theorem 3.6a E(a0Xb)¼a0E(Xb) ¼a0E(b1x1þb2x2þ/C1/C1/C1þ bpxp) [by ( 2 :37)] ¼a0[b1E(x1)þb2E(x2)þ/C1/C1/C1þ bpE(xp)] ¼a0[E(x1),E(x2),...,E(xp)]b [by ( 2 :37)] ¼a0E(X)b: (iii)E(AXB )¼Ea01 a02 ...",
    "2)þ/C1/C1/C1þ bpE(xp)] ¼a0[E(x1),E(x2),...,E(xp)]b [by ( 2 :37)] ¼a0E(X)b: (iii)E(AXB )¼Ea01 a02 ... a0 k0 BBBBB@1 CCCCCAX(b1,b2,...,bp)2 6666643 777775ANSWERS AND HINTS TO THE PROBLEMS 533 --- Page 543 --- ¼Ea0 1Xb1a01Xb2/C1/C1/C1a01Xbp a02Xb1a02Xb2/C1/C1/C1a02Xbp ......... a0 kXb1a0kXb2/C1/C1/C1a0kXbp0 BBBBB@1 CCCCCA ¼a 0 1E(X)b1a01E(X)b2/C1/C1/C1a01E(X)bp a02E(X)b1a02E(X)b2/C1/C1/C1a02E(X)bp ......... a0 kE(X)b1a0kE(X)b2/C1/C1/C1a0kE(X)bp0 BBBBB@1 CCCCCA ¼a 0 1 a02 ...",
    ")b2/C1/C1/C1a02E(X)bp ......... a0 kE(X)b1a0kE(X)b2/C1/C1/C1a0kE(X)bp0 BBBBB@1 CCCCCA ¼a 0 1 a02 ... a0 k0 BBBBB@1 CCCCCAE(X)(b 1,b2,...,bp)¼AE(X)B: 3.15 By (3.21), E(Ayþb)¼E(Ay)þE(b)¼AE(y)þb. Show that E(b)¼b: ifbis a constant vector.",
    ".,bp)¼AE(X)B: 3.15 By (3.21), E(Ayþb)¼E(Ay)þE(b)¼AE(y)þb. Show that E(b)¼b: ifbis a constant vector. 3.16 By (3.10) and Theorem 3.6a, we obtain cov(a0y,b0y)¼E[(a0y/C0a0m)(b0y/C0b0m)] ¼E[(a0(y/C0m)(y/C0m)0b] [by ( 2 :18)] ¼a0E[(y/C0m)(y/C0m)0]b [by Theorem 3 :6b (ii)] ¼a0Sb [by ( 3 :24)] : 3.17 (i) By Theorem 3.6b parts (i) and (iii), we obtain cov(Ay)¼E[(Ay/C0Am)(Ay/C0Am)0] ¼E[(A(y/C0m)(y/C0m)0A0] ¼AE[(y/C0m)(y/C0m)0]A0 ¼ASA0[by ( 3 :24)] : (ii)By (3.34) and Theorem 3.6b(i), cov( Ay,By)¼E½(Ay/C0Am)(By/C0 Bm)0/C138.",
    "]A0 ¼ASA0[by ( 3 :24)] : (ii)By (3.34) and Theorem 3.6b(i), cov( Ay,By)¼E½(Ay/C0Am)(By/C0 Bm)0/C138. Show that this is equal to ASB0.534 ANSWERS AND HINTS TO THE PROBLEMS --- Page 544 --- 3.18 By (3.24) and (3.41), we have cov(Ayþb)¼E[Ayþb/C0(Amþb)][Ayþb/C0(Amþb)]0 ¼E[Ay/C0Am][Ay/C0Am]0: Show that this is equal to ASA0. 3.19 Letz¼y x v w0 BB@1 CCA,K¼AOOO OBOO OOCO OOOD0 BB@1 CCA,L¼/C0 IIOO/C1 , and M¼ /C0 OOII/C1 .",
    "A0. 3.19 Letz¼y x v w0 BB@1 CCA,K¼AOOO OBOO OOCO OOOD0 BB@1 CCA,L¼/C0 IIOO/C1 , and M¼ /C0 OOII/C1 . Then cov( AyþBx,CvþDw) ¼LKcov(z)K0M0 ¼/C0 ABOO/C1SyySxySyvSym SxySxxSxvSxw SvySvxSvvSvw SwySwxSwvSww0 BBB@1 CCCAO 0 O0 C0 D00 BBB@1 CCCA ¼AS yvC0þBSxvC0þASywD0þBSxwD0: 3.20 (a) E(z)¼8,var(z)¼2: (b)E(z)¼3 /C04/C18/C19 ,cov(z)¼21/C014 /C014 45/C18/C19 : 3.21 (a) E(w)¼6 /C010 60 @1A,cov(w)¼6/C014 18 /C014 67 /C049 18/C049 570@1A: (b)cov(z,w)¼11/C025 34 /C085 3 /C031/C18/C19 : Chapter 4 4.1 Use (3.2) and (3.8) and integrate directly.",
    "ov(z,w)¼11/C025 34 /C085 3 /C031/C18/C19 : Chapter 4 4.1 Use (3.2) and (3.8) and integrate directly. 4.2 By (2.67) jS /C01=2j¼j(S1=2)/C01j¼jS/C01=2j/C01:We now use (2.77) to obtain jSj¼jS1=2S1=2j¼jS1=2jjS1=2j¼jS1=2j2,form which it follows thatjS1=2j¼ jSj1=2:ANSWERS AND HINTS TO THE PROBLEMS 535 --- Page 545 --- 4.3 Using Theorem 2.14a and the chain rule for differentiation (and assuming that we can interchange integration and differentiation), we obtain @et0y @t¼et0y@t0y @t¼yet0y, @My(t) @t¼@ @tð /C1/C1/C1ð et0yf(y)dy¼ð /C1/C1/C1ð@ @tet0yf(y)dy, @My(0) @t¼ð /C1/C1/C1ð yf(y)dy¼E(y)½by ( 3 :2) and ( 3 :20)/C138: 4.4@2et0y @tr@ts¼@ @tret0y@t0y @ts/C18/C19 ¼@ @tr(yset0y)¼yryset0y: 4.5 Multiply out the third term on the right side in terms of y2mandSt.",
    "C19 ¼@ @tr(yset0y)¼yryset0y: 4.5 Multiply out the third term on the right side in terms of y2mandSt. 4.6 My/C0m(t)¼E½et0(y/C0m)/C138¼E(et0y/C0t0m)¼e/C0t0mE(et0y)¼e/C0t0met0mþ(1=2)t0St: 4.7 E(et0Ay)¼E(e(A0t)0y). Now use Theorem 4.3 with A0tin place of tto obtain E(et0Ay)¼e(A0t)0mþ(1=2)(A0t)SA0t¼et0(Am)þ(1=2)t0(ASA0)t: 4.8 LetK(t)¼ln½M(t)/C138. Then K0(t)¼M0(t) M(t)andK00(t)¼M00(t) M(t)/C0M0(t) M(t)hi2 . Since M(0 )¼1,K00(0 )¼M00(0 )/C0½M0(0 )/C1382¼s2.",
    "¼M0(t) M(t)andK00(t)¼M00(t) M(t)/C0M0(t) M(t)hi2 . Since M(0 )¼1,K00(0 )¼M00(0 )/C0½M0(0 )/C1382¼s2. 4.9CSC0¼C(s2I)C0¼s2CC0¼s2I:Use Theorem 4.4a (ii).",
    "Since M(0 )¼1,K00(0 )¼M00(0 )/C0½M0(0 )/C1382¼s2. 4.9CSC0¼C(s2I)C0¼s2CC0¼s2I:Use Theorem 4.4a (ii). 4.10 The moment generating function for z¼Ayþbis MzðtÞ¼Eðet0zÞ¼Eðet0ðAyþbÞÞ¼Eðet0ðAyþt0bÞÞ¼et0bEðet0AyÞ ¼et0bet0ðAmÞþt0ðASA0Þt=2½byð4:25Þ/C138 ¼et0ðAmþbÞþt0ðASA0Þt=2, which is the moment generating function for a multivariate normal random vector with mean vector Amþband covariance matrix ASA0.536 ANSWERS AND HINTS TO THE PROBLEMS --- Page 546 --- 4.11 Use (2.35) and (2.36).",
    "iance matrix ASA0.536 ANSWERS AND HINTS TO THE PROBLEMS --- Page 546 --- 4.11 Use (2.35) and (2.36). 4.12 By Theorem 3.6d(ii), cov( Ay,By)¼ASB0: 4.13 Write g(y,x) in terms ofmy mx/C18/C19 andS¼SyySyx SxySxx/C18/C19 .F o rjSjandS/C01, see (2.72) and (2.50).",
    "g(y,x) in terms ofmy mx/C18/C19 andS¼SyySyx SxySxx/C18/C19 .F o rjSjandS/C01, see (2.72) and (2.50). After canceling h(x) in (4.28), show that f(yjx) can be written in the form f(yjx)¼1 (2p)p=2jSy/C1xj1=2e/C0(y/C0my/C1x)0S/C01 y/C1x(y/C0my/C1x)=2, wheremy/C1x¼myþSyxS/C01 xx(x/C0mx) andSy/C1x¼Syy/C0SyxS/C01 xxSxy: 4.14 cov(y/C0Bx,x)¼cov ( I,/C0B)y x/C18/C19 ,(O,I)yx/C18/C19 /C20/C21 :Use Theorem 3.6d(ii) 4.16 (a)y 1 y3/C18/C19 isN213/C18/C19 ,4/C01 /C015/C18/C19/C20/C21 : (b)y 2isN(2, 6).",
    "eorem 3.6d(ii) 4.16 (a)y 1 y3/C18/C19 isN213/C18/C19 ,4/C01 /C015/C18/C19/C20/C21 : (b)y 2isN(2, 6). (c)zisN(24, 79). (d)z¼z1 z2/C18/C19 isN22 9/C18/C19 ,11 /C06 /C06 154/C18/C19/C20/C21 : (e)f(y1,y2jy3,y4)¼N21þy3þ3 2y4 y3þ1 2y4/C18/C19 ,22 24/C18/C19 /C20/C21 .",
    "/C06 154/C18/C19/C20/C21 : (e)f(y1,y2jy3,y4)¼N21þy3þ3 2y4 y3þ1 2y4/C18/C19 ,22 24/C18/C19 /C20/C21 . (f)E(y1,y3jy2,y4)¼13/C18/C19 þ223/C04/C18/C19 6/C02 /C024/C18/C19 /C01y2/C02 y4þ2/C18/C19 , cov( y1,y2jy2,y4)¼4/C01 /C015/C18/C19 /C0223/C04/C18/C19 6/C02 /C024/C18/C19 /C01232/C04/C18/C19 : Thus f(y 1,y3jy2,y4)¼N27 5þ32y2þ45y4 4 5þ15y2/C09 10y4 ! ,6525 2 545 ! \"# : (g)r13¼/C01=2ﬃﬃﬃ 5p . (h)r13/C124¼1=ﬃﬃﬃ 6p . Note that r13/C124is opposite in sign to r13.",
    "45 ! \"# : (g)r13¼/C01=2ﬃﬃﬃ 5p . (h)r13/C124¼1=ﬃﬃﬃ 6p . Note that r13/C124is opposite in sign to r13. (i)Using the partitioning m¼1 2 3 /C020 B@1 CA,S¼4 2/C012 2 63 /C02 /C01 35 /C04 2/C02/C0440 BB@1 CCA,ANSWERS AND HINTS TO THE PROBLEMS 537 --- Page 547 --- we have E(y1jy2,y3,y4)¼1þ(2 /C012 )63 /C02 35 /C04 /C02/C0440 B@1 CA/C01y2/C02 y3/C03 y4þ20 B@1 CA ¼1þ(2 /C012 )1=4/C01=4/C01=8 /C01 45=49 =8 1=89 =82 1 =160 BBB@1 CCCAy2/C02 y3/C03 y4þ20 B@1 CA ¼y2 2þy3 2þ5y4 4þ1 var(y1jy2,y3,y4)¼4/C0(2 /C012 )1=4/C01=4/C01=8 /C01=45 =49 =8 1=89 =82 1 =160 B@1 CA2 /C01 20 B@1 CA ¼4/C03¼1: Thus f(y1jy2,y3,y4)¼N1þ1=2y2þ1=2y3þ5=4y4,1 ðÞ : 4.17 (a) N(17, 79).",
    "B@1 CA2 /C01 20 B@1 CA ¼4/C03¼1: Thus f(y1jy2,y3,y4)¼N1þ1=2y2þ1=2y3þ5=4y4,1 ðÞ : 4.17 (a) N(17, 79). (b)N26 0/C18/C19 ,5442 3/C18/C19/C20/C21 : (c)f(y 2jy1,y3)¼N/C05 2þ14y1þ13y3,17=12/C0/C1 . (d)f(y1jy2,y3)¼N22 /C02þ1 3y3/C18/C19 ,41 15 3/C18/C19 /C20/C21 . (e)r12¼ﬃﬃﬃ 2p =4¼:3536 ,r12/C13¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 3=20p ¼:3873. 4.18 y1andy2are independent, y2andy3are independent. 4.19 y1andy2are independent, ( y1,y2) and ( y3,y4) are independent.",
    "pendent, y2andy3are independent. 4.19 y1andy2are independent, ( y1,y2) and ( y3,y4) are independent. 4.20 Using the expression in (4.38) for Syxin terms of its rows six, show that SyxS/C01 xxSxy¼s0 1xS/C01 xxs1xs01xS/C01 xxs2x...s01xS/C01 xxspx s02xS/C01 xxs1xs02xS/C01 xxs2x...s02xS/C01 xxspx .........",
    "01 xxs1xs01xS/C01 xxs2x...s01xS/C01 xxspx s02xS/C01 xxs1xs02xS/C01 xxs2x...s02xS/C01 xxspx ......... s0 pxS/C01 xxs1xs0 pxS/C01 xxs2x...s0 pxS/C01 xxspx0 BBBB@1 CCCCA:538 ANSWERS AND HINTS TO THE PROBLEMS --- Page 548 --- Chapter 5 5.1Xn i¼1(yi/C0/C22y)2¼Xn i¼1(y2 i/C02/C22yyiþ/C22y2)¼Xn i¼1y2i/C02/C22yX iyiþn/C22y2¼X iy2i/C02n/C22y2þn/C22y2 5.2 By (2.23) ½(1=n)J/C1382¼(1=n2)jj0jj0¼(1=n2)j(n)j0¼(1=n)jj0¼(1=n)J: 5.3 (a) By Theorem 5.2b we obtain var(s2)¼1 (n/C01)2/C20/C21 vary0I/C01 nJ/C18/C19 y/C20/C21 ¼1 (n/C01)2/C20/C21 2tr I/C01nJ/C18/C19 s2I/C20/C212 þ4m2s2j0I/C01nJ/C18/C19 j() ¼1 (n/C01)2/C20/C21 2s4trI/C01 nJ/C18/C19 þ4m2s2(n/C0n)/C20/C21 ¼2s4 n/C01: (b)var(s2)¼vars2u n/C01/C18/C19 ¼s4 (n/C01)2/C20/C21 var(u) ¼s4 (n/C01)2/C20/C21 ( 2)(n/C01)¼2s4 n/C01 5.4 Note that u0V/C01¼m0S/C01.",
    "s4 (n/C01)2/C20/C21 var(u) ¼s4 (n/C01)2/C20/C21 ( 2)(n/C01)¼2s4 n/C01 5.4 Note that u0V/C01¼m0S/C01. Because of symmetry of VandS,w eh a v e V/C01u¼S/C01m.",
    "C01)¼2s4 n/C01 5.4 Note that u0V/C01¼m0S/C01. Because of symmetry of VandS,w eh a v e V/C01u¼S/C01m. Substituting into the expression on the left we obtain Sjj/C01=2I/C02tAS jj/C0(1=2)S/C01/C12/C12/C12/C12/C12/C12/C0(1=2) e/C0½m0S/C01m/C0m0(I/C02AS)/C01S/C01m/C138=2 ¼I/C02tAS jj/C0(1=2)e/C0m0½I/C0(I/C02tAS)/C01/C138S/C01m=2: 5.5 Expanding the second expression we obtain e/C0½m0S/C01m/C0u0V/C01uþy0V/C01y/C02u0V/C01yþ u0V/C01u/C138=2:Substituting u0and V21, simplifying, and noting that u0V/C01¼ m0S/C01, we obtain the ﬁrst expression.",
    "ubstituting u0and V21, simplifying, and noting that u0V/C01¼ m0S/C01, we obtain the ﬁrst expression. 5.6 k0(t)¼/C01 21 jCjdjCj dt/C012m0C/C01dC dtC/C01S/C01m:Using the chain rule , k00(t)¼/C0121 jCj2djCj dt/C20/C212 /C0121 jCjd2jCj dt2þ12m0C/C01dC dtC/C01dC dtC/C01S/C01m /C012m0C/C01d2C dt2C/C01S/C01mþ12m0C/C01dC dtC/C01dC dtC/C01S/C01m:ANSWERS AND HINTS TO THE PROBLEMS 539 --- Page 549 --- 5.7y0Ay¼(y/C0mþm)0A(y/C0mþm) ¼(y/C0m)0A(y/C0m)þ(y/C0m)0Amþm0A(y/C0m)þm0Am ¼(y/C0m)0A(y/C0m)þ2(y/C0m)0Amþm0Am 5.8 To show that E½(y/C0m)(y/C0m)0A(y/C0m)/C138¼0, we need to show that all central third moments of the multivariate normal are zero.",
    "A(y/C0m)/C138¼0, we need to show that all central third moments of the multivariate normal are zero. This can be done by differentiating My/C0m(t) from Corollary 1 to Theorem 4.3a. Show that @3My/C0m(t) @tr@ts@tu¼e(1=2)t0StsurX jtjssj ! þssrX jtjsuj ! \" þX jtjsuj !X jtjssj !X jtjsrj ! þsusX jtjsrj !# Since there is a tjin every term, @3My/C0m(t)=@tr@ts@tu¼0 for t¼0and E½(yr/C0mr)(ys/C0ms)(yu/C0mu)/C138¼0 for all r,s,u.",
    "n every term, @3My/C0m(t)=@tr@ts@tu¼0 for t¼0and E½(yr/C0mr)(ys/C0ms)(yu/C0mu)/C138¼0 for all r,s,u. For the second term, we have [by (3.40)] 2E½(y/C0m)(y/C0m)0Am/C138¼2{E½(y/C0m)(y/C0m)0/C138}Am ¼2SAm: For the third term, we have E½(y/C0m)tr(AS)/C138¼½E(y/C0m/C138½tr(AS)/C138¼0½tr(AS)/C138¼0: 5.9 By deﬁnition cov(By,y0Ay)¼E{[By/C0E(By)][y0Ay/C0E(y0Ay)]} ¼E{[B(y/C0m)][y0Ay/C0E(y0Ay)]} ¼BE{(y/C0m)][y0Ay/C0E(y0Ay)]} ¼Bcov(y,y0Ay)¼2BSAm: 5.10 In (3.34), we have Syx¼E½(y/C0my)(x/C0mx)0/C138.",
    "(y/C0m)][y0Ay/C0E(y0Ay)]} ¼Bcov(y,y0Ay)¼2BSAm: 5.10 In (3.34), we have Syx¼E½(y/C0my)(x/C0mx)0/C138. Show that E(yx0)¼ Syxþmym0 x.",
    "v(y,y0Ay)¼2BSAm: 5.10 In (3.34), we have Syx¼E½(y/C0my)(x/C0mx)0/C138. Show that E(yx0)¼ Syxþmym0 x. Then E(x0Ay)¼E[tr(x0Ay)]¼E[tr(Ayx0)] ¼tr[E(Ayx0)]¼tr[AE(yx0)] ¼tr[A(Syxþmym0 x)]¼tr(ASyxþAmym0x) ¼tr(ASyx)þtr(Amym0x)¼tr(ASyx)þtr(m0xAmy) ¼tr(ASyx)þm0xAmy:540 ANSWERS AND HINTS TO THE PROBLEMS --- Page 550 --- 5.11 (a)Pn i¼1(xi/C0/C22x)(yi/C0/C22y)¼Pni¼1(xiyi/C0/C22xyi/C0/C22yxiþ/C22x/C22y)¼Pni¼1xiyi/C0/C22xP iyi/C0 /C22yP ixiþn/C22x/C22y¼P ixiyi/C0n/C22x/C22y/C0n/C22x/C22yþn/C22x/C22y: (b)With x¼(x1,x2,...,xn)0,y¼(y1,y2,...,yn)0,/C22x¼(1=n)j0x, and /C22y¼(1=n)j0y,w eh a v e n/C22x/C22y¼n1 n/C18/C192 j0xj0y¼1nx 0jj0y¼x01nJ/C18/C19 y, X n i¼1xiyi/C0n/C22x/C22y¼x0y/C0x01nJ/C18/C19 y¼x 0I/C01nJ/C18/C19 y: 5.12 Apply (5.5), (5.9), and (5.8) with m¼0,A¼I,a n dS¼I.",
    "0y/C0x01nJ/C18/C19 y¼x 0I/C01nJ/C18/C19 y: 5.12 Apply (5.5), (5.9), and (5.8) with m¼0,A¼I,a n dS¼I. The results follow. 5.13 By (5.9), var( y0Ay)¼2tr(AS)2þ4m0ASAm. In this case, we seek var( y0y), where yisNn(m,I).",
    "llow. 5.13 By (5.9), var( y0Ay)¼2tr(AS)2þ4m0ASAm. In this case, we seek var( y0y), where yisNn(m,I). Hence, A¼S¼I, and var(y0y)¼2tr(I)2þ4m0m¼2nþ8l: Since Iisn/C2n, tr(I)¼n, and by (5.24), 4 m0m¼8l: 5.14 lnMv(t)¼/C0(n=2)ln(1/C02t)/C0l½1/C0(1/C02t)/C01/C138, dlnMv(t) dt¼n 1/C02t/C0l½/C02( 1/C02t)/C02/C138, dlnMv(0 ) dt¼nþ2l, d2lnMv(t) dt2¼2n (1/C02t)2þ8l(1/C02t)/C03, d2lnMv(0 ) dt2¼2nþ8l: 5.15 Since v1,v2,...,vkare independent, we have MSivi(t)¼E(etSivi)¼E(etv1etv2...etvk) ¼E(etv1)E(etv2)...E(etvk) ¼Yk i¼1Mvi(t)Yk i¼11 (1/C02t)ni=2e/C0li½1/C01=(1/C02t)/C138 ¼1 (1/C02t)Sini=2e/C0½1/C01=(1/C02t)/C138Sili: Thus by (5.25), Siviisx2Sini,Sili ðÞ .ANSWERS AND HINTS TO THE PROBLEMS 541 --- Page 551 --- 5.16 (a)t2¼x2=(u=p)i sF(1,p) since z2isx2(1),uisx2(p), and z2anduare independent.",
    "1 --- Page 551 --- 5.16 (a)t2¼x2=(u=p)i sF(1,p) since z2isx2(1),uisx2(p), and z2anduare independent. (b)t2¼y2=(u=p)i sF(1,p,1 2m2) since y2isx2(1,12m2),uisx2(p), and y2 anduare independent. 5.17 E½S/C01=2(y/C0m)/C138¼S/C01=2½E(y)/C0m/C138¼0:cov½S/C01=2(y/C0m)/C138¼S/C01=2cov(y/C0 m)S/C01=2¼S/C01=2SS/C01=2¼S/C01=2S1=2S1=2S/C01=2¼I. Then by Theorem 4.4a(ii),S/C01=2(y/C0m)i sNn(0,I). 5.18 (a) In this case, S¼s2IandAis replace by A/s2. We thus have ( A/s2) (s2I)¼A, which is indempotent.",
    "8 (a) In this case, S¼s2IandAis replace by A/s2. We thus have ( A/s2) (s2I)¼A, which is indempotent. (b)By Theorem 5.5, y0(A=s2)yisx2(r,l)i f(A/s2)Sis idempotent. In this case,S¼s2I,s o( A=s2)S¼(A=s2)(s2I)¼A.F o rl,w eh a v e l¼1 2m0(A=s2)m¼m0Am=2s2. 5.19 By Theorem 5.5, ( y/C0m)0S/C01(y/C0m)i sx2(n) because AS¼S/C01S¼I (which is idempotent) and E(y/C0m)¼0. The distribution of y0S/C01yis x2(n,l), wherel¼1 2m0S/C01m. 5.20 All of these are direct applications of Theorem 5.5. (a)l¼1 2m0Am¼1200A0¼0.",
    "wherel¼1 2m0S/C01m. 5.20 All of these are direct applications of Theorem 5.5. (a)l¼1 2m0Am¼1200A0¼0. (b)AS¼(I=s2)(s2I)¼I, which is idempotent. l¼12m0(I=s2)m¼ m0m=2s2: (c)In this case “ AS” becomes ( A=s2)(s2S)¼AS. 5.21 BSA¼B(s2I)A¼s2BA, which is OifBA¼O. 5.22 j0½I/C0(1=n)J/C138¼j0½I/C0(1=n)jj0/C138¼j0/C0(1=n)j0jj0¼j0/C0(1=n)(n)j0¼00: 5.23 ASB¼A(s2I)B¼s2AB, which is OifAB¼O. 5.24 (a) Use Theorem 4.4a(i). In this case a¼j/n.",
    "(n)j0¼00: 5.23 ASB¼A(s2I)B¼s2AB, which is OifAB¼O. 5.24 (a) Use Theorem 4.4a(i). In this case a¼j/n. (b)t¼zﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ u=(n/C01)p ¼(/C22y/C0m)=(s=ﬃﬃﬃnp)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ½(n/C01)s2=s2=(n/C01)/C138p . Show that z¼(/C22y/C0m)=(s=ﬃﬃﬃnp)i sN(0, 1). (c)Let v¼(/C22y/C0m0)=(s=ﬃﬃﬃnp). Then E(v)¼(m/C0m0)=(s=ﬃﬃﬃnp)¼d,s a y , and var( v)¼[1=(s2=n)] var( /C22y)¼1.",
    "(/C22y/C0m0)=(s=ﬃﬃﬃnp). Then E(v)¼(m/C0m0)=(s=ﬃﬃﬃnp)¼d,s a y , and var( v)¼[1=(s2=n)] var( /C22y)¼1. Hence visN(d, 1), and by (5.29), we obtain vﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (n/C01)s2=s2 n/C01r ¼/C22y/C0m0 s=ﬃﬃﬃnp ist(n/C01,d): Thusd¼(m/C0m0)=(s=ﬃﬃﬃnp):542 ANSWERS AND HINTS TO THE PROBLEMS --- Page 552 --- 5.25 By Problem 5.2, (1/ n)Jand I2(1/n)Jare idempotent and [ I2(1/n)J] [(1/n)J]¼O. By Example 5.5,Pn i¼1(yi/C0/C22y)2=s2¼y0½I/C0(1=n)J/C138y=s2 isx2(n21).",
    "and [ I2(1/n)J] [(1/n)J]¼O. By Example 5.5,Pn i¼1(yi/C0/C22y)2=s2¼y0½I/C0(1=n)J/C138y=s2 isx2(n21). Show that n/C22y2=s2¼y0½(1=n)J/C138y=s2isx2(1,l), where l¼1 2m0Am¼nm2=2s2. Since ½I/C0(1=n)J(1=n)J/C138¼O, the quadratic forms y0½(1=n)J/C138yandy0½I/C0(1=n)J/C138yare independent. Thus by (5.26), n/C22y2=½Pn i¼1(yi/C0/C22y)2=(n/C01)/C138isF(1,n/C01,l), where l¼nm2=2s2.I f m¼0(H0is true), then l¼0 and n/C22y2=½Pni¼1(yi/C0/C22y)2=(n/C01)/C138is F(1,n21).",
    "here l¼nm2=2s2.I f m¼0(H0is true), then l¼0 and n/C22y2=½Pni¼1(yi/C0/C22y)2=(n/C01)/C138is F(1,n21). 5.26 (b) Since Pni¼1(yi/C0/C22y)2 s2(1/C0r)¼y0½I/C0(1=n)J/C138y s2(1/C0r)¼y0 A s2(1/C0r)/C18/C19 y, we have A s2(1/C0r)S¼s2 s2(1/C0r)¼½I/C0(1=n)J/C138½(1/C0r)IþrJ/C138: Show that this equals ( I/C01 nJ), which is idempotent. 5.27 (a) E(y0Ay)¼tr(AS)þm0Am¼/C016: (b)var(y0Ay)¼2tr (AS)2þ4m0ASAm¼21,138 : (c)Check to see if ASis indepotent. (d)Check to see if Ais idempotent.",
    "y)¼2tr (AS)2þ4m0ASAm¼21,138 : (c)Check to see if ASis indepotent. (d)Check to see if Ais idempotent. 5.28 A¼S/C01¼diag1 2,14,13/C0/C1 ,12m0Am¼2:9167. 5.29 A¼S/C01,1 2m0Am¼27. 5.30 (a) Show that Ais idempotent of rank 2, which is equal to tr ( A). Therefore, y0Ay=s2isx2(2,m0Am=2s2), where12m0Am¼12(1 2 :6)¼6:3: (b)BA¼00 0 10 /C01/C18/C19 =O. Hence y0AyandByare not independent. (c)y1þy2þy3¼j0y. Show that j0A¼00. Hence y0Ayandy1þy2þy3are independent. 5.31 (a) Show that Bis idempotent of rank 1.",
    "Show that j0A¼00. Hence y0Ayandy1þy2þy3are independent. 5.31 (a) Show that Bis idempotent of rank 1. Therefore, y0By=s2is x2(1,m0Bm=2s2). Find1 2m0Bm. (b)Show that BA¼O. Therefore, y0Byandy0Ayare independent. 5.32 (a) A2¼X(X0X)/C01X0X(X0X)/C01X0¼X(X0X)/C01X0¼A:By Theorem 2.13d rank(A)¼tr (A)¼tr [X(X0X)/C01X0].",
    "32 (a) A2¼X(X0X)/C01X0X(X0X)/C01X0¼X(X0X)/C01X0¼A:By Theorem 2.13d rank(A)¼tr (A)¼tr [X(X0X)/C01X0]. By Theorem 2.11(ii), thisANSWERS AND HINTS TO THE PROBLEMS 543 --- Page 553 --- becomes tr [ X(X0X)/C01X0]¼tr(Ip)¼p:Similarly, rank tr ( I/C0A)¼ n/C0p: (b)tr[A(s2I)]¼tr[s2X(X0X)/C01X0]¼ps2:m0Am¼(Xb)0X(X0X)/C01X0(Xb)¼ b0X0Xb:Thus E[y0Ay]¼ps2þb0X0Xb. Show that tr [( I/C0A)(s2I)]¼ s2(n/C0p) andm0(I/C0A)m¼0, ifm¼Xb. Hence E½y0(I/C0A)y/C138¼(n/C0 p)s2.",
    "Show that tr [( I/C0A)(s2I)]¼ s2(n/C0p) andm0(I/C0A)m¼0, ifm¼Xb. Hence E½y0(I/C0A)y/C138¼(n/C0 p)s2. (c)y0Ay=s2isx2(p,l), where l¼m0Am=2s2¼b0X0Xb=2s2: y0(I/C0A)y=s2isx2(n2p). (d)Show that X(X0X)/C01X0½I/C0X(X0X)/C01X0/C138¼O:Then, by Corollary 1 to Theorem 5.6b, y0Ayandy0(I/C0A)yare independent. (e)F(p,n/C0p,l), wherel¼b0X0Xb=2s2.",
    "y Corollary 1 to Theorem 5.6b, y0Ayandy0(I/C0A)yare independent. (e)F(p,n/C0p,l), wherel¼b0X0Xb=2s2. Chapter 6 6.1 Equations (6.3) and (6.4) can be written as Xn i¼1yi/C0n^b0/C0^b1Xn i¼1xi¼0, Xn i¼1xiyi/C0^b0Xn i¼1xi/C0^b1Xn i¼1x2 i¼0: Solving for ^b0from the ﬁrst equation gives ^b0¼P iyi=n/C0^b1P ixi=n¼ /C22y/C0^b1/C22x. Substituting this into the second equation gives the result for ^b1.",
    "C0^b1P ixi=n¼ /C22y/C0^b1/C22x. Substituting this into the second equation gives the result for ^b1. 6.2 (a) Show thatPn i¼1(xi/C0/C22x)(yi/C0/C22y)¼Pni¼1(xi/C0/C22x)yi:Then ^b1¼P i (xi/C0/C22x)yi=c,w h e r e c¼P i(xi/C0/C22x)2.",
    "C0/C22x)(yi/C0/C22y)¼Pni¼1(xi/C0/C22x)yi:Then ^b1¼P i (xi/C0/C22x)yi=c,w h e r e c¼P i(xi/C0/C22x)2. Now, using E(yi)¼b0þb1xi from assumption 1 in Section 6.1, we obtain (assuming that the x’s are constants) E(^b1)¼EXn i¼1(xi/C0/C22x)yi=c\"# ¼X i(xi/C0/C22x)E(yi)=c ¼X i(xi/C0/C22x)(b0þb1xi)=c ¼b0X i(xi/C0/C22x)=cþb1X i(xi/C0/C22x)xi=c ¼0þb1X i(xi/C0/C22x)(xi/C0/C22x)=c¼b1X i(xi/C0/C22x)2=X i(xi/C0/C22x)2¼b1:544 ANSWERS AND HINTS TO THE PROBLEMS --- Page 554 --- (b) E(^b0)¼E(/C22y/C0^b1/C22x)¼EXn i¼1yi=n !",
    ":544 ANSWERS AND HINTS TO THE PROBLEMS --- Page 554 --- (b) E(^b0)¼E(/C22y/C0^b1/C22x)¼EXn i¼1yi=n ! /C0½E(^b1)/C138/C22x ¼X iE(yi)=n/C0b1/C22x¼X i(b0þb1xi)=n/C0b1/C22x ¼X ib0=nþb1X ixi=n/C0b1/C22x¼nb0=nþb1/C22x/C0b1/C22x¼b0: 6.3 (a) Using ^b1¼Pn i¼1(xi/C0/C22x)yi=c, as in the answer to Problem 6.2, and assuming var( yi)¼s2and cov( yi,yj)¼0, we have var( ^b1)¼1 c2Xn i¼1(xi/C0/C22x)2var(yi)¼1 c2Xn i¼1(xi/C0/C22x)2s2 ¼s2Pni¼1(xi/C0/C22x)2 Pni¼1(xi/C0/C22x)2/C2/C32¼s2 Pn i¼1(xi/C0/C22x)2: (b)Show that ^b0can be written in the form ^b0¼Pni¼1yi=n/C0 /C22xPni¼1(xi/C0/C22x)yi=c:Then var( ^b0)¼varXn i¼11 n/C0/C22x(xi/C0/C22x) c/C20/C21 yi() ¼Xn i¼11n/C0/C22x(xi/C0/C22x) c/C20/C212 var(yi) ¼Xn i¼11 n2/C02/C22x(xi/C0/C22x) ncþ/C22x2(xi/C0/C22x)2 c2/C20/C21 s2 ¼s2n n2/C02/C22x ncXn i¼1(xi/C0/C22x)þ/C22x2 c2Xn i¼1(xi/C0/C22x)2\"# ¼s21n/C00þ/C22x2Pn i¼1(xi/C0/C22x)2 ½Pn i¼1(xi/C0/C22x)2/C1382\"# ¼s21 nþ/C22x2 Pn i¼1(xi/C0/C22x)2/C20/C21 :ANSWERS AND HINTS TO THE PROBLEMS 545 --- Page 555 --- 6.4 Suppose that kof the xi’s are equal to aand the remaining n2kxi’s are equal tob.",
    "-- Page 555 --- 6.4 Suppose that kof the xi’s are equal to aand the remaining n2kxi’s are equal tob. Then /C22x¼kaþ(n/C0k)b n, Xn i¼1(xi/C0/C22x)2¼ka/C0kaþ(n/C0k)b n/C20/C212 þ(n/C0k)b/C0kaþ(n/C0k)b n/C20/C212 ¼kn(a/C0b)/C0k(a/C0b) n/C20/C212 þ(n/C0k)k(b/C0a) n/C20/C212 ¼k n2½(n/C0k)(a/C0b)/C1382þn/C0k n2½/C0k(a/C0b)/C1382 ¼(a/C0b)2 n2½k(n/C0k)2þk2(n/C0k)/C138 ¼(a/C0b)2 n2k(n/C0k)(n/C0kþk)¼(a/C0b)2k(n/C0k) n: We then differentiate with respect to kand set the results equal to 0 to solve fork.",
    "0b)2k(n/C0k) n: We then differentiate with respect to kand set the results equal to 0 to solve fork. @Pn i¼1(xi/C0/C22x)2 @k¼(a/C0b)2 n½k(/C01)þn/C0k/C138¼0, k¼n 2: 6.5SSE¼Xn i¼1(yi/C0^yi)2¼X i(yi/C0^b0/C0^b1xi)2 ¼X i(yi/C0/C22yþ^b1/C22x/C0^b1xi)2¼X i½yi/C0/C22y/C0^b1(xi/C0/C22x)/C1382 ¼X i(yi/C0/C22y)2/C02^b1X (yi/C0/C22y)(xi/C0/C22x)þ^b2 1X i(xi/C0/C22x)2: Substitute ^b1from (6.5) to obtain the result.",
    "2^b1X (yi/C0/C22y)(xi/C0/C22x)þ^b2 1X i(xi/C0/C22x)2: Substitute ^b1from (6.5) to obtain the result. 6.6 Show that SSE ¼P(yi/C0/C22y)2/C0^b2 1P(xi/C0/C22x)2:Show that /C22y¼b0þb1/C22xþ/C221, where /C221¼Pn i¼11i=n:Show that EPni¼1(yi/C0/C22y)2/C2/C3 ¼E{P i½b1(xi/C0/C22x)þ 1i/C0/C221/C1382}¼b2 1P i(xi/C0/C22x)2þ(n/C01)s2þ0:By (3.8), E(^b2 1)¼var( ^b1)þ ½E(^b1)/C1382¼s2=P i(xi/C0/C22x)2þb2 1: 6.8 To test H0:b1¼cversus H1:b1=c,we use the test statistic t¼^b1/C0c s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn i¼1(xi/C0/C22x)2q546 ANSWERS AND HINTS TO THE PROBLEMS --- Page 556 --- and reject H0ifjtj/C21ta=2,n/C02.",
    "i/C0/C22x)2q546 ANSWERS AND HINTS TO THE PROBLEMS --- Page 556 --- and reject H0ifjtj/C21ta=2,n/C02. Show that tis distributed as t(n22,d), where d¼b1/C0c s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q 6.9 (a) To test H0:b0¼aversus H1:b0=a, we use the test statistic t¼^b0/C0a sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 nþ/C22x2 Pn i¼1(xi/C0/C22x)2s and reject H0ifjtj.ta=2,n/C02:.",
    "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 nþ/C22x2 Pn i¼1(xi/C0/C22x)2s and reject H0ifjtj.ta=2,n/C02:. Show that tis distributed as t(n22,d), where d¼b0/C0a sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 nþ/C22x2 P i(xi/C0/C22x)2s (b)A 100 (1 2a)% conﬁdence interval for b0is given by ^b+ta=2,n/C02sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1nþ/C22x2 P i(xi/C0/C22x)2:s 6.10 We add and subtract ^yito obtainPn i¼1(yi/C0/C22y)2¼Pni¼1(yi/C0^yiþ^yi/C0/C22y)2: Squaring the right side gives X i(yi/C0/C22y)2¼X i(yi/C0^yi)2þX i(^yi/C0/C22y)2þ2X i(yi/C0^yi)(^yi/C0/C22y): In the third term on the right side, substitute ^yi¼^b0þ^b1xiand then ^b0¼/C22y/C0^b1/C22xto obtain X i(yi/C0^yi)(^yi/C0/C22y)¼X i(yi/C0^b0/C0^b1xi)(^b0þ^b1xi/C0/C22y) ¼X i(yi/C0/C22yþ^b1/C22x/C0^b1xi)(/C22y/C0^b1/C22xþ^b1xi/C0/C22y) ¼X i[(yi/C0/C22y/C0^b1(xi/C0/C22x)][^b1(xi/C0/C22x)] ¼^b1X i(yi/C0/C22y)(xi/C0/C22x)/C0^b2 1X i(xi/C0/C22x)2:ANSWERS AND HINTS TO THE PROBLEMS 547 --- Page 557 --- This is equal to 0 by (6.5).",
    "X i(xi/C0/C22x)2:ANSWERS AND HINTS TO THE PROBLEMS 547 --- Page 557 --- This is equal to 0 by (6.5). 6.11Xn i¼1(^yi/C0/C22y)2¼X i(^b0þ^b1xi/C0/C22y)2¼X i(/C22y/C0^b1/C22xþ^b1xi/C0/C22y)2 ¼^b2 1X i(xi/C0/C22x)2 Substituting this into (6.16) and using (6.5) gives the desired result.",
    "22y)2 ¼^b2 1X i(xi/C0/C22x)2 Substituting this into (6.16) and using (6.5) gives the desired result. 6.12 Since x/C0/C22xj¼(x1/C0/C22x,x2/C0/C22x,...,xn/C0/C22x)0and y/C0/C22yj¼(y1/C0/C22y,y2/C0 /C22y,...,yn/C0/C22y)0, (6.18) can be written as r¼(x/C0/C22xj)0(y/C0/C22yj)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ½(x/C0/C22xj)0(x/C0/C22xj)/C138½(y/C0/C22yj)0(y/C0/C22yj)/C138p : By (2.81), this is the cosine of u, the angle between the vectors x/C0/C22xj andy/C0/C22yj.",
    "/C138p : By (2.81), this is the cosine of u, the angle between the vectors x/C0/C22xj andy/C0/C22yj. 6.13 t¼^b1 s=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x2p ¼^b1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ SSE =(n/C02)p ¼^b1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C02p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(yi/C0/C22y2/C0^b2 1P i(xi/C0/C22x)2q ¼^b1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C02p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ P i(yi/C0/C22y)2/C0P i(xi/C0/C22x)(yi/C0/C22y)/C2/C32P i(xi/C0/C22x)2 P i(xi/C0/C22x)2/C2/C32vuut ¼P i(xi/C0/C22x)(yi/C0/C22y)P i(xi/C0/C22x)2:ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP i(xi/C0/C22x)2qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C02p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ P i(yi/C0/C22y)21/C0P i(xi/C0/C22x)(yi/C0/C22y)/C2/C32 P i(yi/C0/C22y)2P i(xi/C0/C22x)2\"#vuut ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C02p rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1/C0r2p : 6.14 (a) ^b0¼31:752,^b1¼11:368 : (b)t¼11:109,p¼4:108/C210/C015: (c)11:368+2:054,(9:313,13:422) : (d)r2¼SSR SST¼6833 :7663 9658 :0755¼:7076 :548 ANSWERS AND HINTS TO THE PROBLEMS --- Page 558 --- Chapter 7 7.1 By (2.18), ^b0þ^bixi1þ/C1/C1/C1þ ^bkxik¼x0 i^b.",
    "HINTS TO THE PROBLEMS --- Page 558 --- Chapter 7 7.1 By (2.18), ^b0þ^bixi1þ/C1/C1/C1þ ^bkxik¼x0 i^b. By (2.20) and (2.27), we obtain Xn i¼1(yi/C0x0 i^b)2¼(y1/C0x0i^b,y2/C0x02^b,...,yn/C0x0n^b)y1/C0x0 1^b y2/C0x02^b ... yn/C0x0 n^b0 BBBBBB@1 CCCCCCA ¼(y/C0X^ b)0(y/C0X^b): This can also be seen directly by using estimates in the model y¼Xbþ1. Thus ^1¼y/C0X^b,and ˆ10ˆ1¼(y/C0X^b)0(y/C0X^b). 7.2 Multiply (7.9) using (2.17). Keep y2Xbˆtogether and Xbˆ2Xbtogether. Factor Xout of Xbˆ2Xb.",
    "X^b). 7.2 Multiply (7.9) using (2.17). Keep y2Xbˆtogether and Xbˆ2Xbtogether. Factor Xout of Xbˆ2Xb. 7.3 In (7.12), we obtain ^b1¼nP ixiyi/C0ðP ixiÞðP iyiÞ nP ix2 i/C0ðP ixiÞ2¼n½P ixiyi/C0(n/C22x)(n/C22y)=n/C138 n½P ix2i/C0(n/C22x)2=n/C138 ¼P ixiyi/C0n/C22x/C22yP ix2i/C0n/C22x2, which is (6.5).",
    ")(n/C22y)=n/C138 n½P ix2i/C0(n/C22x)2=n/C138 ¼P ixiyi/C0n/C22x/C22yP ix2i/C0n/C22x2, which is (6.5). For bˆ0, we start with (6.6): ^b0¼/C22y/C0^b1/C22x¼P iyi n/C0P ixiyi/C0n/C22x/C22yP ix2i/C0n/C22x2/C18/C19 P ixi n ¼P iyi/C0/C1P ix2 i/C0n/C22x2/C0/C1 nP ix2 i/C0n/C22x2/C0/C1 /C0P ixiyi/C0n/C22x/C22yP ix2 i/C0n/C22x2/C18/C19 P ixi n ¼P ix2 iP iyi/C0nP iyiP ixi=n ðÞ2/C0P ixiðÞP ixiyi ðÞ þnP ixi=n ðÞP iyi=n ðÞP ixiðÞ nP ix2i/C0n/C22x2 ðÞ: The second and fourth terms in the numerator add to 0.ANSWERS AND HINTS TO THE PROBLEMS 549 --- Page 559 --- 7.5 Starting with (6.10), we have var( ^b0)¼s21 nþ/C22x2 P i(xi/C0/C22x)2/C20/C21 ¼s2P i(xi/C0/C22x)2þn/C22x2 nP i(xi/C0/C22x)2/C20/C21 ¼s2P ix2 i/C0n/C22x2þn/C22x2 nP i(xi/C0/C22x)2/C20/C21 : 7.6 The two terms missing in (7.17) are ½A/C0(X0X)/C01X0/C138½(X0X)/C01X0/C1380þ½(X0X)/C01X0/C138½A/C0(X0X)/C01X0/C1380: Using AX¼I, the ﬁrst of these becomes AX(X0X)/C01/C0(X0X)/C01X0X(X0X)/C01¼(X0X)/C01/C0(X0X)/C01¼O: 7.7 (a) For the linear estimator c0yto be unbiased for all possible b,w eh a v e E(c0y)¼c0Xb¼a0b, which requires c0X¼a0.",
    "ar estimator c0yto be unbiased for all possible b,w eh a v e E(c0y)¼c0Xb¼a0b, which requires c0X¼a0. To express var( c0y)i n terms of var( a0^b)¼var½a0(X0X)/C01X0y/C138,w ew r i t ev a r ( c0y)¼s2c0c¼ s2[c/C0X(X0X)/C01aþX(X0X)/C01a/C01]0[c/C0X(X0X)/C01aþX(X0X)/C01a]. Show that with c0X¼a0, this becomes [ c/C0X(X0X)/C01a]0 [c/C0X(X0X)/C01a]þa0(X0X)/C01a, which is minimized by c¼X(X0X)/C01a.",
    "his becomes [ c/C0X(X0X)/C01a]0 [c/C0X(X0X)/C01a]þa0(X0X)/C01a, which is minimized by c¼X(X0X)/C01a. (b)To minimize var( c’y) subject to c’X¼a’, we differentiate v¼s2c0c/C0 (c0X/C0a0)lwith respect to candl(see Section 2.14.3): @v=@l¼/C0X0cþa¼0gives a¼X0c: @v=@c¼2s2c/C0Xl¼0gives c¼Xl=2s2: Substituting c¼Xl=2s2into a¼X0cgives a¼X0Xl=2s2,orl¼ 2s2(X0X)/C01a. Thus c¼Xl=2s2¼X(X0X)/C01a.",
    "=2s2: Substituting c¼Xl=2s2into a¼X0cgives a¼X0Xl=2s2,orl¼ 2s2(X0X)/C01a. Thus c¼Xl=2s2¼X(X0X)/C01a. 7.9^bz¼(Z0Z)/C01Z0y¼(H0X0XH)/C01H0X0y ¼H/C01(X0X)/C01(H0)/C01H0X0y ¼H/C01(X0X)/C01X0y¼H/C01^b: For the ith row of Z¼XH,w eh a v e z0i¼x0iH,orzi¼H0xi.",
    "/C01(H0)/C01H0X0y ¼H/C01(X0X)/C01X0y¼H/C01^b: For the ith row of Z¼XH,w eh a v e z0i¼x0iH,orzi¼H0xi. Thus in general,550 ANSWERS AND HINTS TO THE PROBLEMS --- Page 560 --- z¼H0x, and ^y¼ˆb0 zz¼(H/C01ˆb)0H0x¼ˆb0(H/C01)0H0x ¼ˆb0(H0)/C01H0x¼ˆb0x: 7.10 Since ^b0x¼x0^bis invariant to changes of scale on the x0s,x0 i^bis invariant, where xi0is the ith row of X. Therefore, Xbˆis invariant, and it follows that s2¼(y/C0X^b)0(y/C0X^b)=(n/C0k/C01) is invariant.",
    "X. Therefore, Xbˆis invariant, and it follows that s2¼(y/C0X^b)0(y/C0X^b)=(n/C0k/C01) is invariant. 7.11 (y/C0X^b)0(y/C0X^b)¼y0y/C0y0X^b/C0^b0X0yþ^b0X0X^b. Use (7.8). 7.12 By (7.8), ^b0(X0y)¼^b0(X0X^b). By Theorem 5.2a, E(y0y)¼E(y0Iy)¼ tr(Is2I)þEðy0ÞIEðyÞ¼ns2þb0X0Xb. By Theorems 7.3b and 7.3c, E(^b)¼band cov( ^b)¼s2(X0X)/C01. Thus E(^b0X0X^b)¼tr½(X0X) s2(X0X)/C01/C138þ^b0X0Xb: 7.13 LetX1andbˆ1represent a reduced model with k21x’s, and let Xandbˆ represent the full model with kx’s.",
    "LetX1andbˆ1represent a reduced model with k21x’s, and let Xandbˆ represent the full model with kx’s. Then show that SSE for the full model can be expressed as SSE k¼(y0y/C0^b0 1X0 1y)/C0(^b0X0y/C0^b0 1X0 1y) ¼SSE k/C01/C0( a positive term) : It is shown in Theorem 8.2d and problem 8.10 that ^b0X0y/C0^b0 1X0 1yis a positive deﬁnite quadratic form. 7.15 First show that (1/ n)j0X1¼x¯,w h e r e x¯¼(x¯1,x¯2,...,x¯k), which contains the means of the columns of X1.",
    "ow that (1/ n)j0X1¼x¯,w h e r e x¯¼(x¯1,x¯2,...,x¯k), which contains the means of the columns of X1. Then I/C01 nJ/C18/C19 X1¼X1/C01nJX 1¼X1/C01njj 0X1¼X1/C0j/C22x0 ¼x11x12/C1/C1/C1 x1k x21x22 ... x2k ......... xn1xn2/C1/C1/C1 xnk0 BBBBB@1 CCCCCA/C0/C22x 1/C22x2/C1/C1/C1 /C22xk /C22x1/C22x2/C1/C1/C1 /C22xk ......... /C22x1/C22x2/C1/C1/C1 /C22xk0 BBBBB@1 CCCCCA:ANSWERS AND HINTS TO THE PROBLEMS 551 --- Page 561 --- 7.16 By a comment following (2.25), j0Xccontains the column sums of Xc.",
    "ROBLEMS 551 --- Page 561 --- 7.16 By a comment following (2.25), j0Xccontains the column sums of Xc. The sum of the second column, for example, isPn i¼1(xi2/C0/C22x2)¼Pni¼1xi2/C0n/C22x2¼n/C22x2/C0n/C22x2¼0. Alternatively, j0Xc¼j0½I/C0(1=n)J/C138 X1¼½j0/C0(1=n)j0jj0/C138X1¼00X1¼00since j0j¼n. 7.17 (a) Partition XandbˆasX¼(j,X1) and ^b¼^b0^b1/C18/C19 .",
    "/C0(1=n)j0jj0/C138X1¼00X1¼00since j0j¼n. 7.17 (a) Partition XandbˆasX¼(j,X1) and ^b¼^b0^b1/C18/C19 . Then show that the normal equations X0X^b¼X0yin (7.8) become j0jj0X1 X0 1jX01X1/C18/C19^b0^b1/C18/C19 ¼j0y X01y/C18/C19 , from which we obtain n^b0þj0X1^b1¼n/C22y (1) X0 1j^b0þX01X1^b1¼X01y: (2) Show that (1) becomes ^b0þ/C22x0^b1¼/C22y,or^a¼/C22y. Show that (2) becomes n/C22x^b0þX0 1X1^b1¼X01y: (3 ) By (7.33), Xc¼½I/C0(1=n)J/C138X1. Show that X0 cXc¼X01X1/C0(1=n)X01 JX1¼X01X01/C0n/C22x/C22x0.",
    "(3 ) By (7.33), Xc¼½I/C0(1=n)J/C138X1. Show that X0 cXc¼X01X1/C0(1=n)X01 JX1¼X01X01/C0n/C22x/C22x0. Similarly, show that X0 cy¼X01y/C0(1=n) X01Jy¼X01y/C0n/C22x/C22y. Now show that the normal equations in (7.34) for the centered model can be written in the form n00 0X0 cXc/C18/C19 ^a ^b1/C18/C19 ¼n/C22y X0cy/C18/C19 , which becomes n^a¼n/C22y, (4) X0 cXc^b1¼X0cy: (5) Thus (4) is the same as (1). Using /C22x0^b1¼^a/C0^b0, show that (5) is the same as (3).",
    "1¼X0cy: (5) Thus (4) is the same as (1). Using /C22x0^b1¼^a/C0^b0, show that (5) is the same as (3). (b)Using (2.50) with A11¼n,A12¼n/C22x0,A21¼n/C22x,A22¼X0 1X1, and X0cXc¼X01X1/C0n/C22x/C22x0, show that (X0X)/C01¼nj0X1 X0 1jX 1X1/C18/C19 /C01 ¼nn /C22x0 n/C22xX0 1X1/C18/C19 /C01 ¼1 nþ/C22x0(X0 cXc)/C01/C22x/C0/C22x0(X0 cXc)/C01 /C0(X0cXc)/C01/C22x (X0 cXc)/C010 B@1 CA552 ANSWERS AND HINTS TO THE PROBLEMS --- Page 562 --- and verify by multiplication that ( X0X)/C01X0X¼I.",
    "NSWERS AND HINTS TO THE PROBLEMS --- Page 562 --- and verify by multiplication that ( X0X)/C01X0X¼I. With this partitioned form of ( X0X)/C01, show that ^b¼(X0X)/C01X0y¼½(j,X1)0(j,X1)/C138/C01j0y X0 1y/C18/C19 ¼/C22y/C0/C22x0(X0 cXc)/C01X0cy (X0cXc)/C01X0cy ! ¼/C22y/C0^^b0 1/C22x (X0 cXc)/C01X0cy ! , which is the same as (7.37) and (7.38). 7.18 Substitute x1¼/C22x1,x2¼/C22x2,...,xk¼/C22xkin/C22y¼/C22yþ^b1(x1/C0/C22x1)þ/C1/C1/C1 þ ^bk(xk/C0/C22xk) to obtain ^y¼/C22y.",
    "¼/C22x2,...,xk¼/C22xkin/C22y¼/C22yþ^b1(x1/C0/C22x1)þ/C1/C1/C1 þ ^bk(xk/C0/C22xk) to obtain ^y¼/C22y. 7.19 y0y/C0^b0X0y¼y0y/C0(^b0,^b0 1)(j,X1)0y ¼y0y/C0(^b0,^b01)j0y X0 1y/C18/C19 ¼y0y/C0^b0n/C22y/C0^b0 1X01y ¼y0y/C0(/C22y/C0^b0 1/C22x)n/C22y/C0^b0 1X0 1y ¼y0y/C0n/C22y2/C0^b01(X01y/C0n/C22y/C22x) ¼Xn i¼1(yi/C0/C22y)2/C0^b0 1X0cy: 7.20 (a) By Theorem 2.2c(i), Xc0Xcis obtained as products of columns of Xc.B y (7.33), these products are of the form illustrated in the numerators of (7.41) and (7.42).",
    "of Xc.B y (7.33), these products are of the form illustrated in the numerators of (7.41) and (7.42). (b)By (7.43), the numerator of the second element of syxisPn i¼1 (xi2/C0/C22x2)(yi/C0/C22y). This can be written asP i(xi2/C0/C22x2)yi/C0P i(xi2/C0/C22x)/C22y, the second term of which vanishes. Note thatP i(xi2/C0/C22x2)yiis the second element of X0 cy.",
    "C22y, the second term of which vanishes. Note thatP i(xi2/C0/C22x2)yiis the second element of X0 cy. 7.21 (b) Expand the last term of ln L(b,s2) in (7.51) to obtain lnL(b,s2)¼/C0n 2ln(2p)/C0n2ln s2/C01 2s2(y0y/C02y0Xbþ^b0X0Xb): Then @lnL(b,s2) @b¼/C00/C00/C0n 2s2(0/C02X0yþ2X0Xb): Setting this equal to 0gives (7.48)ANSWERS AND HINTS TO THE PROBLEMS 553 --- Page 563 --- (c)Use lnL(b,s2) as in (7.51), to obtain @lnL(b,s2) @s2¼/C00/C0n 2s2þ1 2(s2)2(y/C0Xb)0(y/C0Xb): Setting this equal to 0 (and substituting bˆfrom @lnL=@b¼0) yields (7.49).",
    "2(s2)2(y/C0Xb)0(y/C0Xb): Setting this equal to 0 (and substituting bˆfrom @lnL=@b¼0) yields (7.49). 7.22 (ii) By (7.26), SSE ¼y0½I/C0X(X0X)/C01X0/C138y.S h o wt h a t I/C0X(X0X)/C01X0is idempotent of rank n/C0k/C01, given that Xisn/C2(kþ1) of rank kþ1. Then by Corollary 2 to Theorem 5.5a, SSE/ s2isx2(n/C0k/C01,l),where l¼m0Am=2s2¼(Xb)0½I/C0X(X0X)/C01X0/C138(Xb=2s2). Show that l¼0. (iii) Show that ( X0X)/C01X0½I/C0X(X0X)/C01X0/C138¼O.",
    "½I/C0X(X0X)/C01X0/C138(Xb=2s2). Show that l¼0. (iii) Show that ( X0X)/C01X0½I/C0X(X0X)/C01X0/C138¼O. Then by Corollary 1 to Theorem 5.6a, ^b¼(X0X)/C01X0yand SSE ¼y0½I/C0X(X0X)/C01X0/C138yare independent. 7.23 The two missing terms in (7.52) are /C0(y/C0X^b)0X(^b/C0b)/C0(^b/C0b)0X0(y/C0X^b) ¼/C0(X0y/C0X0X^b)0(^b/C0b)/C0(^b/C0b)0(X0y/C0X0X^b) ¼/C000(^b/C0b)/C0(^b/C0b)00: Note that X0y/C0X0Xˆb¼0by the normal equations X0Xˆb¼X0yin (7.8).",
    "0X^b) ¼/C000(^b/C0b)/C0(^b/C0b)00: Note that X0y/C0X0Xˆb¼0by the normal equations X0Xˆb¼X0yin (7.8). 7.25 ^b0X0y¼^b0^b1/C18/C190 (j,X1)0y¼(^b0,^b0 1)j0 X0 1/C18/C19 y¼n^b0/C22yþ^b0 1X01y: With ^b0¼/C22y/C0ˆb0 1/C22xfrom (7.38) and Xc¼½I/C0(1=n)J/C138X1from (7.33), this becomes ˆb0X0y¼n(/C22y/C0ˆb0 1/C22x)/C22yþˆb0 1X0 cþ1 nX0 1J/C18/C19 y ¼n/C22y2/C0n(ˆb0 1/C22x)/C22yþˆb0 1X0 cyþ1 nˆb0 1X0 1Jy: The last term can be written as (1 =n)ˆb0 1X0 1Jy¼(1=n)ˆb0 1X0 1jj0y¼(1=n) ˆb0 1n2/C22x/C22y,s ot h a t ˆb0X0y¼n/C22y2þˆb01X0 cy: 7.26 If^b1¼^b2¼/C1/C1/C1¼ ^bk¼0,then ˆb1¼0and ˆb0 1X0 cXcˆb1¼0.",
    "s ot h a t ˆb0X0y¼n/C22y2þˆb01X0 cy: 7.26 If^b1¼^b2¼/C1/C1/C1¼ ^bk¼0,then ˆb1¼0and ˆb0 1X0 cXcˆb1¼0. If yi¼^yi, i¼1,2,...,n,theny¼^y¼X^band ˆb0X0y/C0n/C22y2¼y0y/C0n/C22y2:Also see formulas below (7.61). 7.27 This follows from the statement following Theorem 7.3f, which notes that an additional xreduces SSE (see Problem 7.13). 7.28 (a) A set of full-rank linear transformations on the x’s can be represented by W¼XH, where His a nonsingular matrix.",
    "l-rank linear transformations on the x’s can be represented by W¼XH, where His a nonsingular matrix. Show that ^bw¼(W0W)/C01 W0y¼H/C01(X0X)/C01X0y¼H/C01^bx:Show that ˆb0 wW0y¼ˆb0xW0y:Then R2 w¼(ˆb0 wW0y/C0n/C22y2)=(y0y/C0n/C22y2)¼(ˆb0xX0y/C0n/C22y2)=(y0y/C0n/C22y2)¼R2 x:554 ANSWERS AND HINTS TO THE PROBLEMS --- Page 564 --- (b)Replacing ybyz¼cy,w eh a v e /C22z¼(1=n)j0z¼(1=n)j0cy¼c/C22yand ^bz¼(X0X)/C01X0z¼(X0X)/C01X0cy¼cˆby:Then R2 z¼^b0 zX0z/C0n/C22z2 z0z/C0n/C22z2¼cˆb0 yX0cy/C0n(c/C22y)2 (cy)0(cy)/C0n(c/C22y)2¼c2 c2R2 y: 7.30Xn i¼1^yi=n¼j0ˆy n¼j0X^b n¼j0(j,X1)^b0 ˆb1 !",
    "X0cy/C0n(c/C22y)2 (cy)0(cy)/C0n(c/C22y)2¼c2 c2R2 y: 7.30Xn i¼1^yi=n¼j0ˆy n¼j0X^b n¼j0(j,X1)^b0 ˆb1 ! n ¼n^b0 nþj0X1ˆb1 n¼^b0þ/C22x0^b1¼^b0þ(/C22y/C0^b0), by (7.38) 7.31 By (7.61), we obtain cos2u¼^y0^y/C0/C22y^y0j/C0/C22yj0^yþ/C22y2j0jPn i¼1(yi/C0/C22y)2¼^b0X0X^b/C0n/C22y2 P i(yi/C0/C22y)2, since j0yˆ¼ny¯by Problem 7.30. By (7.8), bˆ0X0Xbˆ¼bˆ0X0y.",
    "22y)2¼^b0X0X^b/C0n/C22y2 P i(yi/C0/C22y)2, since j0yˆ¼ny¯by Problem 7.30. By (7.8), bˆ0X0Xbˆ¼bˆ0X0y. 7.33 (a) Using ˆb¼(X0V/C01X)/C01X0V/C01y, expand ( y/C0Xˆb)0V/C01(y/C0Xˆb)t o obtain y0V/C01y/C0y0V/C01X(X0V/C01X)/C01X0V/C01y, the second term of which appears twice more with opposite signs. (b)Use Theorem 5.2a with A¼V/C01/C0V/C01X(X0V/C01X)/C01X0V/C01,S¼s2V, andm¼Xb.",
    "with opposite signs. (b)Use Theorem 5.2a with A¼V/C01/C0V/C01X(X0V/C01X)/C01X0V/C01,S¼s2V, andm¼Xb. 7.34 lnL(b,s2)¼/C0n 2ln( 2p)/C0n2ln s2/C012lnjVj/C01 2s2(y/C0Xb)0V/C01(y/C0Xb): Expand the last term to obtain 1 2s2(y0V/C01y/C0y0V/C01Xb/C0b0X0V/C01yþb0X0V/C01Xb): Differentiate to obtain @lnL(b,s2) @b¼/C00/C00/C00/C01 2s2(0/C02X0V/C01yþ2X0V/C01Xb), @lnL(b,s2) @s2¼/C00/C0n 2s2/C00þ1 2(s2)2(y/C0Xb)0V/C01(y/C0Xb): Setting these equal to 0and 0, respectively, gives the results.ANSWERS AND HINTS TO THE PROBLEMS 555 --- Page 565 --- 7.35 Show that J2¼nJ.",
    "vely, gives the results.ANSWERS AND HINTS TO THE PROBLEMS 555 --- Page 565 --- 7.35 Show that J2¼nJ. Then multiply VbyV21to get I,w h e r e VandV21are given by (7.67) and (7.68) respectively. 7.36 (a) j0V/C01j¼aj0(I/C0brJ)j¼aj0j/C0abrj0jj0j¼an/C0abr2n2¼an(1/C0brn). Substitute for aandbto show that this is equal to n/[1þ(n21)r]¼bn. Then j0V/C01Xc¼aj0(I/C0brJ)Xc¼aj0Xc/C0abrj0jj0Xc¼00because j0Xc¼ 00. Show that X0cV21Xc¼aX0cXc. 7.37 cov( ^b/C3)¼(X0X)/C01X0cov(y)X(X0X)/C01¼s2(X0X)/C01X0VX(X0X)/C01.",
    "0. Show that X0cV21Xc¼aX0cXc. 7.37 cov( ^b/C3)¼(X0X)/C01X0cov(y)X(X0X)/C01¼s2(X0X)/C01X0VX(X0X)/C01. 7.38 (a) (X0V/C01X)/C01X0V/C01y¼P i1 xin nP ixi0 B@1 CA/C01P iyi xiP iyi0 @1A ¼ 1 P ixiP i1 xi/C0n2P ixi/C0n /C0nP i1 xi0B@1 CAP iyi xiP iyi0 @1A: 7.40 (a) varP i(xi/C0/C22x)yiP i(xi/C0/C22x)2/C18/C19 ¼1 ½P i(xi/C0/C22x)2/C1382X i(xi/C0/C22x)2var(yi) ¼1 ½P i(xi/C0/C22x)2/C1382X i(xi/C0/C22x)2s2xi: 7.42 cov( ^b/C3 1)¼E½^b/C31/C0E(^b/C31)/C138½^b/C31/C0E(ˆb/C3 1)/C1380:Using E(^b/C3 1)¼b1þAb2from (7.80), we have ^b/C31/C0E(^b/C31)¼^b/C31/C0b1/C0(X0 1X1)/C01X01X2b2 ¼(X01X1)/C01X01y/C0(X01X1)/C01X01X2b2/C0b1 ¼(X01X1)/C01X01(y/C0X2b2)/C0b1:556 ANSWERS AND HINTS TO THE PROBLEMS --- Page 566 --- Show that this can be written as ( X0 1X1)/C01X01(y/C0X1b1/C0X2b2), so that cov( ^b/C3 1)¼E½(X0 1X1)/C01X01(y/C0Xb)(y/C0Xb)0X1(X01X1)/C01/C138: 7.43 Use Theorem 7.9a and note that x00b¼(x001,x002)b1 b2/C18/C19 ¼x001b1þx002b2: 7.44 Multiply out ( X01/C0A0X01)0G22(X01/C0A0X01), substitute A¼G/C01 11G12, and use (2.50).",
    "x002b2: 7.44 Multiply out ( X01/C0A0X01)0G22(X01/C0A0X01), substitute A¼G/C01 11G12, and use (2.50). 7.45 E(X0 01^b/C3 1)¼x0 01(b1þAb2)=X0 01b1: 7.46 var(x01^b1)/C21var(x01^b/C3 1) ¼s2(x0 01G11x01/C0x001G/C01 11x01) ¼s2x0 01(G11/C0G/C01 11)x01 /C210because G11/C0G/C01 11¼AB/C01Awhich is positive definite [see Theorem 7.9c(ii)] : 7.47 tr½I/C0X1(X01X1)/C01X01/C138¼tr(I)/C0tr½X01X1(X01X1)/C01/C138¼n/C0(pþ1), b0X0½I/C0X1(X0 1X1)/C01X01/C138Xb¼(b0 1X0 1þb0 2X0 2)½I/C0X1(X01X1)/C01X01/C138 (X1b1þX2b2): Show that three of the resulting four terms vanish, leaving the desired result.",
    "01/C138 (X1b1þX2b2): Show that three of the resulting four terms vanish, leaving the desired result. 7.48@Pn i¼1(yi/C0^b/C3 1xi)2 @^b/C31¼0; 2X i(yi/C0^b/C31xi)(/C0xi)¼0: 7.49 For the full model yi¼b0þb1xiþ1i,w eh a v e X¼1x1 ...... 1xn0 B@1 CA:ANSWERS AND HINTS TO THE PROBLEMS 557 --- Page 567 --- For the reduced model yi¼b/C3 1xiþ1/C3 i,w eh a v e X1¼(x1,x2,...,xn)0. Thus, X2¼(1,1,...,1)0.",
    "7 --- For the reduced model yi¼b/C3 1xiþ1/C3 i,w eh a v e X1¼(x1,x2,...,xn)0. Thus, X2¼(1,1,...,1)0. Then from (7.80), we obtain E(^b/C3 1)¼b1þAb2¼b1þ(X0 1X1)/C01X01X2b2 ¼b1þXn i¼1x2i !/C01Xn i¼1xi/C1b0: 7.50 (a) X¼1/C039 /C027 1/C024 /C08 1/C011 /C01 10 0 0 11 1 1 12 4 813 9 2 70 BBBBBBBB@1 CCCCCCCCA: The ﬁrst two columns constitute X 1, and the last two columns become X2.",
    "2 70 BBBBBBBB@1 CCCCCCCCA: The ﬁrst two columns constitute X 1, and the last two columns become X2. Then by (7.80), we obtain E(^b/C3 1)¼b1þ(X0 1X1)/C01X01X2b2: Show that this gives E(^b/C3 1)¼b0 b1/C18/C19 þ70 02 8/C18/C19 /C0128 0 0 196/C18/C19b2 b3/C18/C19 ¼b0 b1/C18/C19 þ40 07/C18/C19b2 b3/C18/C19 , so that E(^b/C3 0)¼b0þ4b2andE(^b/C31)¼b1þ7b3.",
    "3/C18/C19 ¼b0 b1/C18/C19 þ40 07/C18/C19b2 b3/C18/C19 , so that E(^b/C3 0)¼b0þ4b2andE(^b/C31)¼b1þ7b3. 7.51 X0 1X2:1¼X01½X2/C0X1(X01X1)/C01X01X2/C138¼X01X2/C0X01X1(X01X1)/C01X01X2: 7.52 In the partitioned form, the normal equations X0Xˆb¼X0ybecome X01 X02/C18/C19 (X1,X2)^b1^b2/C18/C19 ¼X01 X02/C18/C19 y, X01X1X01X2 X0 2X1X02X2/C18/C19^b1^b2/C18/C19 ¼X0 1y X0 2y/C18/C19 , X0 1X1^b1þX0 1X2^b2¼X01y, (1) X02X1^b1þX02X2^b2¼X02y: (2)558 ANSWERS AND HINTS TO THE PROBLEMS --- Page 568 --- Solve for ^b1from (1) to obtain ^b1¼(X0 1X1)/C01(X01y/C0X01X2^b2), and substitute this into (2) to obtain ½X0 2X2/C0X02X1(X01X1)/C01X01X2/C138^b2¼X02y/C0X02X1(X01X1)/C01X01y:(3) Multiplying (7.98) by X02.1, we obtain ^b2¼(X0 2:1X2:1)/C01X02:1½^y/C0^y(X1)/C138.",
    "01X1)/C01X01y:(3) Multiplying (7.98) by X02.1, we obtain ^b2¼(X0 2:1X2:1)/C01X02:1½^y/C0^y(X1)/C138. Show that this is the same as (3).",
    ") by X02.1, we obtain ^b2¼(X0 2:1X2:1)/C01X02:1½^y/C0^y(X1)/C138. Show that this is the same as (3). 7.53 (a)^b¼1:0150 /C00:0286 0:2158 /C04:3201 8:97490 BBBB@1 CCCCA,s 2¼7:4529 : (b)s2(X0X)/C01¼3:4645 :0145 /C0:0638 /C01:1620 1 :0723 0:0145 :0082 /C0:0019 /C00:1630 0 :0784 /C00:0638 /C0:0019 :0046 0 :1039 /C00:1250 /C01:1620 /C0:1630 :1039 8 :1280 /C07:2045 1:0723 :0784 /C0:1250 /C07:2045 7 :68750 BBBB@1 CCCCA: (c) ^b1¼S/C01 xxsyx¼380 :6684 237 :6684 27 :0709 25 :3549 237 :6684 247 :5071 17 :8557 18 :3362 27:0709 17 :8557 2 :1090 1 :9909 25:3549 18 :3362 1 :9909 1 :93690 BBB@1 CCCA/C01 /C2151 :0121 134 :0444 11:8365 12:01400 BBB@1 CCCA¼/C00:0286 0:2158 /C04:3201 8:97490 BBB@1 CCCA: ^ b0¼/C22y/C0^b0 1/C22x¼31:125/C0(/C00:0286 ,0:2158 ,/C04:3201 ,8:9749) /C257:9063 55:9063 4:4222 4:32380 BBB@1 CCCA¼1:0150 : (d) R 2¼:9261 ,R2 a¼:9151 :ANSWERS AND HINTS TO THE PROBLEMS 559 --- Page 569 --- 7.54 (a)^b¼332 :111 /C01:546 /C01:425 /C02:2370 BB@1 CCA,s2¼5:3449 : (b)cov( ^b)¼s2(X0X)/C01 ¼5:344965:37550 /C0:33885 /C0:31252 /C0:02041 /C00:33885 :00184 :00127 /C0:00043 /C00:31252 :00127 :00408 /C0:00176 /C00:02041 /C0:00043 /C0:00176 :021610 BB@1 CCA: (c)R2¼:9551 ,R2 a¼:9462 : (d)^b¼964 :929 /C07:442 /C011:508 /C02:140 0:012 0:033 /C00:294 0:054 0:038 /C00:1020 BBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCA,s 2¼5:1342 : (e) R2¼:9741 ,R2 a¼:9483 : 7.55 (a)^b¼:6628 :7803 :5031 /C017:10020 BB@1 CCAs2¼67:9969 : (b)^b1¼S/C01 xxsyx¼504 :2783 9 :4698 /C01:7936 9:4698 201 :9399 1 :0617 /C01:7936 1 :0617 0 :02350 B@1 CA/C01428 :9086 90:8333 /C01:26670 B@1 CA ¼0:7803 0:5031 /C017:10020 B@1 CA ^b0¼/C22y/C0^b0 1/C22x¼41:1553 /C0(:7803 ,:5031 ,/C017:1002) /C242:945 20:169 0:1850 B@1 CA¼:6628 : (c) R2¼:8667 ,R2 a¼:8534 :560 ANSWERS AND HINTS TO THE PROBLEMS --- Page 570 --- Chapter 8 8.1 Substitute ^b1¼(X0 cXc)/C01X0cyinto SSR ¼^b0 1X0 cy.",
    "TO THE PROBLEMS --- Page 570 --- Chapter 8 8.1 Substitute ^b1¼(X0 cXc)/C01X0cyinto SSR ¼^b0 1X0 cy. 8.2 (a)HcI/C01 nJ/C18/C19 ¼Hc/C01nH cJ ¼Xc(X0 cXc)/C01X0c/C01 nXc(X0 cXc)/C01X0cjj0 ¼Xc(X0cXc)/C01X0c/C0O since X0 cjj0¼Oj0¼O: (b) Show that H2c¼Hc, where Hc¼Xc(X0cXc)/C01X0c. Then, since Hcis idempotent, rank ( Hc)¼tr(Hc) by Theorem 2.13d. The centered matrix Xcisn/C2kof rank k[see (7.33)].",
    "idempotent, rank ( Hc)¼tr(Hc) by Theorem 2.13d. The centered matrix Xcisn/C2kof rank k[see (7.33)]. (c)I/C01 nJ/C0Hc/C18/C192 ¼I/C01nJ/C18/C19 2 /C0I/C01nJ/C18/C19 H c/C0HcI/C01nJ/C18/C19 þH 2 c ¼I/C01 nJ/C0Hc/C0HcþHc: Then rank I/C01nJ/C0H c/C18/C19 ¼trI/C01nJ/C0H c/C18/C19 : (d)HcI/C01nJ/C0H c/C18/C19 ¼HcI/C01nJ/C18/C19 /C0H 2 c¼Hc/C0Hc¼O: 8.3m0Hcm¼b0X0Xc(X0 cXc)/C01X0cXb:By (7.32), we have Xb¼ajþXcb1. Hencem0Hcm¼(aj0þb0 1X0 c)Xc(X0cXc)/C01X0c(ajþXcb1).",
    "(X0 cXc)/C01X0cXb:By (7.32), we have Xb¼ajþXcb1. Hencem0Hcm¼(aj0þb0 1X0 c)Xc(X0cXc)/C01X0c(ajþXcb1). Three of the resulting four terms vanish because j0Xc¼00(see Problem 7.16). 8.4 By corollary 2 to Theorem 5.5a, SSE =s2isx2(n/C0k/C01,l2). Alsol2¼ m0½I/C01 nJ/C0Hc/C138m=s2¼(aj0þb0 1X0 c)½I/C01 nJ/C0Hc/C138(ajþXcb1)=s2. Show that all terms involving either j0½I/C01 nJ/C138orj0Hcvanish. Show that b0 1X0 c½I/C01 nJ/C138Xcb1¼b0 1X0 cXcb1and thatb0 1X0 cHcXcb1¼b0 1X0 cXcb1.",
    "8orj0Hcvanish. Show that b0 1X0 c½I/C01 nJ/C138Xcb1¼b0 1X0 cXcb1and thatb0 1X0 cHcXcb1¼b0 1X0 cXcb1. 8.6 Most of these results are proved in Problem 5.32, with the adjustment kþ 1¼p.",
    "Xcb1¼b0 1X0 cXcb1. 8.6 Most of these results are proved in Problem 5.32, with the adjustment kþ 1¼p. 8.7 By (8.14), HH 1¼X(X0X)/C01X0X1(X01X1)/C01X01¼X1(X01X1)/C01X01¼H1:ANSWERS AND HINTS TO THE PROBLEMS 561 --- Page 571 --- 8.9 m0(H/C0H1)m¼b0X0(H/C0H1)Xb ¼b0X0X(X0X)/C01X0Xb/C0b0X0X1(X0 1X1)/C01X01Xb ¼b0X0Xb/C0b0X0X1(X0 1X1)/C01X01Xb ¼(b01X01þb02X02)(X1b1þX2b2) /C0(b0 1X0 1þb0 2X0 2)X1(X01X1)/C01X01(X1b1þX2b2): 8.10 Denote the matrix X0XbyG.",
    "b02X02)(X1b1þX2b2) /C0(b0 1X0 1þb0 2X0 2)X1(X01X1)/C01X01(X1b1þX2b2): 8.10 Denote the matrix X0XbyG. Then in partitioned form, we have G¼X0X¼(X1,X2)0(X1,X2)¼X0 1 X02/C18/C19 (X1,X2) ¼X01X1X01X2 X02X1X02X2/C18/C19 ¼G11G12 G21G22/C18/C19 : If we denote the four corresponding blocks of G21byGij, then by (2.48), G22¼(G222G21G21 11G12)21. By Theorem 2.6e, G21is positive deﬁnite. By Theorem 2.6f, G22is positive deﬁnite.",
    "22G21G21 11G12)21. By Theorem 2.6e, G21is positive deﬁnite. By Theorem 2.6f, G22is positive deﬁnite. By Theorem 2.6e, ( G22)21¼ G222G21G21 11G12¼X02X22X02X1(X01X1)21X01X2is positive deﬁnite. 8.11 By Theorem 8.2b(ii), SS( b2jb1)=s2isx2(h,l1). Then E½SS(b2jb1)=s2/C138¼ hþ2l1by (5.23). 8.12s2þb2 k½x0 kxk/C0x0kX1(X0 1X1)/C01X01xk/C138. 8.13 For the reduced model y¼b/C3 0jþ1/C3,w eh a v e ^b/C30¼(j0j)/C01j0y¼ (1=n)Pn i¼1yi¼/C22yand SS(b/C3 0)¼^b/C30j0y¼/C22yP iyi¼n/C22y2.",
    "1/C3,w eh a v e ^b/C30¼(j0j)/C01j0y¼ (1=n)Pn i¼1yi¼/C22yand SS(b/C3 0)¼^b/C30j0y¼/C22yP iyi¼n/C22y2. 8.14 After multiplying to obtain eight terms, three of the ﬁrst four terms cancel three of the last four terms. For example, the second of the last four is ^b0 1X0 1X1A^b2¼b0 1X0 1X1(X01X1)/C01X01X2^b2¼^b0 1X0 1X2^b2, which is the same as the second of the ﬁrst four terms.",
    "0 1X0 1X1(X01X1)/C01X01X2^b2¼^b0 1X0 1X2^b2, which is the same as the second of the ﬁrst four terms. 8.15 Add and substract n/C22y2in both numerator and denominator of (8.24) and then divide numerator and denominator by y0y/C0n/C22y2. 8.16 Express SSH as a quadratic form in yby substituting ^b¼(X0X)/C01X0y. Then use Corollary 1 to Theorem 5.6b. 8.17 This follows from Corollary 1 to Theorem 2.6b. 8.18 By the answer to Problem 7.28, ^b0 wW0y¼^b0X0y.",
    "This follows from Corollary 1 to Theorem 2.6b. 8.18 By the answer to Problem 7.28, ^b0 wW0y¼^b0X0y. Thus SSE ¼y0y/C0^b0X0yis invariant to the full-rank transformation W¼XH. For the numerator of (8.27), we note that Cis transformed the same way as is X,s o thatCw^bw¼CHH/C01^bx¼Cˆb.",
    "numerator of (8.27), we note that Cis transformed the same way as is X,s o thatCw^bw¼CHH/C01^bx¼Cˆb. Thus the numerator of (8.27) becomes (Cw^bw)0½Cw(W0W)/C01C0 w/C138/C01Cw^bw¼(C^b)0{CH½(XH)0XH/C138/C01(CH)0}/C01C^b ¼(C^b)0{CH½H0(X0X)H/C138/C01H0C0}/C01C^b562 ANSWERS AND HINTS TO THE PROBLEMS --- Page 572 --- ¼(C^b)0½CHH/C01(X0X)/C01(H0)/C01H0C0/C138/C01C^b ¼(C^b)0½C(X0X)/C01C0/C138/C01C^b: Show that the transformation z¼cyalso leaves Funchanged. 8.19 (a) See Section 2.14.3 .@u @l¼Cb.",
    "C^b: Show that the transformation z¼cyalso leaves Funchanged. 8.19 (a) See Section 2.14.3 .@u @l¼Cb. Setting this equal to 0gives Cbˆc¼0. (b) u¼y0y/C0y0Xb/C0b0X0yþb0X0Xbþl0Cb ¼y0y/C02b0X0yþb0X0Xbþl0Cb, ¼@u @b¼0/C02X0yþ2X0XbþC0l: Setting this equal to 0gives ^bc¼(X0X)/C01X0y/C01 2(X0X)/C01C0l ¼^b/C01 2(X0X)/C01C0l: (1) (c) C^bc¼C^b/C012C(X0X)/C01C0l¼0, l¼2½C(X0X)/C01C0/C138/C01C^b: Substituting this into (1) in part (b) gives the result.",
    "0X)/C01C0l¼0, l¼2½C(X0X)/C01C0/C138/C01C^b: Substituting this into (1) in part (b) gives the result. 8.20 ^b0 cX0X^bc¼^b0cX0X{^b0/C0(X0X)/C01C0½C(X0X)/C01C0/C138/C01C^b} ¼^b0cX0X^b/C0^b0cC0½C(X0X)/C01C0/C138/C01C^b ¼^b0 cX0y/C000½C(X0X)/C01C0/C138/C01C^b: Show that ^b0 cC0¼00. 8.21 Substituting ^b0 cin (8.30) into SSH ¼^b0X0y/C0^b0cX0yin (8.31) gives SSH ¼^b0X0y/C0{^b0/C0^b0C0½C(X0X)/C01C0/C138/C01C(X0X)/C01}X0y ¼^b0X0y/C0^b0X0yþ^b0C0½C(X0X)/C01C0/C138/C01C^b, since ^b¼(X0X)/C01X0y.",
    "C01C0/C138/C01C(X0X)/C01}X0y ¼^b0X0y/C0^b0X0yþ^b0C0½C(X0X)/C01C0/C138/C01C^b, since ^b¼(X0X)/C01X0y. 8.22 In Theorem 8.4e(ii), we have cov( ^bc)¼cov{I/C0(X0X)/C01C0½C(X0X)/C01C0/C138/C01C}^b ¼cov(A^b)¼Acov( ^b)A0¼s2A(X0X)/C01A0: Show that A(X0X)/C01A0¼(X0X)/C01/C0(X0X)/C01C0½C(X0X)/C01C0/C138/C01C(X0X)/C01.ANSWERS AND HINTS TO THE PROBLEMS 563 --- Page 573 --- 8.23 Replacebˆby (X0X)21X0yin SSH in Theorem 8.4d(ii) to obtain SSH ¼ [C(X0X)21X0y2t]0[C(X0X)21C0]21[C(X0X)21X0y2t].",
    "(X0X)21X0yin SSH in Theorem 8.4d(ii) to obtain SSH ¼ [C(X0X)21X0y2t]0[C(X0X)21C0]21[C(X0X)21X0y2t]. Show that C(X0X)21X0y2t¼C(X0X)21X0[y2XC0(CC0)21t], so that SSH becomes SSH¼[y2XC0(C0C)21t]0A[y2XC0(CC0)21t], where A¼X(X0X)21C0 [C(X0X)21C0]21C(X0X)21X0. Show that SSE ¼[y2XC0(CC0)21t]0B[y2 XC0(CC0)21t], where B¼I2X(X0X)21X0. Show that AB¼O. Show that y2XC0(CC0)21tisNn[Xb2XC0(CC0)21t,s2I]. Then by Corollary 1 to Theorem 5.6b, SSH and SSE are independent. 8.24 See Section 2.14.3.",
    "21t,s2I]. Then by Corollary 1 to Theorem 5.6b, SSH and SSE are independent. 8.24 See Section 2.14.3. Follow the steps in Problems 8.19 using u¼(y/C0Xb)0(y/C0Xb)þl0(Cb/C0t) ¼y0y/C02b0Xyþb0X0Xbþl0(Cb/C0t): Differentiating with respect to landb, we obtain @u @l¼Cb/C0t, @u @b¼0/C02X0yþ2X0XbþC0l: Setting those equal to 0gives C^bc¼tand ^bc¼^b/C01 2(X0X)/C01C0l: (1) Multiplying (1) by Cand using C^bc¼tgivesl¼2[C(X0X)21 C0]21(C^b2t). Substituting this into (1) gives the result.",
    "(1) by Cand using C^bc¼tgivesl¼2[C(X0X)21 C0]21(C^b2t). Substituting this into (1) gives the result. 8.25 By Theorem 8.4d, we can use the general linear hypothesis test. Use a0¼ (0,..., 0,1) in place of Cin (8.30) to obtain ^bc¼^b/C0(X0X)/C01a½a0(X0X)/C01a/C138/C01a0^b ¼^b/C0(X0X)/C01aa0^b gkk: By (2.37), ( X0X)21ais a linear combination of the columns of ( X0X)21.",
    "^b ¼^b/C0(X0X)/C01aa0^b gkk: By (2.37), ( X0X)21ais a linear combination of the columns of ( X0X)21. Thus ^bc¼^b/C0gk^bk gkk, where gkkis the kth diagonal element of ( X0X)21andgkis the kth column of564 ANSWERS AND HINTS TO THE PROBLEMS --- Page 574 --- (X0X)21. Substituting this expression for bˆcinto ^b0X0y/C0^b0 cX0y, we obtain ^b0X0y/C0^b0 cX0y¼^b0X0y/C0 ^b0X0y/C0^bk gkkg0 kX0y ! ¼^bk^bk gkk¼^b2 k gkk, since ^g0 kX0yis the kth element of bˆ.",
    "y¼^b0X0y/C0 ^b0X0y/C0^bk gkkg0 kX0y ! ¼^bk^bk gkk¼^b2 k gkk, since ^g0 kX0yis the kth element of bˆ. 8.26P/C0ta=2,n/C0k/C01/C20a0^b/C0a0b sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ a0(X0X)/C01ap /C21ta=2,n/C0k/C01\"# ¼1/C0a Solve the inequality for a0b. 8.27 In the answer to Problem 7.17b, we have (X0X)/C01¼1 nþ/C22x0 1(X0cXc)/C01/C22x1/C0/C22x0 1(X0 cXc)/C01 /C0(X0cXc)/C01/C22x1 (X0 cXc)/C010 @1A, where /C22x 1¼(/C22x1,/C22x2,...,/C22xk)0.",
    "2x0 1(X0 cXc)/C01 /C0(X0cXc)/C01/C22x1 (X0 cXc)/C010 @1A, where /C22x 1¼(/C22x1,/C22x2,...,/C22xk)0. Using this form of ( X0X)21, show that X0 0(X0X)/C01x0¼(1,x001)(X0X)/C011 x01/C18/C19 ¼1 nþ(x01/C0/C22x1)0(X0 cXc)/C01(x01/C0/C22x1): 8.28 In this case, x012x¯1¼x02x¯and Xc¼x1/C0/C22x x2/C0/C22x ... xn/C0/C22x0 BBB@1 CCCA: 8.29 E(y 0/C0^y0)¼E(y0/C0x0 0^b)¼x00b/C0x00b¼0.",
    "Xc¼x1/C0/C22x x2/C0/C22x ... xn/C0/C22x0 BBB@1 CCCA: 8.29 E(y 0/C0^y0)¼E(y0/C0x0 0^b)¼x00b/C0x00b¼0. By (8.59), var ( y0/C0^y0)¼s2½1þ x00(X0X)/C01x0/C138.T h e r e f o r e ,( y0/C0^y0)=ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ s2½1þx0 0(X0X)/C01x0q /C138isN(0,1) by Theorems 7.6b(i) and 4.4a(i). By Theorem 7.6b(ii), ( n/C0k/C01)s2=s2is x2(n/C0k/C01). By Theorem 7.6b(iii), ^y0ands2are independent.",
    "Theorem 7.6b(ii), ( n/C0k/C01)s2=s2is x2(n/C0k/C01). By Theorem 7.6b(iii), ^y0ands2are independent. Use (5.33) to show that t¼(y0/C0^y0)=sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01x0q is distributed as t(n/C0k/C01). 8.30 (a) Show that E(/C22y0/C0^y0)¼E(/C22y0/C0x0 0^b)¼0 and that var( /C22y0/C0^y0)¼ s2½1=qþx00(X0X)/C01x0/C138.",
    "w that E(/C22y0/C0^y0)¼E(/C22y0/C0x0 0^b)¼0 and that var( /C22y0/C0^y0)¼ s2½1=qþx00(X0X)/C01x0/C138. For the remaining steps, follow the answer to Problem 8.29.ANSWERS AND HINTS TO THE PROBLEMS 565 --- Page 575 --- 8.31 Invert (take the reciprocal of) all three numbers of the inequality (which changes the directions of the two inequalities) and multiply by ( n/C0k/C01)s2. 8.32 Lety0¼(y01,y02,...,y0d)0be the vector of dfuture observations, and let Xd¼x0 01 ...",
    "C0k/C01)s2. 8.32 Lety0¼(y01,y02,...,y0d)0be the vector of dfuture observations, and let Xd¼x0 01 ... x0 0d0 B@1 CA be the d/C2(kþ1) matrix of corresponding values x01,x02,...,x0d. Show that y0/C0Xd^bisNd(0,s2Vd), where Vd¼IdþXd(X0X)/C01X0 dandXis the X matrix for the original nobservations. Show that (y0/C0Xd^b)0V/C01 d(y0/C0Xd^b) ds2isF(d,n/C0k/C01) [for the distribution of the numerator, see (5.27) or Problem 5.12e].",
    "y0/C0Xd^b) ds2isF(d,n/C0k/C01) [for the distribution of the numerator, see (5.27) or Problem 5.12e]. By Theorem 8.5 and (8.71) with kþ1¼d, we have the simultaneous intervals /C0sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ da0V/C01 daFa,d,n/C0k/C01q /C20a0(y0/C0Xd^b)/C20sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃda 0V/C01 daFa,d,n/C0k/C01,q which hold for all awith conﬁdence coefﬁcient 1 2a.",
    "ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃda 0V/C01 daFa,d,n/C0k/C01,q which hold for all awith conﬁdence coefﬁcient 1 2a. Setting a0 1¼(1,0,...,0),...,a0d¼(0,...,0,1), we obtain x0 0i^b/C0sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ d½1þx0 0i(X0X)/C01x0i/C138Fa,d,n/C0k/C01q /C20y0i/C20x0 0i^bþsﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ d½1þx0 0i(X0X)/C01x0i/C138Fa,d,n/C0k/C01q : These intervals hold with conﬁdence coefﬁcient at least 1 2a.",
    "0 0i(X0X)/C01x0i/C138Fa,d,n/C0k/C01q : These intervals hold with conﬁdence coefﬁcient at least 1 2a. 8.33 For (8.77), we have @lnL(0,s2) @s2¼@ @s2ln1 (2ps2)n=2e/C0y0y=2s2/C20/C21 ¼@ @s2/C0n 2ln2p/C0n2ln s2/C0y0y=2s2hi ¼/C00/C0n 2s2þy0y 2(s2)2¼0; ^s2 0¼y0y n:566 ANSWERS AND HINTS TO THE PROBLEMS --- Page 576 --- For (8.78), we have max HoL(b,s2)¼max L(0,s2)¼L(0,^s2 0) ¼1 ð2p^s2 0)n=2e/C0y0y=2^s2 0 ¼1 ð2pÞn=2ðy0y=nÞn=2e/C0y0y=2ðy0y=nÞ ¼nn=2e/C0n=2 ð2pÞn=2ðy0yÞn=2 For (8.79), we have (y/C0X^b)0(y/C0X^b) y0y\"#n=2 ¼y0y/C0^b0X0y y0y/C0^b0X0yþ^b0X0y\"#n=2 ¼1 1þ^b0X0y=y0y/C0^b0X0y\"#n=2 : 8.34 Expanding ( y2Xb)0(y2Xb), we have v¼/C0(n=2)ln(2p)/C0(n=2)lns2/C0 ½y0y/C02y0Xbþb0X0Xb/C138=2s2þl0Cb.",
    "Expanding ( y2Xb)0(y2Xb), we have v¼/C0(n=2)ln(2p)/C0(n=2)lns2/C0 ½y0y/C02y0Xbþb0X0Xb/C138=2s2þl0Cb. Differentiation with respect to bgives the result. 8.35 From (8.80), we obtain ^b0¼(X0X)/C01X0yþ^s2 0(X0X)/C01C0l: (1) Multiplying bˆ0byCgives C^b0¼C^bþ^s2 0C(X0X)/C01C0l.",
    "obtain ^b0¼(X0X)/C01X0yþ^s2 0(X0X)/C01C0l: (1) Multiplying bˆ0byCgives C^b0¼C^bþ^s2 0C(X0X)/C01C0l. By (8.77), C^b0¼0, and we have l¼/C0 ½C(X0X)/C01C0/C138/C01C^b ^s2 0: Substituting this into (1) gives ^b0¼^b/C0(X0X)/C01C0½C(X0X)/C01C0/C138/C01C^b, where ^b¼(X0X)/C01X0y.ANSWERS AND HINTS TO THE PROBLEMS 567 --- Page 577 --- 8.36 Substituting (8.83) into (8.84) gives (y/C0X^b0)0(y/C0Xb0)¼y/C0X^bþX(X0X)/C01C0½C(X0X)/C01C0/C138/C01C^bno0 /C2y/C0X^bþX(X0X)/C01C0½C(X0X)/C01C0/C138/C01C^bno ¼(y/C0X^b)0(y/C0X^b)þ0þ0þ(C^b)0½C(X0X)/C01C0/C138/C01C^b: Show that the second and third terms vanish and the fourth term is equal to (C^b)0½C(X0X)/C01C0/C138/C01C^bas indicated.",
    "and third terms vanish and the fourth term is equal to (C^b)0½C(X0X)/C01C0/C138/C01C^bas indicated. 8.37 (a) Source df SS MS F pValue Due tob1 4 2520.2724 630.0681 84.540 7:216/C210/C015 Error 27 201.2276 7.4529 Total 31 2721.5000 (This pvalue would typically be reported as p,.0001). The Fvalue can also be found using (8.23): F¼R2=k (1/C0R2)=(n/C0k/C01)¼:9261 =4 (1/C0:9261) =27¼84:540 : (b)For the reduced model yi¼b/C3 0þb/C32xi2þb/C34xi4þ1/C3 i, we obtain ^b/C30 1X0 1y/C0n/C22y2¼2483 :1136.",
    "the reduced model yi¼b/C3 0þb/C32xi2þb/C34xi4þ1/C3 i, we obtain ^b/C30 1X0 1y/C0n/C22y2¼2483 :1136. From the analysis of variance table in part ( a), we have ^b0X0y/C0n/C22y2¼2520 :2724. The difference is ^b0X0y/C0^b/C30 1X01y¼37:1588. By (8.17), we have F¼37:1588 =2 7:4529¼2:4929 , with p¼.102.",
    "rence is ^b0X0y/C0^b/C30 1X01y¼37:1588. By (8.17), we have F¼37:1588 =2 7:4529¼2:4929 , with p¼.102. (c)The values of tj¼^bj=sﬃﬃﬃﬃﬃgjjpin (8.39) are given in the following table: Variable ^bjsﬃﬃﬃﬃﬃgjjptj pValue x1 20.0286 .0906 20.316 .755 x2 0.2158 .0677 3.187 .00362 x3 24.3201 2.8510 21.515 .141 x4 8.9749 2.7726 3.237 .00319 Comparing each (two-sided) pvalue to .05, we would reject H0:bj¼0 forb2andb4.",
    "49 2.7726 3.237 .00319 Comparing each (two-sided) pvalue to .05, we would reject H0:bj¼0 forb2andb4. Comparing each pvalue to the Bonferroni value of :05=4¼:0125, we reject H0forb2andb4also.568 ANSWERS AND HINTS TO THE PROBLEMS --- Page 578 --- (d) To test H0:b1¼b2¼12b3¼12b4, we write H0:Cb¼0where C¼01 /C0100 00 1 /C012 0 00 0 1 /C010 @1A: We test H 0using (8.26). For H01:b1¼b2,H02:b2¼12b3, and H03:b3¼b4, we test each row of Cseparately using (8.37).",
    "sing (8.26). For H01:b1¼b2,H02:b2¼12b3, and H03:b3¼b4, we test each row of Cseparately using (8.37). For H04:b1¼b2andb3¼b4, we use the ﬁrst and third rows of Cand test with (8.26). The results are as follows: H0F¼236 :3268 =3 201 :2276 =27¼10:5698 p¼0:0000899 H01F¼26:7486 7:4529¼3:5890 p¼0:0689 H02F¼17:2922 7:4529¼2:3202 p¼0:139 H03F¼43:5851 7:4529¼5:8481 p¼0:0226 H04F¼206 :2962 =2 7:4529¼13:8400 p¼0:0000729 (e)Forv¼27, we have t.025,27¼2.0518 and t.00625,27 ¼2.6763.",
    "¼206 :2962 =2 7:4529¼13:8400 p¼0:0000729 (e)Forv¼27, we have t.025,27¼2.0518 and t.00625,27 ¼2.6763. Using (8.45) and (8.65) and the values in the answer to part (c), we obtain the following lower and upper conﬁdence limits: ^bj+t:025sﬃﬃﬃﬃﬃgjjp^bj+t:00625 sﬃﬃﬃﬃﬃg jjp 20.2145 0.1573 20.2711 0.2139 0.0769 0.3548 0.0346 0.3970 210.1698 1.5297 211.9500 3.3099 3.2859 14.6639 1.5546 16.3952 8.38 (a) Source df SS MS Fp Value Due tob1 3 13266.8574 4422.2858 65.037 3.112 /C210213 Error 30 2039.9062 67.9969 Total 33 15306.7636 TheFvalue can also be found using (8.23): F¼R2 (1/C0R2)=(n/C0k/C01)¼:8667 =3 (1:8667) =30¼65:037ANSWERS AND HINTS TO THE PROBLEMS 569 --- Page 579 --- (b)The values of tj¼^bj=sﬃﬃﬃﬃﬃgjjpin (8.39) are given in the following table: Variable ^bjﬃﬃﬃﬃﬃgjjptj pValue x1 0.7803 0.0810 9.631 1.09 /C210210 x2 0.5031 0.1251 4.020 0.000361 x3 217.1002 13.5954 21.258 0.218 Comparing each (two-sided) pvalue to .05, we would reject H0:bj¼0 forb1andb2.",
    "2 13.5954 21.258 0.218 Comparing each (two-sided) pvalue to .05, we would reject H0:bj¼0 forb1andb2. Comparing each pvalue to the Bonferroni value of .05/3¼.0167, we reject H0forb1andb2also. (c)Forv¼30, we have t.025,30¼2.0423 and t.00833,30 ¼2.5357.",
    ".05/3¼.0167, we reject H0forb1andb2also. (c)Forv¼30, we have t.025,30¼2.0423 and t.00833,30 ¼2.5357. Using (8.47) and (8.67) and the values in the answer to part (b), we obtain the following lower and upper conﬁdence limits: ^bj+t:025sﬃﬃﬃﬃﬃgjjp^bj+t:00833 sﬃﬃﬃﬃﬃg jjp 0.6148 0.9457 0.5748 0.9857 0.2475 0.7587 0.1858 0.8204 244.8656 10.6652 251.5745 17.3740 (d)Using (8.52), we have x0 0^b+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ x0 0(X0X)/C01X0q 18:9103+2:0423(8 :2460)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :1615p 18:9103+6:7677 ; (12:1426 ,25:6780) (e)Using (8.61), we have x0 0^b+ta=2,n/C0k/C01sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01X0q 18:9103+2:0423(8 :2460)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1:1615p 18:9103+18:1496 ; (:7609 ,37:0599) 8.39 (a) x0 0^b+t:025,15sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ x0 0(X0X)/C01X0q 55:2603+(2:1314)(4 :0781)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ :19957p 55:2603+3:8849 , (51:3754 ,59:1451)570 ANSWERS AND HINTS TO THE PROBLEMS --- Page 580 --- (b)x0 0^b+t:025,15sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1þx0 0(X0X)/C01X0q 55:2603+(2:1314)(4 :0781)ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1:19975p 55:2603+9:5205 , (45:7394 ,64:7811) (c)Using C¼01 /C010 00 2 /C01/C18/C19 , we obtain Cˆb¼:1116 /C0:4478/C18/C19 , C(X0X)/C01C0¼:003366 /C0:006943 /C0:006943 :044974/C18/C19 ,F¼:1577 ,p¼:856 : 8.40 (a) ^b0X0y/C0n/C22y2/C0(^b/C30 1X0 1y/C0n^y2)¼1741 :1233 /C01707 :1580 , F¼5:6609 5:1343¼1:1026 ,p¼:430 (b) F¼:9741 /C0:9551 =6 1/C0:9741 =9¼1:1026 (c) b0:332 :1110+39:8430 (292 :2679 ,371 :9540) ; b1:/C01:5460+:21109 (/C01:7571 ,/C01:3349) , b2:/C01:4246+:3147 (1:7393 ,/C01:1098) , b3: /C02:2374+:7243 (2:9617 ,/C01:5130) (d) b1:/C01:5460+:2668 (/C01:8127 ,/C01:2792) ; b2:/C01:4246+:3977 (/C01:8223 ,/C01:0268) , b3:/C02:2347+:9154 (/C03:1528 ,/C01:3220) (e) 20:2547+2:2024 (18:0524 ,22:4571) (f)20:2547+5:3975 (14:8573 ,25:6522)ANSWERS AND HINTS TO THE PROBLEMS 571 --- Page 581 --- Chapter 9 9.1 (a) By (9.5), we obtain E(^1)¼E½(I/C0H)y/C138¼(I/C0H)E(y) ¼½I/C0X(X0X0)/C01X0/C138Xb¼Xb/C0Xb: (b)We ﬁrst note that I2His symmetric and idempotent [see Theorem 2.13e(i)].",
    "0)/C01X0/C138Xb¼Xb/C0Xb: (b)We ﬁrst note that I2His symmetric and idempotent [see Theorem 2.13e(i)]. Then by Theorem 3.6d(i), we obtain cov( ^1)¼cov½(I/C0H)y/C138¼(I/C0H)s2I(I/C0H)0 ¼s2(I/C0H)2¼s2(I/C0H): (c)By Theorem 3.6d(ii), we have cov( ˆ1,y)¼cov½(I/C0H)y,Iy/C138 ¼(I/C0H)(s2I)I¼s2(I/C0H): (d) cov( ˆ1,y)¼cov½(I/C0H)y,Hy/C138¼(I/C0H)(s2I)H ¼s2(H/C0H2)¼s2(H/C0H) (e)/C22^1¼Pn i¼1^1i=n¼ˆ10j=n:By (9.4) and (9.5), ^10j¼y0(I/C0H)j¼y0(j/C0j): (f)By (9.5), ˆ10y¼y0(I/C0H)y: (g)By (9.2) and (9.5), ˆ10yˆ¼y0(I/C0H)Hy¼y0(H/C0H2)y¼y0(H/C0H)y: (h)By (9.3) and (9.5), ˆ10X¼y0(I/C0H)X¼y0(X/C0HX)¼y0(X/C0X): 9.2 (a)d dh(h/C0h2)¼1/C02h¼0,h¼1 2,12/C012/C18/C19 2 ¼14:572 ANSWERS AND HINTS TO THE PROBLEMS --- Page 582 --- (b)LetXc¼Aand (X0 cXc)/C01¼B.",
    "2/C18/C19 2 ¼14:572 ANSWERS AND HINTS TO THE PROBLEMS --- Page 582 --- (b)LetXc¼Aand (X0 cXc)/C01¼B. Then ABA0¼a0 1 a02 ... a0 n0 BBBBB@1 CCCCCAB(a 1,a2,...,an) ¼a0 1 a02 ... a0 n0 BBBBB@1 CCCCCA(Ba 1,Ba2,...,Ban) ¼a0 1Ba1a01Ba2... a01Ban a02Ba1a02Ba2... a02Ban ......... a0 nBa1a0nBa2...",
    "CCCCCA(Ba 1,Ba2,...,Ban) ¼a0 1Ba1a01Ba2... a01Ban a02Ba1a02Ba2... a02Ban ......... a0 nBa1a0nBa2... a0nBan0 BBBBB@1 CCCCCA: (c)tr(H)¼tr½X(X0X)/C01X0/C138¼tr½(X0X)/C01X0X/C138¼tr(Ikþ1)¼kþ1: 9.3 By Theorem 9.2(iii), hii¼(1=n)þ(x1i/C0/C22x1)0(X0 cXc)/C01(x1i/C0/C22x1): By (2.101) and (2.104), this can be written as hii¼1 nþ(x1i/C0/C22x1)0Xk r¼11 lrara0 r !",
    "/C0/C22x1): By (2.101) and (2.104), this can be written as hii¼1 nþ(x1i/C0/C22x1)0Xk r¼11 lrara0 r ! (x1i/C0/C22x1) ¼Xk r¼11 lr½(x1i/C0/C22x1)0ar/C138½a0 r(x1i/C0/C22x1)/C138 ¼X r1 lr½(x1i/C0/C22x1)0ar/C1382, wherelris the rth eigenvalue of X0 cXcandaris the corresponding (normalized) eigenvector of X0cXc.",
    ", wherelris the rth eigenvalue of X0 cXcandaris the corresponding (normalized) eigenvector of X0cXc. By (2.81), the consine of the angle uirbetween x1i/C0/C22x1ANSWERS AND HINTS TO THE PROBLEMS 573 --- Page 583 --- andaris cosuir¼(x1i/C0/C22x1)0arﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ½(x1i/C0/C22x1)0(x1i/C0/C22x1)/C138(a0 rar)p ¼(x1i/C0/C22x1)0arﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (x1i/C0/C22x1)0(x1i/C0/C22x1)p since a0 rar¼1.",
    "i/C0/C22x1)0arﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (x1i/C0/C22x1)0(x1i/C0/C22x1)p since a0 rar¼1. Thus, if we multiply and divide by ( x1i/C0/C22x1)0(x1i/C0/C22x1), we can express hiias hii¼1 nþ(x1i/C0/C22x1)0(x1i/C0/C22x1)Xk r¼11 lr½(x1i/C0/C22x1)0ar/C1382 (x1i/C0/C22x1)0(x1i/C0/C22x1) ¼1 nþ(x1i/C0/C22x1)0(x1i/C0/C22x1)X r1 lrcos2uir: 9.4 (a) Using (2.51), we obtain H/C3¼(X,y)X0Xþ(X0X)/C01X0yy0X(X0X)/C01 b/C0(X0X)/C01X0y b /C0y0X(X0X)/C01 b1 b0 BB@1 CCAX 0 y0/C18/C19 , where b¼y0y/C0y0X(X0X)/C01X0y.",
    "C01 b/C0(X0X)/C01X0y b /C0y0X(X0X)/C01 b1 b0 BB@1 CCAX 0 y0/C18/C19 , where b¼y0y/C0y0X(X0X)/C01X0y. Show that b¼y0(I/C0H)y¼ˆ10ˆ1.",
    "X0X)/C01 b1 b0 BB@1 CCAX 0 y0/C18/C19 , where b¼y0y/C0y0X(X0X)/C01X0y. Show that b¼y0(I/C0H)y¼ˆ10ˆ1. Show that H/C3¼X(X0X)/C01X0þX(X0X)/C01X0yy0X(X0X)/C01X0 b /C0yy0X(X0X)/C01X0 b/C0X(X0X)/C01X0yy0 bþyy0 b ¼Hþ1 b(Hyy0H/C0yy0H/C0Hyy0þyy0): (b) H/C3¼Hþ1b½(Hyy 0/C0yy0)Hþyy0/C0Hyy0/C138 ¼Hþ1b½(yy 0/C0Hyy0)(I/C0H)/C138 ¼Hþ1b½(I/C0H)yy 0(I/C0H)/C138 ¼Hþˆ1ˆ10 101½by(9:5)/C138:574 ANSWERS AND HINTS TO THE PROBLEMS --- Page 584 --- By (2.21), the diagonal elements of ˆ1ˆ10are^12 1,^12 2,...,^12 n.",
    "TO THE PROBLEMS --- Page 584 --- By (2.21), the diagonal elements of ˆ1ˆ10are^12 1,^12 2,...,^12 n. Therefore, h/C3ii¼hiiþ^12 i=ˆ10ˆ1. (c)Since H/C3is a hat matrix, we have by Theorem 9.2(i),1 n/C20h/C3 ii/C201. Therefore, (1 =n)/C20hiiþ^12 i=ˆ10ˆ1/C201. 9.5 (a) X0X¼x01 x02 ... x0 n0 BBBBB@1 CCCCCA0x0 1 x02 ... x0 n0 BBBBB@1 CCCCCA¼(x 1,x2,...,xn)x0 1 x02 ... x0 n0 BBBBB@1 CCCCCA ¼X n j¼1xjx0 j¼X j=ixjx0jþxix0i¼X0(i)X(i)þxix0i X0y¼(x1,x2,...,xn)y1 y2 ...",
    "... x0 n0 BBBBB@1 CCCCCA ¼X n j¼1xjx0 j¼X j=ixjx0jþxix0i¼X0(i)X(i)þxix0i X0y¼(x1,x2,...,xn)y1 y2 ... yn0 BBBBB@1 CCCCCA¼X n j¼1xjyj ¼X j=1xjyjþxiyi¼X0 (i)y(i)þxiyi: (b) ˆb¼(X0X)/C01X0y¼(X0X)/C01(X0(i)y(i)þxiyi) ¼(X0X)/C01X0(i)y(i)þ(X0X)/C01xiyi: (c)From H¼X(X0X)/C01X0,w eh a v e hii¼x0 i(X0X)/C01xi, where x0iis the ith row of X.",
    "þ(X0X)/C01xiyi: (c)From H¼X(X0X)/C01X0,w eh a v e hii¼x0 i(X0X)/C01xi, where x0iis the ith row of X. Then using the result of part (a) and the inverse in the statement of the problem, we obtain ^b(i)¼(X0 (i)X(i))/C01X0(i)y(i) ¼(X0X/C0x0ixi)/C01X0(i)y(i) ¼(X0X)/C01þ(X0X)/C01xix0 i(X0X)/C01 1/C0x0 i(X0X)/C01xi/C20/C21 X0 (i)y(i) ¼(X0X)/C01þ(X0X)/C01xix0 i(X0X)/C01 1/C0hii/C20/C21 X0 (i)y(i):ANSWERS AND HINTS TO THE PROBLEMS 575 --- Page 585 --- (d)From parts (b) and (c), we have ˆb(i)¼(X0X)/C01X0 (i)y(i)þ(X0X)/C01xix0 i(X0X)/C01X0 (i)y(i) 1/C0hii ¼ˆb/C0(X0X)/C01xiyiþ(X0X)/C01xix0 i½ˆb/C0(X0X)/C01xiy/C138 1/C0hii: With x0i^b¼^yiandx0i(X0X)/C01xi¼hii,w eh a v e ^b(i)/C0^b¼/C0(X0X)/C01xiyi 1/C0hiiþ(X0X)/C01xi^yi 1/C0hii ¼^yi/C0yi 1/C0hii(X0X)/C01xi¼/C0^1i 1/C0hii(X0X)/C01xi: 9.6 By (9.27) and (9.29), we obtain ^1(i)¼yi/C0x0 i^b(i)¼yi/C0x0i^b/C0^1i 1/C0hii(X0X)/C01xi/C20/C21 ¼yi/C0x0 i^bþ^1i 1/C0hiix0 i(X0X)/C01xi ¼yi/C0^yiþ^1ihii 1/C0hii ¼^1iþ^1ihii 1/C0hii¼^1i 1/C0hii: 9.7 (a) Assuming that Xis ﬁxed (constant), we have var(^1(i))¼var^1i 1/C0hii/C18/C19 ¼1 (1/C0hii)2var(^1i)¼s2(1/C0hii) (1/C0hii)2: 9.8 (a) y0y¼Pn j¼1y2 j¼P j=iy2jþy2i¼y0(i)y(i)þy2i.",
    "C19 ¼1 (1/C0hii)2var(^1i)¼s2(1/C0hii) (1/C0hii)2: 9.8 (a) y0y¼Pn j¼1y2 j¼P j=iy2jþy2i¼y0(i)y(i)þy2i. (b)y0 (i)X(i)^b(i)¼y0X^b/C0yix0i^b/C0^1i 1/C0hiiy0X(X0X)/C01xi þ^1i 1/C0hiiyix0 i(X0X)/C01xi 1/C0hii ¼y0X^b/C0yi^yi/C0^1i 1/C0hii^b0xiþ^1i 1/C0hiiyihii ¼y0X^b/C0yi^yi/C0^1i^yi 1/C0hiiþ^1i 1/C0hiiyihii:576 ANSWERS AND HINTS TO THE PROBLEMS --- Page 586 --- Substituting ^yi¼yi/C0^1i, this becomes y0 (i)X(i)^b(i)¼y0X^bþ/C0yi(yi/C0^1i)(1/C0hii)/C0^1i(yi/C0^1i)þ^1iyihii 1/C0hii ¼y0X^bþ/C0(1/C0hii)y2 iþ^12 i 1/C0hii: (c) SSE (i)¼y0y/C0y2 i/C0y0X^b/C0y2iþ^12 i 1/C0hii/C18/C19 ¼y0y/C0y0X^b/C0^12 i 1/C0hii: 9.9 Substituting (9.29) into (9.35) gives Di¼^12 i (1/C0hii)2x0i(X0X)/C01X0X(X0X)/C01xi (kþ1)s2 ¼^12 i (1/C0hii)2hii (kþ1)s2: By (9.25), this becomes Di¼r2 i kþ1hii 1/C0hii: 9.10 Residuals and Inﬂuence Measures for the Gas Vapor Data in Table 7.3a Observations yi yˆi 1ˆi hii ri ti Di 1 29 27.86 1.139 .197 0.466 0.459 .011 2 24 23.76 0.236 .219 0.098 0.096 .0013 26 25.88 0.120 .179 0.049 0.048 .0004 22 23.96 21.961 .289 20.852 20.848 .059 5 27 28.42 21.419 .128 20.557 20.550 .009 6 21 21.67 20.672 .121 20.262 20.258 .002 7 33 31.78 1.222 .053 0.460 0.453 .002 8 34 34.22 20.218 .042 20.082 20.080 .000 9 32 31.98 0.017 .055 0.006 0.006 .000 10 34 33.33 0.666 .039 0.249 0.244 .00011 20 21.54 21.544 .124 20.604 20.597 .010 12 36 32.15 3.846 .040 1.438 1.468 .01713 34 33.73 0.271 .072 0.103 0.101 .00014 23 23.98 20.982 .191 20.400 20.394 .008 15 24 19.71 4.287 .418 2.058 2.200 .609 16 32 32.84 20.841 .060 20.318 20.312 .001 ContinuedANSWERS AND HINTS TO THE PROBLEMS 577 --- Page 587 --- Observations yi yˆi 1ˆi hii ri ti Di 17 40 40.76 20.762 .285 20.330 20.324 .009 18 46 44.39 1.614 .493 0.831 0.826 .134 19 55 52.92 2.083 .243 0.877 0.873 .04920 52 52.02 20.018 .224 20.007 20.007 .000 21 29 32.38 23.377 .177 21.364 1.387 .080 22 22 23.15 21.155 .169 20.464 20.457 .009 23 31 36.59 25.586 .227 22.328 22.555 .319 24 45 47.91 22.909 .185 21.180 21.190 .063 25 37 32.61 4.391 .087 1.683 1.746 .054 26 37 31.89 5.106 .109 1.981 2.103 .09627 33 30.22 2.775 .124 1.086 1.090 .03328 27 31.59 24.593 .102 21.775 21.854 .071 29 34 34.40 20.399 .068 20.151 20.149 .000 30 19 19.32 20.324 .091 20.124 20.122 .000 31 16 19.62 23.623 .102 21.400 21.427 .044 32 22 19.39 2.607 .086 0.999 0.999 .019 aPRESS ¼310.443, SSE ¼201.228.",
    "3.623 .102 21.400 21.427 .044 32 22 19.39 2.607 .086 0.999 0.999 .019 aPRESS ¼310.443, SSE ¼201.228. 9.11 Residuals and Inﬂuence Measures for the Land Rent data of Table 7.5a Observations yi yˆi 1ˆi hii ri ti Di 1 18.38 17.332 1.048 0.080 0.132 0.130 .000 2 20.00 23.948 23.948 0.062 20.494 20.488 .004 3 11.50 13.855 22.355 0.141 20.308 20.303 .004 4 25.00 26.242 21.242 0.070 20.156 20.154 .000 5 52.50 68.180 215.680 0.186 22.107 22.245 .253 6 82.50 66.431 16.069 0.083 2.035 2.156 .094 7 25.00 32.920 27.920 0.067 20.994 20.994 .018 8 30.67 32.642 21.792 0.068 20.248 20.244 .001 9 12.00 7.715 4.285 0.187 0.576 0.570 .019 10 61.25 57.481 3.769 0.103 0.483 0.476 .00711 60.00 50.208 9.792 0.058 1.224 1.234 .02312 57.50 68.846 211.346 0.100 21.451 21.479 .059 13 31.00 31.768 20.768 0.076 20.097 20.095 .000 14 60.00 61.864 21.864 0.067 20.234 20.230 .001 15 72.50 66.773 5.727 0.109 0.736 0.730 .017 16 60.33 66.702 26.372 0.168 20.847 20.843 .036 17 49.75 59.663 29.913 0.114 21.278 21.292 .053 18 8.50 10.790 22.290 0.192 20.309 20.304 .006 19 36.50 24.643 11.857 0.068 1.489 1.522 .040 20 60.00 65.606 25.606 0.181 20.751 20.746 .031 21 16.25 18.016 21.766 0.505 20.304 20.300 .024 22 50.00 47.424 2.576 0.035 0.318 0.313 .001 23 11.50 19.366 24.866 0.118 20.628 20.622 .013 Continued578 ANSWERS AND HINTS TO THE PROBLEMS --- Page 588 --- Observations yi yˆi 1ˆi hii ri ti Di 24 35.00 38.577 23.577 0.064 20.448 20.442 .003 25 75.00 61.694 13.306 0.063 1.667 1.702 .047 26 31.56 35.257 23.697 0.035 20.456 20.450 .002 27 48.50 24.200 6.300 0.063 0.789 0.784 .01028 77.50 69.889 7.611 0.242 1.060 1.062 .089 29 21.67 22.063 20.393 0.060 20.049 20.048 .000 30 19.75 21.221 21.471 0.096 20.188 20.185 .001 31 56.00 48.174 7.826 0.051 0.974 0.974 .013 32 25.00 41.300 216.300 0.217 22.234 22.406 .346 33 40.00 26.907 16.093 0.214 1.791 1.864 .219 34 56.67 56.585 0.085 0.060 0.011 0.010 .000 bPRESS ¼2751.18, SSE ¼2039.91.",
    "3 0.214 1.791 1.864 .219 34 56.67 56.585 0.085 0.060 0.011 0.010 .000 bPRESS ¼2751.18, SSE ¼2039.91. 9.12 Residuals and Inﬂuence Measures for the Chemical Data with Dependent Variable y2a Observations yi yˆi1ˆi hii ri ti Di 1 45.9 49.34 23.442 0.430 21.118 21.128 .235 2 53.3 54.51 21.211 0.310 20.358 20.347 .014 3 57.5 53.46 4.039 0.155 1.078 1.084 .053 4 58.8 56.56 2.238 0.139 0.592 0.578 .0145 60.6 56.04 4.559 0.129 1.198 1.271 .053 6 58.0 59.14 21.143 0.140 20.302 20.293 .004 7 58.6 57.51 1.094 0.228 0.305 0.296 .0078 52.4 60.61 28.208 0.186 22.231 22.638 .258 9 56.9 56.30 0.598 0.053 0.151 0.146 .000 10 55.4 60.35 24.947 0.233 21.385 21.433 .146 11 46.9 52.26 25.356 0.240 21.507 21.580 .179 12 57.3 57.77 20.467 0.164 20.125 20.121 .001 13 55.0 54.84 0.163 0.146 0.043 0.042 .00014 58.9 59.40 20.503 0.245 20.142 20.137 .002 15 50.3 53.20 22.900 0.250 20.821 20.812 .056 16 61.1 58.15 2.950 0.258 0.840 0.831 .061 17 62.9 58.15 4.750 0.258 1.352 1.394 .15918 60.0 56.41 3.592 0.217 0.996 0.955 .069 19 60.6 56.41 4.192 0.217 1.162 1.177 .094 cPRESS ¼416.039, SSE ¼249.462.",
    "592 0.217 0.996 0.955 .069 19 60.6 56.41 4.192 0.217 1.162 1.177 .094 cPRESS ¼416.039, SSE ¼249.462. Chapter 10 10.1 Since ( vi/C0/C22v)0¼(yi/C0/C22y,xi1/C0/C22x1,...,xik/C0/C22xk), the element in the (1, 1) position of ( vi/C0/C22v)(vi/C0/C22v)0is (yi/C0/C22y)2. When this is summed over ias in (10.13), we havePn i¼1(yi/C0/C22y)2¼(n/C01)syyas in (10.14).",
    "C22y)2. When this is summed over ias in (10.13), we havePn i¼1(yi/C0/C22y)2¼(n/C01)syyas in (10.14). Similarly, the (1, 2) element of ( vi/C0/C22v)(vi/C0/C22v)0is (yi/C0/C22y)(xi1/C0/C22x1), which sums to (n/C01)sy1, and the (2, 3) element of ( vi/C0/C22v)(vi/C0/C22v)0is (xi1/C0/C22x1)(xi2/C0/C22x2), which sums to ( n/C01)s12.ANSWERS AND HINTS TO THE PROBLEMS 579 --- Page 589 --- 10.2 By a note following Theorem 7.6b, mˆandSare jointly sufﬁcient for mand S, if the likelihood function ( joint density) in (10.11) factors as L(m,S)¼g(^m,S,m,S)h(v1,v2,...,vn), where v0 i¼(yi,x0i), as in the proof of Theorem 10.2a.",
    "factors as L(m,S)¼g(^m,S,m,S)h(v1,v2,...,vn), where v0 i¼(yi,x0i), as in the proof of Theorem 10.2a. Noting that a scalar is equal to its trace, we write the exponent in (10.11) in the form Xn i¼1(vi/C0m)0S/C01(vi/C0m)¼Xn i¼1tr(vi/C0m)0S/C01(vi/C0m) ¼trS/C01Xn i¼1(vi/C0m)(vi/C0m)0\"# : Adding and subtracting v¯, the sum becomes Xn i¼1(vi/C0m)(vi/C0m)0¼Xn i¼1(vi/C0/C22vþ/C22v/C0m)(vi/C0/C22vþ/C22v/C0m)0 ¼Xn i¼1(vi/C0/C22v)(vi/C0/C22v)0þn(/C22v/C0m)(/C22v/C0m)0 ¼(n/C01)Sþn(/C22v/C0m)(/C22v/C0m)0: Show that the other two terms vanish.",
    "0þn(/C22v/C0m)(/C22v/C0m)0 ¼(n/C01)Sþn(/C22v/C0m)(/C22v/C0m)0: Show that the other two terms vanish. Then show that L(m,S) can be written as L(m,S)¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )n(kþ1)jSjn=2e/C0½(n/C01)tr(S/C01S)þn(/C22v/C0m)0S/C01(v/C0m)/C138=2: 10.3 DRD ¼sy00 0D x/C18/C191r0 yx ryxRxx/C18/C19sy00 0D x/C18/C19 ¼sy syr0yx DxryxDxRxx/C18/C19sy00 0D x/C18/C19 ¼s2y syr0yxDx syDxryxDxRxxDx !",
    "8/C19sy00 0D x/C18/C19 ¼sy syr0yx DxryxDxRxx/C18/C19sy00 0D x/C18/C19 ¼s2y syr0yxDx syDxryxDxRxxDx ! : 10.4 Express yand win terms ofy x/C18/C19 as follows: y = (1, 0, ...,0 ) y x/C18/C19 ¼a0y x/C18/C19 ,w¼(0,s0yxS/C01 xx)y x/C18/C19 + constant = b0y x/C18/C19 þconstant. Then use (3.42) and (3.43) with Spartitioned as in (10.3).580 ANSWERS AND HINTS TO THE PROBLEMS --- Page 590 --- 10.5 Express wandyasw¼(0,a0)y x/C18/C19 andy¼(1,0,...,0)y x/C18/C19 .",
    "THE PROBLEMS --- Page 590 --- 10.5 Express wandyasw¼(0,a0)y x/C18/C19 andy¼(1,0,...,0)y x/C18/C19 . Then cov(y,w)¼cov (1 ,0,...,0)y x/C18/C19 ,(0,a0)y x/C18/C19 /C20/C21 ¼(1,0,...,0)syys0 yx syxSxx/C18/C190 a/C18/C19 ¼(syy,s0 yx)0 a/C18/C19 ¼s0yxa, r2yw¼½cov(y,w)/C1382 var(y) var( w)¼(a0syx)2 syy(a0Sxxa): (1) Differentiate ryw2with respect to aand set the result equal to 0to obtain a¼(a0Sxxa=a0syx)S/C01 xxsyx, with can be substituted into (1) to obtain maxar2 yw¼s0yxS/C01 xxsyx=syy.",
    "(a0Sxxa=a0syx)S/C01 xxsyx, with can be substituted into (1) to obtain maxar2 yw¼s0yxS/C01 xxsyx=syy. 10.6 Show that for Spartitioned as in (10.3), (2.75) becomes jSj¼jSxxj (syy/C0s0yxS/C01 xxsyx). Solve for s0yxS/C01 xxsyxand substitute into (10.27). 10.7 s0 uv¼cov(u,v)¼cov(ay,Bx)¼cov ( a,0,...,0)y x/C18/C19 ,(0,B)y x/C18/C19 /C20/C21 ¼(a,0,...,0)s2 ys0yx syxSxx !",
    "u,v)¼cov(ay,Bx)¼cov ( a,0,...,0)y x/C18/C19 ,(0,B)y x/C18/C19 /C20/C21 ¼(a,0,...,0)s2 ys0yx syxSxx ! 00 B0/C18/C19 ¼as0 yxB0, Svv¼cov(Bx)¼BSxxB0, suu¼a2syy, r2ujv¼s0 uvS/C01 vvsuv suu¼as0yxB0(BSxxB0)/C01aBsyx a2syy ¼a2s0yxB0(B0)/C01S/C01 xxB/C01Bsyx a2syy:ANSWERS AND HINTS TO THE PROBLEMS 581 --- Page 591 --- 10.8y/C0w¼y/C0my/C0s0 yxS/C01 xx(x/C0mx) ¼(1,/C0s0yxS/C01 xx)y x/C18/C19 þconstant ¼a0y x/C18/C19 þconstant , x¼(0,I)y x/C18/C19 ¼By x/C18/C19 , cov(y/C0w,x)¼a0SB0¼(1,/C0s0yxS/C01 xx)s2 ys0yx syxSxx !",
    "onstant , x¼(0,I)y x/C18/C19 ¼By x/C18/C19 , cov(y/C0w,x)¼a0SB0¼(1,/C0s0yxS/C01 xx)s2 ys0yx syxSxx ! 00 I/C18/C19 ¼(s2 y/C0s0yxS/C01 xxsyx,s0yx/C0s0yxS/C01 xxSxx)00 I/C18/C19 ¼00: 10.9 (a) By deﬁnition, r2 y^y¼½Pn i¼1(yi/C0/C22y)(^yi/C0/C22^y)/C1382=Pn i¼1(yi/C0/C22y)2 Pn i¼1(^yi/C0/C22^y)2. Show that /C22^y¼/C22yby using X0¼j0 X0 1/C18/C19 inX0X^b¼X0y to obtain j0X^b¼j0y, from which,Pn i¼1^yi¼Pni¼1yi. Show thatPni¼1(yi/C0/C22y)(^yi/C0/C22^y)¼P iyi^yi/C0n/C22y2¼y0^y/C0n/C22y2¼y0X^b/C0n/C22y2.",
    "Pni¼1yi. Show thatPni¼1(yi/C0/C22y)(^yi/C0/C22^y)¼P iyi^yi/C0n/C22y2¼y0^y/C0n/C22y2¼y0X^b/C0n/C22y2. Show thatP i(^yi/C0/C22^y)2¼P i^y2 i/C0n/C22y2¼^y0^y/C0n/C22y2¼^bX0X^b/C0n/C22y2¼ ^b0X0y/C0n/C22y2. Use (7.54). (b)This follows directly from estimation of (10.25) and the expression following (10.26): ry^y¼sy^y sys^y¼s0 yxS/C01 xxsyx syﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ s0 yxS/C01 xxsyxq 10.10 r2 y,a0x¼(sy,a0x)2=s2 ys2a0x. Express yand a0xasy¼(1,0,...,0)y x/C18/C19 and a0x¼(0,a0)y x/C18/C19 .",
    "r2 y,a0x¼(sy,a0x)2=s2 ys2a0x. Express yand a0xasy¼(1,0,...,0)y x/C18/C19 and a0x¼(0,a0)y x/C18/C19 . By analogy with (3.40), show that sy,a0x¼ (1,0,...,0)S0 a/C18/C19 ¼s0 yxa,w h e r e Sis partitioned as in (10.10). Similarly, by analogy with (3.42) show that s2a0x¼a0Sxxa. Solve (@=@a)(s0yxa)2=s2ya0Sxxa¼0foraand substitute back into r2 y,a0xabove to show that max ar2 y,a0x¼R2.",
    "(@=@a)(s0yxa)2=s2ya0Sxxa¼0foraand substitute back into r2 y,a0xabove to show that max ar2 y,a0x¼R2. 10.11 Substitute (10.20) and (10.21) into (10.34).582 ANSWERS AND HINTS TO THE PROBLEMS --- Page 592 --- 10.12 Adapt (2.51) to obtain ab0 bC/C18/C19/C01 ¼1=d /C0b0C/C01=d /C0C/C01b=dC/C01þC/C01bb0C/C01=d/C18/C19 ; (1) where Cis symmetric and d¼a/C0b0C/C01b.",
    "/C0b0C/C01=d /C0C/C01b=dC/C01þC/C01bb0C/C01=d/C18/C19 ; (1) where Cis symmetric and d¼a/C0b0C/C01b. Apply (1) to Rpartitioned as in (10.18), R¼1r0 yx ryxRxx/C18/C19 : Then ryy¼1 d¼1 a/C0b0C/C01b ¼1 1/C0r0 yxR/C01 xxryx¼1 1/C0R2: 10.13 Use (2.71) to show that for Spartitioned as in (10.14), (2.75) can be adapted to the form jSj¼jSxxj(syy/C0s0 yxS/C01 xxsyx). Solve for s0yxS/C01 xxsyxand substitute into (10.34).",
    "to the form jSj¼jSxxj(syy/C0s0 yxS/C01 xxsyx). Solve for s0yxS/C01 xxsyxand substitute into (10.34). 10.14 As in Problem 10.4, deﬁne u¼ayand v¼Bx, so that s0uv¼as0yxB0, Svv¼BSxxB0, andsuu¼a2syy. Then by Theorem 10.2c, the maximum likelihood estimators of these are s0uv¼as0yxB0,Svv¼BSxxB0, and suu¼a2syy, respectively.",
    "the maximum likelihood estimators of these are s0uv¼as0yxB0,Svv¼BSxxB0, and suu¼a2syy, respectively. Substitute these into R2 u,v¼s0 uvS/C01 vvsuv suu: 10.15L(^m,S0)¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )n(kþ1)jS0jn=2e/C0Pn i¼1(vi/C0^m)0P/C01 0(vi/C0^m)=2: Using vi¼yi xi/C18/C19 ,^min (10.9), amd S0in (10.45), show that L(^m,S0) becomes L(^m,S0)¼1 (ﬃﬃﬃﬃﬃﬃ 2pp )nsn=2 yye/C0Pn i¼1(yi/C0/C22y)2=2syy /C21 (ﬃﬃﬃﬃﬃﬃ 2pp )knjSxxjn=2e/C0Pn i¼1(xi/C0/C22x)0P/C01 xx(xi/C0/C22x)=2: The ﬁrst factor is maximized by sˆyyand the second factor by Sˆxx.",
    "/C0/C22x)0P/C01 xx(xi/C0/C22x)=2: The ﬁrst factor is maximized by sˆyyand the second factor by Sˆxx. Show that when these are substituted in L(^m,S0), the result is given by (10.47).ANSWERS AND HINTS TO THE PROBLEMS 583 --- Page 593 --- 10.16 j^Sj¼n/C01 nS/C12/C12/C12/C12/C12/C12/C12/C12¼n/C01 n/C18/C19kþ1 jSj,j^Sxxj¼n/C01 n/C18/C19k jSxxj: 10.17 Multiply by 1 =ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ n/C03p , subtract z, multiply by 21 (which reverses the direc- tion of the inequalities), then take tanh (hyperbolic tangent) of all three members.",
    "rses the direc- tion of the inequalities), then take tanh (hyperbolic tangent) of all three members. 10.18 (a) V¼1 n1/C0300 01 n2/C030 001 n3/C030 B@1 CA (b)Using Theorem 4.4a(ii) and (5.35), ½C(z/C0mz)/C1380½CVC0/C138/C01 ½C(z/C0mz)/C138isx2(2). (c)Calculate u¼z0C0½CVC0/C138/C01Cz.",
    "(5.35), ½C(z/C0mz)/C1380½CVC0/C138/C01 ½C(z/C0mz)/C138isx2(2). (c)Calculate u¼z0C0½CVC0/C138/C01Cz. Reject if u/C21x2 2,1/C0a: 10.19 The sample covariance matrix involving yandwcan be expressed in the form S¼s2 ys0yw sywSww/C18/C19 , andsywandSwwcan be further partitioned as syw¼syx syz/C18/C19 and Sww¼Szxszx s0zxs2 ^z/C18/C19 : (1) By (10.34), the squared multiple correlation of yregressed on wcan be written as R2 yw¼s0 ywS/C01 wwsyw s2 y: (2) Using (2.51) for the inverse of the partitioned matrix Swwin (1), show that s0 ywS/C01 wwsyw¼1 s2 z/C1x(s2 z/C1xs0yxS/C01 xxsyxþs0yxS/C01 xxszxs0zxS/C01 xxsyx /C0syzs0zxS/C01 xxsyx/C0syzs0yxS/C01 xxszxþs2yz) ¼1 s2 z/C1xs2 z/C1xs2yR2yxþ(^b0 zxsyx/C0syz)2hi , where s2 z/C1x¼s2z/C0s0zxS/C01 xxszxand ^bzx¼S/C01 xxszxis the vector of regression584 ANSWERS AND HINTS TO THE PROBLEMS --- Page 594 --- coefﬁcients of zregressed on the x’s.",
    "gression584 ANSWERS AND HINTS TO THE PROBLEMS --- Page 594 --- coefﬁcients of zregressed on the x’s. Then show that (2) becomes R2 yw¼R2yxþ(^b0 zxsyx/C0syz)2 s2 ys2z(1/C0R2zx): (3) Simplify (3) to the correlation form shown in (10.58). 10.20 Ifzis orthogonal to the x’s, then szx¼0. Show that this leads to ^ryx¼0 and R2 zx¼0. 10.21 For a linear function b0þb0 1x, the mean squared error is given by m¼E(y/C0b0/C0b01x)2.",
    "zx¼0. 10.21 For a linear function b0þb0 1x, the mean squared error is given by m¼E(y/C0b0/C0b01x)2. Adding and subtracting myandb0mxleads to m¼E½(y/C0my)/C0(b0/C0myþb01mx)/C0b01(x/C0mx)/C1382: Show that this becomes m¼s2yþ(b0/C0myþb01mx)2þb01Sxxb1/C02b01syx: Differentiate mwith respect to b0and with respect to b1and set the results equal to zero.",
    "C02b01syx: Differentiate mwith respect to b0and with respect to b1and set the results equal to zero. 10.22 Follow the steps in the answer to Problem 10.19 using /C22y,/C22x,Sxx, and syxin place of my,mx,Sxx, andsyxand using a sample mean in place of expectation.",
    "C22y,/C22x,Sxx, and syxin place of my,mx,Sxx, andsyxand using a sample mean in place of expectation. 10.23 From the expression preceding (10.71), we obtain Xn i¼1w1iw2i¼X i(y1i/C0/C22y1)(y2i/C0/C22y2)/C0^b12X i(y1i/C0/C22y1)(y3i/C0/C22y3) /C0^b11X i(y3i/C0/C22y3)(y2i/C0/C22y2)þ^b11^b12X i(y3i/C0/C22y3)2: Using (10.67) and (10.68), this becomes Xn i¼1w1iw2i¼X i(y1i/C0/C22y1)(y2i/C0/C22y2)/C0^b12^b11X i(y3i/C0/C22y3)2 /C0^b11^b12X i(y3i/C0/C22y3)2þ^b11^b12X i(y3i/C0/C22y3)2: 10.24 Follow the steps in the answer to Problem 10.23.ANSWERS AND HINTS TO THE PROBLEMS 585 --- Page 595 --- 10.25 Denote yki/C0/C22ykbyy/C3 ki,k¼1,2,3.",
    "3.ANSWERS AND HINTS TO THE PROBLEMS 585 --- Page 595 --- 10.25 Denote yki/C0/C22ykbyy/C3 ki,k¼1,2,3. Then by (10.76), we obtain rw1w2¼P iy/C31iy/C32i/C0^b11^b12P iy/C32 3iﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP iy/C32 1i/C0^b2 11P iy/C32 3i/C0/C1 P iy/C32 2i/C0^b2 12P iy/C32 3i/C0/C1q Substituting for bˆ11andbˆ12from (10.67) and (10.68), we have rw1w2¼P iy/C31iy/C32i/C0P iy/C3 1iy/C33iP iy/C32 3i/C18/C19P iy/C32iy/C33iP iy/C32 3i/C18/C19P iy/C32 3i/C0/C1 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ P iy/C32 1i/C0P iy/C3 1iy/C33iP iy/C32 3i/C18/C192 (P iy/C32 3i)\"# P iy/C32 2i/C0P iy/C32iy/C33iP iy/C32 3i/C18/C192 (P iy/C32 3i)\"#vuut: Dividing numerator and denominator byﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP iy/C32 1iP iy/C32 2ip , we obtain rw1w2¼P iy/C3 1iy/C32iﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP iy/C32 1iP iy/C32 2ip /C0P iy/C3 1iy/C33iﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP iy/C32 1iP iy/C32 3ipP iy/C3 2iy/C33iﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP iy/C32 2iP iy/C32 3ip ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP iy/C32 1iP iy/C32 1i/C0(P iy/C3 1iy/C33i)2P iy/C32 1iP iy/C32 3i/C20/C21 P iy/C32 2iP iy/C32 2i/C0(P iy/C32iy/C33i)2P iy/C32 2iP iy/C32 3i/C20/C21s ¼r12/C0r13r23ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (1/C0r2 13)(1/C0r2 23)p : 10.26 By (10.78), Xn i¼1[yi/C0^yi(x)]¼Xn i¼1[yi/C0/C22y/C0SyxS/C01 xx(xi/C0/C22x)] ¼X i(yi/C22y)/C0SyxS/C01 xxX i(xi/C0/C22x) ¼0/C0SyzS/C01 xx0:586 ANSWERS AND HINTS TO THE PROBLEMS --- Page 596 --- 10.27 By deﬁnition, the partitioned S can be written as S¼SyySyx SxySxx/C18/C19 ¼1 n/C01Xn i¼1yi xi/C18/C19 /C0/C22y /C22x/C18/C19/C20/C21yi xi/C18/C19 /C0/C22y /C22x/C18/C19/C20/C21 ¼1 n/C01X iyi/C0/C22y xi/C0/C22x/C18/C19yi/C0/C22y xi/C0/C22x/C18/C19 ¼1 n/C01X iyi/C0/C22y xi/C0/C22x/C18/C19 [(yi/C0/C22y)0,(xi/C0/C22x)0] ¼1 n/C01X i(yi/C0/C22y)(yi/C0/C22y)0(yi/C0/C22y)(xi/C0/C22x)0 (xi/C0/C22x)(yi/C0/C22y)0(xi/C0/C22x)(xi/C0/C22x)0/C20/C21 : 10.28 (a)Sxx¼271 :9298 10 :0830 1 :4011 10:0830 1 :4954 0 :0514 1:4011 0 :0514 0 :00740 B@1 CA,syx¼:2204 :0220 :00170 B@1 CA, ^b1¼/C00:0212 0:0143 4:17810 B@1 CA,^b0¼:2659 ,s2¼:004978 (b)Rxx¼1:000 0 :500 0 :990 0:500 1 :000 0 :490 0:990 0 :490 1 :0000 B@1 CA,ryx¼:151 :203 :2280 B@1 CA, ^b/C3 1¼/C03:960 0:198 4:0520 B@1 CA (c)R2¼:3639 (d)F¼R2=3 (1/C0R2)=15¼2:86045 ,p¼:072 10.29 (a) x2:r1¼.5966, r2¼.0721 z1¼.6878, z2¼.722, v¼2.0642, limits for r1are .2721 and .7992, limits for r2are2.3325 and .4543.",
    "721 z1¼.6878, z2¼.722, v¼2.0642, limits for r1are .2721 and .7992, limits for r2are2.3325 and .4543. (b)x3:r1¼.7012, r2¼.8209, z1¼.8697, z2¼1.1594, v¼2.9716, limits for1are .4309 and .8561, limits for r2are .6301 and .9182. (c)x4:r1¼.0400, r2¼.0714, z1¼.04002, z2¼.07154, v¼2.1057, limits for r1are2.3528 and .4208, limits for r2are2.3331 and .4537.",
    "z1¼.04002, z2¼.07154, v¼2.1057, limits for r1are2.3528 and .4208, limits for r2are2.3331 and .4537. (d)x5:r1¼.3391, r2¼.2683, z1¼.3531, z2¼.2751, v¼.2617, limits for r1are2.0555 and .6421, limits for r2are2.1418 and .5999.ANSWERS AND HINTS TO THE PROBLEMS 587 --- Page 597 --- 10.30zr ˆyz ryz Rzx2Ryw22Ryx2 Fp Value x1 .227 .0228 .9808 .3010 7.099 .018 x2 .055 .0413 .2513 .0292 .689 .417 x3 .149 .0518 .9806 .3193 7.531 .015 10.31 (a)To ﬁnd ry1/C123, the sample covariance matrix is partitioned as S¼:0078 :2204 :0220 :0017 :2204 271 :9298 10:0830 1 :4011 :0220 10 :0830 1:4954 :0514 :0017 1 :4011 :0514 :00740 BBB@1 CCCA¼SyySyx SxySxx/C18/C19 , where y¼(y,x1)’ and x¼(x2,x3)’.",
    ":0514 :0017 1 :4011 :0514 :00740 BBB@1 CCCA¼SyySyx SxySxx/C18/C19 , where y¼(y,x1)’ and x¼(x2,x3)’. From Syy,Syx,Sxy, and Sxx, we obtain Ds¼:0856 0 02 :2846/C18/C19 ,Ry/C1x¼1:000 /C0:567 /C0:567 1 :000/C18/C19 : Thus ry1/C123¼2.567, as compared to ry1¼.151.",
    "C18/C19 ,Ry/C1x¼1:000 /C0:567 /C0:567 1 :000/C18/C19 : Thus ry1/C123¼2.567, as compared to ry1¼.151. (b)Forry2/C113,w eh a v e y¼(y,x2)0andx¼(x1,x3)0, Syy¼:0078 0 :0220 :0220 1 :4954/C18/C19 ,Ds¼01 :0581 :0722 0/C18/C19 , Ry/C1x¼1:000 0 :2097 0:2097 1 :000/C18/C19 : (c)ForRy:xcorresponding to y¼(y,x1,x2)0andx¼x3,w eh a v e Syy¼:0078 :2204 :0220 :2204 271 :9298 10 :0830 :0220 10 :0830 1 :49540 B@1 CA, Ds¼:0861 0 0 02 :3015 0 00 1 :06600 B@1 CA Ry/C1x¼1:000 /C00:546 0 :108 /C00:546 1 :000 0 :121 1:108 0 :121 1 :0000 B@1 CA588 ANSWERS AND HINTS TO THE PROBLEMS --- Page 598 --- Chapter 11 11.1 (b/C0f)0V/C01(b/C0f)þ(y/C0Xb)0(y/C0Xb)þd/C3 ¼b0V/C01b/C02b0V/C01fþf0V/C01fþy0y/C02b0X0yþbX0Xbþd/C3 ¼b0(V/C01þX0X)b/C02b0(V/C01þX0X)(V/C01þX0X)/C01(V/C01fþX0y) þ(V/C01fþX0y)0(V/C01þX0X)/C01(V/C01þX0X)(V/C01þX0X)/C01 /C2(V/C01fþX0y)/C0(V/C01fþX0y)0(V/C01þX0X)/C01(V/C01þX0X) (V/C01þX0X)/C01(V/C01fþX0y)þf0V/C01fþy0yþd/C3 ¼b0V/C01 /C3b/C02b0V/C01 /C3f/C3þf0 /C3V/C01 /C3f/C3/C0f0/C3V/C01 /C3f/C3 þf0V/C01fþy0yþd/C3 ¼(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þd/C3/C3: 11.2ð1 0tae/C0btdt¼b/C0að1 0(bt)ae/C0(bt)dt ¼b/C0að1 0(bt)ae/C0(bt)d(bt) ¼b/C0(aþ1)ð1 0sae/C0sds(letting s¼bt) ¼b/C0(aþ1)G(aþ1)½by definition of G(aþ1)/C138: 11.3 (a) Use (2.54) with A¼I,P¼XV,B¼V21, and Q¼VX0.",
    ") ¼b/C0(aþ1)G(aþ1)½by definition of G(aþ1)/C138: 11.3 (a) Use (2.54) with A¼I,P¼XV,B¼V21, and Q¼VX0. (b)(IþXVX0)/C01X/C0X(X0XþV/C01)/C01V ¼½I/C0X(X0XþV/C01)/C01X0/C138X/C0X(X0XþV/C01)/C01V/C01(Problem 11.3a) ¼X/C0X(X0XþV/C01)/C01X0X/C0X(X0XþV/C01)/C01V ¼X/C0X(X0XþV/C01)/C01(X0XþV/C01) ¼X/C0X ¼O:ANSWERS AND HINTS TO THE PROBLEMS 589 --- Page 599 --- (c)V/C01/C0V/C01(X0XþV/C01)/C01V/C01 ¼½Vþ(X0X)/C01(X0X)(X0X)/C01/C138/C01[use (2.54) in reverse] ¼½(X0X)/C01þV/C138/C01(simplify) ¼X0X/C0X0X(X0XþV/C01)/C01X0X[use (2.54)] ¼X0½I/C0X(X0XþV/C01)/C01X0/C138X (factor) ¼X0(IþXVX0)/C01X (Problem 11.3a).",
    "/C01)/C01X0X[use (2.54)] ¼X0½I/C0X(X0XþV/C01)/C01X0/C138X (factor) ¼X0(IþXVX0)/C01X (Problem 11.3a). 11.4 y0yþf0V/C01f/C0f0 /C3V/C01 /C3f/C3 ¼(y/C0Xf)0(IþXVX0)/C01(y/C0Xf)/C0(y/C0Xf)0 /C2(IþXVX0)/C01(y/C0Xf)þy0yþf0V/C01f/C0f0 /C3V/C01 /C3f/C3 ¼(y/C0Xf)0(IþXVX0)/C01(y/C0Xf)/C0y0(IþXVX0)/C01yþ2y0(IþXVX0)/C01 /C2Xf/C0f0X0(IþXVX0)/C01Xfþy0yþf0V/C01f/C0(X0yþV/C01f)0 /C2(X0XþV/C01)/C01(X0XþV/C01)(X0XþV/C01)/C01(X0yþV/C01f) ¼(y/C0Xf)0(IþXVX0)/C01(y/C0Xf)þy0½I/C0(IþXVX0)/C01 /C0X(X0XþV/C01)/C01X0/C138yþ2y0½(IþXVX0)/C01X/C0X(X0XþV/C01)/C01V/C01/C138f þf0½V/C01/C0X0(IþXVX0)/C01X/C0V/C01(X0XþV/C01)/C01V/C01/C138f ¼(y/C0Xf)0(IþXVX0)/C01(y/C0Xf)þy0Oyþ2y0Ofþf0Of (see Problems 11.3a ,b,and c) ¼(y/C0Xf)0(IþXVX0)/C01(y/C0Xf): 11.5 The prior density for bisp1(b)¼c1.",
    "see Problems 11.3a ,b,and c) ¼(y/C0Xf)0(IþXVX0)/C01(y/C0Xf): 11.5 The prior density for bisp1(b)¼c1. Since the prior density for ln(t21) is uniform, the prior density for tisp2(t)¼c2t21. The like- lihood for yjb,tis the multivariate normal density with mean Xb and covariance matrix t21I.",
    "he like- lihood for yjb,tis the multivariate normal density with mean Xb and covariance matrix t21I. Using Bayes theorem in (11.4), the joint posterior density is g(b,tjy)¼c4c1c2t/C01c3tn=2e/C0t(y/C0Xb)0(y/C0Xb)=2 ¼c5t(n/C02)=2e/C0t(y/C0Xb)0(y/C0Xb)=2:590 ANSWERS AND HINTS TO THE PROBLEMS --- Page 600 --- The marginal posterior density of bjyis u(bjy)¼c5ð1 0t(n/C02)=2e/C0t(y/C0Xb)0(y/C0Xb)=2dt ¼c5G(n=2)½(y/C0Xb)0(y/C0Xb)=2/C138/C0n=2(Problem 11.2) ¼c6½(n/C0k/C01)s2þ(b/C0^b)0(X0X)(b/C0^b)/C138/C0n=2 (proof to Theorem 7.6c) ¼c7½1þ(b/C0^b)0(X0X)(b/C0^b)=(n/C0k/C01)s2/C138/C0n=2 ¼c7½1þ(b/C0^b)0½s2(X0X)/C01/C138/C01(b/C0^b)=(n/C0k/C01)/C138/C0½(n/C0k/C01)þ(kþ1)/C138=2 which is the density function of the multivariate t-distribution (Gelman, et al.",
    "k/C01)þ(kþ1)/C138=2 which is the density function of the multivariate t-distribution (Gelman, et al. 2004, pp.",
    "1)/C138=2 which is the density function of the multivariate t-distribution (Gelman, et al. 2004, pp. 576–577) with parameters ( n/C0k/C01,^b,s2(X0X)/C01): 11.6 Using (7.64), the generalized least squares estimate of bfor the augmented data is X I/C18/C19 0IO OV/C18/C19 /C01X I/C18/C19 \"#/C01X I/C18/C19 0IO OV/C18/C19 /C01y f/C18/C19 ¼(X0I0)IO OV/C18/C19 /C01X Y/C18/C19 \"#/C01 X0I0ðÞIO OV/C18/C19 /C01y f/C18/C19 ¼(X0XþV/C01)/C01(X0yþV/C01f):ANSWERS AND HINTS TO THE PROBLEMS 591 --- Page 601 --- 11.7 E(t)¼ð1 0tda G(a)ta/C01e/C0dtdt ¼da G(a)ð1 0tae/C0dtdt ¼da G(a)d/C0(aþ1)G(aþ1) [using prob.",
    "--- 11.7 E(t)¼ð1 0tda G(a)ta/C01e/C0dtdt ¼da G(a)ð1 0tae/C0dtdt ¼da G(a)d/C0(aþ1)G(aþ1) [using prob. 11.2] ¼a d: var(t)¼ð1 0t2da G(a)ta/C01e/C0dtdt/C0a d/C16/C172 ¼da G(a)ð1 0taþ1e/C0dtdt/C0a d/C16/C172 ¼da G(a)d/C0(aþ2)G(aþ2)/C0a d/C16/C172 ¼da G(a)d/C0(aþ2)G(aþ2)/C0a d/C16/C172 ¼(aþ1)(a) d2/C0a d/C16/C172 ¼a d2: 11.8 The density function of tjyis given in (11.14).",
    "a d/C16/C172 ¼(aþ1)(a) d2/C0a d/C16/C172 ¼a d2: 11.8 The density function of tjyis given in (11.14). Using the change-of-variable technique, the marginal posterior density of s2jyis w(s2jy)¼c5(s/C02)(aþn=2)/C01e/C0½(/C0f0 /C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2/C138(s/C02)(s2)/C02 ¼c6(s2)/C0(aþn=2)/C01e/C0½(/C0f0/C3V/C01 /C3f/C3þf0V/C01fþy0yþ2d)=2/C138=s/C02: 11.9 (a) This is the model of Section 11.2.1, with k¼1,f¼0 0/C18/C19 , andV¼s2 00 0s2 1/C18/C19 .",
    "2: 11.9 (a) This is the model of Section 11.2.1, with k¼1,f¼0 0/C18/C19 , andV¼s2 00 0s2 1/C18/C19 . Using Theorem 11.2b and the expression in (11.18), b1jtis t-distributed with parameters nþ2a,f/C32, and w/C322where f/C32¼(s/C02 0þn)P ixiyi/C0P iyiP ixi (s/C02 0þn)(s/C02 1þP ix2 i)/C0(P ixi)2592 ANSWERS AND HINTS TO THE PROBLEMS --- Page 602 --- and w/C322¼y0(I/C0XVX0)/C01yþ2d nþ2a(s/C02 0þn) (s/C02 0þn)(s/C02 0þP ix2 i)/C0(P ixi)2: (b)A point estimate is given by f*2and a (1 2v)/C2100% conﬁdence interval is given by f/C32+tv=2,nþ2av/C322: 11.10 (a) The joint prior density is p(b,t)¼p1(bjt)p2(t) ¼c1e/C0(b/C0f)0V/C01(b/C0f)=2ta/C01e/C0dt ¼c1ta/C01e/C0(b/C0f)0V/C01(b/C0f)=2/C0dt: Using (11.4), the joint posterior density is g(b,tjy)¼cp(b,t)L(b,tjy) ¼c2ta/C01e/C0(b/C0f)0V/C01(b/C0f)=2/C0dttn=2e/C0t(y/C0Xb)0(y/C0Xb)=2 ¼c2tn=2þa/C01e/C0½(b/C0f)0V/C01(b/C0f)þt(y/C0Xb)0(y/C0Xb)/C138=2/C0dt: (b)Picking the terms out of the joint density g(b,tjy) that involve b, and considering everything else to be part of the normalizing constant, the conditional posterior density of bjt,yis w(bjt,y)¼c3e/C0½(b/C0f)0V/C01(b/C0f)þt(y/C0Xb)0(y/C0Xb)/C138=2 ¼c3e/C0t(t/C01b0V/C01b/C02t/C01b0V/C01fþt/C01f0V/C01fþy0y/C02b0X0yþb0X0Xb)=2 ¼c4e/C0t½b0(X0Xþt/C01V/C01)b/C02b0(X0yþt/C01V/C01f)/C138=2 ¼c4e/C0t½b0(X0Xþt/C01V/C01)b/C02b0(X0Xþt/C01V/C01)(X0Xþt/C01V/C01)/C01(X0yþt/C01V/C01f)/C138=2 ¼c5e/C0t(b0V/C01 nb/C02b0V/C01 nfnþf0 nV/C01 nfn)=2 where Vn¼(X0Xþt/C01V/C01)/C01andfn¼Vn(X0yþt/C01V/C01f) ¼c5e/C0t½(b/C0fn)0V/C01 n(b/C0fn)/C138=2: Hencebjt,yisNkþ1(fn,t/C01Vn).",
    "C01andfn¼Vn(X0yþt/C01V/C01f) ¼c5e/C0t½(b/C0fn)0V/C01 n(b/C0fn)/C138=2: Hencebjt,yisNkþ1(fn,t/C01Vn). (c)Picking the terms out of the joint density g(b,tjy) that involve t, and considering everything else to be part of the normalizing constant, theANSWERS AND HINTS TO THE PROBLEMS 593 --- Page 603 --- conditional posterior density of tjb,yis c(tjb,y)¼c6tn=2þa/C01e/C0t(y/C0Xb)0(y/C0Xb)=2þdt ¼c6tn=2þa/C01e/C0½(y/C0Xb)0(y/C0Xb)=2þd/C138t: Hencetjb,yisGamma ½n=2þa,(y/C0Xb)0(y/C0Xb)=2þd/C138.",
    "¼c6tn=2þa/C01e/C0½(y/C0Xb)0(y/C0Xb)=2þd/C138t: Hencetjb,yisGamma ½n=2þa,(y/C0Xb)0(y/C0Xb)=2þd/C138. (d) †Specify 1/ s2from (7.23) as a starting value t0. †Fori¼1t o M: calculate Vn,i/C01¼(X0Xþt/C01 i/C01V/C01)/C01, calculate fn,i/C01¼Vn,i/C01(X0yþt/C01 i/C01V/C01f), drawbifrom Nkþ1(fn,i/C01,t/C01 i/C01Vn,i/C01), drawtifrom Gamma ½n=2þa,(y/C0Xbi)0(y/C0Xbi)=2þd/C138, calculate t/C01 i †Consider all draws ( bi,t/C01 i) to be from the joint posterior distribution.",
    "38, calculate t/C01 i †Consider all draws ( bi,t/C01 i) to be from the joint posterior distribution. 11.11 (a) Bayesian estimates of b1,b2, andb3are 0.7820, 0.5007, 216.6443. Lower 95% conﬁdence limits are 0.6281, 0.2627, 242.2511. Upper 95% conﬁdence limits are 0.9358, 0.7386, 8.9625. (b)Answers will vary. We obtained Bayesian estimates of 0.7817, 0.4990, 216.5490, lower 95% conﬁdence limits of 0.6332, 0.2627, 242.6158, and upper 95% conﬁdence limits of 0.9358, 0.7358, 9.5144.",
    "ﬁdence limits of 0.6332, 0.2627, 242.6158, and upper 95% conﬁdence limits of 0.9358, 0.7358, 9.5144. (c)Answers will vary. We obtained a Bayesian prediction of 18.9113, withlower and upper 95% limits of 2.3505 and 35.7526. (d)Answers will vary. We obtained Bayesian estimates of 0.8170, 0.4399, 26.1753, lower 95% conﬁdence limits of 0.6831, 0.2142, 221.4675, and upper 95% conﬁdence limits of 0.9523, 0.6567, 9.7605.",
    "ﬁdence limits of 0.6831, 0.2142, 221.4675, and upper 95% conﬁdence limits of 0.9523, 0.6567, 9.7605. 11.12 Use Problem 11.2 with t¼ t,a¼(a/C3/C3þkþ2)=2, and b¼½(b/C0f/C3)0V/C01 /C3(b/C0f/C3)þ(y0/C0x0 0b)2þd/C3/C3/C138=2. Chapter 12 12.1/C22m1:þ/C22m2:¼m11þm12 2þm21þm22 2¼m11þm12þm21þm22 2 ¼2m11þm12þm21þm22 4/C16/C17 ¼2/C22m::594 ANSWERS AND HINTS TO THE PROBLEMS --- Page 604 --- 12.2 The deﬁciency in the rank of Xdoes not affect the differentiation of ^10^1in (12.10).",
    "e 604 --- 12.2 The deﬁciency in the rank of Xdoes not affect the differentiation of ^10^1in (12.10). Thus @ˆ10ˆ1 @^b¼0/C02X0yþ2X0X^b¼0, which yields (12.11). 12.3 For Theorem 2.7a, the coefﬁcient matrix is A¼X0X, and the augmented matrix is B¼(X0X,X0y). We can write BasX0(X,y). which leads to rank( B)/C20rank( X0)¼rank( A). On the other hand, rank( B)/C21rank( A) because augmenting a matrix by a column vector cannot decrease the column rank.",
    "rank( B)/C21rank( A) because augmenting a matrix by a column vector cannot decrease the column rank. Hence rank( B)¼rank( A); that is, rank( X0X,X0y)¼ rank( X0X), and the system is consistent. 12.4 (a) We can obtain l0X0X(X0X)/C0¼l0from the expression X(X0X)/C0X0X¼X given in Theorem 2.8c(iii). Since l0¼a0X, multiplying by a0gives the result; that is, a0X(X0X)/C0X0X¼a0Ximpliesl0(X0X)/C0X0X¼l0.",
    "Since l0¼a0X, multiplying by a0gives the result; that is, a0X(X0X)/C0X0X¼a0Ximpliesl0(X0X)/C0X0X¼l0. (b)The condition X0X(X0X)/C0l¼lfollows from Theorem 2.8f, which states that Ax¼chas a solution if and only if AA2c¼cfor any gener- alized inverse of A. Thus, X0Xr¼l, has a solution if and only if X0X(X0X)/C0l¼l. 12.5 (a) a0X¼(0,0,0,1,0,0)X¼(1,0,1).X0Xr¼l,w h e r e r¼(0,0,1 3)0. Show that X0X(X0X)/C0l¼(1,0,1)0. These values of aandrare illustra- tive. Many others are possible.",
    "Show that X0X(X0X)/C0l¼(1,0,1)0. These values of aandrare illustra- tive. Many others are possible. (b)We attempt to ﬁnd a vector asuch that a0X¼l0¼(0,1,1). Since Xhas only two distinct rows, a0Xis of the form a1(1,1,0)þ a2(1,0,1)¼(a1þa2,a1,a2) which must equal (0, 1, 1). This gives a1þa2¼0,a1¼1, and a2¼1, which is clearly impossible. By Theorem 2.8f, the system of equations X0Xr¼lhas a solution if and only if X0X(X0X)/C0l¼l. This is also condition (iii) of Theorem 11.2b.",
    "s X0Xr¼lhas a solution if and only if X0X(X0X)/C0l¼l. This is also condition (iii) of Theorem 11.2b. We ﬁnd that X0X(X0X)/C0l¼011 010 0010 @1A0 1 10 @1A¼2 1 10 @1A, which is not equal to l.ANSWERS AND HINTS TO THE PROBLEMS 595 --- Page 605 --- 12.6 Multiply the two sets of normal equations by r0, where r0X0X¼l0: r0X0X^b1¼r0X0y r0X0X^b2¼r0X0y: Since the right sides are equal, we obtain r0X0X^b1¼r0X0X^b2,o r l0^b1¼l0^b2.",
    "¼r0X0y r0X0X^b2¼r0X0y: Since the right sides are equal, we obtain r0X0X^b1¼r0X0X^b2,o r l0^b1¼l0^b2. 12.7 In the answer to Problem 11.5a, a solution to X0Xr¼lis given as r¼(0,0,1 3)0. Thus r0X0y¼(0,0,13)y:: y1: y2:0 @1A¼ y2: 3¼/C22y2:: Forl0^b,w eu s e ^b¼^m /C22y1:/C0^m /C22y2:/C0^m0@1A from Example 12.3.1.",
    ": y2:0 @1A¼ y2: 3¼/C22y2:: Forl0^b,w eu s e ^b¼^m /C22y1:/C0^m /C22y2:/C0^m0@1A from Example 12.3.1. Then l0^b¼(1,0,1)^m /C22y1:/C0^m /C22y2:/C0^m0@1A¼^ mþ/C22y2:/C0^m¼/C22y2:: 12.8 (a) X0Xr¼lis given by 633 330 3030 B@1 CAr1 r2 r30 B@1 CA¼1 1 00 B@1 CA,or 6r1þ3r2þ3r3¼1 3r1þ3r2¼1 3r1þ3r3¼0 Using the last two equations, we obtain r1¼/C0r3 r2¼r3þ1 3,596 ANSWERS AND HINTS TO THE PROBLEMS --- Page 606 --- or r¼r1 r2 r30 @1A¼/C0r 3 r3þ1 3 r30 @1A¼r 3/C01 1 10 @1Aþ0 1 3 00 @1A, where r 3is an arbitrary constant that we can denote by c.",
    "3 r30 @1A¼r 3/C01 1 10 @1Aþ0 1 3 00 @1A, where r 3is an arbitrary constant that we can denote by c. (b)The BLUE, r0X0y, is given by r0X0y¼(/C0c,cþ1 3,c)y:: y1: y2:0 B@1 CA ¼/C0cy::þcy1:þ1 3y1:þcy2: ¼/C0c(y1:þy2:)þcy1:þ13y1:þcy2:¼13y1:: Show that y::¼y1:þy2: 12.9 (a) From Example 12.2.2(b), we have b¼m a1 a2 b1 b20 BBBB@1 CCCCA, mþa1þb1¼(1,1,0,1,0)b¼l0 1b: Show that X0X¼42222 22011 20211 21120211020 BBBB@1 CCCCA,X 0y¼y:: y1: y2: y:1 y:20 BBBB@1 CCCCA: The value r0¼(0,1 2,0,14,/C014) gives r0X0X¼l0 1.",
    "BB@1 CCCCA,X 0y¼y:: y1: y2: y:1 y:20 BBBB@1 CCCCA: The value r0¼(0,1 2,0,14,/C014) gives r0X0X¼l0 1. Then r0X0y¼y1: 2þy:1 4/C0y:2 4: Forl0 2b¼b1/C0b2¼(0,0,0,1,/C01)b, a convenient value for ris r0¼(0,0,0,1 2,/C012), which gives r0X0y¼12y:1/C012y:2¼/C22y:1/C0/C22y:2.",
    "a convenient value for ris r0¼(0,0,0,1 2,/C012), which gives r0X0y¼12y:1/C012y:2¼/C22y:1/C0/C22y:2. The function l0 3b¼a1/C0a2¼(0,1,/C01,0,0)bcan be obtained using r0¼(0,1 2,/C012,0,0), which leads to r0X0y¼12y1:/C012y2:¼/C22y1:/C0/C22y2:ANSWERS AND HINTS TO THE PROBLEMS 597 --- Page 607 --- (b) E(r0 1X0y)¼Ey1: 2þy:1 4/C0y:2 4/C16/C17 ¼1 4E½2(y11þy12)þ(y11þy21)/C0(y12þy22)/C138 ¼1 4E(3y11þy12þy21/C0y22) ¼14½3(mþa1þb1)þmþa1 þb2þmþa2þb1/C0(mþa2þb2)/C138 ¼14(4mþ4a1þ4b1)¼mþa1þb1¼l0 1b: 12.10 (a) The function l0b¼(0,c1,c2,...,ck)b¼Pk i¼1citiis estimable if there exists a vector a such that l0¼a0X.",
    "e function l0b¼(0,c1,c2,...,ck)b¼Pk i¼1citiis estimable if there exists a vector a such that l0¼a0X. The kdistinct rows of Xare of the formx0 i¼(1,0,...,0,1,0,...,0), so that l0¼a0X¼Xk i¼1aix0 i¼X iai,a1,a2,...,ak ! : Equating this to l0¼(0,c1,c2,...,ck), we obtainPk i¼1ai¼0,ai¼cii¼1,2,...,k. ThusP ici¼0.",
    ",...,ak ! : Equating this to l0¼(0,c1,c2,...,ck), we obtainPk i¼1ai¼0,ai¼cii¼1,2,...,k. ThusP ici¼0. (b)Any estimable function can be found as a0Xb, which gives a0Xb¼a0E(y)¼Xk i¼1Xn j¼1aijE(yij) ¼X iX jaij(mþti)¼X i(mþti)X jaij\"# ¼X i(mþti)ai:¼mX iai:þX iai:ti ¼mX iciþX iciti, where ci¼ai:¼P jaij. ThusP icitiis estimable if and only ifP ici¼0.",
    "X iai:þX iai:ti ¼mX iciþX iciti, where ci¼ai:¼P jaij. ThusP icitiis estimable if and only ifP ici¼0. 12.11 In Example 12.2.2(a), part (ii), we have X0X¼633 330 3030 @1A:598 ANSWERS AND HINTS TO THE PROBLEMS --- Page 608 --- Then for l¼(0,1,/C01)0,X0Xr¼lbecomes 6r1þ3r2þ3r3¼0 3r1þ3r2¼1 3r1þ3r3¼/C01: Show that all solutions are given by r¼c1 /C01 /C010 @1Aþ 1 30 1 /C010 @1A, where cis arbitrary. Show that r 0X0y¼/C22y1:/C0/C22y2:for all values of c.",
    "0 @1Aþ 1 30 1 /C010 @1A, where cis arbitrary. Show that r 0X0y¼/C22y1:/C0/C22y2:for all values of c. 12.12 Use to Corollary 2 Theorem 3.6d(ii) to obtain cov( r0 1X0y,r02X0y)¼ r01X0cov(y)Xr2and cov(l0 1^b,l02^b)¼l01cov( ^b)l2. 12.13 (a) (y/C0X^b)0(y/C0X^b)¼y0y/C0y0X^b/C0^b0X0yþ^b0X0X^b. Since y0X^bis a scalar, it is equal to its transpose ^b0X0y. The last term, ^bX0X^b, becomes ^b0X0ybecause X0X^b¼X0y.",
    "calar, it is equal to its transpose ^b0X0y. The last term, ^bX0X^b, becomes ^b0X0ybecause X0X^b¼X0y. (b)Using ^b¼(X0X)/C0X0y,w eh a v e y0y/C0^b0X0y¼y0y/C0y0X½(X0X)/C0/C1380X0y ¼y0y/C0y0X(X0X)/C0X0y by Theorem 2.8c(ii). 12.14b0X0½I/C0X(X0X)/C0X0/C138Xb¼b0X0Xb/C0b0X0X(X0X)/C0X0Xb: By (2.58), X0X(X0X)/C0X0X¼X0X. 12.15 Follow the steps in the answer to Problem 7.21. Is there any step that must be altered because Xis not full-rank?",
    "eps in the answer to Problem 7.21. Is there any step that must be altered because Xis not full-rank? 12.16 (a) Since ^b¼(X0X)/C0X0yis a linear function of yfor a particular choice of (X0X)/C0, we can use Theorem 4.4a(ii) directly. (b)Show that I/C0X(X0X)/C0X0is idempotent. Then use Corollary 2 to Theorem 5.5. (c)Show that ( X0X)/C0X0½I/C0X(X0X)/C0X0/C138¼O, and then invoke Corollary 1 to Theorem 5.6a. 12.17 Sinceg¼UbandXb¼Zg,w eh a v e l0b¼a0Xb¼a0Zg¼b0g. Thus dl0b¼cb0g¼b0^g.",
    "ollary 1 to Theorem 5.6a. 12.17 Sinceg¼UbandXb¼Zg,w eh a v e l0b¼a0Xb¼a0Zg¼b0g. Thus dl0b¼cb0g¼b0^g. Similarly, with d¼VbandXb¼Wd,w eh a v e l0b¼a0Xb¼a0Wd¼c0danddl0b¼cc0d¼c0^d.ANSWERS AND HINTS TO THE PROBLEMS 599 --- Page 609 --- 12.18 XU0¼21 21 12 120 BB@1 CCA,UU0¼21 12/C18/C19 : 12.19 Z¼1010 1/C01 1/C010 BB@1 CCA,U¼11 0 01 /C01/C18/C19 : 12.20 The normal equations are given by 4^ mþ2^t1þ2^t2¼y:: 2^mþ2^t1¼y1: 2^mþ2^t2¼y2: Substituting ^bin (12.39) into the ﬁrst of these, for example, gives 4/C22y::þ2(/C22y1:/C0/C22y::)þ2(/C22y2:/C0/C22y::)¼y:: 4y:: 4þ2y1: 2/C0y:: 4/C16/C17 þ2y2: 2/C0y:: 4/C16/C17 ¼y:: y::þy1:þy2:/C0y::¼y:: y::¼y:: 12.21a1/C0a2¼0 givesa1¼a2.",
    "0y:: 4/C16/C17 þ2y2: 2/C0y:: 4/C16/C17 ¼y:: y::þy1:þy2:/C0y::¼y:: y::¼y:: 12.21a1/C0a2¼0 givesa1¼a2. Substituting this into a1þa2/C02a3¼0 gives 2a2/C02a3¼0o ra2¼a3. 12.22 Express SSH as a quadratic form in yby substituting ^b¼(X0X)/C0X0y. Show that SSH is independent of SSE in (12.21) by use of Corollary 1 to Theorem 5.6b. Use either C(X0X)/C0X0X¼CorX0X(X0X)/C0X0¼X0.",
    "f SSE in (12.21) by use of Corollary 1 to Theorem 5.6b. Use either C(X0X)/C0X0X¼CorX0X(X0X)/C0X0¼X0. 12.23 The ﬁrst normal equation, for example, is 6 ^mþ2^a1þ2^a2þ2^a3þ3^b1þ 3^b2¼y::, which simpliﬁes to 6 ^m¼y::when we use the two side conditions.600 ANSWERS AND HINTS TO THE PROBLEMS --- Page 610 --- 12.24 X2¼110 110110 101 1011010 BBBBBB@1 CCCCCCA,X 0 2X2¼633 330 3030 @1A, b2¼m b1 b20@1A, X 0 2y¼y:: y:1 y:20 @1A: Then X 0 2X2^b2¼X02ygives the result in (12.51).",
    "3030 @1A, b2¼m b1 b20@1A, X 0 2y¼y:: y:1 y:20 @1A: Then X 0 2X2^b2¼X02ygives the result in (12.51). 12.25 (a) X¼1100 11001100 1010 1010 1010 10011001 10010 BBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCA,X 0X¼9333 3300 3030 30030 BBB@1 CCCA, X0y¼y:: y1: y2: y3:0 BBB@1 CCCA: The normal equations are given by 9333 3300 3030 30300 BBB@1 CCCA^ m ^t1 ^t2 ^t30 BBB@1 CCCA¼y:: y1: y2: y3:0 BBB@1 CCCA,or 9^mþ3^t1þ3^t2þ3^t3¼y:: 3^mþ3^ti¼yi:i¼1,2,3ANSWERS AND HINTS TO THE PROBLEMS 601 --- Page 611 --- (b)Three possible sets of linearly independent estimable functions are {mþt1,mþt2,mþt3} {3mþt1þt2þt3,t1/C0t2,t2/C0t3} {mþt1,t1/C0t2,t2/C0t3}: (c)The side condition ^t1þ^t2þ^t3¼0 gives ^m¼y:: 9¼/C22y:: ^ti¼1 3yi:/C019y::¼/C22yi:/C0/C22y::i¼1,2,3: (d)The hypothesis H0:t1¼t2¼t3is equivalent to H0:t1/C0t2¼0 and t1/C0t3¼0; hence H0is testable: SS(m,t)¼^b0X0y¼^my::þX3 i¼1^tiyi: ¼/C22y::y::þX3 i¼1yi: 3/C0y:: 9/C16/C17 yi: ¼y2 :: 9þX3 i¼1y2i: 3/C0y2:: 9¼X3 i¼1y2i: 3: The reduced model is yijþmþ1ij, the X2matrix reduces to a single column of 1’s, and the normal equations become 9^m¼y:: ^m¼y:: 9¼/C22y::: Hence SS(m)¼^b0 2X0 2y¼/C22y::y::¼y2 :: 9: (e) Analysis of Variance for H0:t15t25t3 Sum of Squares df FStatistic SS(tjm)¼P3 i¼1y2 i: 3/C0y2:: 92 SS(tjm)=2 SSE =6 SSE¼P ijy2ij/C0P iy2i: 36 SST¼P ijy2ij/C0y2:: 98602 ANSWERS AND HINTS TO THE PROBLEMS --- Page 612 --- 12.26 (a) The normal equations X0X^b¼X0yare given by 1 266663333 6 60333300 6 06330033 6 33603030 6 33060303 3 303030003 03300030 3 030300030 BBBBBBBBBBBBB@1 CCCCCCCCCCCCCA, or^ m ^a1 ^a2 ^b1 ^b2 ^g11 ^g12 ^g21 ^g220 BBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCA¼y ...",
    "CCCCCCCCCCCCA, or^ m ^a1 ^a2 ^b1 ^b2 ^g11 ^g12 ^g21 ^g220 BBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCA¼y ... y1:: y2:: y:1: y:2: y11: y12: y21: y22:0 BBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCA,or 12^ mþ6X2 i¼1^aiþ6X2 j¼1^bjþ3X ij^gij¼y:: 6^mþ6^aiþ3X2 j¼1^bjþ3X2 j¼1^gij¼yi::i¼1,2 6^mþ3X2 i¼1^aiþ6^bjþ3X2 i¼2^gij¼y:j:j¼1,2 3^mþ3^aiþ3^bjþ3^gij¼yij:i¼1,2j¼1,2 (b)The rank of X0Xis 4.",
    "6^mþ3X2 i¼1^aiþ6^bjþ3X2 i¼2^gij¼y:j:j¼1,2 3^mþ3^aiþ3^bjþ3^gij¼yij:i¼1,2j¼1,2 (b)The rank of X0Xis 4. From the last four rows, which are linearly indepen- dent, we obtain mþa1þb1þg11 mþa1þb2þg12 mþa2þb1þg21 mþa2þb2þg22 or mþa1þb1þg11 a1/C0a2þg11/C0g21(ora1/C0a2þg12/C0g22) b1/C0b2þg11/C0g12(orb1/C0b2þg21/C0g22) g11/C0g12/C0g21þg22:ANSWERS AND HINTS TO THE PROBLEMS 603 --- Page 613 --- 12.27 (a) The normal equations are 8444444 4402222 4042222 4224022 4220422 4222240 42222040 BBBBBBBB@1 CCCCCCCCA^ m ^a1 ^a2^b1^b2 ^g1 ^g20 BBBBBBBB@1 CCCCCCCCA¼y ...",
    "2 4220422 4222240 42222040 BBBBBBBB@1 CCCCCCCCA^ m ^a1 ^a2^b1^b2 ^g1 ^g20 BBBBBBBB@1 CCCCCCCCA¼y ... y1:: y2:: y:1: y:2: y::1 y::20 BBBBBBBB@1 CCCCCCCCA (b) mþa1þb1þg1 a1/C0a2 b1/C0b2 g1/C0g2 (c)Using the side conditions ^a1þ^a2¼0,^b1þ^b2¼0,^g1þ^g2¼0, we obtain ^m¼/C22y...,^ai¼/C22yi::/C0/C22y...,^bj¼/C22y:j:/C0/C22y...,^gk¼/C22y::k/C0/C22y.... (d) SS(m,a,b,g)¼^b0X0y¼/C22y...y...þX i(/C22yi::/C0/C22y...)yi:: þX j(/C22y:j:/C0/C22y...)y:j:þX k(/C22y::k/C0/C22y...)y::k ¼y2 ... 8þX iy2i:: 4/C0y2...",
    "C22y...)yi:: þX j(/C22y:j:/C0/C22y...)y:j:þX k(/C22y::k/C0/C22y...)y::k ¼y2 ... 8þX iy2i:: 4/C0y2... 8þX jy2:j: 4/C0y2... 8þX ky2::k 4/C0y2... 8 ¼SS(m)þSS(a)þSS(b)þSS(g): Using this same notation, the reduced normal equations under H0:a1=a2become SS( m,b,g)þSS(m)þSS(b)þSS(g).",
    "this same notation, the reduced normal equations under H0:a1=a2become SS( m,b,g)þSS(m)þSS(b)þSS(g). (e) Analysis of Variance for H0:t15t25t3 Source df Sum of Squares F SS(ajm,b,g) 1 SS( m,a,b,g)/C0SS(m,b,g)¼SS(a) SS(ajm,b,g) SSE =4 Error 4 SSE¼P ijky2 ijk/C0SSE(m,a,b,g)604 ANSWERS AND HINTS TO THE PROBLEMS --- Page 614 --- 12.28 X0X¼844442222 440222200 404220022402402020 422040202 220202000220020200 202200020 2020200020 BBBBBBBBBBBB@1 CCCCCCCCCCCCA: Chapter 13 13.1 X 0X¼kj0jj0jj0j...j0j j0jj0j0 ...",
    "202200020 2020200020 BBBBBBBBBBBB@1 CCCCCCCCCCCCA: Chapter 13 13.1 X 0X¼kj0jj0jj0j...j0j j0jj0j0 ... 0 j0j0j0j... 0 ............ j0j00 ...j0j0 BBBBBBB@1 CCCCCCCA¼kn n n ... n nn 0... 0 n0n... 0 ............ n00 ... n0 BBBBBBB@1 CCCCCCCA, X 0y¼P ij0yi j0y1 j0y2 ... j0yk0 BBBBBBB@1 CCCCCCCA¼P iyi: y1: y2: ... yk:0 BBBBBBB@1 CCCCCCCA¼y :: y1: y2: ... yk:0 BBBBBBB@1 CCCCCCCA: 13.2 ^ b¼(X0X)/C0X0y¼00 ... 0 01 =n... 0 ......... 00 ... 1=n0 BBB@1 CCCAy :: y1: ... yk:0 BBB@1 CCCA¼0 y 1:=n ...",
    "/C0X0y¼00 ... 0 01 =n... 0 ......... 00 ... 1=n0 BBB@1 CCCAy :: y1: ... yk:0 BBB@1 CCCA¼0 y 1:=n ... yk:=n0 BBB@1 CCCA¼0 /C22y 1: ... /C22yk:0 BBB@1 CCCA: 13.3 X ij(yij/C0/C22yi:)2¼Xk i¼1Xn j¼1(y2 ij/C02yij/C22yi:þ/C22y2i:) ¼X ijy2ij/C02X iyi: nX jyij ! þnX iy2 i: n2 ¼X ijy2 ij/C02X iy2 i: nþX iy2i: n:ANSWERS AND HINTS TO THE PROBLEMS 605 --- Page 615 --- 13.4 X(X0X)/C0X0¼jj0 ...0 j0j ...0 ............ j00 ...j0 BBBBB@1 CCCCCA00 0 ... 0 01 =n 0 ... 0 001 =n... 0 ............ 00 0 ...",
    "...0 ............ j00 ...j0 BBBBB@1 CCCCCA00 0 ... 0 01 =n 0 ... 0 001 =n... 0 ............ 00 0 ... 1=n0 BBBBBBB@1 CCCCCCCA /C2j0j0j0...j0 j00000...00 00j000...00 ............ 000000...j00 BBBBBBB@1 CCCCCCCA ¼0 1 nj0 ... 0 001 nj... 0 ............ 00 0...1 nj0 BBBBBB@1 CCCCCCAj 0j0j0...j0 j00000...00 00j000...00 ............ 000000...j00 BBBBBBB@1 CCCCCCCA ¼1 njj0O ...O Oj j0...O .........",
    "0000...00 00j000...00 ............ 000000...j00 BBBBBBB@1 CCCCCCCA ¼1 njj0O ...O Oj j0...O ......... OO ...jj00 BBBBB@1 CCCCCA, y 0½I/C0X(X0X)/C0X0/C138y¼y0y/C0y0X(X0X)/C0X0y ¼X ijy2 ij/C01 n(y0 1,y02,...,y0k)jj0O ...O Oj j0...O ......... OO ...jj00 BBBBB@1 CCCCCAy1 y2 ...",
    "y ¼X ijy2 ij/C01 n(y0 1,y02,...,y0k)jj0O ...O Oj j0...O ......... OO ...jj00 BBBBB@1 CCCCCAy1 y2 ... yk0 BBBBB@1 CCCCCA ¼X ijy2 ij/C01 nXk i¼1y0 ijj0yi¼X ijy2ij/C01 nX iy2 i: 13.5 (a) Witha/C3 i¼mi/C0/C22m:in (13.5), H0:a/C3i¼a/C32¼/C1/C1/C1¼a/C3kin (13.18) becomes H0:m1/C0/C22m:¼m2/C0/C22m:¼/C1/C1/C1¼mk/C0/C22m:or H0:m1¼ m2¼/C1/C1/C1mk, which is equivalent to (13.7). (b)Denote by a/C3the common value of a/C3 iinH0:a/C31¼a/C32¼/C1/C1/C1¼a/C3kin (13.18).",
    "nt to (13.7). (b)Denote by a/C3the common value of a/C3 iinH0:a/C31¼a/C32¼/C1/C1/C1¼a/C3kin (13.18). ThenP ia/C3i¼0 givea/C3¼0, sincePk i¼1a/C3¼a/C3 i¼0,i¼ 1,2,...,k.",
    "1¼a/C32¼/C1/C1/C1¼a/C3kin (13.18). ThenP ia/C3i¼0 givea/C3¼0, sincePk i¼1a/C3¼a/C3 i¼0,i¼ 1,2,...,k. Thus,a/C3i¼0,i¼1,2,...,k:606 ANSWERS AND HINTS TO THE PROBLEMS --- Page 616 --- 13.6nXk i¼1(/C22yi:/C0/C22y::)2¼nX i(y2 i:/C02/C22yi:/C22y::þ/C22y2::) ¼nX i/C22y2i:/C02n/C22y::X i/C22yi:þkn/C22y2:: ¼nX iyi: n/C16/C172 /C02ny:: knX iyi: nþkny:: kn/C16/C172 ¼1 nX iy2 i:/C02y:: ky:: nþy2 :: kn: 13.7 See the ﬁrst part of the answer to Problem 13.3.",
    "/C16/C172 ¼1 nX iy2 i:/C02y:: ky:: nþy2 :: kn: 13.7 See the ﬁrst part of the answer to Problem 13.3. 13.9 Using Xin (13.6), we have C(X0X)/C0X0¼01 /C0100 01 0 /C010 01 0 0 /C010 B@1 CA00000 01000 00100 00010 000010 BBBBBB@1 CCCCCCA /C2j njn00 0 jn0jn00 jn0 00jn0 B@1 CA0 ¼01 /C0100 01 0 /C010 01 0 0 /C010 B@1 CAj0 nj0nj0nj0n j0n000000 00j0n0000 0000j0 n00 000000j0 n0 BBBBBB@1 CCCCCCA ¼j 0 n/C0j0n0000 j0n00/C0j0n00 jn 000/C0j0n0 B@1 CA:ANSWERS AND HINTS TO THE PROBLEMS 607 --- Page 617 --- 13.10 By (2.37), we obtain Aj3¼jnjnjn /C0jn00 0/C0jn0 00 /C0jn0 BBB@1 CCCA1 1 10 B@1 CA ¼jn /C0jn 0 00 BBB@1 CCCAþj n 0 /C0jn 00 BBB@1 CCCAþj n 0 0 /C0jn0 BBB@1 CCCA¼3j n /C0jn /C0jn /C0jn0 BBB@1 CCCA, AJ 3A0¼Aj3j0 3A0¼3jn /C0j0 n /C0j0 n /C0j0n0 BBB@1 CCCA(3jn,/C0jn,/C0jn,/C0jn) ¼9Jn/C03Jn/C03Jn/C03Jn /C03Jn Jn Jn Jn /C03Jn Jn Jn Jn /C03Jn Jn Jn Jn0 BBB@1 CCCA: 13.11 (a) E(1ij)2¼E(1ij/C00)2¼E½1ij/C0E(1ij)/C1382¼var(1ij)¼s2, E(1ij1i0j0)¼E½1ij/C00)(1i0j0/C00)/C138 ¼E½1ij/C0E(1ij)/C138½1i0j0/C0E(1i0j0)/C138 ¼cov(1ij,1i0j0)¼0: (b) E(y2 ::)¼EXn ijyij !2 ¼EXn i¼1Xn j¼1(m/C3þa/C3iþ1ij)\"#2 ¼Ek nm/C3þnX ia/C3iþX ij1ij\"#2 ¼Ek2n2m/C32þX ij1ij !2 þ2knm/C3X ij1ij2 435 ¼Ek 2n2m/C32þX ij12 ijþX ij=lm1ij1lmþ2knm/C3X ij1ij !",
    "#2 ¼Ek2n2m/C32þX ij1ij !2 þ2knm/C3X ij1ij2 435 ¼Ek 2n2m/C32þX ij12 ijþX ij=lm1ij1lmþ2knm/C3X ij1ij ! ¼k2n2m/C32þkns2,608 ANSWERS AND HINTS TO THE PROBLEMS --- Page 618 --- EXk i¼1y2 i: ! ¼EXk i¼1Xn j¼1yij !22 435 ¼EX iX j(m/C3þa/C3 iþ1ij)\"#28 < :9 = ; ¼EX inm/C3þna/C3 iþX j1ij !22 435 ¼EX in2m/C32þn2a/C32 iþX j1ij !2 þ2n2m/C3a/C3 i2 48 < : þ2nm/C3X j1ijþ2nX ja/C3 i1ij#) ¼Ek n2m/C32þn2X ia/C32 iþX iX j12 ijþX j=11ij1il !",
    "m/C3a/C3 i2 48 < : þ2nm/C3X j1ijþ2nX ja/C3 i1ij#) ¼Ek n2m/C32þn2X ia/C32 iþX iX j12 ijþX j=11ij1il ! \" þ2n2m/C3X ia/C3 iþ2nm/C3X ij1ijþ2nX iX ja/C3i1ij# ¼kn2m/C32þn2X ia/C32 iþkns2, E½SS(ajm)/C138¼E1 nX iy2 i:/C01 kny2:: ! ¼1 nkn2m/C32þn2X ia/C32 iþkns2 ! /C01 kn(k2n2m/C32þkns2) ¼knm/C32þnX ia/C32 iþks2/C0knm/C32/C0s2 ¼(k/C01)s2þnX ia/C32 i: (c) EXk i¼1Xn j¼1y2 ij !",
    "m/C32þkns2) ¼knm/C32þnX ia/C32 iþks2/C0knm/C32/C0s2 ¼(k/C01)s2þnX ia/C32 i: (c) EXk i¼1Xn j¼1y2 ij ! ¼EX ij(m/C3þa/C3iþ1ij)2\"# ¼EX ij(m/C32þa/C32 iþ12ijþ2m/C3a/C3iþ2m/C31ijþ2a/C3i1ij)\"#ANSWERS AND HINTS TO THE PROBLEMS 609 --- Page 619 --- ¼Ek nm/C32þnX ia/C32 iþX ij12 ijþ2nm/C3X ia/C3i\" þ2m/C3X ij1ijþ2X ija/C3i1ij# ¼knm/C32þnX ia/C32 iþkns2, E(SSE) ¼EX ijy2ij/C01 nX iy2 i: !",
    "3X ia/C3i\" þ2m/C3X ij1ijþ2X ija/C3i1ij# ¼knm/C32þnX ia/C32 iþkns2, E(SSE) ¼EX ijy2ij/C01 nX iy2 i: ! ¼knm/C32þnX ia/C32 iþkns2/C0knm/C32/C0nX ia/C32 i/C0ks2 ¼k(n/C01)s2: 13.13 By (2.37), C0j3¼000 111 /C0100 0/C010 00 /C010 BBBB@1 CCCCA1 110 @1A¼0 1 /C01 0 00 BBBB@1 CCCCAþ0 1 0 /C01 00 BBBB@1 CCCCAþ0 1 00 /C010 BBBB@1 CCCCAþ0 3 /C01 /C01 /C010 BBBB@1 CCCCA: Thus C0J3¼C0(j3,j3,j3)¼(C0j3,C0j3,C0j3)¼000 333 /C01/C01/C01 /C01/C01/C01 /C01/C01/C010 BBBBBB@1 CCCCCCA, C0J3C¼000 333 /C01/C01/C01 /C01/C01/C01 /C01/C01/C010 BBBBBB@1 CCCCCCA01 /C0100 01 0 /C010 0 100 /C010 B@1 CA ¼00000 09 /C03/C03/C03 0/C03111 0/C03111 0/C031110 BBBBBB@1 CCCCCCA:610 ANSWERS AND HINTS TO THE PROBLEMS --- Page 620 --- 13.15 Using ( X0X)2in (13.11) and bˆin (13.12), we obtain c0^b¼(0,c1,c2,...,ck)0 /C22y1: /C22y2: ...",
    "13.15 Using ( X0X)2in (13.11) and bˆin (13.12), we obtain c0^b¼(0,c1,c2,...,ck)0 /C22y1: /C22y2: ... /C22yk:0 BBBBBBB@1 CCCCCCCA¼X k i¼1ci/C22y1:, c0(X0X)/C0c¼(0,c1,c2,...,ck)00 ... 0 01 =n... 0 ......... 00 ... 1=n0 BBBB@1 CCCCA0 c1 c2 ...",
    "22y1:, c0(X0X)/C0c¼(0,c1,c2,...,ck)00 ... 0 01 =n... 0 ......... 00 ... 1=n0 BBBB@1 CCCCA0 c1 c2 ... ck0 BBBBBB@1 CCCCCCA¼X k i¼1c2 i n: 13.16 Using ^b¼(X0X)/C0X0y, the sum of squares for the contrast c0i^bcan be expressed as (c0i^b)2 c0 i(X0X)/C0ci¼^b0cic0 i^b c0 i(X0X)/C0ci¼y0X(X0X)/C0cic0 i(X0X)/C0X0y c0 i(X0X)/C0ci, with a similar expression for the sum of squares for c0 j^b.",
    "0X)/C0cic0 i(X0X)/C0X0y c0 i(X0X)/C0ci, with a similar expression for the sum of squares for c0 j^b. By Corollary 1 to Theorem 5.6b, these two quadratic forms are independent if X(X0X)/C0cic0 i(X0X)/C0X0X(X0X)/C0cjc0j(X0X)/C0X0¼O: This holds if c0 i(X0X)/C0X0X(X0X)/C0cj¼0, which reduces to c0i(X0X)/C0cj¼0, since cibis an estimable function and therefore by Theorem 11.2b(iii), we havec0i(X0X)/C0X0X¼c0i.",
    "0, since cibis an estimable function and therefore by Theorem 11.2b(iii), we havec0i(X0X)/C0X0X¼c0i. Now by Theorem 12.3c, we obtain cov(c0 i^b,c0j^b)¼s2c0i(X0X)/C0cj: 13.17 (Ai)0¼(viv0i)0¼(v0i)0v0i¼viv0i¼Ai: (Ai)2¼viv0iviv0i¼viv0i¼Aisince v0ivi¼1: By Theorem 2.4(iii), rank( Ai)¼rank( vivi0)¼rank( vi)¼1.",
    "Ai: (Ai)2¼viv0iviv0i¼viv0i¼Aisince v0ivi¼1: By Theorem 2.4(iii), rank( Ai)¼rank( vivi0)¼rank( vi)¼1. AiAj¼viv0 ivjv0j¼Obecause v0ivj¼0 by Theorem 2.12c(ii).ANSWERS AND HINTS TO THE PROBLEMS 611 --- Page 621 --- 13.18 (a)J abn/C18/C192 ¼(jj0)2 (abn)2¼jj0jj0 (abn)2¼j(abn)j0 (abn)2¼J abn: (b)J abn/C18/C19 x1¼l1x1¼x1,sincel1¼1 jj0x1 abn¼x1: Clearly x1¼jis a solution, since j0j¼abn.",
    "J abn: (b)J abn/C18/C19 x1¼l1x1¼x1,sincel1¼1 jj0x1 abn¼x1: Clearly x1¼jis a solution, since j0j¼abn. 13.19v0 0v0¼j0 4nj4n 4n¼4n 4n¼1, v0 0v1¼1ﬃﬃﬃﬃﬃ 4npﬃﬃﬃﬃﬃ 2np (j0 n,j0n,j0n,j0n)jn /C0jn 0 00 BB@1 CCA¼0: 13.20 j0x2:01¼j0x2/C0j0x2 j0jj0j/C0x0 1:0x2 x0 1:0x1:0j0x1:0 ¼j0x2/C0j0x2/C00 ½by (13 :66)/C138, x0 1:0x2:01¼x01:0x2/C0j0x2 j0jx01:0j/C0x0 1:0x2 x0 1:0x1:0x0 1:0x1:0 ¼x01:0x2/C00/C0x01:0x2 ½by (13 :66)/C138: 13.21 By (7.97), we have x3:012¼x3/C0Z1(Z0 1Z1)/C01Z01x3, where Z1¼ (j,x1:0,x2:01).",
    "y (13 :66)/C138: 13.21 By (7.97), we have x3:012¼x3/C0Z1(Z0 1Z1)/C01Z01x3, where Z1¼ (j,x1:0,x2:01). Thus x3:012¼x3/C0(j,x1:0,x2:01)j0j 00 0x0 1:0x1:0 0 00 x02:01x2:010 B@1 CA/C01j0x3 x0 1:0x3 x02:01x30 B@1 CA ¼x3/C0j0x3 j0jj/C0x0 1:0x3 x0 1:0x1:0x1:0/C0x0 2:01x3 x0 2:01x2:01x2:01: 13.22 Using (13.66) and (13.69), we have j0x3:012¼j0x3/C0j0x3 j0jj0j/C0x0 1:0x3 x0 1:0x1:0j0x1:0/C0x0 2:01x3 x0 2:01x2:01j0x2:01 ¼j0x3/C0j0x3/C00/C00, x0 1:0x3:012¼x01:0x3/C0j0x3 j0jx01:0j/C0x0 1:0x3 x0 1:0x1:0x0 1:0x1:0/C0x0 2:01x3 x0 2:01x2:01x0 1:0x2:01:612 ANSWERS AND HINTS TO THE PROBLEMS --- Page 622 --- 13.23 Show that the coefﬁcients in (13.70) are given by j0x3 j0j¼100n 4n¼25, x0 1:0x3 x0 1:0x1:0¼208 20¼10:4, x0 2:01x3 x0 2:01x2:01¼30 4¼7:5: Then by (13.70).",
    "3 j0j¼100n 4n¼25, x0 1:0x3 x0 1:0x1:0¼208 20¼10:4, x0 2:01x3 x0 2:01x2:01¼30 4¼7:5: Then by (13.70). z3¼x3/C025j/C010:4x1:0/C07:5x2:01 ¼(/C0:3,...,/C0:3,:9,...,:9,/C0:9,...,/C0:9,:3,...,:3)0, which we divide by .3 to obtain z3¼(/C01,.../C01,3,...3,/C03,...,/C03,1,...,1)0: 13.24 z0¼x0¼j, z1¼x1:0¼2(x1/C02:5j), z2¼x2:01¼x2/C07:5j/C02:5x1:0¼x2/C07:5j/C02:5½2(x1/C02:5j)/C138 ¼x2þ5j/C05x1, z3¼x3/C025j/C010:4x1:0/C07:5x2:01 :3 ¼x3/C025j/C010:4(2x1/C05j)/C07:5(x2þ5j/C05x1) :3 ¼x3 :3/C035jþ16:7 :3/C18/C19 x1/C025x2: Then Xb¼Zucan be written as b0jþb1x1þb2x2þb3x3¼u0jþu1z1þu2z2þu3z3 ¼u0jþu1(2x1/C05j)þu2(x2þ5j/C05x1) þu3x3 :3/C16/C17 /C035jþ16:7 :3/C18/C19 x1/C025x2/C20/C21 ¼(u0/C05u1þ5u2/C035u3)j þ2u1/C05u2þ16:7 :3/C18/C19 u3/C20/C21 x1 þ(u2/C025u3)x2þu3 :3/C18/C19 x3:ANSWERS AND HINTS TO THE PROBLEMS 613 --- Page 623 --- Thus b0¼u/C05u1þ5u2/C035u3 b1¼2u1/C05u2þ16:7 :3/C18/C19 u3 b2¼u2/C025u3 b3¼u3 :3: 13.25 Z0y¼(j,z1,z2,z3)0y¼j0 z0 1 z02 z0 30 BBB@1 CCCAy¼j0y z0 1y z02y z0 3y0 BBB@1 CCCA, ^u¼(Z0Z)/C01Z0y¼j0j 000 0z0 1z1 00 00 z02z2 0 00 0 z03z30 BBB@1 CCCA/C01j0y z0 1y z02y z03y0 BBB@1 CCCA ¼j0y=j0j z0 1y=z01z1 z02y=z02z2 z03y=z03z30 BBB@1 CCCA: 13.26 Since the columns of Zare linear transformations of the columns of X[see (13.65), (13.68), and (13.70)], we can write Z¼XHandZ1¼X1H1, where HandH1are nonsingular.",
    "s of X[see (13.65), (13.68), and (13.70)], we can write Z¼XHandZ1¼X1H1, where HandH1are nonsingular. Thus ^b0X0y/C0^b/C30X0 1y¼y0X(X0X)/C01X0y/C0y0X1(X01X1)/C01X01y ¼y0ZH/C01½(ZH/C01)0(ZH/C01)/C138/C01(ZH/C01)0y /C0y0Z1H/C01 1½(Z1H/C01 1)0(Z1H/C01 1)/C138/C01(Z1H/C01 1)0y: Show that this reduces to ( z0 ky)2=z0kzk.",
    "0Z1H/C01 1½(Z1H/C01 1)0(Z1H/C01 1)/C138/C01(Z1H/C01 1)0y: Show that this reduces to ( z0 ky)2=z0kzk. 13.27Linear :/C03(1)/C0(2)þ2þ3(1)¼0 Quadratic :1/C02/C02þ1¼/C02 Cubic :/C01þ3(2)/C03(2)þ1¼0614 ANSWERS AND HINTS TO THE PROBLEMS --- Page 624 --- 13.28 The orthogonal contrasts that can be used in H0:Pk i¼1cimi¼0 in part (b) are 2m1þ2m2þ2m3/C03m4/C03m5¼0 2m1/C0m2/C0m3¼0 m2/C0m3¼0 m4/C0m5¼0: The results for parts (a) and (b) are given in the following ANOVA table.",
    "0m3¼0 m2/C0m3¼0 m4/C0m5¼0: The results for parts (a) and (b) are given in the following ANOVA table. Source dfSum of SquaresMean Square Fp Value Breed 4 4,276.1327 1069.0332 8.47 .000033 Contrasts A, B, C vs. D, E1 211.7289 211.7289 1.68 .202 A, B, vs. C 1 370.6669 370.6669 2.94 .0933A vs. B 1 708.0500 708.0500 5.61 .0221D vs.",
    "7289 1.68 .202 A, B, vs. C 1 370.6669 370.6669 2.94 .0933A vs. B 1 708.0500 708.0500 5.61 .0221D vs. E 1 2,885.4545 2885.4545 22.86 .0000182 Error 46 5,806.4556 126.2273 Total 50 10,082.5882 13.29 The orthogonal polynomial contrast coefﬁcients are the rows of the follow- ing matrix see Table (13.5): /C02/C0101 2 2/C01/C02/C012 /C0120 /C021 1/C046 /C0410 BB@1 CCA: The results for parts (a) and (b) are given in the following ANOVA table.",
    "21 1/C046 /C0410 BB@1 CCA: The results for parts (a) and (b) are given in the following ANOVA table. Source dfSum of SquaresMean Square Fp Value Glucose 4 154.9210 38.7303 29.77 7.902 /C210211 Contrasts Linear 1 140.1587 140.1587 107.74 3.168 /C210212 Quadratic 1 0.0065 0.0065 0.006 .944 Cubic 1 14.7319 14.7319 11.32 .002Quartic 1 0.0241 0.0241 0.021 .893 Error 35 45.5322 1.3009 Total 39 200.4532ANSWERS AND HINTS TO THE PROBLEMS 615 --- Page 625 --- The means for the ﬁve glucose concentrations are 2.66, 2.69, 4.94, 7.09, and 7.10.",
    "--- Page 625 --- The means for the ﬁve glucose concentrations are 2.66, 2.69, 4.94, 7.09, and 7.10. From the Fs we see that there is a large linear effect and a small cubic effect. 13.30 The contrast coefﬁcients are given in the following matrix: 2/C01/C01 01 /C01/C18/C19 : The results for parts (a) and (b) are given in the following ANOVA table. Source dfSum of SquaresMean Square Fp Value Stimulus 2 561.5714 280.7857 67.81 2.018 /C210213 Contrasts 1 vs.",
    "Sum of SquaresMean Square Fp Value Stimulus 2 561.5714 280.7857 67.81 2.018 /C210213 Contrasts 1 vs. 2, 3 1 525.0000 252.0000 126.78 8.005 /C210214 2 vs. 3 1 36.5714 36.5714 8.83 .00505 Error 39 161.5000 4.1410 Total 41 723.0714 13.31 For contrast coefﬁcients comparing the two types of raw materials, we can use those in the vector (5, 5, 5, 5, 24,24,24,24,24). The results for parts (a) and (b) are in the following ANOVA table.",
    "or (5, 5, 5, 5, 24,24,24,24,24). The results for parts (a) and (b) are in the following ANOVA table. Source dfSum of SquaresMean Square Fp Value Cable 8 1924.2963 240.5370 9.07 2.831 /C21029 Contrast 1 1543.6463 1543.6463 58.18 1.493 /C210211 Error 99 2626.9167 26.5345 Total 107 4551.2130 13.32 Contrast coefﬁcients are given in the following matrix: 11 /C01/C01 1/C0100 001 /C010 @1A: The results for parts (a) and (b) are given in the following ANOVA table.616 ANSWERS AND HINTS TO THE PROBLEMS --- Page 626 --- Source dfSum of SquaresMean Square Fp Value Treatments 3 1045.4583 348.8461 6.03 .0043 Contrasts 1, 2 vs.",
    "e dfSum of SquaresMean Square Fp Value Treatments 3 1045.4583 348.8461 6.03 .0043 Contrasts 1, 2 vs. 3, 41 7.0417 7.0417 0.12 .731 1 vs. 2 1 30.0833 30.0833 0.52 .4793 vs. 4 1 1008.3333 1008.3333 17.44 .0005 Error 20 1156.5000 57.8250 Total 23 2201.9583 13.33 Contrast coefﬁcients are given in the following matrix: 111 /C03 11 /C020 1/C01000 @1A: The results for parts (a) and (b) are given in the following ANOVA table.",
    "C03 11 /C020 1/C01000 @1A: The results for parts (a) and (b) are given in the following ANOVA table. Source df Sum of SquaresMean SquareFp Value Treatments 3 3462.500 1154.167 6.71 .00103 Contrasts 1, 2, 3 vs. 41 1968.300 1968.300 11.44 .00175 1, 2 vs. 3 1 66.150 66.150 .385 .5391 vs.",
    "03 Contrasts 1, 2, 3 vs. 41 1968.300 1968.300 11.44 .00175 1, 2 vs. 3 1 66.150 66.150 .385 .5391 vs. 2 1 1428.050 1428.050 8.30 .0066 Error 36 6193.400 172.039 Total 39 9655.900 Chapter 14 14.1u1¼m11/C0m21¼mþa1þb1þg11/C0(mþa2þb1þg21) u5¼m11/C0m12/C0m31þm32 14.2 By Theorem 12.2b, all estimable functions can be obtained from mij¼ mþaiþbjþgij.",
    "0m12/C0m31þm32 14.2 By Theorem 12.2b, all estimable functions can be obtained from mij¼ mþaiþbjþgij. To obtain an estimable contrast of the formP iciai,ANSWERS AND HINTS TO THE PROBLEMS 617 --- Page 627 --- whereP ici¼0, we consider Xa i¼1cimij¼Xa i¼1cimiþX iciaiþX icibjþX icigij ¼X iciaiþX icigij: Thus an estimable function of the a’s also involves the g’s.",
    "iaiþX icibjþX icigij ¼X iciaiþX icigij: Thus an estimable function of the a’s also involves the g’s. 14.31 3(u3þu0 3þu003)¼1 3(b1/C0b2þg11/C0g12þb1/C0b2þg21/C0g22 þb1/C0b2þg31/C0g32) ¼13(3b1/C03b2þg11þg21þg31/C0g12/C0g22/C0g32): 14.4 (a)Xa i¼1a/C3 i¼X i(/C22mi:/C0/C22m::)¼X i/C22mi:/C0a/C22m:: ¼X imi: b/C0am:: ab¼m:: b/C0m:: b (c)Xa i¼1g/C3ij¼X i(mij/C0/C22mi:/C0/C22m:jþ/C22m::) ¼X imij/C0X i/C22mi:/C0a/C22m:jþa/C22m:: ¼m:j/C0X imi: b/C0am:j aþam:: ab ¼m:j/C0m:: b/C0m:jþm:: b: 14.5 (a) Xa i¼1a/C3i¼X i(ai/C0/C22a:þ/C22gi:/C0/C22g::) ¼X i(ai/C0/C22a:)þX i(/C22gi:/C0/C22g::) ¼a:/C0aa: aþX igi: b/C0g:: ab/C16/C17 ¼a:/C0a:þg:: b/C0ag:: ab: (c)Xa i¼1g/C3ij¼X i(gij/C0/C22gi:/C0/C22g:jþ/C22g::)¼X igij/C0gi: b/C0g:j aþg:: ab/C16/C17 ¼g:j/C0g:: b/C0ag:j aþag:: ab:618 ANSWERS AND HINTS TO THE PROBLEMS --- Page 628 --- 14.6 (b)g/C3 ij¼mij/C0/C22mi:/C0/C22m:jþ/C22m:: ¼mij/C01 bXb j¼1mij/C01aX a i¼1mijþ1 abXa i¼1Xb j¼1mij ¼mþaiþbjþgij/C01bX b j¼1(mþaiþbjþgij) /C01aX a i¼1(mþaiþbjþgij)þ1 abX ij(mþaiþbjþgij) ¼mþaiþbjþgij/C0m/C0ai/C01bX jbj/C01bX jgij /C0m/C01aX iai/C0bj/C01aX igijþmþ1aX iai þ1 bX jbjþ1 abX ijgij ¼gij/C0/C22gi:/C0/C22g:jþ/C22g::: 14.7 E(^ai)¼E(/C22yi::/C0/C22y...)¼Eyi:: bn/C16/C17 /C0Ey...",
    "ijgij ¼gij/C0/C22gi:/C0/C22g:jþ/C22g::: 14.7 E(^ai)¼E(/C22yi::/C0/C22y...)¼Eyi:: bn/C16/C17 /C0Ey... abn/C16/C17 ¼EP jkyijk bn/C18/C19 /C0EP ijkyijk abn/C18/C19 ¼P jkE(yijk) bn/C0P ijkE(yijk) abn ¼P jk(m/C3þa/C3 iþb/C3 jþg/C3 ij) bn/C0P ijk(m/C3þa/C3iþb/C3 jþg/C3 ij) abn ¼bnm/C3þbna/C3iþnP jb/C3 jþnP jg/C3 ij bn ¼abnm/C3þbnP ia/C3iþanP jb/C3 jþnP ijg/C3 ij abn ¼m/C3þa/C3 i/C0m/C3: 14.8 (a)F o r b¼2 and n¼2, we have /C22y11:þ/C22y12:¼1 2X ky11kþ12X ky12k¼12X jky1jk ¼124X jky1jk 4 !",
    "4.8 (a)F o r b¼2 and n¼2, we have /C22y11:þ/C22y12:¼1 2X ky11kþ12X ky12k¼12X jky1jk ¼124X jky1jk 4 ! ¼2/C22y1::ANSWERS AND HINTS TO THE PROBLEMS 619 --- Page 629 --- 14.9 Write X0XasX0X¼A11A12 A21 2I/C18/C19 .",
    "::ANSWERS AND HINTS TO THE PROBLEMS 619 --- Page 629 --- 14.9 Write X0XasX0X¼A11A12 A21 2I/C18/C19 . Then X0X(X0X)/C0¼A11A12 A21 2I/C18/C19OO O1 2I/C18/C19 ¼O1 2A12 OI/C18/C19 , X0X(X0X)/C0X0X¼1 2A12A21A12 A21 2I/C18/C19 : Show that1 2A12A21¼A11; that is, show that 12222222 220000 002200 000022 202020 0202020 BBBBBB@1 CCCCCCA220020 220002 202020 202002 200220 2002020 BBBBBB@1 CCCCCCA ¼1 244466 4 40022 4 040224 00422 6 22260 6 222060 BBBBBB@1 CCCCCCA: 14.10 X ijk(yijk/C0/C22yij:)2¼X ijk(y2 ijk/C02yijk/C22yij:þ/C22y2ij:) ¼X ijky2ijk/C02X ij/C22yij:X kyijkþnX ij/C22y2ij: ¼X ijky2ijk/C02X ij/C22yij:n/C22yij:þnX ij/C22y2ij:: 14.11 Fromm21/C0m22¼m31/C0m32,w eh a v e 0¼m21/C0m22/C0m31þm32 ¼mþa2þb1þg21/C0(mþa2þb2þg22) /C0(mþa3þb1þg31)þmþa3þb2þg32 ¼g21/C0g22/C0g31þg32:620 ANSWERS AND HINTS TO THE PROBLEMS --- Page 630 --- 14.12 X i(/C22yi::/C0/C22y...)yi::¼X iyi:: bn/C0y...",
    "WERS AND HINTS TO THE PROBLEMS --- Page 630 --- 14.12 X i(/C22yi::/C0/C22y...)yi::¼X iyi:: bn/C0y... abn/C16/C17 yi::¼X iy2 i:: bn/C0y... abnX iyi:: ¼X iy2i:: bn/C0y2... abn, X ij(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...)yij:¼X ijyij: n/C0yi:: bn/C0y:j: anþy... abn/C16/C17 yij: ¼X ijy2ij: n/C0X iyi::X jyij: bn ! /C0X jy:j:X iyij: an ! þy...X ijyij: abn ¼X ijy2ij: n/C0X iy2i:: bn/C0X jy2:j: anþy2...",
    "X jyij: bn ! /C0X jy:j:X iyij: an ! þy...X ijyij: abn ¼X ijy2ij: n/C0X iy2i:: bn/C0X jy2:j: anþy2... abn: 14.13 (a) Usingbˆfrom (14.24) and X0yfrom (14.19) (both extended to general a andb), we obtain ˆb0X0y¼X ij/C22yij:yij:¼X ijyij: n/C16/C17 yij:¼X ijy2ij: n: (b) nX ij/C22y2 ij:¼nX ijyij: n/C16/C172 ¼nX ijy2 ij: n2 14.14 In the following array, we see that the gij/C3’s in the margins can all be obtained from the remaining ( a21)(b21)gij/C3’s by using side conditions: g/C311g/C312 ...g/C31,b/C01g/C31b g/C321g/C322 ...g/C32,b/C01g/C32b ............",
    "ng side conditions: g/C311g/C312 ...g/C31,b/C01g/C31b g/C321g/C322 ...g/C32,b/C01g/C32b ............ g/C3 a/C01,1g/C3a/C01,2...g/C3a/C01,b/C01g/C3a/C01,b g/C3a1g/C3a2 ...g/C3a,b/C01g/C3abANSWERS AND HINTS TO THE PROBLEMS 621 --- Page 631 --- 14.15 By (5.1),Pn i¼1(yi/C0/C22y)2¼Pni¼1y2 i/C0n/C22y2.",
    "D HINTS TO THE PROBLEMS 621 --- Page 631 --- 14.15 By (5.1),Pn i¼1(yi/C0/C22y)2¼Pni¼1y2 i/C0n/C22y2. Then nX ij(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...)2 ¼nX ij½(/C22yij:/C0/C22yi::)/C0(/C22y:j:/C0/C22y...)/C1382 ¼nX ij(/C22yij:/C0/C22yi::)2þanX j(/C22y:j:/C0/C22y...)2/C02nX ij(/C22yij:/C0/C22yi::)(/C22y:j:/C0/C22y...) ¼nX ij/C22y2 ij:/C0bnX i/C22y2i::þanX j/C22y2:j:/C0abn/C22y2... /C02nX jy:j: an/C0y...",
    ":/C0/C22y...) ¼nX ij/C22y2 ij:/C0bnX i/C22y2i::þanX j/C22y2:j:/C0abn/C22y2... /C02nX jy:j: an/C0y... abn/C16/C17X iyij: n/C0yi:: bn/C16/C17\"# ¼nX ijy2 ij: n2/C0bnX iy2i:: b2n2þanX jy2:j: a2n2/C0abny2... a2b2n2 /C02nX jy:j: an/C0y... abn/C16/C17y:j: n/C0y... bn/C16/C17hi ¼X ijy2ij n/C0X iy2i:: bnþX jy2:j: an/C0y2... abn/C02 anX jy2 :j:/C02y...y:j: bþy2 ... b2/C18/C19 ¼X ijy2ij n/C0X iy2i:: bnþX jy2:j: an/C0y2... abn/C02 anX jy2 :j:þ2y2 ... abn ¼X ijy2ij n/C0X iy2i:: bn/C0X jy2:j: anþy2...",
    "bnþX jy2:j: an/C0y2... abn/C02 anX jy2 :j:þ2y2 ... abn ¼X ijy2ij n/C0X iy2i:: bn/C0X jy2:j: anþy2... abn: 14.16 By (5.1), we obtain SSE¼X ijk(yijk/C0/C22yij:)2¼X ijX k(yijk/C0/C22yij:)2 ¼X ijX ky2 ijk/C0n/C22y2ij: ! ¼X ijX ky2ijk/C0y2 ij: n !",
    "yijk/C0/C22yij:)2¼X ijX k(yijk/C0/C22yij:)2 ¼X ijX ky2 ijk/C0n/C22y2ij: ! ¼X ijX ky2ijk/C0y2 ij: n ! ¼X ijky2 ijk/C0X ijy2 ij: n:622 ANSWERS AND HINTS TO THE PROBLEMS --- Page 632 --- 14.17 Partitioning XintoX¼(X1,X2), where X1contains the ﬁrst six columns and X2constitutes the last six columns, we have X(X0X)/C0X0¼1 2(X1,X2)OO OI/C18/C19X0 1 X02/C18/C19 ¼1 2(O,X2)X0 1 X02/C18/C19 ¼1 2X2X0 2: We can express X2as X2¼j0 ...0 0j ...0 ......... 00 ...j0 BBB@1 CCCA, where jand0are 2 /C21.",
    "X2X0 2: We can express X2as X2¼j0 ...0 0j ...0 ......... 00 ...j0 BBB@1 CCCA, where jand0are 2 /C21. Hence 1 2X2X0 2assumes the form given in (14.50). 14.18 (a) X0 1X1(X01X1)/C0¼/C0111111 /C01 310013 13 /C01301013 13 /C01 300113 13 /C01 212 12 1210 /C012 12 12 12010 BBBBBB@1 CCCCCCA Multiply by X 0 1X1on the right to show that X01X1(X01X1)/C0X01X1¼X01X1.",
    "2 12 12010 BBBBBB@1 CCCCCCA Multiply by X 0 1X1on the right to show that X01X1(X01X1)/C0X01X1¼X01X1. (b) X1(X0 1X1)/C0¼1 12/C0130020 /C0130020 /C0130002 /C0130002 /C0103020 /C0103020 /C0103002 /C0103002 /C0100320 /C0100320 /C0100302 /C01003020 BBBBBBBBBBBBBBBBBB@1 CCCCCCCCCCCCCCCCCCA Multiply on the right side by X 10to obtain X1(X0 1X1)/C0X01in (14.54).ANSWERS AND HINTS TO THE PROBLEMS 623 --- Page 633 --- 14.19 We ﬁrst consider y.1.andy2 .1.: y:1:¼X ikyi1k¼X ky11kþX ky21kþX ky31k ¼y0 11jþy021jþy031j ¼(y011,y012,y021,y022,y031,y032)j 0 j 0 j 00 BBBBBBBB@1 CCCCCCCCA, y 2 :1:¼y0j O j O j O0 BBBBBBBB@1 CCCCCCCCA(j 0,O0,j0,O0,j0,O0)y¼y0JOJOJO OOOOOO JOJOJO OOOOOO JOJOJO OOOOOO0 BBBBBBBB@1 CCCCCCCCAy: Similarly y 2 :2:¼y0O j O j O j0 BBBBBB@1 CCCCCCA(O 0,j0,O0,j0,O0,j0)y¼y0OOOOOO OJOJOJ OOOOOOOJOJOJ OOOOOO OJOJOJ0 BBBBBB@1 CCCCCCAy: If we denote the above matrices as C 1andC2,w eh a v e 1 6P2 j¼1y2 :j:¼1 6y2 :1:þ1 6y2 :2:¼1 6y0C1yþ16y0C2y¼16y0(C1þC2)y: Then C¼C1þC2¼JOJOJO OJOJOJ JOJOJO OJOJOJ JOJOJO OJOJOJ0 BBBBBB@1 CCCCCCAy:624 ANSWERS AND HINTS TO THE PROBLEMS --- Page 634 --- 14.20 We show the result of1 2A/C014B/C016Cþ1 12Dfor the ﬁrst two “rows”: 1 2JOOOOO OJOOOO/C18/C19 /C014JJOOOO JJOOOO/C18/C19 /C01 6JOJOJO OJOJOJ/C18/C19 þ1 12JJJJJJ JJJJJJ/C18/C19 ¼1 122J/C02J/C0JJ /C0JJ /C02J 2JJ /C0JJ /C0J/C18/C19 since 6 12J/C03 12J/C02 12Jþ1 12J¼2 12J, O/C03 12J/C0Oþ1 12J¼/C02 12J, O/C0O/C02 12Jþ1 12J¼/C01 12J, and so on.",
    "/C03 12J/C02 12Jþ1 12J¼2 12J, O/C03 12J/C0Oþ1 12J¼/C02 12J, O/C0O/C02 12Jþ1 12J¼/C01 12J, and so on. 14.21 Ifa/C3 1¼a/C32¼a/C33¼a/C3, say, thenP3 i¼1a/C3 i¼0 implies 0 ¼P3 i¼1a/C3 i¼P3 i¼1a/C3¼3a/C3,o ra/C3¼0. 14.22 SS(m,a,g)¼^my...þXa i¼1^aiyi::þXa i¼1Xb j¼1^gijyij:: ¼y2 ... abnþX iy2i:: bn/C0y2... abn ! þX ijy2 ij: n/C0X iy2 i:: bn/C0X jy2 :j: anþy2 ... abn !",
    "yij:: ¼y2 ... abnþX iy2i:: bn/C0y2... abn ! þX ijy2 ij: n/C0X iy2 i:: bn/C0X jy2 :j: anþy2 ... abn ! : 14.23 As noted preceding Theorem 14.4b, SS( a,b,gjm)¼SS(ajm,b,g)þ SS(bjm,a,g)þSS(gjm,a,b), where SS( a,b,gjm)¼P ijy2 ij:=n/C0y2...= abn :For a¼3, b¼2, and n¼2, we have by (14.57) and (14.60), SS( a,b,gjm)¼y0(1 2A/C01 12D)y, where A¼JOOOOO OJOOOO OOJOOO OOOJOO OOOOJO OOOOOJ0 BBBBBB@1 CCCCCCA,D¼J 12¼JJJJJJ JJJJJJJJJJJJ JJJJJJ JJJJJJJJJJJJ0 BBBBBB@1 CCCCCCA, andJandOare 2 /C22.",
    "BBBB@1 CCCCCCA,D¼J 12¼JJJJJJ JJJJJJJJJJJJ JJJJJJ JJJJJJJJJJJJ0 BBBBBB@1 CCCCCCA, andJandOare 2 /C22. Show that 1 2A212Dis idempotent, so that condition (c) of Theorem 5.6c is satisﬁed. To show that condition (d) holds, note thatANSWERS AND HINTS TO THE PROBLEMS 625 --- Page 635 --- the degrees of freedom ofP ijy2 ij:=n/C0y2...=abnareab21, which is easily shown to equal ( a/C01)þ(b/C01)þ(a/C01)(b/C01). 14.25 Forb¼2, the sum of squares has only 1 degree of freedom and Chas only one row.",
    "(a/C01)(b/C01). 14.25 Forb¼2, the sum of squares has only 1 degree of freedom and Chas only one row. From (14.11), we obtain H0:b/C3 1/C0b/C32¼0o r H0:b1/C0b2þ 1 3g11þ13g21þ13g31/C013g12/C013g22/C013g32¼0. Thus C¼c0¼(0, 0, 0, 0, 1, /C01,1 3,/C013,13,/C013,13,/C013),c0(X0X)/C0c¼1=3,½c0(XX)/C0c/C138/C01¼3, and X(X0X)/C0c½c0(XX)/C0c/C138/C01c0(X0X)/C0X0¼1 12J/C0JJ /C0JJ /C0J /C0JJ /C0JJ /C0JJ J/C0JJ /C0JJ /C0J /C0JJ /C0JJ /C0JJ J/C0JJ /C0JJ /C0J /C0JJ /C0JJ /C0JJ0 BBBBBB@1 CCCCCCA: where Jis 2/C22.",
    "/C0JJ /C0J /C0JJ /C0JJ /C0JJ J/C0JJ /C0JJ /C0J /C0JJ /C0JJ /C0JJ0 BBBBBB@1 CCCCCCA: where Jis 2/C22. This can be expressed as 1 122JO 2JO 2JO O2JO 2JO 2J 2JO 2JO 2JO O2JO J O 2J 2JO 2JO 2JO O2JO 2JO 2J0 BBBBBB@1 CCCCCCA/C0 1 12J12¼1 12B/C01 12J12: Since1 12Bis the same as1 6Cin (14.59), the result is obtained.",
    "CCCCCCA/C0 1 12J12¼1 12B/C01 12J12: Since1 12Bis the same as1 6Cin (14.59), the result is obtained. 14.26 E(12 ijk)¼E(1ijk/C00)2¼E½1ijk/C0E(1ijk)/C1382¼var(1ijk)¼s2, E(1ijk1lmn)¼E½(1ijk/C00)(1lmn/C00)/C138¼Ef½1ijk/C0E(1ijk)/C138½1lmn/C0E(1lmn)/C138g ¼cov(1ijk,1lmn)¼0:626 ANSWERS AND HINTS TO THE PROBLEMS --- Page 636 --- 14.27 EX iy2 i:: !",
    "n)/C138g ¼cov(1ijk,1lmn)¼0:626 ANSWERS AND HINTS TO THE PROBLEMS --- Page 636 --- 14.27 EX iy2 i:: ! ¼EX iX jkyijk !22 435 ¼EX iX jk(m/C3þa/C3 iþb/C3 jþg/C3ijþ1ijk)\"#28 < :9 = ; ¼EX ibnm/C3þbna/C3 iþnX jb/C3jþnX jg/C3ijþX jk1ijk\"#28 < :9 = ; ¼EX ib2n2m/C32þb2n2a/C32 iþX jk1ijk !2 þ2b2n2m/C3a/C3 i2 48 < : þ2bnm/C3X jk1ijkþ2bna/C3 iX jk1ijk#) ¼Ea b2n2m/C32þb2n2X ia/C32 iþX ijk12ijkþX iX jk=lm1ijk1ilm !",
    ": þ2bnm/C3X jk1ijkþ2bna/C3 iX jk1ijk#) ¼Ea b2n2m/C32þb2n2X ia/C32 iþX ijk12ijkþX iX jk=lm1ijk1ilm ! ( þ2b2n2m/C3X ia/C3iþ2bnm/C3X ijk1ijkþ2bnX ia/C3iX jk1ijk !) ¼ab2n2m/C32þb2n2X ia/C32 iþabns2: 14.30 Usinga:¼3/C22a:,gi:¼2/C22gi:, andg::¼6/C22g::, (14.90) becomes b0Hb¼4P i a2 iþ8P iai/C22giþ4P i/C22gi:/C012/C22a2:/C024/C22a:/C22g::/C012/C22g2::. Show that the 10 terms of 4P i(ai/C0/C22a:þ/C22gi:/C0/C22g::)2in (14.91) collapse to the same expression for b0Hbinvolving 6 terms.",
    "ai/C0/C22a:þ/C22gi:/C0/C22g::)2in (14.91) collapse to the same expression for b0Hbinvolving 6 terms. 14.31 (c)E(/C22yij:)¼E1 2Pn k¼1yijk/C18/C19 ¼12P kE(yijk) ¼1 2P k(mþaiþbjþgij)¼12(2mþ2aiþ2bjþ2gij):ANSWERS AND HINTS TO THE PROBLEMS 627 --- Page 637 --- 14.32 (b) By (14.47), (14.93), (14.94) and Problem 14.31(b, c), we have E½SS(gjm,a,b)/C138¼2s2þ2X ij½E(/C22yij:)/C0E(/C22yi::)/C0E(/C22y:j:)þE(/C22y::)/C1382 ¼2s2þ2X ij½mþaiþbjþgij/C0m/C0ai/C0/C22b:/C0/C22gi: /C0m/C0/C22a:/C0bj/C0/C22g:jþmþ/C22a:þ/C22b:þ/C22g::/C1382 ¼2s2þ2X ij(gij/C0/C22gi:/C0/C22g:jþ/C22g::)2: 14.33 Analysis of Variance for the Lactic Acid Data in Table 13.5 SourceSum of Squares dfMean Square F A 533.5445 1 533.5445 30.028 B 2974.0180 4 746.5045 41.844 AB 441.1580 4 110.2895 6.207 Error 177.6850 10 17.7685 Total 4126.4055 19 Thepvalues for these three F’s are .0003, .000003, and .009.",
    "177.6850 10 17.7685 Total 4126.4055 19 Thepvalues for these three F’s are .0003, .000003, and .009. 14.34 Analysis of Variance for the Hemoglobin Data in Table 13.7SourceSum of Squares dfMean Square F Rate 90.560375 3 30.186792 19.469 Method 2.415125 1 2.415125 1.380Interaction 4.872375 3 1.624125 1.558 Error 111.637000 72 1.550514 Total 209.484875 79 Thepvalue for the ﬁrst Fis 2.404 /C21029.",
    "125 1.558 Error 111.637000 72 1.550514 Total 209.484875 79 Thepvalue for the ﬁrst Fis 2.404 /C21029. The other two pvalues are .2161 and .3769.628 ANSWERS AND HINTS TO THE PROBLEMS --- Page 638 --- Chapter 15 15.1 W0W¼n10 ... 0 0n2... 0 ......... 00 ... nk0 BBBBB@1 CCCCCA,W 0y¼y1: y2: ... yk:0 BBBBB@1 CCCCCA, (W 0W)/C01W0y¼y1:=n1 y2:=n2 ... yk:=nk0 BBBBB@1 CCCCCA¼/C22y 1: /C22y2: ...",
    "... yk:0 BBBBB@1 CCCCCA, (W 0W)/C01W0y¼y1:=n1 y2:=n2 ... yk:=nk0 BBBBB@1 CCCCCA¼/C22y 1: /C22y2: ... /C22yk0 BBBBB@1 CCCCCA: 15.2 (a) The reduced model y ij¼mþ1/C3 ijcan be written in matrix form as y¼mjþ1/C3, from which we have ^m¼/C22y::and ^mj0y¼/C22y::y::¼ y2::=N¼N/C22y2::: (b) SSB¼ˆm0W0y/C0N/C22y2::¼(/C22y1:,/C22y2:,...,/C22yk:)y1: ...",
    "^mj0y¼/C22y::y::¼ y2::=N¼N/C22y2::: (b) SSB¼ˆm0W0y/C0N/C22y2::¼(/C22y1:,/C22y2:,...,/C22yk:)y1: ... yk0 B@1 CA/C0N/C22y/C02 ::: (c)Pk i¼1/C22yi:yi:/C0N/C22y2 ::¼P iyi: niyi:/C0Ny:: N/C16/C172 : 15.3 (a)Xk i¼1ni(/C22yi:/C0/C22y::)2¼X i(ni/C22y2 i:/C02ni/C22yi:/C22y::þni/C22y2::) ¼X ini/C22y2i:/C02/C22y::X ini/C22yi:þ/C22y2::X ini ¼X iniyi: ni/C18/C192 /C02y:: NX iniyi: niþNy:: N/C16/C172 ¼X iy2 i: ni/C02y2:: Nþy2:: N: (b)Xk i¼1Xni j¼1(yij/C0/C22yi:)2¼X iX j(y2 ij/C02yij/C22yi:þ/C22y2i:) ¼X ijy2 ij/C02X iyi: niX jyij !",
    "b)Xk i¼1Xni j¼1(yij/C0/C22yi:)2¼X iX j(y2 ij/C02yij/C22yi:þ/C22y2i:) ¼X ijy2 ij/C02X iyi: niX jyij ! þX iX jyi: ni/C18/C192 ¼X ijy2 ij/C02X iy2 i: niþX iniy2i: n2 i ¼X ijy2 ij/C0X iy2 i: ni:ANSWERS AND HINTS TO THE PROBLEMS 629 --- Page 639 --- 15.4 (c0^m)0½c0(W0W)/C01c/C138/C01c0^m=s2¼P ici/C22yi:/C0/C1P ic2 i=ni/C0/C1/C01P ici/C22yi:=s2: 15.5P iaibi ni¼(25)(0) 10þ(/C020)(1) 20þ(/C05)(/C01) 5¼/C01þ1¼0. 15.6 (W0W)/C01W0y¼n110 ... 0 0 n12 ... 0 ......... 00 ... n230 BBB@1 CCCA/C01y11: y12: ...",
    "/C01þ1¼0. 15.6 (W0W)/C01W0y¼n110 ... 0 0 n12 ... 0 ......... 00 ... n230 BBB@1 CCCA/C01y11: y12: ... y23:0 BBB@1 CCCA¼/C22y 11: /C22y12: ... /C22y23:0 BBB@1 CCCA: 15.7 W^ m¼(/C22y11:,/C22y11:,/C22y12:,/C22y13:,/C22y13:,...,/C22y23:)0:Note that ^mis 6/C21 and W^mis 11/C21. 15.8 This follows by deﬁnition; see, for example, (7.41). 15.9 Since Bm¼0,t h a ti s b0 1m¼0, we can equate b01mandb02mto obtain 2m11/C0m12/C0m13þ2m21/C0m22/C0m23¼m12/C0m13þm22/C0m23, which reduces to 2 m11þ2m21¼2m12þ2m22.",
    "obtain 2m11/C0m12/C0m13þ2m21/C0m22/C0m23¼m12/C0m13þm22/C0m23, which reduces to 2 m11þ2m21¼2m12þ2m22. We can obtain m12þm22¼ m13þm23fromb02m¼0. 15.10 a0^m¼P ijaij/C22yij:anda0(W0W)/C01a¼P ija2 ij=nij: 15.11 a0(W0W)/C01a¼3.833. 15.12 B(W0W)/C01B0¼1 325 /C01 /C017/C18/C19 . 15.13 Show that KG0¼O. 15.14 By (3.42), cov( ^mc)¼K0(KK0)/C01cov( ^dc)(KK0)/C01K.",
    "01 /C017/C18/C19 . 15.13 Show that KG0¼O. 15.14 By (3.42), cov( ^mc)¼K0(KK0)/C01cov( ^dc)(KK0)/C01K. 15.15 (a) Analysis of Variance for the Weight Data of Table 14.6 Source dfSum of SquaresMean Square Fp Value Protein 4 111,762.28 27940.57 8.36 .0000169 Error 56 181,256.71 3236.73 Total 60 293,018.98 (b) FTests for Unweighted Contrasts Contrasts df Contrast SS Fp Value L, C vs. So, Su, M 1 2,473.61 0.76 .386 So, M vs Su 1 36,261.93 11.20 .00147So vs. M 1 5,563.22 1.72 .195L vs.",
    "o, Su, M 1 2,473.61 0.76 .386 So, M vs Su 1 36,261.93 11.20 .00147So vs. M 1 5,563.22 1.72 .195L vs. C 1 65,940.17 20.37 .0000332630 ANSWERS AND HINTS TO THE PROBLEMS --- Page 640 --- (c) FTests for Two Weighted Contrasts Contrasts dfContrast SS Fp Value L, C vs. So, Su, M 1 2,473.61 0.76 .386 So, M vs. Su 1 31,673.79 9.79 .00278 15.17 (a) m¼(m11,m12,m13,m21,m22,m23)0,Wis 47 /C26,ais 6/C21,BandC are each 2 /C26.",
    "3.79 9.79 .00278 15.17 (a) m¼(m11,m12,m13,m21,m22,m23)0,Wis 47 /C26,ais 6/C21,BandC are each 2 /C26. a¼(1,1,1,/C01,/C01) B¼1/C021 1 /C021 10 /C011 0 /C01/C18/C19 C¼1/C021 /C012 /C01 10 /C01/C01/C011/C18/C19 ^m¼/C22y¼(96:50,85:90,95:17,79:20,91:83,82:00)0 SSE¼8436 :1667 ,nE¼41 FA¼3:65104 ,FB¼:022053 ,FC¼2:90567 : (b) G is the same as Cin part (a) K¼j0 a0 B0 B@1 CA¼1 11111 11 1 /C01/C01/C01 1/C021 1 /C021 10 /C011 0 /C010 BBB@1 CCCA ^ mc¼(91:61,91:31,92:66,83:11,82:81,84:15)0 SSE c¼9631 :9072 ,nEc¼43 FAc¼3:687,FBc¼:03083 : (c) Analysis of Variance for Unconstrained Model Source dfSum of SquaresMean Square Fp Value Level 1 751.238 751.238 3.65 .0630 Type 2 9.075 4.538 .02 .978Level /C2type 2 1195.7406 597.870 2.91 .0661 Error 41 8436.167 205.760 Total 46 10474.851ANSWERS AND HINTS TO THE PROBLEMS 631 --- Page 641 --- Analysis of Variance for Constrained Model Source dfSum of SquaresMean Square Fp Value Level 1 826.544 826.544 3.69 .0614 Type 2 13.810 6.905 0.03 .970Error 43 9,631.907 223.998 Total 46 10,474.851 15.18 Analysis of Variance for Data in Table 15.12 Source dfSum of SquaresMean Square Fp Value Fertilizer 3 20,979.042 9663.014 111.52 5:773/C210/C015 Variety 4 306.621 76.655 1.22 0.325 Fertilizer /C2 variety12 997.589 83.132 1.33 0.263 Error 26 1,630.333 62.705 Total 45 28,486.370 Chapter 16 16.1 Verify that I2Pis symmetric and idempotent.",
    "26 1,630.333 62.705 Total 45 28,486.370 Chapter 16 16.1 Verify that I2Pis symmetric and idempotent. Then X0(I2P)X¼X0(I2 P)0(I2P)X, and by Theorem 2.4(iii), rank[ X0(I2P)X]¼rank [( I2P)X]. The matrix ( I2P)Xisn/C2q. To show that rank [( I2P)X¼q, we demon- strate that the columns are linearly independent. By the deﬁnition of linear independence in (2.40), the columns of ( I2P)Xare lineraly independent if (X2PX)a¼0implies a¼0. Suppose that there is a vector a=0such that ( X2PX)a¼0.",
    "ineraly independent if (X2PX)a¼0implies a¼0. Suppose that there is a vector a=0such that ( X2PX)a¼0. Then Xa¼PXa ¼Z(Z0Z)/C0Z0Xa: By Theorem 2.8c(iii), a solution to this is Xa¼zi, where ziis the ith column ofZ. But this is impossible since the columns of Zare linearly independent of those of X. We therefore have Xa¼0, which implies that a¼0, since X is full-rank. This contradicts the possibility that a=0.",
    "have Xa¼0, which implies that a¼0, since X is full-rank. This contradicts the possibility that a=0. 16.2 (a) By (16.11) and (16.14) to (16.17), we obtain SSE y:x¼y0y/C0y0Z(Z0Z)/C0Z0y/C0^b0X0(I/C0P)y ¼y0(I/C0P)y/C0e0 xyE/C01 xxexy¼eyy/C0e0xyE/C01 xxexy: (b)By (12.21), SSE y¼y0[I2Z(Z0Z)2Z0]y, which, by (16.13), equals y0(I2P)y.632 ANSWERS AND HINTS TO THE PROBLEMS --- Page 642 --- 16.3 By Theorem 8.4a(ii), SSH ¼(C^b)0A(C^b), where A¼[cov(C^b)]/C01=s2.I n this case, C¼Iand by (15.19) cov( ^b)¼s2[X(I/C0P)X0]/C01.",
    "C^b)0A(C^b), where A¼[cov(C^b)]/C01=s2.I n this case, C¼Iand by (15.19) cov( ^b)¼s2[X(I/C0P)X0]/C01. 16.4 By (13.12), a solution ^a0is given by ^a0¼(0,/C22y1:,...,/C22yk:)0. By analogy to (13.7), Z0x¼(x::,x1:,...,xk:)0, and by (13.11), a generalized inverse is (Z0Z)-¼diag(0 ,1=n,...,1=n). Then ( Z0Z)/C0Z0x¼(0,/C22x1:,...,/C22xk:)0.",
    "), a generalized inverse is (Z0Z)-¼diag(0 ,1=n,...,1=n). Then ( Z0Z)/C0Z0x¼(0,/C22x1:,...,/C22xk:)0. 16.5 By (16.13) and (16.16), exx¼x0(I/C0P)x¼x0x/C0x0Z(Z0Z)/C0Z0x.F r o mt h e answer to Problem 16.4, x0Z¼(x::,x1:,...,xk:)a n d (Z0Z)/C0Z0x¼(0,/C22x1:,...,/C22xk:)0.T h u s x0Z(Z0Z)/C0Z0x¼Pk i¼1xi:/C22xi: ¼nP i/C22x2 i:,a n d exx¼P ijx2ij/C0nP i/C22x2i:.S h o wt h a t exxcan be written as exx¼P ij(xij/C0/C22xi:)2. The quantities exyandeyycan be found in an analogous manner.",
    "be written as exx¼P ij(xij/C0/C22xi:)2. The quantities exyandeyycan be found in an analogous manner. 16.6 (a) By (16.39) and (16.40), we have X0(I/C0P)¼x0100...00 00x02...00 ......... 0000...x0 k0 BBBBB@1 CCCCCAI/C01 nJO ... O OI /C01nJ... O ......... OO ...I/C01 nJ0 BBBBBBBBBB@1 CCCCCCCCCCA ¼x 0 1(I/C01 nJ) 00... 00 00x0 2(I/C01 nJ)... 00 ......... 0000...x0 k(I/C01 nJ)0 BBBBBBBBBB@1 CCCCCCCCCCA, X 0(I/C0P)X¼x0 1(I/C01 nJ)x1 0 ... 0 0 x0 2(I/C01 nJ)x2... 0 .........",
    "J)0 BBBBBBBBBB@1 CCCCCCCCCCA, X 0(I/C0P)X¼x0 1(I/C01 nJ)x1 0 ... 0 0 x0 2(I/C01 nJ)x2... 0 ......... 00 ...x0 k(I/C01 nJ)xk0 BBBBBBBBBB@1 CCCCCCCCCCA:ANSWERS AND HINTS TO THE PROBLEMS 633 --- Page 643 --- To show that this equal to (16.41), we have, for example x0 2I/C01 nJ/C18/C19 x2¼x0 2x2/C0x2jj0x2 n¼X jx22j/C0x2 2: n: Show that this equalsP j(x2j/C0/C22x2:)2: (b)Using X0(I/C0P) from part (a), we have X0(I/C0P)y¼x01I/C01 nJ/C18/C19 00... 00 00x0 2I/C01 nJ/C18/C19 ... 00 .........",
    "om part (a), we have X0(I/C0P)y¼x01I/C01 nJ/C18/C19 00... 00 00x0 2I/C01 nJ/C18/C19 ... 00 ......... 0000...x0 kI/C01 nJ/C18/C190 BBBBBBBBBBBB@1 CCCCCCCCCCCCAy 1 y2 ... yk0 BBBBB@1 CCCCCA ¼x 0 1I/C01 nJ/C18/C19 y1 x0 2(I/C01 nJ)y2 ...",
    "BBBBBBBB@1 CCCCCCCCCCCCAy 1 y2 ... yk0 BBBBB@1 CCCCCA ¼x 0 1I/C01 nJ/C18/C19 y1 x0 2(I/C01 nJ)y2 ... x0 kI/C01 nJ/C18/C19 yk0 BBBBBBBBBBB@1 CCCCCCCCCCCA: The elements of this vector are, for example x 0 2I/C01 nJ/C18/C19 y2¼x0 2y2/C0x0 2jj0y2 n¼X jx2jy2j/C0/C22x2:/C22y2: n: Show that this equalsP j(x2j/C0/C22x2:)(y2j/C0/C22y2:).634 ANSWERS AND HINTS TO THE PROBLEMS --- Page 644 --- 16.7 X ijk(xijk/C0/C22xij:)(yijk/C0/C22yij:)¼X ijkxijkyijk/C0X ijkxijk/C22yij:/C0X ijk/C22xij:yijk þnX ij/C22xij:/C22yij: ¼X ijkxijkyijk/C0X ij/C22yij:X kxijk !",
    "0X ijkxijk/C22yij:/C0X ijk/C22xij:yijk þnX ij/C22xij:/C22yij: ¼X ijkxijkyijk/C0X ij/C22yij:X kxijk ! /C0X ij/C22xij:X kyijk ! þnX ij/C22xij:/C22yij: ¼X ijkxijkyijk/C0nX ij/C22xij:/C22yij: /C0nX ij/C22xij:/C22yij:þnX ij/C22xij:/C22yij:: 16.8 In (14.40) and (14.41), we have X ijy2 ij: n/C0y2... abn¼X iy2i:: bn/C0y2... abn ! þX jy2:j: an/C0y2... abn ! þX ijy2ij: n/C0X iy2 i:: bn/C0X jy2 :j: anþy2 ... abn ! : By an analogous identity, we have (note that bis replaced by c) X ijxij:yij: n/C0x...y...",
    ".. abn ! : By an analogous identity, we have (note that bis replaced by c) X ijxij:yij: n/C0x...y... acn¼X ixi::yi:: cn/C0x...y... acn ! þX jx:j:y:j: an/C0x...y... acn ! þX ijxij:yij: n/C0X ixi::yi:: cn/C0X jx:j:y:j: anþx...y... acn !",
    "! þX jx:j:y:j: an/C0x...y... acn ! þX ijxij:yij: n/C0X ixi::yi:: cn/C0X jx:j:y:j: anþx...y... acn ! : Show that the right side is equal to cnX i(/C22xi::/C0/C22x...)(/C22yi::/C0/C22y...)þanX j(/C22x:j:/C0/C22x...)(/C22y:j:/C0/C22y...) þnX ij(/C22xij:/C0/C22x^i::/C0/C22x:j:þ/C22x...)(/C22yij:/C0/C22yi::/C0/C22y:j:þ/C22y...) ¼SPAþSPCþSPAC :ANSWERS AND HINTS TO THE PROBLEMS 635 --- Page 645 --- 16.9 (a) SS(CþE)y:x¼SSC ^yþSSE y/C0(SPC þSPE)2 SSC xþSSE x SSC y:x¼SS(CþE)y:x/C0SSE y:x F¼SSAC y:x=(c/C01) SSE y:x=½ac(n/C01)/C01/C138: TheFstatistic is distributed as F[c21,ac(n21)21] if H0is true.",
    "=(c/C01) SSE y:x=½ac(n/C01)/C01/C138: TheFstatistic is distributed as F[c21,ac(n21)21] if H0is true. (b) SS(ACþE)y:x¼SSAC yþSSE y/C0(SPAC þSPE)2 SSAC xþSSE x SSAC y:x:¼SS(ACþE)y:x/C0SSE y:x: F¼SSAC y:x:=(a/C01)(c/C01) SSE y:x=½ac(n/C01)/C01/C138: TheFstatistic is distributed as F[(a21) (c21),ac(n21)21] if H0is true. 16.10 (a)Exx¼X0(I/C0P)X ¼(X0 1,X02,...,X0k)I/C01 nJO ... O OI /C01nJ... O ......... OO ...I/C01 nJ0 BBBBBBBBBB@1 CCCCCCCCCCAX 1 X2 ...",
    ",...,X0k)I/C01 nJO ... O OI /C01nJ... O ......... OO ...I/C01 nJ0 BBBBBBBBBB@1 CCCCCCCCCCAX 1 X2 ... Xk0 BBBBB@1 CCCCCA ¼X 0 1I/C01 nJ/C18/C19 ,X0 2I/C01 nJ/C18/C19 ,...,X0 kI/C01 nJ/C18/C19 /C20/C21X1 X2 ... Xk0 BBBBB@1 CCCCCA ¼Xk i¼lX0 iI/C01 nJ/C18/C19 Xi 16.11 By Theorem 2.2c(i), the diagonal elements of X0ciXciare products of columns of Xci. Thus, for example, the seond diagonal element of X0ciXci in (16.70) is (xi12/C0/C22xi:2,...,xin2/C0/C22xi:2)xi12/C0/C22xi:2 ...",
    "d diagonal element of X0ciXci in (16.70) is (xi12/C0/C22xi:2,...,xin2/C0/C22xi:2)xi12/C0/C22xi:2 ... xin2/C0/C22xi:20 B@1 CA¼Xn j¼1(xij2/C0/C22xi:2)2:636 ANSWERS AND HINTS TO THE PROBLEMS --- Page 646 --- Similarly, the (1,2) element of Xci0Xciis (xi11/C0/C22xi:1,...,xin1/C0/C22xi:1)xi12/C0/C22xi:2 ... xin2/C0/C22xi:20 B@1 CA¼Xn j¼1(xij1/C0/C22xi:1)(xij2/C0/C22xi:2): 16.12 (Z0Z)/C0Z0X^b¼00 ... 0 01 n... 0 ......... 00 ...1 n0 BBBBBBBB@1 CCCCCCCCAj 0j0...j0 j000...00 00j0...00 .........",
    "00 ... 0 01 n... 0 ......... 00 ...1 n0 BBBBBBBB@1 CCCCCCCCAj 0j0...j0 j000...00 00j0...00 ......... 0000...j00 BBBBBBB@1 CCCCCCCAX 1 X2 ... Xk0 BBBBB@1 CCCCCA^ b ¼1 n0000...00 j00...00 00j0...00 ......... 0000...j00 BBBBBBB@1 CCCCCCCAX 1 X2 ... Xk0 BBBBB@1 CCCCCA^ b ¼1 nj0X1 j0X2 ... j0Xk0 BBBBB@1 CCCCCA^ b¼/C22x0 1 /C22x0 2 ... /C22x0 k0 BBBBB@1 CCCCCA^ b¼/C22x0 1^b /C22x0 2^b ...",
    "j0Xk0 BBBBB@1 CCCCCA^ b¼/C22x0 1 /C22x0 2 ... /C22x0 k0 BBBBB@1 CCCCCA^ b¼/C22x0 1^b /C22x0 2^b ... /C22x0 k^b0 BBBBBB@1 CCCCCCA: 16.13X ij(yij/C0/C22y::)2/C0X ij(yij/C0/C22yi:)2 ¼X ijy2 ij/C02/C22y::X ijyijþkn/C22y2:: /C0X ijy2ij/C02X i/C22yi:X jyij !",
    ":)2/C0X ij(yij/C0/C22yi:)2 ¼X ijy2 ij/C02/C22y::X ijyijþkn/C22y2:: /C0X ijy2ij/C02X i/C22yi:X jyij ! þnX i/C22y2i:\"# ¼X ijy2ij/C0kn/C22y2::/C0X ijy2ijþ2nX i/C22y2i:/C0nX i/C22y2i: ¼nX i/C22y2i:/C0kn/C22y2::¼nX i(/C22yi:/C0/C22y::)2: 16.14 (a) exx¼358 :1667 ,exy¼488 :5000 ,eyy¼5937 :8333 ,^b¼1:3639 : (b)SSE y/C1x¼5271 :5730 with 19df ,SST y/C1x¼6651 :1917 with 22 df, SS(ajm,b)¼1379 :6188 with 3df ,F¼1:6575 ,p¼:210 :ANSWERS AND HINTS TO THE PROBLEMS 637 --- Page 647 --- (c)F¼2:4014 ,p¼:138 : (d) ^b1¼1:9950 ,^b2¼/C0 :9878 ,^b3¼/C01:2687 ,^b4¼3:1646 , F¼(5271 :5730 /C04178 :2698) =3 4178 :2698 =16¼1:3955 ,p¼:280 : 16.15 (a) Sum of Squares and Products for xand y SourceSS and SP Corrected for the Mean yx x y A 268,043.37 811.11 14,627.22 C 588,510.81 2485.11 1,468.89 2AC 1,789,999.1 2411.11 22,736.222 Error 7,717,172.7 5632.67 2168,409.7 AþC 7,985,216 6443.78 2183,036.9 CþE 8,305,683.5 8117.78 2182,678.6 ACþE 950,7171.7 8043.78 2171,145.9 (b) SS(AþE)y/C1x¼2,786014 ,SSE y/C1x¼2,681,934 :9,SSA y:x¼104,079 :06: For factor A,F¼104,079 :06=2 2,681,934 :92=35¼:6791 ,p¼:514 : For factor C,F¼1,512,838 :46=5 2,681,934 :92=35¼3:9486 ,p¼:0061 : For interaction AC,F¼3,183,799 :17=10 2,681,934 :92=35¼4:1549 ,p¼:000783 : (c)F¼(SPE)2=SSE x SSE y/C1x=½ac(n/C01)/C01/C138¼(/C0168,409 :7)2=5632 :67 2,681,934 :92=35 ¼65:7113 ,p¼1:516/C210/C09: (d)For factor A ^b1¼SPE 1 SSE x,1¼/C050,869 :67 1,274 :67¼/C039:9082 , ^b2¼SPE 2 SSE x,2¼/C037,796 :33 1,987 :33¼/C019:0187 , ^b3¼SPE 3 SSE x,3¼/C079,743 :67 2,370 :67¼/C033:6377 , SS(F)¼SSE y/C0X3 i¼1(SPE i)2 SSE x,i¼2,285,831 :3, SS(R)¼SSE y/C0(SPE)2 SSE 2¼2,681,934 :9:638 ANSWERS AND HINTS TO THE PROBLEMS --- Page 648 --- By (16.64), we obtain F¼(2,681,934 :9/C02,285,831 :3)=2 2,285,831 :3=33¼2:8592 ,p¼:0716 : For factor C ^b1¼/C032:8195 ,^b2¼/C030:3492 ,^b3¼/C026:2928 ,^b4¼/C027:8251 , ^b5¼/C053:1667 ,^b6¼/C028:2191 , F¼156,728 :91=5 2,525,206 :01=30¼:3724 ,p¼:864 : 16.16 (a) Sums of Squares and Products for xand y SourceSS and SP Corrected for the Mean yx x y A 235.225 176.4 203.70 C 30.625 0.400 23.50 AC 3.025 12.10 26.05 Error 867.500 5170.2 1253.10 AþE 1102.725 5346.6 1456.80 CþE 898.125 5170.6 1249.60 ACþE 870.525 5182.3 1247.05 (b) SS(AþE)y/C1x¼705 :7875 ,SSE y/C1x¼563 :7865 ,SSA y/C1x¼142 :0010 : For factor A,F¼142 :0010 =1 563 :7865 =35¼8:8155 ,p¼:0054 : For factor C,F¼32:3426 =1 563 :7865 =35¼2:0078 ,p¼:165 : For interaction AC,F¼6:6529 =1 563 :7865 =35¼:4130 ,p¼:525 : (c) F¼(SPE)2=SSE x SSE y/C1x=½ac(n/C01)/C01/C138¼(1253 :10)2=5170 :200 563 :7865 =35 ¼18:8546 ,p¼:000115 :ANSWERS AND HINTS TO THE PROBLEMS 639 --- Page 649 --- (d)For factor A ^b1¼SPE 1 SSE x,1¼996 :10 3109 :7¼:3203 , ^b2¼SPE 2 SSE x,2¼257 :00 2060 :50¼:1247 , SS(F)¼SSE y/C0X2 i¼1(SPE i)2 SSE x,i¼516 :3741 , SS(R)¼SSE y/C0(SPE)2 SSE x¼563 :7865 : By (16.64), F¼(563 :7865 /C0516 :3741) =1 516 :3741 =34¼3:1218 ,p¼:0862 : For factor C,^b1¼:2034 ,^b2¼:2870 , F¼8:9930 =1=554 :7935 =34¼:5511 ,p¼:463 : 16.17 (a)Exx¼4548 :2 2877 :4 2877 :4 4876 :9/C18/C19 ,exy5:623 26:219/C18/C19 eyy¼:8452 ,^b¼/C0:003454 :007414/C18/C19 : (b) SSE y/C1x¼:67026 ,SST y/C1x¼:84150 , SS(ajm,b)¼SST y/C1x/C0SSE y/C1x¼:17124 , F¼:17124 =3 :67026 =34¼2:8955 ,p¼:0493 : (c)To test H0:b¼0, we use (16.84): F¼e0 xyE/C01 xxexy=q SSE y/C1x=½k(n/C01)/C0q/C138¼4:4378 ,p¼:0194 :640 ANSWERS AND HINTS TO THE PROBLEMS --- Page 650 --- (d) ^b1¼1268 :9 983 :4 983 :4 1076 :4/C18/C19 /C012:984 5:694/C18/C19 ¼/C0:00599 :01076/C18/C19 , ^b2¼1488 :4 836 :0 836 :0 1512 :0/C18/C19 /C01/C02:636 3:95/C18/C19 ¼/C0:004697 :005209/C18/C19 , ^b3¼502 :9 513 :0 513 :0 1552 :0/C18/C19 /C011:735 12:94/C18/C19 ¼/C0:00763 :01086/C18/C19 , ^b4¼1288 :0 545 :0 545 :0 736 :5/C18/C19 /C013:540 3:635/C18/C19 ¼:000961 :004224/C18/C19 SSE( F)y/C1x¼eyy/C0X4 i¼1e0 xy,iE/C01 xx,iexy,i¼:62284 , SSE( R)y/C1x¼eyy/C0e0xyE/C01 xxexy¼:67026 : By (16.89) F¼½SSE( R)y/C1x/C0SSE( F)y/C1x/C138=q(k/C01) SSE( F)y/C1x=k(n/C0q/C01) ¼:047425 =6 :62284 =28¼:3553 ,p¼:901 : Chapter 17 17.1 IfV¼PP0, let v¼P21yandW¼P21X.",
    "1x=k(n/C0q/C01) ¼:047425 =6 :62284 =28¼:3553 ,p¼:901 : Chapter 17 17.1 IfV¼PP0, let v¼P21yandW¼P21X. Then visN(Wb,s2I),^b¼ (W0W)21W0y, and F¼(C^b/C0t)0½C(W0W)/C01C0/C138(C^b/C0t)=q v0(I/C0W(W0W)/C01W0)v=(n/C0k/C01): (a)By Theorem 8.4g(ii), FisF(q, n2k21). (b)By Theorem 8.4g(i), FisF(q,n2k21, (C^b/C0t)0½C(W0W)/C01C0/C138/C01 (C2t)/2).",
    "ii), FisF(q, n2k21). (b)By Theorem 8.4g(i), FisF(q,n2k21, (C^b/C0t)0½C(W0W)/C01C0/C138/C01 (C2t)/2). 17.2 As in Problem 17.1 and using (8.49), the conﬁdence interval is a0^b+ta=2,n/C0k/C01ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ a0(W0W)/C01aq ora0^b+ta=2,n/C0k/C01ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ a0(X0(X0V/C01X)/C01aq where sis given by (7.67)ANSWERS AND HINTS TO THE PROBLEMS 641 --- Page 651 --- 17.3 LetX0¼j5j5000j50j50000000 j5j50000j50j5000000 j50j500j5000j500000 j50j500 0j5000j50000 j500j50j500000j5000 j500j500j500000j500 j5000j5j50000000j50 j5000j50j50000000j50 BBBBBBBBBBBBB@1 CCCCCCCCCCCCCA: Then X¼X 0 X0 ...",
    "00000j500 j5000j5j50000000j50 j5000j50j50000000j50 BBBBBBBBBBBBB@1 CCCCCCCCCCCCCA: Then X¼X 0 X0 ... X00 BBBBB@1 CCCCCA,Z1¼j400/C1/C1/C10 0j40/C1/C1/C10 ......... 00 /C1/C1/C1j400 BBBBB@1 CCCCCA, Z2¼j100/C1/C1/C10 0j10/C1/C1/C10 ......... 00 /C1/C1/C1j100 BBBBB@1 CCCCCA,andZ 3¼j50/C1/C1/C10 0j5/C1/C1/C10 ......... 00 /C1/C1/C1j50 BBBBB@1 CCCCCA: 17.4 (a) cov(y)¼covX m i¼1Ziaiþ1 !",
    "C1/C1/C10 0j5/C1/C1/C10 ......... 00 /C1/C1/C1j50 BBBBB@1 CCCCCA: 17.4 (a) cov(y)¼covX m i¼1Ziaiþ1 ! ¼Xm i¼1ZiGiZ0 iþR: (b) cov(y)¼ZGZ0þR: 17.5 Using (5.4), E½yK0(KSK0)/C01KZ iZ0iK0(KSK0)/C01Ky/C138 ¼tr½K0(KSK0)/C01KZ iZ0iK0(KSK0)/C01KS/C138 þb0X0K0(KSK0)/C01KZ iZ0iK0(KSK0)/C01KXb ¼tr½KSK0(KSK0)/C01KZ iZ0iK0(KSK0)/C01/C138þ0 (since KX¼O) ¼tr½KZ iZ0iK0(KSK0)/C01/C138 ¼tr½K0(KSK0)/C01KZ iZ0i/C138:642 ANSWERS AND HINTS TO THE PROBLEMS --- Page 652 --- 17.6 q is obvious; tr½K0(KSK0)/C01KZ iZ0 i/C138¼tr½(KSK0)/C01KZ iZ0iK0/C138 ¼tr½(KSK0)/C01KZ iZ0iK0(KSK0)/C01KSK0/C138 ¼trK0(KSK0)/C01KZ iZ0iK0(KSK0)/C01KXm i¼0ZiZ0is2i\"# ¼m0 1s,where the jth element of m1is tr½K0(KSK0)/C01KZ iZ0 iK0(KSK0)/C01KZ jZ0j/C138: 17.7 (a) IfS¼PP0, let v¼P21yandW¼P21X.",
    "ment of m1is tr½K0(KSK0)/C01KZ iZ0 iK0(KSK0)/C01KZ jZ0j/C138: 17.7 (a) IfS¼PP0, let v¼P21yandW¼P21X. Then visN(Wb,I) and L(X0S21X)2L0¼L(W0W)2L0. Since Lis estimable, L¼AX¼ APP0X¼BW. Hence the rows of Lalso deﬁne estimable functions ofv. Thus by Theorem 12.7b, L(X0S21X)2L0is nonsingular. (b)Since Lbˆ2LbisN[0,L(W0W)2L0], note that [ L(W0W)2L0]21 L(W0W)2L0¼I, which is idempotent of rank g. 17.8 x 00bˆis estimable and is N[x00b,x00(W0W)2x0].",
    "0]21 L(W0W)2L0¼I, which is idempotent of rank g. 17.8 x 00bˆis estimable and is N[x00b,x00(W0W)2x0]. Hence x0 0^b/C0x00bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ x0 0(W0W)/C0x0p isN(0,1): Thus a 100(1 2a)% conﬁdence interval for x00bˆis x0 0^b+za=2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ x0 0(W0W)/C0x0q or x0 0^b+za=2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ x0 0(X0S/C01X)/C0x0q :ANSWERS AND HINTS TO THE PROBLEMS 643 --- Page 653 --- 17.9 (X0^S/C01X)/C01¼(I6I6)^S/C01 1 O /C1/C1/C1 O O^S/C01 1/C1/C1/C1 O .........",
    "43 --- Page 653 --- 17.9 (X0^S/C01X)/C01¼(I6I6)^S/C01 1 O /C1/C1/C1 O O^S/C01 1/C1/C1/C1 O ......... OO /C1/C1/C1^S/C01 10 BBBBBBB@1 CCCCCCCAI 6 I6/C18/C192 666666643 77777775/C01 ¼2^S/C01 1 OO O^S/C01 1 O OO^S/C01 10 BBBB@1 CCCCA2 666643 77775/C01 ¼1 2^S1OO O^S1O OO^S10 B@1 CA: 17.10 LetC¼(I6,O) and K¼C(I2H)¼1 2(I6,2I6). Then K0(KSK0)21K ¼12T/C0T /C0TT/C18/C19 where T¼^S/C01 1 OO O^S/C01 1 O OO^S/C01 10 BB@1 CCA:Since Z0¼I12and Z1¼j20/C1/C1/C1 0 0j2/C1/C1/C1 0 .........",
    "C01 1 OO O^S/C01 1 O OO^S/C01 10 BB@1 CCA:Since Z0¼I12and Z1¼j20/C1/C1/C1 0 0j2/C1/C1/C1 0 ......... 00 /C1/C1/C1j20 BBB@1 CCCA,the REML equations become 3t r (S /C01)¼y0½K0(KSK0)/C138/C01KK0(KSK0)K/C138y,and 3t r (S/C01J2)¼y0½K0(KSK0)/C01KJ2K0(KSK0)K/C138y: Noting that^S/C01¼1 ^s2(^s2þ2^s2 1)^s2þ^s2 1 /C0^s21 /C0^s2 1 ^s2þ^s21/C18/C19 ,the REML equations644 ANSWERS AND HINTS TO THE PROBLEMS --- Page 654 --- can be written as 6^s2þ^s2 1 ^s2(^s2þ2^s21)¼1 2y0T2/C0T2 /C0T2T2 !",
    "TO THE PROBLEMS --- Page 654 --- can be written as 6^s2þ^s2 1 ^s2(^s2þ2^s21)¼1 2y0T2/C0T2 /C0T2T2 ! y,and 6 ^s2(^s2þ2^s2 1)¼1 2u (^s2þ2^s2 1)2 where u¼y0 U/C0U /C0UU/C18/C19 yand U¼J2OO OJ 2O OOJ 20 @1A:The second REML equation can be rearranged ^ s2 1¼u 24/C0^s2 2:Substituting this expression into the ﬁrst REML equation and then simplifying, we obtainu2^s2 24þu^s4 2¼u^s4 2þu2 288y0Pywhere P¼ ROO /C0ROO OROO /C0RO OOROO /C0R /C0ROOROO O/C0ROORO OO /C0ROOR0 BBBBBB@1 CCCCCCA,which simpliﬁes to sˆ2¼1 12y0Py.",
    "O /C0RO OOROO /C0R /C0ROOROO O/C0ROORO OO /C0ROOR0 BBBBBB@1 CCCCCCA,which simpliﬁes to sˆ2¼1 12y0Py. 17.11 X(X0X)/C01L0QL(X0X)/C01X0S ¼1 2I6 I6/C18/C191 3s22/C01 /C012/C18/C1912(I 6I6)S ¼1 12s2WW WW/C18/C19S1O/C1/C1/C1 O OS1/C1/C1/C1 O .........",
    "18/C191 3s22/C01 /C012/C18/C1912(I 6I6)S ¼1 12s2WW WW/C18/C19S1O/C1/C1/C1 O OS1/C1/C1/C1 O ......... OO /C1/C1/C1S10 BBBBB@1 CCCCCA where2/C02/C011 /C011 /C0221 /C011 /C01 /C0112 /C02/C011 1/C01/C0221 /C01 /C011 /C0112 /C02 1/C011 /C01/C0220 BBBBBBBB@1 CCCCCCCCA ¼ 1 12WW WW/C18/C19 :ANSWERS AND HINTS TO THE PROBLEMS 645 --- Page 655 --- Now, 1 12WW WW/C18/C191 12WWWW/C18/C19 ¼1 1442W22W2 2W22W2 !",
    "TS TO THE PROBLEMS 645 --- Page 655 --- Now, 1 12WW WW/C18/C191 12WWWW/C18/C19 ¼1 1442W22W2 2W22W2 ! ¼1 12WWWW/C18/C19 : 17.12 1 12s2WW WW/C18/C19 S1 2P ¼1 24WW WW/C18/C19 ROO /C0ROO ORO O /C0RO OOR O O /C0R0 B@1 CA ¼O: 17.13 ½c0(X0^S/C01X)/C01c/C138/C01¼^s2þ2^s2 1¼(^s2þ2^s21)=3s2 w=d: Using results from the solution to Problem 17.10, v¼1 3s2 (1 12y0Pyþ1 12u/C0^s2)¼1 36y0Dywhere Dis a nonzero square matrix not involving s2.",
    "em 17.10, v¼1 3s2 (1 12y0Pyþ1 12u/C0^s2)¼1 36y0Dywhere Dis a nonzero square matrix not involving s2. Hence (1 36s2D)2=1 36s2D: 17.14(n/C0k)½c0(X0^S/C01X)/C0c/C138 ½c0(X0S/C01X)/C0c/C138¼(n/C0k)½c0(X0X)/C0c/C138s2 ½c0(X0X)/C0c/C138s2¼(n/C0k)s2 s2: 17.15@f(s) @s2 i¼@ @s2i½c0(X0S/C01X)/C0c/C138 ¼/C0c0(X0S/C01X)/C0@ @s2 i(X0S/C01X)/C18/C19 (X0S/C01X)/C0c [ by an extension of (2.117)] ¼c0(X0S/C01X)/C0X0S/C01 @ @s2 iS/C18/C19 S/C01X(X0S/C01X)/C0c ¼c0(X0S/C01X)/C0X0S/C01 @ @s2iXm j¼0s2 jZjZ0j !",
    "01X)/C0X0S/C01 @ @s2 iS/C18/C19 S/C01X(X0S/C01X)/C0c ¼c0(X0S/C01X)/C0X0S/C01 @ @s2iXm j¼0s2 jZjZ0j ! S/C01X(X0S/C01X)/C0c ¼c0(X0S/C01X)/C0X0S/C01ZiZ0iS/C01X(X0S/C01X)/C0c:646 ANSWERS AND HINTS TO THE PROBLEMS --- Page 656 --- 17.16 (a) (L^b/C0Lb)0½L(X0^S/C01X/C0/C138/C01(L^b/C0Lb) ¼(L^b/C0Lb)0PD/C01P0(L^b/C0Lb) ¼(L^b/C0Lb)0Xg i¼1pip0 i li(L^b/C0Lb) ¼Xg i¼1½p0i(L^b/C0Lb)/C1382 li: (b) note that var ½p0 i(L^b/C0Lb)/C138¼var(p0iL^b) ¼p0iL(X0^S/C01X)/C0L0pi ¼p0 iL(X0^S/C01X)/C0L0pi ¼½P0L(X0^S/C01X)/C0L0P/C138ii ¼½P0PDP0P/C138ii¼Dii¼li: (c) cov(p0 iL^b,p0i,L^b)¼p0iL(X0^S/C01X)/C0L0p0i ¼p0iPDP0pi0 ¼lip0 ipip0i0pi0¼lip0iOpi0¼0: 17.17 (a) We use Theorems 5.2a and 5.2e to obtain E½a/C0B(y/C0Xb)/C1380½a/C0B(y/C0Xb)/C138 ¼E(a0a)/C0E½a0B(y/C0Xb)/C0E(y/C0Xb)0B0a/C138 þE½(y/C0Xb)0B0B(y/C0Xb)/C138 ¼tr(V)/C0tr½Bcov(y,a)/C138/C0tr½B0cov(a,y/C138þtr(B0BS) ¼tr(V)/C0tr(BZV )0/C0tr(B0VZ0)þtr(BSB0) ¼tr(V)þtr(BZV0/C0B0VZ0þBSB0) ¼tr(V)þtr½B/C0VZ0S/C01)S(B/C0VZ0S/C01)0/C0VZ0S/C01ZV0/C138: (b)Since E(a)¼0,E(y2Xb)¼0, and cov( a,y)¼VZ0,w eh a v e E½a/C0B(y/C0Xb)/C138½a/C0B(y/C0Bb)/C1380 ¼E(aa0)/C0E½a(y/C0Xb)0B0/C138/C0E½B(y/C0Xb)a0/C138 þE½B(y/C0Xb)(y/C0Xb)0B0/C138 ¼cov(a)/C0½cov(a,y)/C138B0/C0Bcov(y,a)þBcov(y)B0 ¼V/C0VZ0B0/C0BZV0þBSB0 ¼Vþ(B/C0VZ0S/C01)S(B/C0VZ0S/C01)0/C0VZ0S/C01ZV0:ANSWERS AND HINTS TO THE PROBLEMS 647 --- Page 657 --- The ﬁrst and third terms do not involve B, and the second term is “minimized” by B¼VZ0S21.",
    "e 657 --- The ﬁrst and third terms do not involve B, and the second term is “minimized” by B¼VZ0S21. By “minimize,” we mean that any other choice for Badds a positive deﬁnite matrix to the result. This holds because Sis positive deﬁnite.",
    "r choice for Badds a positive deﬁnite matrix to the result. This holds because Sis positive deﬁnite. 17.18 ½I/C0X(X0S/C01X)/C0X0S/C01/C138S½I/C0X(X0S/C01X)/C0X0S/C01/C1380 ¼½S/C0X(X0S/C01X)/C0X0/C138½I/C0S/C01X(X0S/C01X)/C0X0/C138 ¼S/C0X(X0S/C01X)/C0X0/C0X(X0S/C01X)/C0X0 þX(X0S/C01X)/C0X0S/C01X(X0S/C01X)/C0X0 ¼S/C0X(X0S/C01X)/C0X0: 17.19 (a) Using Problem 17.17, the BLP OF Ua isE(Uajy)¼UE(ajy)¼ UGZ0S21(y2Xb).",
    "S/C0X(X0S/C01X)/C0X0: 17.19 (a) Using Problem 17.17, the BLP OF Ua isE(Uajy)¼UE(ajy)¼ UGZ0S21(y2Xb). (b)cov½UGZ0S/C01(y/C0Xb)/C138¼UGZ0S/C01SS/C01ZGU0¼UGZ0S/C01ZGU0: (c) cov½UGZ0S/C01(y/C0X^b)/C138 ¼cov{UGZ0S/C01½I/C0X(X0S/C01X)/C0X0S/C01/C138y} ¼UGZ0S/C01½I/C0X(X0S/C01X)/C0X0S/C01/C138 /C2S½I/C0X(X0S/C01X)/C0X0S/C01/C138S/C01ZGU0 ¼UGZ0S/C01½S/C0X(X0S/C01X)/C0X0/C138S/C01ZGU0 [using Problem 17.18] ¼UGZ0½S/C01/C0S/C01X(X0S/C01X)/C0X0S/C01/C138ZGU0:648 ANSWERS AND HINTS TO THE PROBLEMS --- Page 658 --- 17.20S/C01¼(s2I12þs2 1ZZ0)/C01 ¼s2I4þs2 1j4j0 4 O /C1/C1/C1 O Os2I4þs2 1j4j0 4/C1/C1/C1 O .........",
    "17.20S/C01¼(s2I12þs2 1ZZ0)/C01 ¼s2I4þs2 1j4j0 4 O /C1/C1/C1 O Os2I4þs2 1j4j0 4/C1/C1/C1 O ......... OO /C1/C1/C1s2I4þs2 1j4j0 40 BBBBB@1 CCCCCA/C01 ¼(s2I4þs2 1j4j0 4)/C01O /C1/C1/C1 O O (s2I4þs2 1j4j0 4)/C01/C1/C1/C1 O ......... OO /C1/C1/C1(s2I4þs2 1j4j0 4)/C010 BBBBB@1 CCCCCA ½by(2 :52)/C138 ¼1 s2I4/C0s2 s2þ4s2 1J4/C18/C19 O /C1/C1/C1 O O1 s2I4/C0s2 s2þ4s2 1J4/C16/C17 /C1/C1/C1 O .........",
    "¼1 s2I4/C0s2 s2þ4s2 1J4/C18/C19 O /C1/C1/C1 O O1 s2I4/C0s2 s2þ4s2 1J4/C16/C17 /C1/C1/C1 O ......... OO /C1/C1/C11 s2I4/C0s2 s2þ4s2 1J4/C18/C190 BBBBBBBBBBB@1 CCCCCCCCCCCA: ½by(2 :53)/C138 17.21 cov½EBLUP( a)/C138 ¼^GZ 0½^S/C01/C0^S/C01X(X0^S/C01X)/C0X0^S/C01/C138Z^G ¼^s4 1j0 40000 00j0 400 0000j0 40 B@1 CA½^S/C01/C0^S/C01j12(j0 12^S/C01j12)/C01j012^S/C01/C138j400 0j40 00j40 B@1 CA ¼^s4 1j0 40000 00j0400 0000j040 B@1 CA^S/C01/C012 ^s2þ4^s2 1J12\"# j400 0j40 00j40 B@1 CA ¼^s4 1 3(^s2þ4^s2 1)8/C04/C04 /C048 /C04 /C04/C0480 B@1 CA:ANSWERS AND HINTS TO THE PROBLEMS 649 --- Page 659 --- 17.22 cov(a,y)¼cov(a,XbþZaþ1)¼cov(a,Za)¼GZ0.",
    ":ANSWERS AND HINTS TO THE PROBLEMS 649 --- Page 659 --- 17.22 cov(a,y)¼cov(a,XbþZaþ1)¼cov(a,Za)¼GZ0. Hence, cov( ajy)¼ Saa/C0SayS/C01 yySya¼G/C0GZ0S/C01ZG: 17.23 cov(ajy)¼s2 1(I10/C0s21Z0S/C01Z)¼s21I10/C02s4 1 s2þ2s2 1I10:Note that the off- diagonal elements are 0’s. 17.24 SinceS21/2is symmetric, let S21/2¼ab ba/C18/C19 :Then I20/C0H/C3¼I20/C0S/C01=2X(X0S/C01X)/C01X0S/C01=2 ¼I20/C0aa aa ...... a/C0a a/C0a0 BBBB@1 CCCCA1 20a20 01 20a2 !",
    "I20/C0S/C01=2X(X0S/C01X)/C01X0S/C01=2 ¼I20/C0aa aa ...... a/C0a a/C0a0 BBBB@1 CCCCA1 20a20 01 20a2 ! aa /C1/C1/C1 aa aa /C1/C1/C1 /C0 a/C0a/C18/C19 ¼I20/C00:1J10 O O 0:1J10/C18/C19 The off-diagonal elements are either 0 or 20.1 (corresponding to corre- lations of either 0 or 20.11).",
    "e off-diagonal elements are either 0 or 20.1 (corresponding to corre- lations of either 0 or 20.11). Chapter 18 18.1 E(yi)¼(0)P(yi¼0)þ(1)P(yi¼1)¼1:pi¼pi, var(yi)¼E½yi/C0E(yi)/C1382 ¼(0/C0pi)2P(yi¼0)þ(1/C0pi)2P(yi¼1) ¼p2 i(1/C0pi)þ(1/C0pi)2pi ¼pi(1/C0pi)½piþ(1/C0pi)/C138: 18.2 Letui¼b0þb1xi:Then (18.7) becomes pi¼eui=(1þeui):From this we obtain 1/C0pi¼1/C0eui 1þeui¼1þeui/C0eui 1þeui¼1 1þeui, pi 1/C0pi¼eui 1þeui=1 1þeui¼eui(1þeui) 1þeui¼eui, lnpi 1/C0pi/C18/C19 ¼ui:650 ANSWERS AND HINTS TO THE PROBLEMS --- Page 660 --- 18.3 lnL(b0,b1)¼lnYn i¼1pyi i(1/C0pi)1/C0yi\"# ¼Xn i¼1[yilnpiþ(1/C0yi)ln(1/C0pi)] ¼X iyi[lnpi/C0ln(1/C0pi)]þX iln(1/C0pi) ¼X iyilnpi 1/C0pi/C18/C19 þX iln(1/C0pi): By Problem 17.2, this becomes lnL(b0,b1)¼X iyi(b0þb1xi)þX iln(1/C0pi): To show that ln(1/C0pi)¼/C0 ln(1þeb0þb1xi), letui¼ln[pi=(1/C0pi)]:Then eui¼pi 1/C0pi: Solve this no obtain pi¼eui=(1þeui):Then show that 1 /C0pi¼1=(1þeui) and that ln(1/C0pi)¼/C0 ln(1þeui)¼/C0 ln(1þeb0þb1xi): 18.4 @lnL(b0,b1) @b0¼Xn i¼1yi/C0Xn i¼1eb0þb1xi 1þeb0þb1xi: @lnL(b0,b1) @b1¼Xn i¼1xiyi/C0Xn i¼1eb0þb1xi 1þeb0þb1xi: 18.5 b(ui)¼niln(1/C0pi)¼/C0niln(1þeui), as shown in the answer to Problem 17.3.ANSWERS AND HINTS TO THE PROBLEMS 651 --- Page 661 --- References Agresti, A.",
    "answer to Problem 17.3.ANSWERS AND HINTS TO THE PROBLEMS 651 --- Page 661 --- References Agresti, A. (1984). Analysis of Ordinal Categorical Data . New York: Wiley. Agresti, A. (1990). Categorical Data Analysis . New York: Wiley. Anderson, E. B. (1991). The Statistical Analysis of Categorical Data (2nd ed.). New York: Springer-Verlag. Anderson, T. W. (1984). Introduction to Multivariate Statistical Analysis (2nd ed.). New York: Wiley. Andrews, D. F. (1974).",
    "Introduction to Multivariate Statistical Analysis (2nd ed.). New York: Wiley. Andrews, D. F. (1974). A robust method for multiple linear regression. Technometrics 16 , 523–531. Andrews, D. F. and A. M. Herzberg (1985). Data . New York: Springer-Verlag. Bailey, B. J. R. (1977). Tables of the Bonferroni t-statistic. Journal of the American Statistical Association 72 , 469–479. Bates, D. M. and D. G. Watts (1988). Nonlinear Regression and Its Applications . New York: Wiley. Beckman, R. J. and R. D.",
    "Watts (1988). Nonlinear Regression and Its Applications . New York: Wiley. Beckman, R. J. and R. D. Cook (1983). Outliers (with comments). Technometrics 25 , 119–163. Belsley, D. A., E. Kuh, and R. E. Welsch (1980). Regression Diagnostics :Identifying Data and Sources of Collinearity . New York: Wiley. Benjamini, Y. and Y. Hochberg (1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing.",
    "(1995). Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society, Series B: Methodological 57 , 289–300. Benjamini, Y. and D. Yekutieli (2001). The control of the false discovery rate in multiple testing under dependency. The Annals of Statistics 29 (4), 1165–1188. Benjamini, Y. and D. Yekutieli (2005). False discovery rate-adjusted multiple conﬁdence inter- vals for selected parameters.",
    "utieli (2005). False discovery rate-adjusted multiple conﬁdence inter- vals for selected parameters. Journal of the American Statistical Association 100 (469), 71–81. Bingham, C. and S. E. Feinberg (1982). Textbook analysis of covariance—is it correct? Biometrics 38 , 747–753. Birch, J. B. (1980). Some convergence properties of iterated reweighted least squares in the location model. Communications in Statistics B9 (4), 359–369. Bishop, Y., S. Fienberg, and P. Holland (1975).",
    "model. Communications in Statistics B9 (4), 359–369. Bishop, Y., S. Fienberg, and P. Holland (1975). Discrete Multivariate Analysis: Theory and Practice . Cambridge, MA: Massachusetts Institute of Technology Press. Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 653 --- Page 662 --- Bloomﬁeld, P. (2000). Fourier Analysis of Time Series: An Introduction . New York: Wiley. Bonferroni, C. E. (1936).",
    "2000). Fourier Analysis of Time Series: An Introduction . New York: Wiley. Bonferroni, C. E. (1936). Il calcolo delle assicurazioni su gruppi di teste. Studii in Onore del Profesor S. O. Carboni. Roma. Box, G. E. P. and P. V. Youle (1955). The exploration and exploitation of response surfaces: An example of the link between the ﬁtted surface and the basic mechanism of the system. Biometrics 11 , 287–323. Broadbent, K. L. (1993). A Comparison of Six Bonferroni Procedures .",
    "ystem. Biometrics 11 , 287–323. Broadbent, K. L. (1993). A Comparison of Six Bonferroni Procedures . Master’s thesis, Department of Statistics, Brigham Young University. Brown, H. and R. Prescott (1999). Applied Mixed Models in Medicine . New York: Wiley & Sons. Brown, H. and R. Prescott (2006). Applied Mixed Models in Medicine (2nd ed.). Hoboken, NJ: Wiley. Bryce, G. R. (1975). The one-way model. The American Statistician 29 , 69–70. Bryce, G. R. (1998). Personal communication. Bryce, G. R., M.",
    "The American Statistician 29 , 69–70. Bryce, G. R. (1998). Personal communication. Bryce, G. R., M. W. Carter, and M. W. Reader (1976). Nonsingular and singular transform- ations in the ﬁxed model. Annual Meeting of the American Statistical Association, Boston, Aug. 1976. Bryce, G. R., M. W. Carter, and D. T. Scott (1980a). Recovery of Estimability in Fixed Models with Missing Cells . Technical Report SD-022-R, Department of Statistics, Brigham Young University. Bryce, G. R., D. T. Scott, and M.",
    "port SD-022-R, Department of Statistics, Brigham Young University. Bryce, G. R., D. T. Scott, and M. W. Carter (1980b). Estimation and hypothesis testing in linear models—a reparameterization approach. Communications in Statistics—Series A, Theory and Methods 9 , 131–150. Casella, G. and E. I. George (1992). Explaining the Gibbs sampler. The American Statistician 46, 167–174. Chatterjee, S. and A. S. Hadi (1988). Sensitivity Analysis in Linear Regression . New York: Wiley. Christensen, R. (1996).",
    "S. Hadi (1988). Sensitivity Analysis in Linear Regression . New York: Wiley. Christensen, R. (1996). Plane Answers to Complex Questions: The Theory of Linear Models (2nd ed.). New York: Springer-Verlag. Christensen, R. (1997). Log-Linear Models and Logistic Regression (2nd ed.). New York: Springer-Verlag. Cochran, W. G. (1934). The distribution of quadratic forms in a normal system with appli- cations to the analysis of variance. Proceedings, Cambridge Philosophical Society 30 , 178–191.",
    "pli- cations to the analysis of variance. Proceedings, Cambridge Philosophical Society 30 , 178–191. Cochran, W. G. (1977). Sampling Techniques . New York: Wiley. Cook, R. D. (1977). Detection of inﬂuential observations in linear regression. Technometrics 19, 15–18. Cook, R. D. and S. Weisberg (1982). Residuals and Inﬂuence in Regression . New York: Chapman & Hall. Crampton, E. W. and J. W. Hopkins (1934).",
    "als and Inﬂuence in Regression . New York: Chapman & Hall. Crampton, E. W. and J. W. Hopkins (1934). The use of the method of partial regression in the analysis of comparative feeding trial data. Part II. J. Nutrition 8 , 329–339. Daniel, W. W. (1974). Biostatistics: A Foundation for Analysis in the Health Sciences . New York: Wiley. Devlin, S. J., R. Gnanadesikan, and J. R. Kettenring (1975). Robust estimation and outlier detection with correlation coefﬁcients.",
    ", and J. R. Kettenring (1975). Robust estimation and outlier detection with correlation coefﬁcients. Biometrika 62 , 531–546.654 REFERENCES --- Page 663 --- Diggle, P., P. Heagerty, K.-Y. Liang, and S. L. Zeger (2002). Analysis of Longitudinal Data . Oxford University Press. Dobson, A. J. (1990). An Introduction to Generalized Linear Models . New York: Chapman & Hall. Draper, N. R. and H. Smith (1981). Applied Regression Analysis (2nd ed.). New York: Wiley. Draper, N. R. and H. Smith (1998).",
    "h (1981). Applied Regression Analysis (2nd ed.). New York: Wiley. Draper, N. R. and H. Smith (1998). Applied Regression Analysis . New York: Wiley. Driscoll, M. F. (1999). An improved result relating quadratic forms and chi-square distri- butions. The American Statistician 53 , 273–275. Eubank, R. L. and R. L. Eubank (1999). Nonparametric Regression and Spline Smoothing . New York: Marcel Dekker. Evans, M. and T. Swartz (2000). Approximating Integrals via Monte Carlo and Deterministic Methods .",
    "Evans, M. and T. Swartz (2000). Approximating Integrals via Monte Carlo and Deterministic Methods . Oxford University Press. Ezekiel, M. (1930). Methods of Correlation Analysis . New York: Wiley. Fai, A. H.-T. and P. L. Cornelius (1996). Approximate F-tests of multiple degree of freedom hypotheses in generalized least squares analyses of unbalanced split-plot experiments. Journal of Statistical Computation and Simulation 54 , 363–378. Fisher, R. A. (1921).",
    "t experiments. Journal of Statistical Computation and Simulation 54 , 363–378. Fisher, R. A. (1921). On the probable error of a coefﬁcient of correlation deduced from a small sample. Metron 1 , 1–32. Fitzmaurice, G. M., N. M. Laird, and J. H. Ware (2004). Applied Longitudinal Analysis . Hoboken, NJ: Wiley. Flury, B. W. (1989). Understanding partial statistics and redundancy of variables in regression and discriminant analysis. The American Statistician 43 (1), 27–31. Fox, J. (1997).",
    "es in regression and discriminant analysis. The American Statistician 43 (1), 27–31. Fox, J. (1997). Applied Regresion Analysis, Linear Models, and Related Methods . Thousand Oaks, CA: SAGE Publications. Freund, R. J. and P. D. Minton (1979). Regression Methods: A Tool for Data Analysis . New York: Marcel Dekker. Fuller, W. A. and G. E. Battese (1973). Transformations for estimation of linear models with nested-error structure. Journal of the American Statistical Association 68 , 626–632.",
    "ar models with nested-error structure. Journal of the American Statistical Association 68 , 626–632. Gallant, A. R. (1975). Nonlinear regression. The American Statistician 29 , 73–81. Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (2004). Bayesian Data Analysis (2nd ed.). Chapman & Hall /CRC. Ghosh, B. K. (1973). Some monotonicity theorems for chi-square. Fandtdistributions with applications . Journal of the Royal Statistical Society 35 , 480–492. Giesbrecht, F. G. and J. C. Burns (1985).",
    "s . Journal of the Royal Statistical Society 35 , 480–492. Giesbrecht, F. G. and J. C. Burns (1985). Two-stage analysis based on a mixed model: Large- sample asymptotic theory and small-sample simulation results. Biometrics 41 , 477–486. Gilks, W. R. E., S. E. Richardson, and D. J. E. Spiegelhalter (1998). Markov Chain Monte Carlo in Practice . London: Chapman & Hall. Gomez, E., G. Schaalje, and G. Fellingham (2005).",
    "Monte Carlo in Practice . London: Chapman & Hall. Gomez, E., G. Schaalje, and G. Fellingham (2005). Performance of the kenward-roger method when the covariance structure is selected using aic and bic. Communications in Statistics: Simulation and Computation 34 (2), 377–392. Graybill, F. A. (1954). On quadratic estimates of variance components. Annals of Mathematical Statistics 25 (2), 367–372. Graybill, F. A. (1969). Introduction to Matrices with Applications in Statistics .",
    "25 (2), 367–372. Graybill, F. A. (1969). Introduction to Matrices with Applications in Statistics . Belmont, CA: Wadsworth Publishing Company.REFERENCES 655 --- Page 664 --- Graybill, F. A. (1976). Theory and Application of the Linear Model . North Scituate, MA: Duxbury Press. Graybill, F. A. and H. K. Iyer (1994). Regression Analysis: Concepts and Applications . North Scituate, MA: Duxbury Press. Graybill, F. A. and A. W. Wortham (1956).",
    "epts and Applications . North Scituate, MA: Duxbury Press. Graybill, F. A. and A. W. Wortham (1956). A note on uniformly best, unbiased estimators for variance components. Journal of the American Statistical Association 51 , 266–268. Gutsell, J. S. (1951). The effect of sulfamerazine on the erythrocyte and hemoglobin content of trout blood. Biometrics 7 (2), 171–179. Guttman, I. (1982). Linear Models: An Introduction . New York: Wiley. Hald, A. (1952).",
    "(2), 171–179. Guttman, I. (1982). Linear Models: An Introduction . New York: Wiley. Hald, A. (1952). Statistical Theory with Engineering Applications . New York: Wiley. Hamilton, D. (1987). Sometimes R2.ryx12þr2 yx2: Correlated variables are not always redun- dant. The American Statistician 41 (2), 129–132. Hampel, F. R. (1974). The inﬂuence curve and its role in robust estimation. Journal of the American Statistical Association 69 , 383–393. Hartley, H. O. (1956).",
    "ust estimation. Journal of the American Statistical Association 69 , 383–393. Hartley, H. O. (1956). Programming analysis of variance for general purpose computers. Biometrics 12 , 110–122. Harville, D. A. (1997). Matrix Algebra from a Statistician’s Perspective . New York: Springer- Verlag. Healy, M. J. R. and M. Westmacott (1969). Missing values in experiments analysed on auto- matic computers. Applied Statistics 5 , 203–206. Helland, I. S. (1987).",
    "xperiments analysed on auto- matic computers. Applied Statistics 5 , 203–206. Helland, I. S. (1987). On the interpretation and use of R2in regression analysis. Biometrics 43 , 61–69. Henderson, C. R. (1950). Estimation of genetic parameters. Annals of Mathetmatical Statistics 21, 309–310. Henderson, C. R. and A. J. McAllister (1978). The missing subclass problem in two-way ﬁxed models. Journal of Animal Science 46 , 1125–1137. Hendrix, L. J. (1967, Aug.).",
    "oblem in two-way ﬁxed models. Journal of Animal Science 46 , 1125–1137. Hendrix, L. J. (1967, Aug.). Auditory Discrimination Differences between Culturally Deprived and Nondeprived Preschool Children . PhD thesis, Brigham Young University. Hendrix, L. J., M. W. Carter, and J. Hintze (1978). A comparison of ﬁve statistical methods for analyzing pretest-post designs. Journal of Experimental Education 47 , 96–102. Hendrix, L. J., M. W. Carter, and D. T. Scott (1982).",
    "Journal of Experimental Education 47 , 96–102. Hendrix, L. J., M. W. Carter, and D. T. Scott (1982). Covariance analysis with heterogeneity of slopes in ﬁxed models. Biometrics 38 , 641–650. Hilbe, J. M. (1994). Generalized linear models. The American Statistician 48 , 255–265. Hoaglin, D. C. and R. E. Welsch (1978). The hat matrix in regression and ANOVA. The American Statistician 32 , 17–22. Hochberg, Y. (1988). A sharper Bonferroni procedure for multiple tests of signiﬁcance.",
    "n 32 , 17–22. Hochberg, Y. (1988). A sharper Bonferroni procedure for multiple tests of signiﬁcance. Biometrika 75 , 800–802. Hocking, R. R. (1976). The analysis and selection of variables in linear regression. Biometrics 32, 1–51. Hocking, R. R. (1985). The Analysis of Linear Models . Monterey, CA: Brooks /Cole. Hocking, R. R. (1996). Methods and Applications of Linear Models . New York: Wiley. Hocking, R. R. (2003). Methods and Applications of Linear Models (2nd ed.).",
    "odels . New York: Wiley. Hocking, R. R. (2003). Methods and Applications of Linear Models (2nd ed.). New York: Wiley.656 REFERENCES --- Page 665 --- Hocking, R. R. and F. M. Speed (1975). A full rank analysis of some linear model problems. Journal of the American Statistical Association 70 , 706–712. Hoerl, A. E. and R. W. Kennard (1970). Ridge regression: Biased estimation for nonorthogonal problems. Technometrics 12 , 55–67. Hogg, R. V. and A. T. Craig (1995).",
    "estimation for nonorthogonal problems. Technometrics 12 , 55–67. Hogg, R. V. and A. T. Craig (1995). Introduction to Mathematical Statistics (5th ed.). Englewood Cliffs, NJ: Prentice-Hall. Holland, B. (1991). On the application of three modiﬁed Bonferroni procedures to pairwise multiple comparisons in balanced repeated measures designs. Computational Statistics Quarterly 3 , 219–231. Holland, B. and M. D. Copenhaver (1987). An improved sequentially rejective Bonferroni test procedure.",
    "lland, B. and M. D. Copenhaver (1987). An improved sequentially rejective Bonferroni test procedure. Biometrics 43 , 417–423. Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics 6 , 65–70. Hommel, G. (1988). A stagewise rejective multiple test procedure based on a modiﬁed Bonferroni test. Biometrika 75 , 383–386. Hosmer, D., B. Jovanovic, and S. Lemeshow (1989). Best subsets logistic regression. Biometrics 45 , 1265–1270. Hosmer, D. W.",
    ", and S. Lemeshow (1989). Best subsets logistic regression. Biometrics 45 , 1265–1270. Hosmer, D. W. and S. Lemeshow (1989). Applied Logistic Regression . New York: Wiley. Huber, P. J. (1973). Robust regression: Asymptotics, conjectures, and Monte Carlo. Annals of Statistics 1 , 799–821. Hummel, T. J. and J. Sligo (1971). Empirical comparison of univariate and multivariate analy- sis of variance procedures. Psychological Bulletin 76 , 49–57. Jammalamadaka, S. R. and D. Sengupta (2003).",
    "variance procedures. Psychological Bulletin 76 , 49–57. Jammalamadaka, S. R. and D. Sengupta (2003). Linear Models an Integrated Approach . Singapore: World Scientiﬁc Publications. Jeske, D. R. and D. A. Harville (1988). Prediction-interval procedures and (ﬁxed-effects) conﬁdence-interval procedures for mixed linear models. Communications in Statistics: Theory and Methods 17 , 1053–1087. Jørgensen, B. (1993). The Theory of Linear Models . New York: Chapman & Hall. Kackar, R. N. and D. A.",
    "ørgensen, B. (1993). The Theory of Linear Models . New York: Chapman & Hall. Kackar, R. N. and D. A. Harville (1984). Approximations for standard errors of estimators of ﬁxed and random effects in mixed linear models. Journal of the American Statistical Association 79 , 853–862. Kendall, M. G. and A. Stuart (1969). The Advanced Theory of Statistics (3rd ed.), Vol. 1. New York: Hafner. Kenward, M. G. and J. H. Roger (1997).",
    "ced Theory of Statistics (3rd ed.), Vol. 1. New York: Hafner. Kenward, M. G. and J. H. Roger (1997). Small sample inference for ﬁxed effects from restricted maximum likelihood. Biometrics 53 , 983–997. Keselman, H. J., R. K. Kowalchuk, J. Algina, and R. D. Wolﬁnger (1999). The analysis of repeated measurements: A comparison of mixed-model Satterthwaite Ftests and a non- pooled adjusted degrees of freedom multivariate test. Communications in Statistics: Theory and Methods 28, 2967–2999.",
    "egrees of freedom multivariate test. Communications in Statistics: Theory and Methods 28, 2967–2999. Kleinbaum, D. G. (1994). Logistic Regression . New York: Springer-Verlag. Krasker, W. S. and R. Welsch (1982). Efﬁcient bounded-inﬂuence regression estimation. Journal of the American Statistical Association 77 , 595–604. Kshirsagar, A. M. (1983). A Course in Linear Models . New York: Marcel Dekker. Ku, H. H. and S. Kullback (1974). Loglinear models in contingency table analysis.",
    "rk: Marcel Dekker. Ku, H. H. and S. Kullback (1974). Loglinear models in contingency table analysis. The American Statistician 28 , 115–122.REFERENCES 657 --- Page 666 --- Kutner, M. H., C. J. Nachtsheim, J. Neter, and W. Li (2005). Applied Linear Statistical Models (5th ed.). New York: McGraw-Hill /Irwin. Lehmann, E. L. (1999). Elements of Large-Sample Theory . New York: Springer-Verlag. Lindley, D. V. and A. F. M. Smith (1972). Bayes estimates for the linear model (with discus- sion).",
    "Lindley, D. V. and A. F. M. Smith (1972). Bayes estimates for the linear model (with discus- sion). Journal of the Royal Statistical Society, Series B: Methodological 34 , 1–41. Lindsey, J. K. (1997). Applying Generalized Linear Models . New York: Springer-Verlag. Little, R. J. A. and D. B. Rubin (2002). Statistical Analysis with Missing Data . Hoboken, NJ: Wiley. Mahalanobis, P. C. (1936). On the generalized distance in statistics.",
    "ing Data . Hoboken, NJ: Wiley. Mahalanobis, P. C. (1936). On the generalized distance in statistics. Proceedings of the National Institute of Science of India 12 , 49–55. Mahalanobis, P. C. (1964). Professor Ronald Aylmer Fisher. Biometrics 20 , 238–250. Marcuse, S. (1949). Optimum allocation and variance components in nested sampling with an application to chemical analysis. Biometrics 5 (3), 189–206. McCullagh, P. and J. A. Nelder (1989). Generalized Linear Models (2nd ed.).",
    "ometrics 5 (3), 189–206. McCullagh, P. and J. A. Nelder (1989). Generalized Linear Models (2nd ed.). New York: Chapman & Hall. McCulloch, C. E. and S. R. Searle (2001). Generalized, Linear, and Mixed Models . New York: Wiley. Mclean, R. A., W. L. Sanders, and W. W. Stroup (1991). A unifed approach to mixed linear models. American Statistician 45 , 54–64. Mendenhall, W. and T. Sincich (1996). A Second Course in Statistics: Regression Analysis . Englewood Cliffs, NJ: Prentice-Hall. Milliken, G. A.",
    "ond Course in Statistics: Regression Analysis . Englewood Cliffs, NJ: Prentice-Hall. Milliken, G. A. and D. E. Johnson (1984). Analysis of Messy Data, Vol. 1: Designed Experiments . New York: Van Nostrand-Reinhold. Montgomery, D. C. and E. A. Peck (1992). Introduction to Linear Regresion Analysis (2nd ed.). New York: Wiley. Morrison, D. F. (1983). Applied Linear Statistical Methods . Englewood Cliffs, NJ: Prentice- Hall. Mosteller, F. and J. W. Tukey (1977). Data Analysis and Regression .",
    "ood Cliffs, NJ: Prentice- Hall. Mosteller, F. and J. W. Tukey (1977). Data Analysis and Regression . Reading, MA: Addison- Wesley. Muller, K. E. and M. C. Mok (1997). The distribution of Cook’s Dstatistic. Communications in Statistics: Theory and Methods 26 , 525–546. Myers, R. H. (1990). Classical and Modern Regression with Applications (2nd ed.). Boston: Duxbury Press. Myers, R. H. and J. S. Milton (1991). A First Course in the Theory of Linear Statistical Models . Boston: PWS-Kent. Nelder, J.",
    "ton (1991). A First Course in the Theory of Linear Statistical Models . Boston: PWS-Kent. Nelder, J. A. (1974). Letter to the editor. Journal of the Royal Statistical Society ,Series C 23 , 232. Nelder, J. A. and P. W. Lane (1995). The computer analysis of factorial experiments: In mem- oriam—Frank Yates. The American Statistician 49 , 382–385. Nelder, J. A. and R. W. M. Wedderburn (1972). Generalized linear models. Journal of the Royal Statistical Society, Series A 135 , 370–384. Ogden, R. T.",
    "alized linear models. Journal of the Royal Statistical Society, Series A 135 , 370–384. Ogden, R. T. (1997). Essential Wavelets for Statistical Applications and Data Analysis . Birkhauser.658 REFERENCES --- Page 667 --- Ostle, B. and L. C. Malone (1988). Statistics in Research: Basic Concepts and Techniques for Research Workers (4th ed.). Ames: Iowa State University Press. Ostle, B. and R. W. Mensing (1975). Statistics in Research (3rd ed.). Ames: Iowa State University Press. Patterson, H. D.",
    "ensing (1975). Statistics in Research (3rd ed.). Ames: Iowa State University Press. Patterson, H. D. and R. Thompson (1971). Recovery of inter-block information when block sizes are unequal. Biometrika 58 , 545–554. Pawitan, Y. (2001). In All Likelihood: Statistical Modelling and Inference Using Likelihood . Oxford University Press. Pearson, E. S., R. L. Plackett, and G. A. Barnard (1990). Student: A Statistical Biography of William Sealy Gossett . New York: Oxford University Press. Plackett, R.",
    ": A Statistical Biography of William Sealy Gossett . New York: Oxford University Press. Plackett, R. L. (1981). The Analysis of Categorical Data (2nd ed.). London: Grifﬁn. Rao, C. R. (1965). Linear Statistical Inference and Its Applications . New York: Wiley. Rao, P. S. R. S. (1997). Variance Components Estimation . London: Chapman & Hall. Ratkowsky, D. A. (1983). Nonlinear Regression Modelling: A Uniﬁed Approach . New York: Marcel Dekker. Ratkowsky, D. A. (1990).",
    "onlinear Regression Modelling: A Uniﬁed Approach . New York: Marcel Dekker. Ratkowsky, D. A. (1990). Handbook of Nonlinear Regression Models . New York: Marcel Dekker. Read, T. R. C. and N. A. C. Cressie (1988). Goodness-of-Fit Statistics for Discrete Multivariate Data . New York: Springer-Verlag. Reader, M. W. (1973). The Analysis of Covariance with a Single Linear Covariate Having Heterogeneous Slopes . Master’s thesis, Department of Statistics, Brigham Young University. Rencher, A. C. (1993).",
    "Slopes . Master’s thesis, Department of Statistics, Brigham Young University. Rencher, A. C. (1993). The contribution of individual variables to Hotelling’s T2, Wilks’Land R2.Biometrics 49 , 217–225. Rencher, A. C. (1995). Methods of Multivariate Analysis . New York: Wiley. Rencher, A. C. (1998). Multivariate Statistical Inference and Applications . New York: Wiley. Rencher, A. C. (2002). Multivariate Statistical Inference and Applications . Hoboken, NJ: Wiley. Rencher, A. C. and D. T.",
    ". Multivariate Statistical Inference and Applications . Hoboken, NJ: Wiley. Rencher, A. C. and D. T. Scott (1990). Assessing the contribution of individual variables following rejection of a multivariate hypothesis. Communications in Statistics—Series B, Simulation and Computation 19 , 535–553. Rom, D. M. (1990). A sequentially rejective test procedure based on a modiﬁed Bonferroni inequality. Biometrika 77 , 663–665. Ross, S. M. (2006). Introduction to Probability Models (9th ed.).",
    "equality. Biometrika 77 , 663–665. Ross, S. M. (2006). Introduction to Probability Models (9th ed.). San Diego, CA: Academic Press. Royston, J. P. (1983). Some techniques for assessing multivariate normality based on the Shapiro-Wilk W. Applied Statistics 32 , 121–133. Ryan, T. P. (1997). Modern Regression Methods . New York: Wiley. Santner, T. J. and D. E. Duffy (1989). The Statistical Analysis of Discrete Data . New York: Springer-Verlag. Satterthwaite, F. E. (1941). Synthesis of variances.",
    "s of Discrete Data . New York: Springer-Verlag. Satterthwaite, F. E. (1941). Synthesis of variances. Psychometrika 6 , 309–316. Saville, D. J. (1990). Multiple comparison procedures: The practical solution (C /R: 91V45 p165–168). The American Statistician 44 , 174–180.REFERENCES 659 --- Page 668 --- Schaalje, G. B., J. B. McBride, and G. W. Fellingham (2002). Adequacy of approximations to distributions of test statistics in complex mixed linear models.",
    "002). Adequacy of approximations to distributions of test statistics in complex mixed linear models. Journal of Agricultural, Biological, and Environmental Statistics 7 (4), 512–524. Scheffe ´, H. (1953). A method of judging all contrasts in the analysis of variance. Biometrika 40, 87–104. Scheffe ´, H. (1959). The Analysis of Variance . New York: Wiley. Schott, J. R. (1997). Matrix Analysis for Statistics . New York: Wiley. Schwarz, G. (1978). Estimating the dimension of a model.",
    "Analysis for Statistics . New York: Wiley. Schwarz, G. (1978). Estimating the dimension of a model. Annals of Statistics 6 , 461–464. Searle, S. R. (1971). Linear Models . New York: Wiley. Searle, S. R. (1977). Analysis of Variance of Unbalanced Data from 3-Way and Higher-Order Classiﬁcations . Technical Report BU-606-M, Cornell University, Biometrics Units. Searle, S. R. (1982). Matrix Algebra Useful for Statistics . New York: Wiley. Searle, S. R., G. Casella, and C. E. McCulloch (1992).",
    "ebra Useful for Statistics . New York: Wiley. Searle, S. R., G. Casella, and C. E. McCulloch (1992). Variance Components . New York: Wiley. Searle, S. R., F. M. Speed, and H. V. Henderson (1981). Some computational and model equiv- alencies in analysis of variance of unequal-subclass-numbers data. The American Statistician 35 , 16–33. Seber, G. A. F. (1977). Linear Regression Analysis . New York: Wiley. Seber, G. A. F. and A. J. Lee (2003). Linear Regression Analysis (2nd ed.).",
    "lysis . New York: Wiley. Seber, G. A. F. and A. J. Lee (2003). Linear Regression Analysis (2nd ed.). Hoboken, NJ: Wiley. Seber, G. A. F. and C. J. Wild (1989). Nonlinear Regression . New York: Wiley. Sen, A. and M. Srivastava (1990). Regression Analaysis: Theory, Methods, and Applications . New York: Springer-Verlag. Shaffer, J. P. (1986). Modiﬁed sequentially rejective multiple test procedures. Journal of the American Statistical Association 81 , 826–831. Silverman, B. W. (1999).",
    "t procedures. Journal of the American Statistical Association 81 , 826–831. Silverman, B. W. (1999). Density Estimation for Statistics and Data Analysis . London: Chapman & Hall. Simes, R. J. (1986). An improved Bonferroni procedure for multiple tests of signiﬁcance. Biometrika 73 , 751–754. Snedecor, G. W. (1948). Answer to query. Biometrics 4 (2), 132–134. Snedecor, G. W. and W. G. Cochran (1967). Statistical Methods (6th ed.). Ames: Iowa State University Press. Snee, R. D. (1977).",
    "ochran (1967). Statistical Methods (6th ed.). Ames: Iowa State University Press. Snee, R. D. (1977). Validation of regression models: Methods and examples. Technometrics 19, 415–428. Speed, F. M. (1969). A New Approach to the Analysis of Linear Models . Technical report, National Aeronautics and Space Administration, Houston, TX; a NASA Technical memo, NASA TM X-58030. Speed, F. M., R. R. Hocking, and O. P. Hackney (1978). Methods of analysis of linear models with unbalanced data.",
    "R. R. Hocking, and O. P. Hackney (1978). Methods of analysis of linear models with unbalanced data. Journal of the American Statistical Association 73 , 105–112. Spiegelhalter, D. J., N. G. Best, B. P. Carlin, and A. van der Linde (2002). Bayesian measures of model complexity and ﬁt (pkg: P583-639). Journal of the Royal Statistical Society, Series B: Statistical Methodology 64 (4), 583–616. Stapleton, J. H. (1995). Linear Statistical Models . New York: Wiley. Stigler, S. M. (2000).",
    "83–616. Stapleton, J. H. (1995). Linear Statistical Models . New York: Wiley. Stigler, S. M. (2000). The problematic unity of biometrics. Biometrics 56 (3), 653–658.660 REFERENCES --- Page 669 --- Stokes, M. E., C. S. Davis, and G. G. Koch (1995). Categorical Data Analysis Using the SAS System . Cary, NC: SAS Institute. Theil, H. and C. Chung (1988). Information-theoretic measures of ﬁt for univariate and multi- variate linear regressions. The American Statistician 42 , 249–252. Tiku, M. L.",
    "nivariate and multi- variate linear regressions. The American Statistician 42 , 249–252. Tiku, M. L. (1967). Tables of the power of the F-test. Journal of the American Statistical Association 62 , 525–539. Turner, D. L. (1990). An easy way to tell what you are testing in analysis of variance. Communications in Statistics—Series A, Theory and Methods 19 , 4807–4832. Urquhart, N. S., D. L. Weeks, and C. R. Henderson (1973). Estimation associated with linear models: A revisitation.",
    ", D. L. Weeks, and C. R. Henderson (1973). Estimation associated with linear models: A revisitation. Communications in Statistics 1 , 303–330. Verbeke, G. and G. Molenberghs (2000). Linear Mixed Models for Longitudinal Data . Springer-Verlag. Wald, A. (1943). Tests of statistical hypotheses concerning several parameters when the number of observations is large. Transactions of the American Mathematical Society 54 , 426–483. Wang, S. G. and S. C. Chow (1994).",
    ". Transactions of the American Mathematical Society 54 , 426–483. Wang, S. G. and S. C. Chow (1994). Advanced Linear Models: Theory and Applications . New York: Marcel Dekker. Weisberg, S. (1985). Applied Linear Regression . New York: Wiley. Welsch, R. E. (1975). Conﬁdence regions for robust regression. Paper presented at Annual Meeting of the American Statistical Association , Washington, DC. Winer, B. J. (1971). Statistical Principles in Experimental Design (2nd ed.). New York: McGraw-Hill.",
    "Winer, B. J. (1971). Statistical Principles in Experimental Design (2nd ed.). New York: McGraw-Hill. Working, H. and H. Hotelling (1929). Application of the theory of error to the interpretation of trends. Journal of the American Statistical Association ,Suppl. (Proceedings) 24 , 73–85. Yates, F. (1934). The analysis of multiple classiﬁcations with unequal numbers in the different classes.",
    "s, F. (1934). The analysis of multiple classiﬁcations with unequal numbers in the different classes. Journal of the American Statistical Association 29 , 52–66.REFERENCES 661 --- Page 670 --- Index Adjusted R2, 162 Alias matrix, 170 Analysis of covariance, 443–478 assumptions, 443–444 covariates, 444 estimation, 446–448 model, 444–445 one-way model with one covariate, 449–451 estimation of parameters, 449–450 model, 449 testing hypotheses, 448, 450–451 equality of treatment effects, 450–452 homogeneity of slopes, 452–456 interpretation, 456 slope, 452 one-way model with multiple covariates, 464–472 estimation of parameters, 465–468 model, 464–465testing hypotheses, 468–469 equality of treatment effects, 468–469 homogeneity of slope vectors, 470–472 slope vector, 470 power, 444testing hypotheses, 448 two-way model with one covariate, 457–464 model, 457 testing hypotheses, 458–464 homogeneity of slopes, 463–464main effects and interactions, 458–462 slope, 462 unbalanced models, 473–474 cell means model, 473 constrained model, 473–474Analysis of variance, 295–338 estimability of bin the empty cells model, 432, 434–435 estimability of bin the non-full-rank model, 302–304 estimable functions l0b, 305–308 conditions for estimability of l0b, 305–307 estimators of l0b, 309–313 BLUE properties of, 313 covariance of, 312 variance of, 311 estimation of s2in the non-full-rank model, 313–314 model, 3–4, 295–301 one-way.",
    "12 variance of, 311 estimation of s2in the non-full-rank model, 313–314 model, 3–4, 295–301 one-way. SeeOne-way model two-way.",
    "tion of s2in the non-full-rank model, 313–314 model, 3–4, 295–301 one-way. SeeOne-way model two-way. SeeTwo-way model normal equations, 302–303 solution using generalized inverse, 302–303 normal model, 314–316 estimators of bands2, 314–315 properties of, 316 and regression, 4reparameterization to full-rank model, 318–320 side conditions, 320–322, 433SSE in the non-full-rank model, 313–314 testable hypotheses, 323–324 testable hypotheses in the empty cells model, 433 testing hypotheses, 323–329 full and reduced model, 324–326 general linear hypothesis, 326–329 treatments or natural groupings of units, 4 unbalanced data.",
    "–326 general linear hypothesis, 326–329 treatments or natural groupings of units, 4 unbalanced data. SeeUnbalanced data in ANOVA Angle between two vectors, 41–42, 136, 163, 238 Linear Models in Statistics ,Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc.",
    ",Second Edition , by Alvin C. Rencher and G. Bruce Schaalje Copyright #2008 John Wiley & Sons, Inc. 663 --- Page 671 --- Asymptotic inference for large samples, 260–262, 491, 515 Augmented matrix, 29 Bayes’ theorem, 278–279 Bayesian linear model, 279–284, 480 Bayesian linear mixed model, 497 Best linear predictor, 499 Best linear unbiased estimators (BLUE), 147, 165, 313 Best quadratic unbiased estimators, 151, 486 Beta weights, 251BIC. SeeInformation criterion BLUE.",
    "13 Best quadratic unbiased estimators, 151, 486 Beta weights, 251BIC. SeeInformation criterion BLUE. SeeBest linear unbiased estimators Causality, 3, 130–131, 443 Chi-square distribution, 112–114 central chi-square, 112 moment-generating function, 112–113 noncentral chi-square, 112–114 noncentrality parameter, 112, 124 Cluster correlation, 479–480, 481–485 Coefﬁcient of determination in multiple regression, 161–164in simple linear regression, 133–134 Coefﬁcient(s), regression, 2, 127Conditional density, 73, 95–99, 278–284, 498–499 Conﬁdence interval(s) for b1in simple linear regression, 133 in Bayesian regression, 278, 285 in linear mixed models, 491, 495 in multiple regression.",
    "ssion, 133 in Bayesian regression, 278, 285 in linear mixed models, 491, 495 in multiple regression. SeeRegression, multiple linear with ﬁxed x’s, conﬁdence interval(s) in random- xregression, 261–262 Contrasts, 308, 341, 357–371 Control of output, 3 Correlation bivariate, 134 Correlation matrix (matrices) population, 77–78 relationship to covariance matrix, 77–78 sample, 247 relationship to covariance matrix, 247–248 Covariance matrix (matrices) forbˆ, 145 for partitioned random vector, 78 population, 75–76 sample, 156, 246–247 for two random vectors, 82Data space, 153, 163, 316–317 Dependent variable, 1, 137, 295 Derivative, matrix and vector, 56–59, 91, 109, 142, 158, 495 Determinant, 37–41Determination, coefﬁcient of.",
    "ve, matrix and vector, 56–59, 91, 109, 142, 158, 495 Determinant, 37–41Determination, coefﬁcient of. SeeCoefﬁcient of determination Diagnostics, regression, 227–238 also Hat matrix; Inﬂuential observations;Outliers; Residual(s) Diagonal matrix, 8DIC. SeeInformation criterion Distance Mahalanobis, 77standardized, 77 Distribution(s) chi-square, 112–114 F, 114–116 gamma, 280 inverse gamma, 284 multivariate t, 282–283, 285 normal.",
    "s) chi-square, 112–114 F, 114–116 gamma, 280 inverse gamma, 284 multivariate t, 282–283, 285 normal. SeeNormal distribution t, 216, 283 Effect of each variable on R 2, 262–265 Eigenvalues. SeeMatrix, eigenvalues Eigenvectors. SeeMatrix, eigenvectors Empty cells, 432–439 Error sum of squares.",
    "Matrix, eigenvalues Eigenvectors. SeeMatrix, eigenvectors Empty cells, 432–439 Error sum of squares. SeeSSE Error term, 1, 137Estimated best linear unbiased predictor, 499 Estimated generalized least squares estimation, 490 Exchangeability, 277 Expected mean squares, 173–174, 179, 182, 312–317, 362–367, 433 Expected value of bilinear form [ E(x 0Ay)], 111 of least squares estimators, 131–132 of quadratic form [ E(y0Ay)], 107 ofR2, 162 of random matrix, 75–76 of random variable [ E(y)], 70 of random vector [ E(y)], 75–76 of sample covariance [ E(sxy)], 112 of sample variance [ E(s2)], 108, 131, 150 of sum of random variables, 70of sum of random vectors, 75–76 Exponential family, 514664 INDEX --- Page 672 --- F-Distribution, 114–116 central F, 114 mean of central F, 115 noncentral F, 115 noncentrality parameter, 115 variance of central F, 115 F-Tests.",
    "of central F, 115 noncentral F, 115 noncentrality parameter, 115 variance of central F, 115 F-Tests. See also Regression, multiple linear with ﬁxed x’s, tests of hypoth- eses; Tests of hypotheses general linear hypothesis test, 198–203for overall regression, 185 power, 115 subset of the b’s, 189 False discovery rate, 206 First order multivariate Taylor series, 495 Fixed effects models, 480 Gauss-Markov theorem, 146–147, 276.",
    "order multivariate Taylor series, 495 Fixed effects models, 480 Gauss-Markov theorem, 146–147, 276. See also Best linear unbiased estimators Generalized least squares, 164–169, 285–286, 479, 503 Generalized linear models, 513–516 exponential family, 514 likelihood function, 512 linear predictor, 513–514link function, 514 model, 514 Generalized inverse, 32–37, 302–303, 343, 384 of symmetric matrix, 33 Generalized variance, 77, 88–89 Geometry of least squares, 151–154, 163, 316–317 angle between two vectors, 163 prediction space, 153–154, 163, 316–317 data space, 153, 163, 316–317parameter space, 152, 154, 316–317 Gibbs sampling, 289, 291 Hadamard product, 16, 425 Hat matrix, 230–231Hessian matrix, 495 Highest density interval, 279, 285 Hyperprior distribution, 280, 287Hypothesis tests.",
    "an matrix, 495 Highest density interval, 279, 285 Hyperprior distribution, 280, 287Hypothesis tests. SeeTests of hypotheses Idempotent matrix for chi-square distribution, 117–118 deﬁnition and properties, 54–55 in linear mixed models, 487 Identity matrix, 8Independence of contrasts, 358–362 independence and zero covariance, 93–94 of linear functions and quadratic forms, 119–120 of quadratic forms, 120–121of random variables, 71, 94 of random vectors, 93, 94of SSR and SSE, 187 Inﬂuential observations, 235–238 Cook’s distance, 236–237 leverage, 236 Information criterion, 286 Iterative methods for ﬁnding estimates, 490 Invariance ofF, 149, 200 of maximum likelihood estimators, 247–248 ofR 2, 149 ofs2, 149 oft, 149 ofyˆ, 148–149 Inverse matrix.",
    "f maximum likelihood estimators, 247–248 ofR 2, 149 ofs2, 149 oft, 149 ofyˆ, 148–149 Inverse matrix. SeeMatrix, inverse jvector, 8 Jmatrix, 8 Kenward–Roger adjustment, 496–497 Lagrange multiplier, 60, 68, 179, 201, 220, 223, 429 Least squares, 128, 131, 141, 143, 145–151, 302, 507 properties of estimators, 129–133, 143, 145–147 Likelihood function, 158, 513–514 Likelihood ratio tests, 258–262 Linear estimator, 143.",
    "43, 145–147 Likelihood function, 158, 513–514 Likelihood ratio tests, 258–262 Linear estimator, 143. See also Best linear unbiased estimators Linear mixed model, 480 randomized blocks, 481–482 subsampling, 482 split plot studies, 483–484, 492–494 one-way random effects, 484, 489 random coefﬁcients, 484–485 heterogeneous variances, 485–486 Linear model, 2, 137 Linear models, generalized.",
    "efﬁcients, 484–485 heterogeneous variances, 485–486 Linear model, 2, 137 Linear models, generalized. See Generalized linear models Logistic regression, 508–511 binary y, 508 estimation, 510INDEX 665 --- Page 673 --- Logistic regression ( Continued ) logit transformation, 509 model, 509–510 polytomous model, 511 categorical, 511 ordinal, 511 several x’s, 510 Logit transformation, 509 Loglinear models, 511–512 contingency table, 511 likelihood ratio test, 512 maximum likelihood estimators, 512 LSD test, 209 Mahalanobis distance, 77 Markov Chain Monte Carlo, 288–289, 291–292 Matrix (matrices), 5–68 addition of, 9–10 algebra of, 5–60 augmented matrix, 29bilinear form, 16 Cholesky decomposition, 27 conditional inverse, 33 conformable matrices, 9 deﬁnition, 5derivatives, 56–58 determinant, 37–41 of partitioned matrix, 38–40 diagonal of a matrix, 7 diagonal matrix, 8 diagonalizing a matrix, 52 differentiation, 56–57 eigenvalues, 46–53, 496 characteristic equation, 47 and determinant, 51–52 of functions of a matrix, 49–50 of positive deﬁnite matrix, 53 square root matrix, 53 of product, 50–53of symmetric matrix, 51 and trace, 51 eigenvectors, 46–47, 496equality, 6generalized inverse, 32–37, 302, 343, 384, 391–395 of symmetric matrix, 36 Hadamard product, 16, 425idempotent matrix, 54 and eigenvalues, 54 identity matrix, 8inverse, 21–23 conditional inverse, 33 generalized inverse, 32–37of partitioned matrix, 23–24 of product, 22 jvector, 8 Jmatrix, 8 multiplication of, 10 conformal matrices, 10 nonsingular matrix, 21 notation, 5 O(zero matrix), 8 orthogonal matrix, 41–43 partitioned matrix, 16–18 multiplication of, 17 positive deﬁnite matrix, 24–28 positive semideﬁnite matrix, 25–28product, 10 commutativity, 10 as linear combination of columns, 17 matrix and diagonal matrix, 16 matrix and j,1 2 matrix and scalar, 10 product equal to zero, 20 rank of product, 21 quadratic form, 16.",
    "ix and j,1 2 matrix and scalar, 10 product equal to zero, 20 rank of product, 21 quadratic form, 16. See also Quadratic form(s) random matrix, 69 rank, 19–21. See also Rank of a matrix spectral decomposition, 51, 360, 362, 495–496 square root matrix, 53 sum of, 9 symmetric matrix, 7 spectral decomposition, 51 trace, 44–46transpose, 7 of product, 13 triangular matrix, 8 vector(s). SeeVector(s) zero matrix ( O) and zero vector ( 0), 8 Matrix product.",
    "riangular matrix, 8 vector(s). SeeVector(s) zero matrix ( O) and zero vector ( 0), 8 Matrix product. SeeMatrix, product Maximum likelihood estimators forbands2in ANOVA, 315 forbands2in ﬁxed- xregression, 158–159 properties, 159–161 forb0,b1, ands2in random- x regression, 245–248 properties, 248–249 invariance of, 249 in loglinear models, 511 for partial correlation, 266–268 MCMC. SeeMarkov Chain Monte Carlo Mean. See also Expected value sample mean.",
    "al correlation, 266–268 MCMC. SeeMarkov Chain Monte Carlo Mean. See also Expected value sample mean. SeeSample mean population mean, 70666 INDEX --- Page 674 --- Missing at random, 432 Misspeciﬁcation of cov( y), 167–169. See also Generalized least squares Misspeciﬁcation of model, 169–174 alias matrix, 170 overﬁtting, 170–172 underﬁtting, 170–172 Model diagnostics, 227–238. See also Hat matrix; Inﬂuential observations; Outliers; Residual(s) Model, linear, 2, 137Model validation, 227–238.",
    "trix; Inﬂuential observations; Outliers; Residual(s) Model, linear, 2, 137Model validation, 227–238. See also Hat matrix; Inﬂuential observations;Outliers; Residual(s); Moment-generating function, 90–92, 96, 99–100, 103–104, 108 Multiple linear regression, 90–92, 108, 112–114, 117–119, 122.",
    "tion, 90–92, 96, 99–100, 103–104, 108 Multiple linear regression, 90–92, 108, 112–114, 117–119, 122. See Regression, multiple linear with ﬁxed x’s Multivariate delta method, 495Multivariate normal distribution, 87–103 conditional distribution, 95–97 density function, 88–89 independence and zero covariance, 93–94 linear functions of, 89 marginal distribution, 93moment generating-function of, 90–92 partial correlation, 100–101 properties of, 92–100 Noncentrality parameter for chi-square, 112 forF, 114, 187, 192, 325 fort, 116, 132 Nonlinear regression, 507 conﬁdence intervals, 507 least squares estimators, 507 tests of hypotheses, 507 Nonsingular matrix, 21 Normal distribution multivariate.",
    "es estimators, 507 tests of hypotheses, 507 Nonsingular matrix, 21 Normal distribution multivariate. SeeMultivariate normal distribution univariate, 87–88 standard normal, 87 Normalizing constant, 278, 281, 284 O(zero matrix), 8 One-way model (balanced), 3, 295–298, 339–376 contrasts, 357–371 and eigenvectors, 360–362 hypothesis test for, 344–351orthogonal contrasts, 358–371 independence of, 363–364 orthogonal polynomial contrasts, 363–371 partitioning of sum of squares, 360–361 estimable functions, 340–341 contrasts, 341 estimation of s2, 343–344 expected mean squares, 351–357 full-reduced–model method, 352–354 general linear hypothesis method, 354–356 normal equations, 341–344 solution using generalized inverse, 343 solution using side conditions, 342–343 overparameterized model, 297 assumptions, 297–298 parameters not unique, 297reparameterization, 298 side conditions, 298SSE, 314 testing the hypothesis H 0:m1¼m2¼ ...¼mk, 344–351 full and reduced model, 344–348 general linear hypothesis, 348–351 Orthogonal matrix, 41–43 Orthogonal polynomials, 363–371 Orthogonal vectors, 40 Orthogonal x’s in regression models, 149, 174–178 Orthogonality of columns of Xin balanced ANOVA models, 333–335 Orthogonality of rows of Ain unbalanced ANOVA models, 293–296 Orthogonalizing the x’s in regression models, 174–178 and partial regression coefﬁcients, 175–176 Outliers, 232–235 mean shift outlier model, 235 PRESS (prediction sum of squares), 235 Overﬁtting, 170–172 p-Value forF-test, 188–189 fort-test, 132 Parameter space, 152, 154, 316–317 Partial correlation(s), 100–101, 266–273 matrix of (population) partial correlations, 100–101 sample partial correlations, 177–178, 266–173INDEX 667 --- Page 675 --- Partial interaction constraints, 434 Poisson distribution, 512 Poisson regression, 512–513 likelihood function, 513 model, 513 Polynomials, orthogonal.",
    "bution, 512 Poisson regression, 512–513 likelihood function, 513 model, 513 Polynomials, orthogonal. SeeOrthogonal polynomials Positive deﬁnite matrix, 24–28 Positive semideﬁnite matrix, 25–28Posterior distribution, 278–284 conditional, 289marginal, 282 Posterior predictive distribution, 279, 290–292 Prediction, 2–3, 137, 142, 148, 156, 161 Precision, 280 Prediction of a random effect, 497–499 Prediction interval, 213–215Prediction space, 153–154, 163, 316–317 Prediction sum of squares (PRESS), 235PRESS (prediction sum of squares), 235 Prior distribution, 278–284 diffuse, 281, 287informative, 281 conjugate, 281, 289speciﬁcation, 280 Projection matrix, 228 Quadratic form(s), 16, 489 distribution of, 117–118 expected value of, 107 idempotent matrix, 106 independence of, 119–121 moment-generating function of, 108 variance of, 108 r 2in simple linear regression, 133–134 R2(squared multiple correlation), 161–164, 254–257 effect of each variable on R2, 262–265 ﬁxed x’s, 161–164 adjusted R2, 162 angle between two vectors, 163 properties of R2andR, 162 random x’s, 254–257 population multiple correlation, 254 properties, 255 sample multiple correlation, 256 properties, 256–257 Random matrix, 69 Random model, 480Random variable(s), 69 correlation, 74 covariance, 71 and independence, 71–74 expected value (mean), 70independent, 71, 94 mean (expected value), 70 standard deviation, 71 variance, 70 Random vector(s), 69–74 correlation matrix, 77–78covariance matrix, 75–76, 83 linear functions of, 79–83 mean of, 80 variances and covariances of, 81–83 mean vector, 75–76 partitioned, 78–79 Random x’s in regression.",
    "variances and covariances of, 81–83 mean vector, 75–76 partitioned, 78–79 Random x’s in regression. SeeRegression, random x’s Rank of a matrix, 19–21 full rank, 19 rank of product, 20–21 Regression coefﬁcients ( b’s), 2, 138, 251 partial regression coefﬁcients, 138 standardized coefﬁcients (beta weights), 251 Regression, logistic. SeeLogistic regression Regression, multiple linear with ﬁxed x’s, 2–3, 137–184 assumptions, 138–139 centered x’s, 154–157 coefﬁcients.",
    "multiple linear with ﬁxed x’s, 2–3, 137–184 assumptions, 138–139 centered x’s, 154–157 coefﬁcients. SeeRegression coefﬁcients conﬁdence interval(s) forb, 209 forE(y), 211–212 for one a0b, 211 for onebj, 210–211 fors2, 215 for several ai0b’s, 216–217 for several bj’s, 216 design matrix, 138 diagnostics, 227–238.",
    "ors2, 215 for several ai0b’s, 216–217 for several bj’s, 216 design matrix, 138 diagnostics, 227–238. See also Diagnostics, regression estimation of b0,b1,...,bk, 141–145 with centered x’s, 154–157 least squares, 2, 143–144 maximum likelihood, 158–159 properties of estimators, 145–149 with sample covariances, 157668 INDEX --- Page 676 --- estimation of s2 maximum likelihood estimator, 158–159 minimum variance unbiased estimator, 158–159 unbiased estimator,149–151 best quadratic unbiased estimator, 151 generalized least squares, 164–169 minimum variance estimators, 158–159 misspeciﬁcation of error structure, 151–153 misspeciﬁcation of model, 169–174.",
    "e estimators, 158–159 misspeciﬁcation of error structure, 151–153 misspeciﬁcation of model, 169–174. See also Misspeciﬁcation of model model, 137–140 multiple correlation ( R), 161–162 normal equations, 141–142orthogonal x’s, 149, 174–178 orthogonalizing the x’s, 174–178 outliers, 232–235. See also Outliers partial regression, 141prediction.",
    "ogonalizing the x’s, 174–178 outliers, 232–235. See also Outliers partial regression, 141prediction. SeePrediction prediction equation, 142prediction interval, 213–215 properties of estimators, 145–149 purposes of, 2–3random x’s.SeeRegression, random x’s residuals, 227–230.",
    "ties of estimators, 145–149 purposes of, 2–3random x’s.SeeRegression, random x’s residuals, 227–230. See also Residuals sufﬁcient statistics, 159–160tests of hypotheses all possible a 0b, 193–194 expected mean squares, 173–174 general linear hypothesis test H0:Cb¼0, 198–203 estimation under reduced model, 324–326 full and reduced model, 324–326 H0:Cb¼t, 203–204 likelihood ratio tests, 217–221 distribution of likelihood ratio, 218–219 likelihood ratio, 218 forH0:b¼0, 219–220 forH0:Cb¼0, 220–221 linear combination a0b, 204–205 onebj, 204–205 F-test, 204–205 t-test, 205 overall regression test, 185–189in terms of R2, 196–198 several ai0b’s, 205 severalbj’s Bonferonni method, 206–207 experimentwise error rate, 206 overalla-level, 206 Scheffe ´method, 207–209 subset of the b’s, 189–196 expected mean squares, 193, 196 full and reduced model, 190 noncentrality parameter, 192–193 quadratic forms, 190–193, 195 in terms of R2, 196 weighted least squares, 168 Xmatrix, 138–139 Regression, nonlinear.",
    "190–193, 195 in terms of R2, 196 weighted least squares, 168 Xmatrix, 138–139 Regression, nonlinear. SeeNonlinear regression Regression, Poisson. SeePoisson regression Regression, random x’s, 243–273 multivariate normal model, 244 conﬁdence intervals, 258–262 estimation of b0,b1, ands2, 245–249 properties of estimators, 249 standardized coefﬁcients (beta weights), 251 in terms of correlations, 249–154 R2, 254–257.",
    "ors, 249 standardized coefﬁcients (beta weights), 251 in terms of correlations, 249–154 R2, 254–257. See also R2, random x’s effect of each variable on R2, 262–265 tests of hypotheses, 258–262 comparison with tests for ﬁxed x’s, 258 correlations, tests for, 260–261 Fisher’s z-transformation, 261 likelihood ratio tests, 258–260 nonnormal data, 265–266 estimation of bˆ0andbˆ1, 266 sample partial correlations, 266–273 maximum likelihood estimators, 268 other estimators, 269–271 Regression, simple linear (one x), 1, 127–136 assumptions, 127 coefﬁcient of determination r2, 133–134 conﬁdence interval for b0, 134 conﬁdence interval for b1, 132–133 correlation r, 133–134 in terms of angle between vectors, 135 estimation of b0andb1, 128–129 estimation of s2, 131–132INDEX 669 --- Page 677 --- Regression, simple linear ( Continued ) model, 127 properties of estimators, 131 test of hypothesis for b0, 119 test of hypothesis for b1, 132–133 test of hypothesis for r, 134 Regression sum of squares.",
    "b0, 119 test of hypothesis for b1, 132–133 test of hypothesis for r, 134 Regression sum of squares. SeeSSR Regression to the mean, 498Residual(s), 131, 227–230 deleted residuals, 234 externally studentized residual, 234 hat matrix, 228, 230–232 in linear mixed models, 501–502 plots of, 230properties of, 237–230 residual sum of squares (SSE), 131, 150–151.",
    "xed models, 501–502 plots of, 230properties of, 237–230 residual sum of squares (SSE), 131, 150–151. SeeSSE studentized residual, 233 Response variable, 1, 137, 150 Robust estimation methods, 232 Sample mean deﬁnition, 105–106 independent of sample variance, 119–120 Sample space (data space), 152–153Sample variance ( s 2), 107–108 best quadratic unbiased estimator, 151 distribution, 118 expected value, 108, 127 independent of sample mean, 120 Satterthwaite, 494Scalar, 6 Scientiﬁc method, 1 Selection of variables, 2, 172 Serial correlation, 479 Shrinkage estimator, 287, 500 Signiﬁcance level ( a), 132 Simple linear regression.",
    "correlation, 479 Shrinkage estimator, 287, 500 Signiﬁcance level ( a), 132 Simple linear regression. SeeRegression, simple linear Singular matrix, 22 Small sample inference for mixed linear models, 491–491, 494–497 Span, 153Spectral decomposition, 51, 495–496 Square root matrix, 53 SSE (error sum of squares) balanced ANOVA one-way model, 343–344two-way model, 385, 390–391 independence of SSR and SSE, 187 multiple regression, 150–156, 179 non-full-rank model, 313–314simple linear regression, 131–132 unbalanced ANOVA one-way model, 417two-way model constrained, 428 unconstrained, 432 SSH (for general linear hypothesis test) in ANOVA, 326–329, 348–351, 401–403 in regression, 199, 203 SSR (regression sum of squares), 133–134, 161, 164, 186–189 Standardized distance, 77 Subspace, 153, 317Sufﬁcient statistics, 159–160 Sum(s) of squares Analysis of covariance, 449–463, 468–473 ANOVA, balanced one-way, 345–346, 348–351 contrasts, 358–363, 367–331 two-way, 388–395, 395–403 ANOVA, unbalanced one-way, 417 contrasts, 417–421 two-way, 426, 431–432 full-and-reduced-model test in ANOVA, 324–326 SSE.",
    "way, 417 contrasts, 417–421 two-way, 426, 431–432 full-and-reduced-model test in ANOVA, 324–326 SSE. SeeSSE SSH (for general linear hypothesis test). SeeSSH SSR (for overall regression test). SeeSSR as quadratic form, 105–107 test of a subset of b’s, 190–192 Symmetric matrix, 7 Systems of equations, 28–32 consistent and inconsistent, 29 and generalized inverse, 37–39 t-Distribution, 116–117, 123 central t, 117 noncentral t, 116–117, 132 noncentrality parameter, 116–117, 132 p-value.",
    "16–117, 123 central t, 117 noncentral t, 116–117, 132 noncentrality parameter, 116–117, 132 p-value. See p -Value t-Tests, 123, 131–132, 134, 205 p-value. See p -Value Tests of hypotheses. See also Analysis of variance, testing hypotheses;One-way model (balanced), testing the hypothesis H 0:m1¼m2¼...¼ mk; Two-way model (balanced), tests of hypotheses forb1in simple linear regression, 131–132670 INDEX --- Page 678 --- in Bayesian regression, 286 F-tests.",
    "1in simple linear regression, 131–132670 INDEX --- Page 678 --- in Bayesian regression, 286 F-tests. See F -Tests general linear hypothesis test, 198–204 for individual b’s or linear combinations.",
    "sts. See F -Tests general linear hypothesis test, 198–204 for individual b’s or linear combinations. SeeRegression, multiple linear with ﬁxed x’s, tests of hypotheses likelihood ratio tests, 217–221 in linear mixed models, 491, 495 overall regression test, 185–189, 196for rin bivariate normal distribution, 134 regression tests in terms of R2, 196–198 signiﬁcance level ( a), 132 subset of the b’s, 189–196 t-tests.",
    "ession tests in terms of R2, 196–198 signiﬁcance level ( a), 132 subset of the b’s, 189–196 t-tests. See t -Tests Trace of a matrix, 44–46Transpose, 7 Treatments, 4, 295, 339, 377 Triangular matrix, 8Two-way model (balanced), 3, 299–301, 377–408 estimable functions, 378–382 estimates of, 382–384 interaction terms, 380 main effect terms, 380–381 estimation of s2, 384–385 expected mean squares, 403–408 quadratic form approach, 405 sums of squares approach, 403–405 interaction, 301, 377 model, 377–378 assumptions, 378 no-interaction model, 329–335 estimable functions, 330–331 testing a hypothesis, 331–333 normal equations, 382–384orthogonality of columns of X, 333–335 reparameterization, 299–300side conditions, 300–301, 381 SSE, 384, 390 tests of hypotheses interaction full-and-reduced-model test, 388–391 generalized inverse approach, 391–395 hypothesis, 385–388 main effects full-and-reduced-model approach, 395–401 general linear hypothesis approach, 401–403 hypothesis, 396Unbalanced data in ANOVA cell means model, 414 one-way model, 415–421 contrasts, 417–421 conditions for independence, 418orthogonal contrasts, 418 weighted orthogonal contrasts, 419 estimation, 415–416 SSE, 416testing H 0:m1¼m2¼ ...¼mk,4 1 6 overparameterized model, 414 serial correlation, 479 two-way model, 421–432 cell means model, 421, 422 constrained model, 428–432 estimation, 430model, 429 SSE, 431 testing hypotheses, 431–432 type I, II and III sums of squares, 414 unconstrained model, 421–428 contrasts, 424–425 estimator of s2, 423 Hadamard product, 425 SSE, 423 testing hypotheses, 425–428 two-way model with empty cells, 432–439 estimability of empty cell means, 435 estimation for the partially constrained model, 434 isolated cells, 432 missing at random, 432 testing the interaction, 433–434 SSE, 433 weighted squares of means, 414 Underﬁtting, 170–172 Validation of model, 227–238.",
    ", 433–434 SSE, 433 weighted squares of means, 414 Underﬁtting, 170–172 Validation of model, 227–238. See also Hat matrix; Inﬂuential observations; Outliers; Residual(s) Variable(s) dependent, 1, 137independent, 1, 137 predictor, 1, 137response, 1, 137 selection of variables, 2, 172 Variance of estimators of l0b, 311 generalized, 77 of least squares estimators, 130–131 population, 70–71 of quadratic form, 107 sample, 95.",
    "alized, 77 of least squares estimators, 130–131 population, 70–71 of quadratic form, 107 sample, 95. See also Sample varianceINDEX 671 --- Page 679 --- Variance components, 480 estimating equations, 488 estimation, 486–489 Vector(s) angle between two vectors, 41–42, 136, 163, 238 column vector, 6 jvector, 8–9 length of, 12 linear independence and dependence, 19 normalized vector, 42 notation, 6orthogonal vectors, 37 orthonormal vectors, set of, 38 product of, 10–11 random vector.",
    "2 notation, 6orthogonal vectors, 37 orthonormal vectors, set of, 38 product of, 10–11 random vector. SeeRandom Vectors row vector, 6 zero vector ( 0), 8 Weighted least squares, 168 Zero matrix ( O), 8 Zero vector ( 0), 8672 INDEX",
    "--- Page 2 --- MACHINE LEARNING“This book provides a great way to start off with deep learning, with plenty of examples and well-explained concepts. A perfect book for readers of all levels who are interested in the domain.” —Vishwesh Ravi Shrimali ADAS EngineerFundamentals of Deep Learning US $69.99 CAN $87 .99 ISBN: 978-1-492-08128-7Twitter: @oreillymedia linkedin.com/company/oreilly-media youtube.com/oreillymedia We’re in the midst of an AI research explosion.",
    "n.com/company/oreilly-media youtube.com/oreillymedia We’re in the midst of an AI research explosion. Deep learning has unlocked superhuman perception to power our push toward creating self-driving vehicles, defeating human experts at a variety of difficult games including Go, and even generating essays with shockingly coherent prose. But deciphering these breakthroughs often takes a PhD in machine learning and mathematics.",
    "nt prose. But deciphering these breakthroughs often takes a PhD in machine learning and mathematics. The updated second edition of this book describes the intuition behind these innovations without jargon or complexity. Python-proficient programmers, software engineering professionals, and computer science majors will be able to reimplement these breakthroughs on their own and reason about them with a level of sophistication that rivals some of the best developers in the field.",
    "ason about them with a level of sophistication that rivals some of the best developers in the field. • Learn the mathematics behind machine learning jargon • Examine the foundations of machine learning and neural networks • Manage problems that arise as you begin to make networks deeper • Build neural networks that analyze complex images • Perform effective dimensionality reduction using autoencoders • Dive deep into sequence analysis to examine language • Explore methods in interpreting complex machine learning models • Gain theoretical and practical knowledge on generative modeling • Understand the fundamentals of reinforcement learningNithin Buduma is a machine learning scientist at Cresta, a leader in the contact center intelligence space.",
    "Buduma is a machine learning scientist at Cresta, a leader in the contact center intelligence space. Nikhil Buduma is cofounder and chief scientist of Ambience Healthcare, a San Francisco-based company that makes autonomous technologies for healthcare delivery. Joe Papa , founder of TeachMe.AI, has over 25 years of experience in research and development. He’s led AI research teams with PyTorch at Booz Allen and Perspecta Labs.",
    "research and development. He’s led AI research teams with PyTorch at Booz Allen and Perspecta Labs. Buduma, Buduma & Papa --- Page 3 --- Nithin Buduma, Nikhil Buduma, and Joe Papa with contributions by Nicholas LocascioFundamentals of Deep Learning Designing Next-Generation Machine Intelligence AlgorithmsSECOND EDITION Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing --- Page 4 --- 978-1-492-08218-7 LSIFundamentals of Deep Learning by Nithin Buduma, Nikhil Buduma, and Joe Papa Copyright © 2022 Nithin Buduma and Mobile Insights Technology Group, LLC.",
    "ikhil Buduma, and Joe Papa Copyright © 2022 Nithin Buduma and Mobile Insights Technology Group, LLC. All rights reserved. Printed in the United States of America. Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles ( http://oreilly.com ).",
    "or sales promotional use. Online editions are also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com .",
    "ation, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com . Acquisitions Editor: Rebecca Novack Development Editor: Melissa Potter Production Editor: Katherine Tozer Copyeditor: Sonia Saruba Proofreader: Stephanie EnglishIndexer: Judith McConville Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea June 2017: First Edition May 2022: Second Edition Revision History for the Second Edition 2022-05-16: First Release See http://oreilly.com/catalog/errata.csp?isbn=9781492082187 for release details.",
    "-16: First Release See http://oreilly.com/catalog/errata.csp?isbn=9781492082187 for release details. The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Fundamentals of Deep Learning , the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.",
    "s of Deep Learning , the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work.",
    "ng without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights. --- Page 5 --- Table of Contents Preface. . . . . . . . . . .",
    "lies with such licenses and/or rights. --- Page 5 --- Table of Contents Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix 1.Fundamentals of Linear Algebra for Deep Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "als of Linear Algebra for Deep Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Data Structures and Operations 1 Matrix Operations 3 Vector Operations 6 Matrix-Vector Multiplication 7 The Fundamental Spaces 7 The Column Space 7 The Null Space 10 Eigenvectors and Eigenvalues 13 Summary 15 2.Fundamentals of Probability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "ity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Events and Probability 17 Conditional Probability 20 Random Variables 22 Expectation 24 Variance 25 Bayes’ Theorem 27 Entropy, Cross Entropy, and KL Divergence 29 Continuous Probability Distributions 32 Summary 36 3.The Neural Network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 Building Intelligent Machines 39 The Limits of Traditional Computer Programs 40 The Mechanics of Machine Learning 41 iii --- Page 6 --- The Neuron 45 Expressing Linear Perceptrons as Neurons 47 Feed-Forward Neural Networks 48 Linear Neurons and Their Limitations 51 Sigmoid, Tanh, and ReLU Neurons 51 Softmax Output Layers 54 Summary 54 4.Training Feed-Forward Neural Networks. . . . . . . . . . .",
    "51 Softmax Output Layers 54 Summary 54 4.Training Feed-Forward Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "Forward Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 The Fast-Food Problem 55 Gradient Descent 57 The Delta Rule and Learning Rates 58 Gradient Descent with Sigmoidal Neurons 60 The Backpropagation Algorithm 61 Stochastic and Minibatch Gradient Descent 63 Test Sets, Validation Sets, and Overfitting 65 Preventing Overfitting in Deep Neural Networks 71 Summary 76 5.Implementing Neural Networks in PyTorch. . . . . . . . . . . . . . . . . . . . . . .",
    "71 Summary 76 5.Implementing Neural Networks in PyTorch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 Introduction to PyTorch 77 Installing PyTorch 77 PyTorch Tensors 78 Tensor Init 78 Tensor Attributes 79 Tensor Operations 80 Gradients in PyTorch 83 The PyTorch nn Module 84 PyTorch Datasets and Dataloaders 87 Building the MNIST Classifier in PyTorch 89 Summary 93 6.Beyond Gradient Descent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "yond Gradient Descent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 The Challenges with Gradient Descent 95 Local Minima in the Error Surfaces of Deep Networks 96 Model Identifiability 97 How Pesky Are Spurious Local Minima in Deep Networks?",
    "s of Deep Networks 96 Model Identifiability 97 How Pesky Are Spurious Local Minima in Deep Networks? 98 Flat Regions in the Error Surface 101 When the Gradient Points in the Wrong Direction 104 Momentum-Based Optimization 106 A Brief View of Second-Order Methods 109 Learning Rate Adaptation 111 iv | Table of Contents --- Page 7 --- AdaGrad—Accumulating Historical Gradients 111 RMSProp—Exponentially Weighted Moving Average of Gradients 112 Adam—Combining Momentum and RMSProp 113 The Philosophy Behind Optimizer Selection 115 Summary 116 7.Convolutional Neural Networks.",
    "SProp 113 The Philosophy Behind Optimizer Selection 115 Summary 116 7.Convolutional Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "al Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 Neurons in Human Vision 117 The Shortcomings of Feature Selection 118 Vanilla Deep Neural Networks Don’t Scale 121 Filters and Feature Maps 122 Full Description of the Convolutional Layer 127 Max Pooling 131 Full Architectural Description of Convolution Networks 132 Closing the Loop on MNIST with Convolutional Networks 134 Image Preprocessing Pipelines Enable More Robust Models 136 Accelerating Training with Batch Normalization 137 Group Normalization for Memory Constrained Learning Tasks 139 Building a Convolutional Network for CIFAR-10 141 Visualizing Learning in Convolutional Networks 143 Residual Learning and Skip Connections for Very Deep Networks 147 Building a Residual Network with Superhuman Vision 149 Leveraging Convolutional Filters to Replicate Artistic Styles 152 Learning Convolutional Filters for Other Problem Domains 154 Summary 155 8.Embedding and Representation Learning.",
    "olutional Filters for Other Problem Domains 154 Summary 155 8.Embedding and Representation Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "and Representation Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 Learning Lower-Dimensional Representations 157 Principal Component Analysis 158 Motivating the Autoencoder Architecture 160 Implementing an Autoencoder in PyTorch 161 Denoising to Force Robust Representations 171 Sparsity in Autoencoders 174 When Context Is More Informative than the Input Vector 177 The Word2Vec Framework 179 Implementing the Skip-Gram Architecture 182 Summary 188 9.Models for Sequence Analysis.",
    "ramework 179 Implementing the Skip-Gram Architecture 182 Summary 188 9.Models for Sequence Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "nalysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 Analyzing Variable-Length Inputs 189 Tackling seq2seq with Neural N-Grams 190 Implementing a Part-of-Speech Tagger 192 Table of Contents | v --- Page 8 --- Dependency Parsing and SyntaxNet 197 Beam Search and Global Normalization 203 A Case for Stateful Deep Learning Models 206 Recurrent Neural Networks 207 The Challenges with Vanishing Gradients 210 Long Short-Term Memory Units 213 PyTorch Primitives for RNN Models 218 Implementing a Sentiment Analysis Model 219 Solving seq2seq Tasks with Recurrent Neural Networks 224 Augmenting Recurrent Networks with Attention 227 Dissecting a Neural Translation Network 230 Self-Attention and Transformers 239 Summary 242 10.",
    "227 Dissecting a Neural Translation Network 230 Self-Attention and Transformers 239 Summary 242 10. Generative Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 Generative Adversarial Networks 244 Variational Autoencoders 249 Implementing a V AE 259 Score-Based Generative Models 264 Denoising Autoencoders and Score Matching 269 Summary 274 11. Methods in Interpretability. . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "9 Summary 274 11. Methods in Interpretability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 Overview 275 Decision Trees and Tree-Based Algorithms 276 Linear Regression 280 Methods for Evaluating Feature Importance 281 Permutation Feature Importance 281 Partial Dependence Plots 282 Extractive Rationalization 283 LIME 288 SHAP 292 Summary 297 12. Memory Augmented Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "Memory Augmented Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "ented Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 Neural Turing Machines 299 Attention-Based Memory Access 301 NTM Memory Addressing Mechanisms 303 Differentiable Neural Computers 307 Interference-Free Writing in DNCs 309 DNC Memory Reuse 310 Temporal Linking of DNC Writes 311 vi | Table of Contents --- Page 9 --- Understanding the DNC Read Head 312 The DNC Controller Network 313 Visualizing the DNC in Action 314 Implementing the DNC in PyTorch 317 Teaching a DNC to Read and Comprehend 321 Summary 323 13.",
    "on 314 Implementing the DNC in PyTorch 317 Teaching a DNC to Read and Comprehend 321 Summary 323 13. Deep Reinforcement Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325 Deep Reinforcement Learning Masters Atari Games 325 What Is Reinforcement Learning?",
    ". . . . . . 325 Deep Reinforcement Learning Masters Atari Games 325 What Is Reinforcement Learning? 326 Markov Decision Processes 328 Policy 329 Future Return 330 Discounted Future Return 331 Explore Versus Exploit 331 ϵ-Greedy 333 Annealed ϵ-Greedy 333 Policy Versus Value Learning 334 Pole-Cart with Policy Gradients 335 OpenAI Gym 335 Creating an Agent 335 Building the Model and Optimizer 337 Sampling Actions 337 Keeping Track of History 337 Policy Gradient Main Function 338 PGAgent Performance on Pole-Cart 340 Trust-Region Policy Optimization 341 Proximal Policy Optimization 345 Q-Learning and Deep Q-Networks 347 The Bellman Equation 347 Issues with Value Iteration 348 Approximating the Q-Function 348 Deep Q-Network 348 Training DQN 349 Learning Stability 349 Target Q-Network 350 Experience Replay 350 From Q-Function to Policy 350 DQN and the Markov Assumption 351 DQN’s Solution to the Markov Assumption 351 Playing Breakout with DQN 351 Building Our Architecture 354 Table of Contents | vii --- Page 10 --- Stacking Frames 354 Setting Up Training Operations 354 Updating Our Target Q-Network 354 Implementing Experience Replay 355 DQN Main Loop 356 DQNAgent Results on Breakout 358 Improving and Moving Beyond DQN 358 Deep Recurrent Q-Networks 359 Asynchronous Advantage Actor-Critic Agent 359 UNsupervised REinforcement and Auxiliary Learning 360 Summary 361 Index.",
    "tage Actor-Critic Agent 359 UNsupervised REinforcement and Auxiliary Learning 360 Summary 361 Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363 viii | Table of Contents --- Page 11 --- Preface With the reinvigoration of neural networks in the 2000s, deep learning has become an extremely active area of research that is paving the way for modern machine learning.",
    "has become an extremely active area of research that is paving the way for modern machine learning. This book uses exposition and examples to help you understand major concepts in this complicated field. Large companies such as Google, Microsoft, and Facebook have taken notice and are actively growing in-house deep learning teams. For the rest of us, deep learning is still a pretty complex and difficult subject to grasp.",
    "g teams. For the rest of us, deep learning is still a pretty complex and difficult subject to grasp. Research papers are filled to the brim with jargon, and scattered online tutorials do little to help build a strong intuition for why and how deep learning practitioners approach problems. Our goal is to bridge this gap. In this second edition, we provide more rigorous background sections in mathemat‐ ics with the aim of better equipping you for the material in the rest of the book.",
    "ions in mathemat‐ ics with the aim of better equipping you for the material in the rest of the book. In addition, we have updated chapters in sequence analysis, computer vision, and reinforcement learning with deep dives into the latest advancements in the fields. And finally, we have added new chapters in the fields of generative modeling and interpretability to provide you with a broader view of the field of deep learning.",
    "tive modeling and interpretability to provide you with a broader view of the field of deep learning. We hope that these updates inspire you to practice deep learning on their own and apply their learnings to solve meaningful problems in the real world. Prerequisites and Objectives This book is aimed at an audience with a basic operating understanding of calculus and Python programming.",
    "ook is aimed at an audience with a basic operating understanding of calculus and Python programming. In this latest edition, we provide extensive mathematical background chapters, specifically in linear algebra and probability, to prepare you for the material that lies ahead.",
    "rs, specifically in linear algebra and probability, to prepare you for the material that lies ahead. By the end of the book, we hope you will be left with an intuition for how to approach problems using deep learning, the historical context for modern deep learning approaches, and a familiarity with implementing deep learning algorithms using the PyTorch open source library. ix --- Page 12 --- How Is This Book Organized?",
    "ing algorithms using the PyTorch open source library. ix --- Page 12 --- How Is This Book Organized? The first chapters of this book are dedicated to developing mathematical maturity via deep dives into linear algebra and probability, which are deeply embedded in the field of deep learning. The next several chapters discuss the structure of feed-forward neu‐ ral networks, how to implement them in code, and how to train and evaluate them on real-world datasets.",
    "networks, how to implement them in code, and how to train and evaluate them on real-world datasets. The rest of the book is dedicated to specific applications of deep learning and understanding the intuition behind the specialized learning techniques and neural network architectures developed for those applications. Although we cover advanced research in these latter sections, we hope to provide a breakdown of these techniques that is derived from first principles and digestible.",
    "ope to provide a breakdown of these techniques that is derived from first principles and digestible. Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.",
    "variable or function names, databases, data types, environment variables, statements, and keywords. This element signifies a general note. This element indicates a warning or caution. Using Code Examples Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book . If you have a technical question or a problem using the code examples, please email bookquestions@oreilly.com .",
    "a technical question or a problem using the code examples, please email bookquestions@oreilly.com . x | Preface --- Page 13 --- This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. Y ou do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission.",
    "mple, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission. We appreciate, but do not require, attribution.",
    "our product’s documentation does require permission. We appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “ Fundamentals of Deep Learning by Nithin Buduma, Nikhil Buduma, and Joe Papa (O’Reilly). Copyright 2022 Nithin Buduma and Mobile Insights Technology Group, LLC, 978-1-492-08218-7.",
    "’Reilly). Copyright 2022 Nithin Buduma and Mobile Insights Technology Group, LLC, 978-1-492-08218-7. ” If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com . O’Reilly Online Learning For more than 40 years, O’Reilly Media has provided technol‐ ogy and business training, knowledge, and insight to help companies succeed.",
    "a has provided technol‐ ogy and business training, knowledge, and insight to help companies succeed. Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers.",
    "oding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit https://oreilly.com . How to Contact Us Please address comments and questions concerning this book to the publisher: O’Reilly Media, Inc.",
    "Us Please address comments and questions concerning this book to the publisher: O’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax) Preface | xi --- Page 14 --- We have a web page for this book, where we list errata, examples, and any addi‐ tional information. Y ou can access this page at https://oreil.ly/fundamentals-of-deep- learning-2e .",
    "ional information. Y ou can access this page at https://oreil.ly/fundamentals-of-deep- learning-2e . Email bookquestions@oreilly.com to comment or ask technical questions about this book. For news and information about our books and courses, visit https://oreilly.com . Find us on LinkedIn: https://www.linkedin.com/company/oreilly-media . Follow us on Twitter: https://twitter.com/oreillymedia . Watch us on Y ouTube: https://www.youtube.com/oreillymedia .",
    "ter: https://twitter.com/oreillymedia . Watch us on Y ouTube: https://www.youtube.com/oreillymedia . Acknowledgements We’ d like to thank several people who have been instrumental in the completion of this text. We’ d like to start by acknowledging Mostafa Samir and Surya Bhupatiraju, who contributed heavily to the content of Chapters 7 and 8.",
    "ing Mostafa Samir and Surya Bhupatiraju, who contributed heavily to the content of Chapters 7 and 8. We also appreciate the contributions of Mohamed (Hassan) Kane and Anish Athalye, who worked on early versions of the code examples in this book’s GitHub repository. Nithin and Nikhil This book would not have been possible without the never-ending support and expertise of our editor, Shannon Cutt.",
    "d not have been possible without the never-ending support and expertise of our editor, Shannon Cutt. We’ d also like to appreciate the commentary provided by our reviewers, Isaac Hodes, David Andrzejewski, Aaron Schumacher, Vishwesh Ravi Shrimali, Manjeet Dahiya, Ankur Patel, and Suneeta Mall, who pro‐ vided thoughtful, in-depth, and technical commentary on the original drafts of the text.",
    "l, who pro‐ vided thoughtful, in-depth, and technical commentary on the original drafts of the text. Finally, we are thankful for all of the insight provided by our friends and family members, including Jeff Dean, Venkat Buduma, William, and Jack, as we finalized the manuscript of the text. Joe Updating the code for this book with PyTorch has been an enjoyable and exciting experience. No endeavor like this can be achieved by one person alone.",
    "een an enjoyable and exciting experience. No endeavor like this can be achieved by one person alone. First, I would like to thank the PyTorch community and its 2,100+ contributors for continuing to grow and improve PyTorch and its deep learning capabilities. It is because of you that we can demonstrate the concepts described in this book. I am forever grateful to Rebecca Novack for bringing me into this project and for her confidence in me as an author.",
    "teful to Rebecca Novack for bringing me into this project and for her confidence in me as an author. Many thanks to Melissa Potter and the O’Reilly production staff in making this updated version come to life. xii | Preface --- Page 15 --- For his encouragement and support, I’ d like to thank Matt Kirk. He’s been my rock through it all. Thank you for our countless chats full of ideas and resources.",
    "rk. He’s been my rock through it all. Thank you for our countless chats full of ideas and resources. Special thanks to my kids, Savannah, Caroline, George, and Forrest, for being patient and understanding when Daddy had to work. And, most of all, thank you to my wife, Emily, who has always supported my dreams throughout life. While I diligently wrote code, she cared for our newborn through sleepless nights while ensuring the “big” kids had their needs met too.",
    "ared for our newborn through sleepless nights while ensuring the “big” kids had their needs met too. Without her, my contributions to this project would not be possible. Preface | xiii --- Page 17 --- CHAPTER 1 Fundamentals of Linear Algebra for Deep Learning In this chapter, we cover important prerequisite knowledge that will motivate our discussion of deep learning techniques in the main text and the optional sidebars at the end of select chapters.",
    "f deep learning techniques in the main text and the optional sidebars at the end of select chapters. Deep learning has recently experienced a renaissance, both in academic research and in the industry. It has pushed the limits of machine learning by leaps and bounds, revolutionizing fields such as computer vision and natural language processing.",
    "by leaps and bounds, revolutionizing fields such as computer vision and natural language processing. However, it is important to remember that deep learning is, at its core, a culmination of achievements in fields such as calculus, linear algebra, and probability. Although there are deeper connections to other fields of mathematics, we focus on the three listed here to help us broaden our perspective before diving into deep learning.",
    "focus on the three listed here to help us broaden our perspective before diving into deep learning. These fields are key to unlocking both the big picture of deep learning and the intricate subtleties that make it as exciting as it is. In this first chapter on background, we cover the fundamentals of linear algebra.",
    "exciting as it is. In this first chapter on background, we cover the fundamentals of linear algebra. Data Structures and Operations The most important data structure in linear algebra (whenever we reference linear algebra in this text, we refer to its applied variety) is arguably the matrix , a 2D array of numbers where each entry can be indexed via its row and column.",
    "arguably the matrix , a 2D array of numbers where each entry can be indexed via its row and column. Think of an Excel spreadsheet, where you have offers from Company X and Company Y as two rows, and the columns represent some characteristic of each offer, such as starting salary, bonus, or position, as shown in Table 1-1 . 1 --- Page 18 --- Table 1-1.",
    "r, such as starting salary, bonus, or position, as shown in Table 1-1 . 1 --- Page 18 --- Table 1-1. Excel spreadsheet Company X Company Y Salary $50,000 $40,000 Bonus $5,000 $7,500 Position Engineer Data Scientist The table format is especially suited to keep track of such data, where you can index by row and column to find, for example, Company X’s starting position. Matrices, similarly, are a multipurpose tool to hold all kinds of data, where the data we work in this book is of numerical form.",
    "ltipurpose tool to hold all kinds of data, where the data we work in this book is of numerical form. In deep learning, matrices are often used to represent both datasets and weights in a neural network. A dataset, for example, has many individual data points with any number of associated features. A lizard dataset might contain information on length, weight, speed, age, and other important attributes.",
    "ard dataset might contain information on length, weight, speed, age, and other important attributes. We can represent this intuitively as a matrix or table, where each row represents an individual lizard, and each column represents a lizard feature, such as age.",
    "each row represents an individual lizard, and each column represents a lizard feature, such as age. However, as opposed to Table 1-1 , the matrix stores only the numbers and assumes that the user has kept track of which rows correspond to which data points, which columns correspond to which feature, and what the units are for each feature, as you can see in Figure 1-1 . Figure 1-1.",
    "o which feature, and what the units are for each feature, as you can see in Figure 1-1 . Figure 1-1. A comparison of tables and matrices On the right side, we have a matrix, where it’s assumed, for example, that the age of each lizard is in years, and Komodo Ken weighs a whopping 50 kilograms! But why even work with matrices when tables clearly give the user more information?",
    "ng 50 kilograms! But why even work with matrices when tables clearly give the user more information? Well, in linear algebra and even deep learning, operations such as multiplication and addition are done on the tabular data itself, but such operations can only be computed efficiently when the data is in solely numerical format.",
    "f, but such operations can only be computed efficiently when the data is in solely numerical format. Much of the work in linear algebra centers on the emergent properties of matrices, which are especially interesting when the matrix has certain base attributes, and operations on these data structures. Vectors , which can be seen as a subset type of matrices, are a 1D array of numbers.",
    "ata structures. Vectors , which can be seen as a subset type of matrices, are a 1D array of numbers. This data structure can be used to represent an individual data point or the weights in a linear regression, for example. We cover properties of matrices and vectors as well as operations on both.",
    "regression, for example. We cover properties of matrices and vectors as well as operations on both. 2 | Chapter 1: Fundamentals of Linear Algebra for Deep Learning --- Page 19 --- Matrix Operations Matrices can be added, subtracted, and multiplied—there is no division of matrices, but there exists a similar concept called inversion . When indexing a matrix, we use a tuple, where the first index represents the row number and the second index represents the column number.",
    ", where the first index represents the row number and the second index represents the column number. To add two matrices A and B, one loops through each index (i,j) of the two matrices, sums the two entries at the current index, and places that result in the same index (i,j) of a new matrix C, as can be seen in Figure 1-2 . Figure 1-2. Matrix addition This algorithm implies that we can’t add two matrices of different shapes, since indices that exist in one matrix wouldn’t exist in the other.",
    "wo matrices of different shapes, since indices that exist in one matrix wouldn’t exist in the other. It also implies that the final matrix C is of the same shape as A and B. In addition to adding matrices, we can multiply a matrix by a scalar. This involves simply taking the scalar and multiplying each of the entries of the matrix by it (the shape of the resultant matrix stays constant), as depicted in Figure 1-3 . Figure 1-3.",
    "ix by it (the shape of the resultant matrix stays constant), as depicted in Figure 1-3 . Figure 1-3. Scalar-matrix multiplication These two operations, addition of matrices and scalar-matrix multiplication, lead us directly to matrix subtraction, since computing A – B is the same as computing the matrix addition A + (– B), and computing –B is the product of a scalar –1 and the matrix B. Multiplying two matrices starts to get interesting.",
    "is the product of a scalar –1 and the matrix B. Multiplying two matrices starts to get interesting. For reasons beyond the scope of this text (motivations in a more theoretical flavor of linear algebra where matrices represent linear transformations), we define the matrix product A·B as: Equation 1-1.",
    "where matrices represent linear transformations), we define the matrix product A·B as: Equation 1-1. Matrix multiplication formula A·Bi,j= ∑k′= 1kAi,k′Bk′,j Data Structures and Operations | 3 --- Page 20 --- In simpler terms, this means that the value at the index (i,j) of A·B is the sum of the product of the entries in the ith row of A with those of the jth column of B. Figure 1-4 is an example of matrix multiplication. Figure 1-4.",
    "A with those of the jth column of B. Figure 1-4 is an example of matrix multiplication. Figure 1-4. Matrix multiplication It follows that the rows of A and the columns of B must have the same length, so two matrices can be multiplied only if the shapes align. We use the term dimension to formally represent what we have referred to so far as shape : i.e., A is of dimen‐ sion m by k, meaning it has m rows and k columns, and B is of dimension k by n.",
    "i.e., A is of dimen‐ sion m by k, meaning it has m rows and k columns, and B is of dimension k by n. If this weren’t the case, the formula for matrix multiplication would give us an indexing error. The dimension of the product is m by n, signifying an entry for every pair of rows in A and columns in B. This is the computational way of thinking about matrix multiplication, and it doesn’t lend itself well to theoretical interpretation.",
    "thinking about matrix multiplication, and it doesn’t lend itself well to theoretical interpretation. We’ll call Equation 1-1 the dot product interpretation of matrix multiplication, which will make more sense after reading “Vector Operations” on page 6 . Note that matrix multiplication is not commutative, i.e., A·B≠B·A. Of course, if we were to take a matrix A that is 2 by 3 and a matrix B that is 3 by 5, for example, by the rules of matrix multiplication, B·A doesn’t exist.",
    "nd a matrix B that is 3 by 5, for example, by the rules of matrix multiplication, B·A doesn’t exist. However, even if the product were defined due to both matrices being square, where square means that the matrix has an equal number of rows and columns, the two products will not be the same (this is an exercise for you to explore on your own). However, matrix multiplication is associative, i.e., A·B+C=A·B+A·C. Let’s delve into matrix multiplication a bit further.",
    "plication is associative, i.e., A·B+C=A·B+A·C. Let’s delve into matrix multiplication a bit further. After some algebraic manipulation, we can see that another way to formulate matrix multiplication is: A·B· ,j=A·B· ,j This states that the jth column of the product A·B is the matrix product of A and the jth column of B, a vector. We’ll call this column vector interpretation of matrix multiplication, as can be seen in Figure 1-5 .",
    "e’ll call this column vector interpretation of matrix multiplication, as can be seen in Figure 1-5 . 4 | Chapter 1: Fundamentals of Linear Algebra for Deep Learning --- Page 21 --- Figure 1-5. Matrix multiplication: another view In a later section, we cover matrix-vector multiplication and different ways to think about this computation, which leads to more exciting properties regarding matrices.",
    "nt ways to think about this computation, which leads to more exciting properties regarding matrices. One of the most important matrices in linear algebra is the identity matrix , which is a square matrix with 1s along the main diagonal and 0s in every other entry. This matrix is usually denoted as I. When computing the product of I with any other matrix A, the result is always A—thus its name, the identity matrix.",
    "the product of I with any other matrix A, the result is always A—thus its name, the identity matrix. Try multiplying a few matrices of your choosing with the appropriate-sized identity matrix to see why this is the case. As noted at the beginning of the section, there is no such division operation for matrices, but there is the concept of inversion.",
    "he section, there is no such division operation for matrices, but there is the concept of inversion. The inverse of matrix A is matrix B, such that AB = BA = I, the identity matrix (similar in idea to a number’s reciprocal— when dividing by a number on both sides of an equation, we can also think of this operation as multiplying both sides by its reciprocal). If such a B exists, we denote it as A−1.",
    "his operation as multiplying both sides by its reciprocal). If such a B exists, we denote it as A−1. From this definition, we know that A must be, at the very least, a square matrix since we are able to multiply A on either side by the same matrix A−1, as you can see in Figure 1-6 . Matrix inversion is deeply tied to other properties of matrices that we will discuss soon, which are the backbone of fundamental data science techniques.",
    "f matrices that we will discuss soon, which are the backbone of fundamental data science techniques. These techniques influenced their more complex neural variants, which researchers still use to this day. Figure 1-6. Matrix inversion Data Structures and Operations | 5 --- Page 22 --- When trying to solve an equation such as Ax=b for x, we would multiply both sides on the left by A−1 to get x=A−1b if A is invertible.",
    "uch as Ax=b for x, we would multiply both sides on the left by A−1 to get x=A−1b if A is invertible. There exists another necessary condition for A to be invertible, which we’ll discuss later. Vector Operations Vectors can be seen as a subset of matrices, so a lot of the operations follow from the properties of addition, subtraction, multiplication, and inversion. However, there is some vector-specific terminology we should cover.",
    ", multiplication, and inversion. However, there is some vector-specific terminology we should cover. When a vector is of dimension 1 × n, we call this vector a row vector , and when the vector is of dimension n × 1, we call it a column vector . When taking matrix product of a row vector and a column vector, we can see that the result is a single number—we call this operation the dot product. Figure 1-7 is an example of the dot product of two vectors. Figure 1-7.",
    "s operation the dot product. Figure 1-7 is an example of the dot product of two vectors. Figure 1-7. Dot product Now the reason for the name dot product interpretation of matrix multiplication might make more sense. Looking back at Equation 1-1 , we see that every entry in the matrix product A·Bi,j is just the dot product of the corresponding row Ai, · and the corresponding column B· ,j. When the dot product of two vectors is 0, we term the two vectors to be orthogonal .",
    "g column B· ,j. When the dot product of two vectors is 0, we term the two vectors to be orthogonal . Orthogonality is a generalization of perpendicularity to any dimension, even those far beyond the ones we can imagine. Y ou can check in the 2D case, for example, that any two vectors are perpendicular if and only if (also termed iff) they have a dot product of 0. When we instead take the matrix product of a column vector and a row vector, we see that the result is quite surprisingly a matrix!",
    "product of a column vector and a row vector, we see that the result is quite surprisingly a matrix! This is termed the outer product . Figure 1-8 is the outer product of the same two vectors from the dot product example, except their roles as row and column vectors have been reversed. Figure 1-8.",
    "he dot product example, except their roles as row and column vectors have been reversed. Figure 1-8. Outer product 6 | Chapter 1: Fundamentals of Linear Algebra for Deep Learning --- Page 23 --- Matrix-Vector Multiplication When multiplying a matrix A and a vector v, we can again do this via the dot product interpretation of matrix multiplication, as described previously.",
    "again do this via the dot product interpretation of matrix multiplication, as described previously. However, if we instead manipulate the expression slightly, we’ll see that another way to formulate this product is: Av= ∑jvjA· ,j Where each vj is a constant to be multiplied with its corresponding column of A. Fig‐ ure 1-9 is an example of this method in action. Figure 1-9.",
    "with its corresponding column of A. Fig‐ ure 1-9 is an example of this method in action. Figure 1-9. Matrix-vector multiplication This section introduced matrix and vector operations, which are fundamental to understanding the inner workings of a neural network. In the next section, we will use our knowledge of matrix and vector operations to concretely define some matrix properties, which serve as the basis for important data science and deep learning techniques.",
    "matrix properties, which serve as the basis for important data science and deep learning techniques. The Fundamental Spaces In this section, we will formally discuss some important matrix properties and provide some background knowledge on key algorithms in deep learning, such as representation learning. The Column Space Consider the set of all possible vectors v and their products Av. We term this the col‐ umn space of A, or C(A).",
    "set of all possible vectors v and their products Av. We term this the col‐ umn space of A, or C(A). The term column space is used because C(A) represents all possible linear combinations of the columns of A, where a linear combination of vectors is a sum of constant scalings of each vector. The constant scaling for each The Fundamental Spaces | 7 --- Page 24 --- column vector of A is determined by the choice of v, as we just saw in the previous section.",
    "24 --- column vector of A is determined by the choice of v, as we just saw in the previous section. The column space is an example of a vector space, which is the space defined by a list of vectors and all possible linear combinations of this collection. Properties for formally defining a vector space pop up directly from this intuition. For example, if a set of vectors is a vector space, then the vector that arises from multiplying any vector in the space by a scalar must also be in the space.",
    "e vector that arises from multiplying any vector in the space by a scalar must also be in the space. In addition, if we were to add any two vectors in the space, the result should still be in the space. In both of these operations, the vectors we start with are known to be in the vector space, and thus can be formulated as linear combinations of the original list.",
    "n to be in the vector space, and thus can be formulated as linear combinations of the original list. By performing scalar multiplication or addition on the vectors in question, we are just computing linear combinations of linear combinations, which are still linear combinations, as can be seen in Figure 1-10 . Figure 1-10.",
    "near combinations, which are still linear combinations, as can be seen in Figure 1-10 . Figure 1-10. The sum, or linear combination, of the two linear combinations 3a and 2b + 2c is still a linear combination of the original vectors a, b, and c 8 | Chapter 1: Fundamentals of Linear Algebra for Deep Learning --- Page 25 --- We term these key properties of vector spaces closed under scalar multiplication and closed under addition.",
    "these key properties of vector spaces closed under scalar multiplication and closed under addition. If a set of vectors doesn’t always satisfy either of these properties, then the set clearly doesn’t contain all possible linear combinations of the original list and is not a vector space. An example of a vector space you’re probably familiar with is ℝ3, or the entire space defined by the x-y-z coordinate axis.",
    "space you’re probably familiar with is ℝ3, or the entire space defined by the x-y-z coordinate axis. The reason for the notation ℝ3 is that each coordinate can take on any value in the reals, or ℝ, and there are three coordinates that uniquely define any such vector in this space. A collection of vectors that defines this space are the vectors (0,0,1),(0,1,0),(1,0,0), the unit vectors of each axis.",
    "tors that defines this space are the vectors (0,0,1),(0,1,0),(1,0,0), the unit vectors of each axis. Any vector ( a,b,c) in the space can be written as a*(1,0,0) + b*(0,1,0) + c*(0,0,1), a linear combination of the collection. In the other direction, any possible linear combination of the three vectors represents some vector ( a,b,c) that lies in ℝ3. Often, there exist matrices A for which some columns are linear combinations of other columns.",
    "n ℝ3. Often, there exist matrices A for which some columns are linear combinations of other columns. For example, imagine if in our lizard dataset from Figure 1-2 , we had an additional feature for each lizard’s weight, but instead in pounds. This is a clear redundancy in the data since this feature is completely determined by the feature for weight in kilograms.",
    "ancy in the data since this feature is completely determined by the feature for weight in kilograms. In other words, the new feature is a linear combination of the other features in the data—simply take the column for weight in kilograms, multiply it by 2.2, and sum it with all the other columns multiplied by zero to get the column for weight in pounds. Logically, if we were to remove these sorts of redundancies from A, then C(A) shouldn’t change.",
    "nds. Logically, if we were to remove these sorts of redundancies from A, then C(A) shouldn’t change. One method to do this is to first create a list of all the original column vectors of A, where order is assigned arbitrarily. When iterating through the list, check to see if the current vector is a linear combination of all the vectors that precede it. If so, remove this vector from the list and continue.",
    "ombination of all the vectors that precede it. If so, remove this vector from the list and continue. It’s clear that the removed vector provided no additional information beyond the ones we’ve already seen. The resulting list is called the basis of C(A), and the length of the basis is the dimen‐ sion of C(A). We say that the basis of any vector space spans the space, which means that all of the elements in the vector space can be formulated as a linear combination of basis vectors.",
    "all of the elements in the vector space can be formulated as a linear combination of basis vectors. In addition, the basis vectors are linearly independent , which means that none of the vectors can be written as a linear combination of the others, i.e., no redundancies. Going back to the example where we defined vector space, (0,0,1), (0,1,0),(1,0,0) would be a basis for the space ℝ3, since no vector in the list is a linear combination of the others, and this list spans the entire space.",
    "e no vector in the list is a linear combination of the others, and this list spans the entire space. And instead, the list (0,0,1),(0,1,0),(1,0,0),(2,5,1) spans the entire space, but is not linearly independent because (2,5,1) can be written as a linear combination of the first three vectors (we call such a list of vectors a spanning list , and of course the set of bases for a vector space is a subset of the set of spanning lists for the same space).",
    "se the set of bases for a vector space is a subset of the set of spanning lists for the same space). As we alluded to in the discussion of our lizard dataset, the basis of the column space, given each lizard feature is a column, is a concise representation of the information The Fundamental Spaces | 9 --- Page 26 --- represented in the feature matrix.",
    "ion of the information The Fundamental Spaces | 9 --- Page 26 --- represented in the feature matrix. In the real world, where we often have thousands of features (e.g., each pixel in an image), achieving a concise representation of our data is quite desirable. Though this is a good start, identifying the clear redundancies in our data often isn’t enough, as the randomness and complexity that exist in the real world tend to obscure these redundancies.",
    "h, as the randomness and complexity that exist in the real world tend to obscure these redundancies. Quantifying relationships between features can inform concise data representations, as we discuss at the end of this chapter and in Chapter 9 on representation learning. The Null Space Another key vector space is the null space of a matrix A, or N(A) . This space consists of the vectors v such that Av = 0. We know that v = 0 , the trivial solution, will always satisfy this property.",
    "rs v such that Av = 0. We know that v = 0 , the trivial solution, will always satisfy this property. If only the trivial solution is in the null space of a matrix, we call the space trivial. However, it is possible that there exist other solutions to this equation depending on the properties of A, or a nontrivial null space. For a vector v to satisfy Av = 0, v must be orthogonal to each of the rows of A, as shown in Figure 1-11 . Figure 1-11.",
    "atisfy Av = 0, v must be orthogonal to each of the rows of A, as shown in Figure 1-11 . Figure 1-11. The implication that the dot product between each row and the vector v must be equal to 0 Let’s assume A is of dimension 2 by 3, for example. In our case, A’s rows cannot span ℝ3 due to A having only two rows (remember from our recent discussion that all bases have the same length, and all spanning lists are at least as long as all bases, so A’s rows can be neither of these).",
    "gth, and all spanning lists are at least as long as all bases, so A’s rows can be neither of these). At best, A’s rows define a plane in the 3D coordinate system. The other two options are that the rows define a line or a point. The former occurs when A either has two nonzero rows, where one is a multiple of the other, or has one zero row and one nonzero row. The latter occurs when A has two zero rows, or in other words, is the zero matrix.",
    "one nonzero row. The latter occurs when A has two zero rows, or in other words, is the zero matrix. In the case where A’s row space defines a plane (or even a line for that matter), all we’ d need to do to find a vector in N(A) is: 10 | Chapter 1: Fundamentals of Linear Algebra for Deep Learning --- Page 27 --- 1.Pick any vector v that doesn’t lie in A’s row space. 1. 2.Find its projection v’ onto the row space, where the projection of v is defined 2. as the vector in the space closest to v.",
    "onto the row space, where the projection of v is defined 2. as the vector in the space closest to v. Geometrically, the projection looks as if we had dropped a line down from the tip of v perpendicular to the space, and connected a vector from the origin to that point on the space. 3.Compute v – v’ , which is orthogonal to the row space and thus, each row vector. 3. Figure 1-12 depicts this. Figure 1-12.",
    "is orthogonal to the row space and thus, each row vector. 3. Figure 1-12 depicts this. Figure 1-12. Finding a vector in N(A) Note that v – v’ is perpendicular to R(A) , the row space of A, since v’ was formed by dropping a perpendicular to the plane down from its tip.",
    "the row space of A, since v’ was formed by dropping a perpendicular to the plane down from its tip. The Fundamental Spaces | 11 --- Page 28 --- An important takeaway is that nontrivial solutions to Av = 0 exist when the rows of A do not span ℝ3; more generally, if A is of dimension m by n, nontrivial solutions to Av = 0 exist when the row vectors do not span ℝn.",
    "A is of dimension m by n, nontrivial solutions to Av = 0 exist when the row vectors do not span ℝn. The process is similar to that shown: pick a vector in ℝn not in the row space, find its projection onto the row space, and subtract to get a vector in null space. But we still must show that N(A) itself is a vector space. We can easily see that any linear combination of nontrivial solutions to Av = 0 is still a solution.",
    "We can easily see that any linear combination of nontrivial solutions to Av = 0 is still a solution. For example, given two nontrivial solutions v1 and v2 and their linear combination c1v1+c2v2, where c1 and c2 are constants, we see that: Ac1v1+c2v2 =Ac1v1+Ac2v2 =c1Av1+c2Av2 =c1* 0 +c2* 0 = 0 Where the first equality arises from the associativity of matrix multiplication and the second from the fact that c1 and c2 are constants.",
    "he associativity of matrix multiplication and the second from the fact that c1 and c2 are constants. Note that this logic can be used for any number of nontrivial solutions, not just two. Thus, the null space is defined by some collection of vectors that can be boiled down to a basis, and contains all possible linear combinations of these vectors. These characteristics make the null space a vector space.",
    "ible linear combinations of these vectors. These characteristics make the null space a vector space. This is all deeply connected to one of the key matrix operations presented, the matrix inverse . We can think of a matrix’s inverse as undoing the action of a matrix upon any other entity. For example, if we were to compute Av and multiply on the left by A−1, we should be left with our initial v. However, depending on the properties of A, there can exist ambiguities as to how to “undo” its action.",
    "wever, depending on the properties of A, there can exist ambiguities as to how to “undo” its action. For example, let’s say v was some nonzero vector, but for some reason, Av = 0 . If we were to multiply on the left by A−1, we’ d be left with v = 0 instead of our initial v. This unfortunately goes against the properties of an inverse, and we declare that such a matrix is noninvertible, or singular . But why does this happen in the first place? This goes back to our observation about ambiguities.",
    ". But why does this happen in the first place? This goes back to our observation about ambiguities. Because an inverse is supposed to undo the action of a matrix, if there are multiple initial vectors that map to the same vector via the matrix’s action, trying to undo this action is impossible. Going back to our example, we know that nonzero vectors are mapped to 0 by A when A has a nontrivial null space. Thus, any matrix with a nontrivial null space is also singular.",
    "when A has a nontrivial null space. Thus, any matrix with a nontrivial null space is also singular. 12 | Chapter 1: Fundamentals of Linear Algebra for Deep Learning --- Page 29 --- Next, we will cover eigenvectors and eigenvalues, which puts all of the information we’ve learned so far into practice. Eigenvectors and Eigenvalues Matrices can act on vectors in many different ways.",
    "far into practice. Eigenvectors and Eigenvalues Matrices can act on vectors in many different ways. For most combinations of matrices and vectors, plotting the vector and its transformation doesn’t provide us with any interesting patterns. However, for certain matrices and specific vectors for those matrices, the action of the matrix upon the vector gives us an informative and surprising result: the transformation is a scalar multiple of the original.",
    "es us an informative and surprising result: the transformation is a scalar multiple of the original. We call these vectors eigenvectors , and the scalar multiple its corresponding eigenvalue. In this section, we discuss these very special vectors, relate back to the material presented in the previously, and begin the discussion connecting the theory of linear algebra with the practice of data science.",
    "and begin the discussion connecting the theory of linear algebra with the practice of data science. More formally, an eigenvector for a matrix A is a nonzero vector v such that Av = cv, where c is some constant (including zero, potentially), as shown in Figure 1-13 . Figure 1-13.",
    "= cv, where c is some constant (including zero, potentially), as shown in Figure 1-13 . Figure 1-13. The vector (1,1) is an eigenvector of our matrix, with a corresponding eigenvalue of 3 Note that if we were to pick any random vector, such as (2,5), the transformation wouldn’t look as meaningful as it does in Figure 1-13 . Of course, if A is a rectangular matrix, it’s impossible for A to have any eigenvectors.",
    "gure 1-13 . Of course, if A is a rectangular matrix, it’s impossible for A to have any eigenvectors. The original vector and its transformation have different sizes, and thus transfor‐ mation couldn’t be a scalar multiple of the original. For this reason, we limit our discussion in this section to square matrices. The simplest example is the identity matrix.",
    "imit our discussion in this section to square matrices. The simplest example is the identity matrix. Every nonzero vector is an eigenvector of the identity matrix since Iv = v for all v, with each having an eigenvalue of Eigenvectors and Eigenvalues | 13 --- Page 30 --- 1. Oftentimes, however, the eigenvectors of a matrix won’t be so obvious. How do we find these vectors and their corresponding eigenvalues?",
    "s of a matrix won’t be so obvious. How do we find these vectors and their corresponding eigenvalues? We know the conditions of any potential eigenvector; that is, if v is an eigenvector, it must satisfy Av = cv for some scalar c: Av=cvAv−cv= 0A−cIv= 0 The implication here is that if Av = cv, then A – cI must have a nontrivial null space. In the other direction, if we find a c such that A – cI has a nontrivial null space, the nonzero vectors in the null space are eigenvectors of A.",
    "hat A – cI has a nontrivial null space, the nonzero vectors in the null space are eigenvectors of A. Of course, if A itself has a nontrivial null space, then all nonzero v in the null space satisfy the above implication when c is 0. More generally, however, we must find the c such that A – cI has a nontrivial null space. As established previously, checking for a nontrivial null space is equivalent to testing for a matrix being singular.",
    "eviously, checking for a nontrivial null space is equivalent to testing for a matrix being singular. For reasons beyond the scope of this text, one way to test if A−cI for some c is a singular matrix is to check whether its determinant is 0. We won’t go into too much depth here, but we can think about the determinant as a function, or polynomial, that encodes properties of the matrix and results in a value of 0 iff the matrix is singular.",
    "omial, that encodes properties of the matrix and results in a value of 0 iff the matrix is singular. However, it would be inefficient, and frankly impossible, for us to test every possible c for a zero determinant. We can instead think of c as a variable in an equation and solve for it via the characteristic polynomial , which is the determinant of the matrix A – cI set equal to 0. The roots of this polynomial give us the eigenvalues of A.",
    "nant of the matrix A – cI set equal to 0. The roots of this polynomial give us the eigenvalues of A. To find their corresponding eigenvectors, we can plug each solution for c into A – cI and then solve for the v that make(s) (A – cI)v = 0. Calculating the determinant for any matrix of reasonable size is quite prohibitive in terms of computational cost.",
    "e determinant for any matrix of reasonable size is quite prohibitive in terms of computational cost. Although we won’t delve further into this, algorithms today use a version of the QR algorithm (named after the QR matrix decomposition) to calculate the eigenvalues of a matrix. If you’ d like to learn more about these and similar such algorithms, we highly recommend lecture notes or books on numerical linear algebra.",
    "and similar such algorithms, we highly recommend lecture notes or books on numerical linear algebra. How does our study of eigenvalues and eigenvectors connect to that of data science?",
    "linear algebra. How does our study of eigenvalues and eigenvectors connect to that of data science? Principal component analysis, or PCA, is one of the most famous algorithms in data science, and it uses the eigenvectors and eigenvalues of a special matrix called the correlation matrix, which represents the quantifiable relationships between features alluded to earlier, to perform dimensionality reduction on the original data matrix.",
    "etween features alluded to earlier, to perform dimensionality reduction on the original data matrix. We will discuss correlation and related concepts in the next chapter on probability, and learn more about PCA in Chapter 8 . 14 | Chapter 1: Fundamentals of Linear Algebra for Deep Learning --- Page 31 --- Summary In this chapter, we investigated some of the basics of applied linear algebra.",
    "- Page 31 --- Summary In this chapter, we investigated some of the basics of applied linear algebra. We learned about the key data structures and operations that rule both applied linear algebra and deep learning, and different ways to view these fundamental operations. For example, we learned that the dot product view of matrix multiplication was important from a computational lens, while the column vector approach led us into our discussion on the fundamental spaces quite naturally.",
    "ile the column vector approach led us into our discussion on the fundamental spaces quite naturally. We also got a peek at some of the surprising hidden properties of matrices, such as eigenvalues and eigenvectors, and how these properties are widely utilized in data science even to this day. In the next chapter, we will learn about the field of probability, which is often used in tandem with linear algebra to build complex, neural models used in the world.",
    "which is often used in tandem with linear algebra to build complex, neural models used in the world. Summary | 15 --- Page 33 --- CHAPTER 2 Fundamentals of Probability Probability is a field of mathematics that quantifies our uncertainty regarding events. For example, when rolling dice or flipping a coin, barring any irregularities in the dice or coin themselves, we are uncertain about the result to come. However, we can quantify our belief in each of the potential outcomes via probabilities.",
    "lt to come. However, we can quantify our belief in each of the potential outcomes via probabilities. We say, for example, that on every coin toss the probability of the coin showing up heads is 1 2. And on every dice roll, we say the probability of a die facing up with a five is 1 6. These are the sorts of probabilities we talk about with ease in our daily lives, but how can we define and utilize them effectively?",
    "ties we talk about with ease in our daily lives, but how can we define and utilize them effectively? In this chapter we’ll discuss the fundamentals of probability and how they connect to key concepts in deep learning. Events and Probability When running a trial such as rolling a dice or tossing a coin, we intuitively assign some belief to the trial’s possible outcomes. In this section, we aim to formalize some of these concepts.",
    "elief to the trial’s possible outcomes. In this section, we aim to formalize some of these concepts. In particular, we will begin by working in this discrete space, where discrete signifies a finite or countably infinite number of possibilities. Both rolling a dice and tossing a coin are in the discrete space—when rolling a fair dice there are six possible outcomes and when tossing a fair coin there are two. We term the entire set of possibilities for an experiment the sample space.",
    "fair coin there are two. We term the entire set of possibilities for an experiment the sample space. For example, the numbers one through six would make up the sample space for rolling a fair dice. We can define events as subsets of the sample space. The event of rolling at least a three corresponds with the dice facing up any number in the subset of three, four, five, and six in the sample space defined previously.",
    "ng up any number in the subset of three, four, five, and six in the sample space defined previously. A set of probabilities that sum to one over all outcomes in the sample space is termed a probability distribution over that sample space, and these distributions will be the main focus of our discussion.",
    "stribution over that sample space, and these distributions will be the main focus of our discussion. 17 --- Page 34 --- In general, we won’t worry too much about where exactly these probabilities come from, as that requires a much more rigorous and thorough examination beyond the scope of this text. However, we will give some intuition about the different interpretations.",
    "nd the scope of this text. However, we will give some intuition about the different interpretations. At a high level, the frequentist view sees the probability of an outcome as arising from its frequency over a long-run experiment.",
    "st view sees the probability of an outcome as arising from its frequency over a long-run experiment. In the case of fair dice, this view claims we can say the probability of any side of the dice showing up on a given roll is 1 6, since performing a large number of rolls and counting up the occurrences of each side will give us an estimate that is roughly this fraction.",
    "and counting up the occurrences of each side will give us an estimate that is roughly this fraction. As the number of rolls in the experiment grows, we see that this estimate gets closer and closer to the limit 1 6, the outcome’s probability. On the other hand, the Bayesian view of probability is based more on quantifying our prior belief in hypotheses and how we update our beliefs in light of new data.",
    "re on quantifying our prior belief in hypotheses and how we update our beliefs in light of new data. For a fair dice, the Bayesian view would claim there is no prior information, both from the dice’s structure and the rolling process, that would suggest any side of the dice as being more likely to turn up than any other side. Thus, we would say each outcome has probability 1 6, our prior belief.",
    "turn up than any other side. Thus, we would say each outcome has probability 1 6, our prior belief. The set of probabilities, in this case all being 1 6, associated with each outcome is termed our prior. As we see new data, the Bayesian view gives us a methodology to update our prior accordingly, where we term this new belief our posterior.",
    "gives us a methodology to update our prior accordingly, where we term this new belief our posterior. This Bayesian view is sometimes directly applied to neural network training, where we first assume that each weight in the network has some prior associated with it. As we train the network, we update the prior associated with each weight accordingly to better fit the data we see. At the end of the training, we are left with a posterior distribution associated with each weight.",
    ". At the end of the training, we are left with a posterior distribution associated with each weight. We will assume throughout this chapter that the probabilities associated with any outcome have been determined via reasonable methods, and focus on how we can manipulate these probabilities for use in our analyses. We start with the four tenets of probability, specifically in the discrete space: 1.The sum of probabilities for all possible outcomes in a sample space must be1. equal to one.",
    "pace: 1.The sum of probabilities for all possible outcomes in a sample space must be1. equal to one. In other words, the probability distribution over the sample space must sum to one. This should make sense intuitively, since the set of all outcomes in the sample space must represent the entire set of possibilities. The probability distribution not summing to one would imply the existence of possibilities not accounted for, which is contradictory.",
    "summing to one would imply the existence of possibilities not accounted for, which is contradictory. Mathematically, we say that for any valid probability distribution, ∑oPo= 1, where o represents an outcome. 2.Let E1 be an event, and recall that we define an event as a subset of possi‐ 2. ble outcomes. We call E1c the complement of E1, or all possible outcomes in the sample space that are not in E1. The second tenet of probability is that PE1= 1 −PE1c.",
    "tcomes in the sample space that are not in E1. The second tenet of probability is that PE1= 1 −PE1c. This is just an application of the first tenet—if this were not true, it would clearly contradict the first tenet. In Figure 2-1 , we see an example 18 | Chapter 2: Fundamentals of Probability --- Page 35 --- of this, where S represents the entire space of outcomes, and the event and its complement together form the entirety of S. Figure 2-1.",
    "ire space of outcomes, and the event and its complement together form the entirety of S. Figure 2-1. Event A and its complement interact to form the entire set of possibili‐ ties, S. The complement simply defines all the possibilities not originally in A. 3.Let E1 and E2 be two events, where E1 is a subset (not necessarily strict) of E2. 3. The third tenet is that PE1≤PE2.",
    "two events, where E1 is a subset (not necessarily strict) of E2. 3. The third tenet is that PE1≤PE2. This, again, shouldn’t be too surprising— the second event has at least as many outcomes as the first event, and all the outcomes the first event has since the second is a superset of the first. If this tenet were not true, that would imply the existence of outcomes with negative probability, which is impossible from our definitions.",
    "imply the existence of outcomes with negative probability, which is impossible from our definitions. 4.The fourth and last tenet of probability is the principle of inclusion and4. exclusion, which states that PA∪B =PA+PB−PA∩B . For those not familiar with this terminology, the ∪ denotes the union of the two events, a set operation that takes the two events and returns an event that contains all elements from the two original sets.",
    "hat takes the two events and returns an event that contains all elements from the two original sets. The ∩, or intersection, is a set operation that returns an event that contains all elements belonging to both of the two original sets. The idea behind the equality presented is that by just naively summing the probabilities of A and B, we double-count the elements that belong to both sets.",
    "naively summing the probabilities of A and B, we double-count the elements that belong to both sets. Thus, to accurately obtain the probability of the union, we must subtract the probability of the intersection. In Figure 2-2 , we show two events and what their intersection would look like physically, while the union is all the outcomes in the combined area of the events. Figure 2-2. The middle sliver is the overlap between the two sets, containing all the outcomes that are in both sets.",
    "iddle sliver is the overlap between the two sets, containing all the outcomes that are in both sets. The union is all the events in the combined area of the two circles; if we were to add their probabilities naively, we would double-count all the outcomes in the middle sliver. Events and Probability | 19 --- Page 36 --- These tenets of probability find their way into everything that has to do with the field.",
    "age 36 --- These tenets of probability find their way into everything that has to do with the field. For example, in deep learning, most of our problems fall into one of two categories: regression and classification . In the latter, we train a neural model that can predict the likelihood that the input belongs to one of a discrete number of classes. The famous MNIST digits dataset, for example, provides us with pictures of digits and associated numerical labels in the range of 0 through 9.",
    "le, provides us with pictures of digits and associated numerical labels in the range of 0 through 9. Our objective is to build a classifier that can take in this picture and return the most likely label as its guess. This is naturally formulated as a problem in probability—the classifier produces a probability distribution over the sample space, 0 through 9, for any given input and its best guess is the digit that is assigned the highest probability. How does this relate to our tenets?",
    "est guess is the digit that is assigned the highest probability. How does this relate to our tenets? Since the classifier is producing a probability distribution, it must follow the tenets. For example, the probabilities associated with each digit must sum to one—a quick back-of-the-envelope check to ensure the model isn’t buggy. In the next section, we cover probabilities where we are initially given relevant information that affects our beliefs and how to use that information.",
    "e are initially given relevant information that affects our beliefs and how to use that information. Conditional Probability Knowing information often changes our beliefs, and by consequence, our probabili‐ ties. Going back to our classic dice example, we may roll the dice thinking that it’s fair, while in reality there’s a hidden weight at the dice’s core, making it more likely to land up a number greater than three.",
    "’s a hidden weight at the dice’s core, making it more likely to land up a number greater than three. As we roll the dice, we of course start to notice this pattern, and our belief regarding the dice’s fairness starts to shift. This is at the core of conditional probability itself. Instead of thinking simply about Pbiased or Pfair , we have to think about probabilities like Pbiasedinformation instead.",
    "imply about Pbiased or Pfair , we have to think about probabilities like Pbiasedinformation instead. This quantity, which we term a conditional probability, is spoken as “the probability the dice is biased given the information we’ve seen. ” How do we think about such probabilities intuitively? For starters, we must imagine that we are now in a different universe than the one we started in.",
    "y? For starters, we must imagine that we are now in a different universe than the one we started in. The new universe is one that incorporates the information we’ve seen since the start of the experiment, e.g., our past dice rolls. Going back to our MNIST example, the probability distri‐ bution that the trained neural net produces is actually a conditional probability distribution. The probability that the input image is zero, for example, can be seen as P(0|input).",
    "distribution. The probability that the input image is zero, for example, can be seen as P(0|input). In plain English, we want to find the probability of a zero given all of the pixels that make up the specific input image we fed into our neural net. Our new universe is the universe in which the input pixels have taken on this specific configuration of values. This is distinct from simply looking at P(0), the probability of returning a zero, which we can think about in terms of prior belief.",
    "ing at P(0), the probability of returning a zero, which we can think about in terms of prior belief. Without any knowledge of the input pixel configuration, we’ d have no reason to believe that the possibility of returning a zero is any more or less likely than that of any other digit. 20 | Chapter 2: Fundamentals of Probability --- Page 37 --- Sometimes, seeing certain information does not change our probabilities—we call this property independence.",
    "es, seeing certain information does not change our probabilities—we call this property independence. For example, Tom Brady may have thrown a touchdown pass after the third roll of our experiment, but incorporating that information into our new universe should (hopefully!) have no impact on the likelihood of the dice being biased. We state this independence property as P(biased|Tom Brady throws a touchdown pass) = P(biased).",
    "sed. We state this independence property as P(biased|Tom Brady throws a touchdown pass) = P(biased). Note that any two events E1 and E2 that satisfy this property are independent.",
    "wn pass) = P(biased). Note that any two events E1 and E2 that satisfy this property are independent. Perhaps slightly more counterintuitively, if it happens to be the case that all of our dice rolls so far don’t numerically change our prior belief regarding the dice’s fairness (maybe the dice rolls so far have shown up evenly across one through six and our initial prior belief was that the dice was fair), we’ d still say that these events are independent.",
    "initial prior belief was that the dice was fair), we’ d still say that these events are independent. Finally, note that independence is symmetric: if PE1E2=PE1, then it is also the case that PE2E1=PE2. In the previous section, we introduced intersection and union notation. It turns out that we can break down the intersection operation into a product of probabilities. We have the following equality: PE1∩E 2=PE1E2*PE2. Let’s break down the intuition here.",
    "obabilities. We have the following equality: PE1∩E 2=PE1E2*PE2. Let’s break down the intuition here. On the left side, we have the probability that both events E1 and E2 have occurred. On the right side, we have the same idea, but expressed slightly differently. In the universe where both events have occurred, one way to arrive in this universe is to first have E2 occur, followed by E1.",
    "events have occurred, one way to arrive in this universe is to first have E2 occur, followed by E1. Porting this intuition into mathematical terms, we must first find the probability that E2 has occurred, followed by the probability that E1 has occurred in the universe where E2 has already occurred. How do we combine these two probabilities?",
    "s occurred in the universe where E2 has already occurred. How do we combine these two probabilities? Intuitively, it makes sense that we multiply them—we must have both events occur, the first unconditionally and the second in the universe where the first has already occurred. Note that the order of these events doesn’t really matter, as both paths get us to the same universe. So, more completely, PE1∩E 2=PE1E2*PE2=PE2E1*PE1. However, some of these paths make much more physical sense than others.",
    "PE1∩E 2=PE1E2*PE2=PE2E1*PE1. However, some of these paths make much more physical sense than others. For example, if we think of E1 as the event where someone contracts a disease, and E2 as the event where the patient shows symptoms of the disease, the path in which the patient contracts the disease and then shows symptoms makes much more physical sense than the reverse. In the case where the two events are independent, we have that PE1∩E 2=PE1E2*PE2=PE1*PE2.",
    "e reverse. In the case where the two events are independent, we have that PE1∩E 2=PE1E2*PE2=PE1*PE2. Hopefully this makes some intu‐ itive sense. In the independence scenario, the fact that E2 has occurred doesn’t affect the chances of E1 occurring; i.e., incorporating this information into the new universe doesn’t affect the probability of the next event. In the next section, we cover random variables, which are relevant summaries of events and also have their own probability distributions.",
    "variables, which are relevant summaries of events and also have their own probability distributions. Conditional Probability | 21 --- Page 38 --- Random Variables Once again, let’s consider the coin flipping experiment. If we flip a coin some finite number of times, natural questions start to arise. How many heads did we encounter during our experiment? How many tails? How many tails until the first head? Every outcome in such an experiment has an answer to each of the listed questions.",
    "l the first head? Every outcome in such an experiment has an answer to each of the listed questions. If we flip a coin say, five times, and we receive the sequence TTHHT, we have seen two heads, three tails, and two tails until the first head. We can think of a random variable as a map, or a function, from the sample space to another space, such as the integers in Figure 2-3 .",
    "s a map, or a function, from the sample space to another space, such as the integers in Figure 2-3 . Such a function would take as input the sequence TTHHT and output one of the three answers listed depending on the question we ask. The value that the random variable takes on would be the output associated with result of the experiment.",
    "alue that the random variable takes on would be the output associated with result of the experiment. Although random variables are determinis‐ tic in that they map a given input to a single output, they are not deterministic in that they also have a distribution associated with their output space. This is due to the inherent randomness in the experiment—depending on the probability of the input outcome, its corresponding output may be more or less likely than other outputs. Figure 2-3.",
    "e input outcome, its corresponding output may be more or less likely than other outputs. Figure 2-3. Random variables X, Y, and Z all act on the same sample space, but have varying outputs. It’s important to keep in mind what you’re measuring! Note that multiple inputs could map to the same output. For exam‐ ple, X(HHH) = 3 in addition to X(HHTH) in Figure 2-3 .",
    "nputs could map to the same output. For exam‐ ple, X(HHH) = 3 in addition to X(HHTH) in Figure 2-3 . One easy way to begin is to just think of this map as an identity function—whatever we flip or roll, its map in the output space is exactly the same as the input.",
    "ity function—whatever we flip or roll, its map in the output space is exactly the same as the input. Encoding a heads as a one and a tails as a zero, we can define a random variable representing 22 | Chapter 2: Fundamentals of Probability --- Page 39 --- the coin flip as whether the coin came up heads, i.e., C(1) = 1, where C is our random variable. In the dice scenario, the mapped output is the same as whatever we rolled, i.e., D(5) = 5, where D is our random variable.",
    "the mapped output is the same as whatever we rolled, i.e., D(5) = 5, where D is our random variable. Why should we care about random variables and their distributions? It turns out they play a vital role in deep learning and machine learning as a whole. For example, in Chapter 4 , we will cover the concept of dropout , a technique for mitigating overfitting in neural networks.",
    ", we will cover the concept of dropout , a technique for mitigating overfitting in neural networks. The idea of a dropout layer is that, during training, it independ‐ ently and at random masks every neuron in the previous layer with some probability. This prevents the network from becoming overly dependent on specific connections or subnetworks. We can think of every neuron in the previous layer as representing a coin flip-type experiment.",
    "rks. We can think of every neuron in the previous layer as representing a coin flip-type experiment. The only difference is that we set the probability of this experiment, rather than a fair coin having the default probability 1 2 of showing up either side. Each neuron has a random variable X associated with it, with input one if the dropout layer decides to mask it and zero otherwise. X is an identity function from the input space to the output space, i.e., X(1) = 1 and X(0) = 0.",
    "se. X is an identity function from the input space to the output space, i.e., X(1) = 1 and X(0) = 0. Random variables, in general, need not be the identity map. Most functions you can think of are valid methods of mapping the input space to an output space where the random variable is defined. For example, if the input space were every possible length n sequence of coin flips, the function could be to count the number of heads in the sequence and square it.",
    "nce of coin flips, the function could be to count the number of heads in the sequence and square it. Some random variables can even be expressed as functions of other random variables, or a function of a function, as we will cover later.",
    "pressed as functions of other random variables, or a function of a function, as we will cover later. If we again consider the input space of every possible length n sequence of coin flips, the random variable counting the number of heads in the input sequence is the same as counting whether each individual coin flip turned up heads and taking a sum of all of those values.",
    "counting whether each individual coin flip turned up heads and taking a sum of all of those values. In mathematical terms, we say X= ∑i= 1nCi, where X is the random variable representing the total number of heads, and Ci is the binary random variable associated with the ith coin flip. Back to the dropout example, we can think of the random variable representing the total number of masked-out neurons as the sum of binary random variables representing each neuron.",
    "e total number of masked-out neurons as the sum of binary random variables representing each neuron. In the future, when we want to refer to the event where the random variable takes on a specific value c (the domain being the output space we’ve been referring to, e.g., the number of heads in a sequence of coin flips), we will write this concisely as X = c. We denote the probability that the random variable takes on a specific value as P(X = c), for example.",
    "denote the probability that the random variable takes on a specific value as P(X = c), for example. The probability that the random variable takes on any given value in the output space is just the sum of the probabilities of the inputs that map to it. This should make some intuitive sense, as this is basically the fourth tenet of probability where the intersection between any two events is the empty set since all the events we start from are individual, distinct inputs.",
    "any two events is the empty set since all the events we start from are individual, distinct inputs. Note that P(X) itself is also a probability distribution that follows all the basic tenets of probability described in the first section. In the next section, we consider statistics regarding random variables.",
    "cribed in the first section. In the next section, we consider statistics regarding random variables. Random Variables | 23 --- Page 40 --- Expectation As we discussed, a random variable is a map from input space to output space, where inputs are generated according to some probability distribution. The random variable can be thought of as a relevant summary of the input, and can take on many forms depending on the question we ask.",
    "of as a relevant summary of the input, and can take on many forms depending on the question we ask. Sometimes, it’s useful to understand statistics regarding the random variable. For example, if we flip a coin eight times, how many heads do we expect to see on average? And, of course, we don’t see the average number of heads all the time—how much does the number of heads we see tend to vary?",
    "see the average number of heads all the time—how much does the number of heads we see tend to vary? The first quantity is what we call the random variable’s expectation , and the second is the random variable’s variance . For a random variable X, we denote its expectation as EX. We can think of this as the average value that X takes on, weighted by the probability of each of those outcomes. Mathematically, this is written as EX= ∑oo*PX=o.",
    "ghted by the probability of each of those outcomes. Mathematically, this is written as EX= ∑oo*PX=o. Note that if all outcomes o are equally likely, we get a simple average of all the outcomes. It makes sense to use the probability of the outcome as a weighting, since some outcomes are more likely than others, and the average value we observe will be skewed toward such outcomes. For a single fair coin flip, the expected number of heads would be ∑o ∈ 0, 1o*Po= 0 * 0 . 5 + 1 * 0 . 5 = 0 . 5 .",
    "fair coin flip, the expected number of heads would be ∑o ∈ 0, 1o*Po= 0 * 0 . 5 + 1 * 0 . 5 = 0 . 5 . In other words, we’ d expect to see half of a head for any given fair coin flip. Of course, this makes no physical sense in that we could never possibly flip half of a head, but this gives you an idea of the proportions we’ d expect to see over a long run experiment. Returning to our example of length n sequences of coin flips, let’s try to find the expected number of heads in such a sequence.",
    "length n sequences of coin flips, let’s try to find the expected number of heads in such a sequence. We have n + 1 possible number of heads, and according to our formula, we’ d need to find the probability of attaining each possible number to use as our weights. Mathematically, we’ d need to compute ∑x ∈ 0, ...,nx*PX=x, where X is the random variable representing the total num‐ ber of heads.",
    "ompute ∑x ∈ 0, ...,nx*PX=x, where X is the random variable representing the total num‐ ber of heads. However, as n gets larger and larger, performing this calculation starts to become more and more complicated. Instead, let’s denote Xi as the binary random variable for the ith coin flip and use the observation we made in the last section of being able to break up the total number of heads into a sum over heads/tails for all the individual coin flips. Since we know X=X1+X2+ ...",
    "r of heads into a sum over heads/tails for all the individual coin flips. Since we know X=X1+X2+ ... +Xn, we can also say that EX=EX1+X2+ ... +Xn. How does making this substitution make our problem easier? We now introduce the concept of linearity of expectation, which states we can break up the right side into the sum EX1+EX2+ ... +EXn. We know that the expected number of heads for each flip is 0.5, so the expected number of heads in a sequence of n flips is just 0.5* n.",
    "heads for each flip is 0.5, so the expected number of heads in a sequence of n flips is just 0.5* n. This is much simpler than going down the previous route, as this approach’s difficulty does not scale with the number of flips. 24 | Chapter 2: Fundamentals of Probability --- Page 41 --- Let’s go over the simplification we made in a bit more detail.",
    "entals of Probability --- Page 41 --- Let’s go over the simplification we made in a bit more detail. Mathematically, if we have any two independent random variables A and B: EA+B= ∑a,ba+b*PA=a,B=b = ∑a,ba+b*PA=a*PB=b = ∑a,ba*PA=a*PB=b+b*PA=a*PB=b = ∑a,ba*PA=a*PB=b+ ∑a,bb*PA=a*PB=b = ∑aa*PA=a∑bPB=b+ ∑bb*PB=b∑aPA=a = ∑aa*PA=a+ ∑bb*PB=b =EA+EB Note that we made the independence assumption we talked about earlier in the chapter here when we broke up the probability of the event A = a and the event B = b into a product of the two individual probabilities.",
    "obability of the event A = a and the event B = b into a product of the two individual probabilities. The rest of the derivation doesn’t require additional assumptions, so we recommend working through the algebra on your own. Although we won’t show this for the depen‐ dent case, linearity of expectation also holds for dependent random variables. Going back to the dropout example, the expectation of the total number of masked neurons can be broken up into a sum of expectations over each neuron.",
    "of the total number of masked neurons can be broken up into a sum of expectations over each neuron. The expected number of masked neurons, similarly to the expected number of heads in a sequence of coin flips, is p*n, where p is the probability of being masked (and the expectation of each individual binary random variable representing a neuron) and n is the number of neurons. As mentioned, we don’t always see the expected number of occurrences of an event in every repetition of an experiment.",
    "on’t always see the expected number of occurrences of an event in every repetition of an experiment. In some cases, such as the expected number of heads in a single, fair coin flip from earlier, we never see it! Next, we will quantify the average deviation, or variance, from the expected value we see in repetitions of an experiment. Variance We define the variance, or Var( X), as EX−μ2, where we let μ=EX.",
    "etitions of an experiment. Variance We define the variance, or Var( X), as EX−μ2, where we let μ=EX. In plain English, this measure represents the average squared difference between the value X takes on and its expectation. Note that X−μ2 itself is also a random variable since it is a function of a function ( X), which is still a function.",
    "self is also a random variable since it is a function of a function ( X), which is still a function. Although we won’t get into Variance | 25 --- Page 42 --- too much detail about why we use this formula in particular, we encourage you to think about why we don’t use a formula such as EX−μ instead.",
    "mula in particular, we encourage you to think about why we don’t use a formula such as EX−μ instead. To obtain a slightly simpler form for the variance, we can perform the following simplification: EX−μ2=EX2− 2μX+μ2 =EX2−E2μX +Eμ2 =EX2− 2μEX+μ2 =EX2− 2EX2+EX2 =EX2−EX2 Let’s take a moment to go through each of these steps. In the first step, we fully express the random variable as all of its component terms via classic binomial expansion.",
    ", we fully express the random variable as all of its component terms via classic binomial expansion. In the second step, we perform linearity of expectation to break out the component terms into their own, individual expectations. In the third step, we note that μ, or EX, and its square are both constants and thus can be pulled out of the surrounding expectation.",
    "or EX, and its square are both constants and thus can be pulled out of the surrounding expectation. They are constants since they are not a function of the value X takes on and are instead evaluated using the entire domain (the set of values X can take on). Constants can be seen as random variables that can take on only one value, which is the constant itself. Thus, their expectations, or the average value the random variable takes on, is the constant itself since we always see the constant.",
    "average value the random variable takes on, is the constant itself since we always see the constant. The final steps are algebraic manipulations that bring us to the simplified result. Let’s use this formula to find the variance of the binary random variable representing a single neuron under dropout, and p is the probability of the neuron being masked out: EX2−EX2= ∑x ∈ 0, 1x2*PX=x−∑x ∈ 0, 1x*PX=x2 = ∑x ∈ 0, 1x2*PX=x−p2 =p−p2 =p1 −p These simplifications should make sense.",
    "x2*PX=x−∑x ∈ 0, 1x*PX=x2 = ∑x ∈ 0, 1x2*PX=x−p2 =p−p2 =p1 −p These simplifications should make sense. We know from “Expectation” on page 24 that the expectation of the binary random variable representing a neuron is just p, and the rest is algebraic simplifications. We highly encourage you to work through these derivations on your own.",
    "is algebraic simplifications. We highly encourage you to work through these derivations on your own. As we start to think about the random variable representing the number of masked neurons in the entire layer, we naturally ask the question of whether there exists a similar linearity property for variance as there does for expectation.",
    "ion of whether there exists a similar linearity property for variance as there does for expectation. Unfortunately, the property does not hold in general: 26 | Chapter 2: Fundamentals of Probability --- Page 43 --- VarA+B=EA+B2−EA+B2 =EA2+ 2 *A*B+B2−EA+EB2 =EA2+ 2EA*B+EB2−EA2− 2EAEB−EB2 =EA2−EA2+EB2−EB2+ 2EA*B− 2EAEB =VarA+VarB+ 2EA*B−EAEB =VarA+VarB+ 2CovA,B As we can see from the last line, the final term in the expression, which we call the covariance between the two random variables, ruins our hope for linearity.",
    "ession, which we call the covariance between the two random variables, ruins our hope for linearity. However, covariance is another key concept in probability—the intuition for covariance is that it measures the dependence between two random variables.",
    "bility—the intuition for covariance is that it measures the dependence between two random variables. As one random variable more completely determines the value of another random variable (think of A as the number of heads in a sequence of coin flips and B as the number of tails in the same sequence of coin flips), the magnitude of the covariance increases.",
    "the number of tails in the same sequence of coin flips), the magnitude of the covariance increases. Thus, it stands to reason that if A and B are independent random variables, the covariance between them should be zero, and linearity should hold in this special case. We highly encourage you to work through the math and show this on your own.",
    "ld in this special case. We highly encourage you to work through the math and show this on your own. Back to the dropout example, the variance of the total number of masked neurons can be broken up into a sum of variances over each neuron, since each neuron is masked independently. The variance of the number of masked neurons is p(1 – p)*n, where p(1 – p) is the variance for any given neuron and n is the number of neurons.",
    "s is p(1 – p)*n, where p(1 – p) is the variance for any given neuron and n is the number of neurons. Expectation and variance in dropout allow us to understand more deeply what we expect to see when applying such a layer in a deep neural network. Bayes’ Theorem Returning to our discussion on conditional probability, we noted that the probability of intersection between two events could be written as a product of a conditional dis‐ tribution and a distribution over a single event.",
    "ould be written as a product of a conditional dis‐ tribution and a distribution over a single event. Let’s translate this into the language of random variables, now that we have introduced this new terminology. We denote A to be one random variable, and B to denote a second. Let a be a value that A can take on, and b be a value that B can take on.",
    "and B to denote a second. Let a be a value that A can take on, and b be a value that B can take on. The analogy to the intersection operation for random variables is the joint probability distribution P(A=a,B=b), which denotes the event where A = a and B = b. We can think of A = a and B = b as individual events, and when we write P(A = a,B = b) , we are considering the probability that both events have occurred, i.e., their intersection PA=a∩B =b.",
    "are considering the probability that both events have occurred, i.e., their intersection PA=a∩B =b. Note that we generally write the joint probability distribution as P(A,B), since this encompasses all possible joint settings of the random variables A and B. Bayes’ Theorem | 27 --- Page 44 --- We mentioned earlier that intersection operations could be written as the product of a conditional distribution and a distribution over a single event.",
    "ould be written as the product of a conditional distribution and a distribution over a single event. Rewriting this in the format for random variables, we have P(A = a,B = b) = P(A = a|B = b)P(B = b) . And more generally, considering all possible joint settings of the two random variables, we have P(A,B) = P(A|B)P(B).",
    "y, considering all possible joint settings of the two random variables, we have P(A,B) = P(A|B)P(B). We also discussed how there always exists a second way of writing this joint distribution as a product: P(A = a,B = b) = P(B = b|A = a)P(A=a), and more generally , P(A,B) = P(B|A)P(A). We noted that sometimes one of these paths makes more sense than the other.",
    "y , P(A,B) = P(B|A)P(A). We noted that sometimes one of these paths makes more sense than the other. For example, in the case where symptoms are represented by A and disease is represented by B, the path in which B takes on a value b, and then A takes on a value a in that universe makes much more sense than the reverse since, biologically, people contract a disease first and only then show symptoms for that disease. However, this doesn’t mean that the reverse isn’t useful.",
    "only then show symptoms for that disease. However, this doesn’t mean that the reverse isn’t useful. It is almost universally the case that people show up at a hospital with mild symptoms, and medical professio‐ nals must try to infer the most likely disease from these symptoms to effectively treat the underlying disease. Bayes’ Theorem gives us a way of calculating the probability of a disease given the observed symptoms.",
    "yes’ Theorem gives us a way of calculating the probability of a disease given the observed symptoms. Since the same joint probability distribution can be written in the two ways mentioned in the previous paragraph, we have the following equality: PBA=PABPB PA If B represents disease, while A represents symptoms, this gives us a method for com‐ puting the likelihood of any disease given the observed symptoms. Let’s analyze the right side to see if the equality also makes intuitive sense.",
    "e observed symptoms. Let’s analyze the right side to see if the equality also makes intuitive sense. The likelihood of symptoms given the disease times the likelihood of the disease is just the joint distribution, which makes sense as the numerator here. The denominator is the likelihood of see‐ ing those symptoms, which can also be expressed as a sum of the numerator over all possible diseases.",
    "ng those symptoms, which can also be expressed as a sum of the numerator over all possible diseases. This is an instance of a more general process called marginalization, or removing a subset of random variables from a joint distribution by summing over all possible configurations of the subset: PA= ∑bPA,B=b In more concise terms, we have: PB=bqueryA=PB=bquery ,A ∑bPB=b,A 28 | Chapter 2: Fundamentals of Probability --- Page 45 --- Bayes’ Theorem is a very valuable application of probability in the real world, espe‐ cially in the case of disease prediction.",
    "luable application of probability in the real world, espe‐ cially in the case of disease prediction. Additionally, if we replace the random variable for symptoms with a random variable representing the result of a test for a specific disease, and the random variable over all diseases with a random variable over pres‐ ence of the specific disease, we can infer the likelihood of actually having a specific disease given a positive test for it using Bayes’ Theorem.",
    "likelihood of actually having a specific disease given a positive test for it using Bayes’ Theorem. This is a common problem in most hospitals, and is especially relevant to epidemiology given the outbreak of COVID-19. Entropy, Cross Entropy, and KL Divergence Probability distributions, by definition, give us a way of comparing the likelihoods of various possible events.",
    "distributions, by definition, give us a way of comparing the likelihoods of various possible events. However, even if we know the most likely event (or events) that is to occur, when running the experiment we are bound to see all sorts of events. In this section, we first consider the problem of defining a single metric that encapsulates all of the uncertainty within a probability distribution, which we will define as the distribution’s entropy . Let’s set up the following scenario.",
    "tribution, which we will define as the distribution’s entropy . Let’s set up the following scenario. I am a researcher who is running an experiment. The experiment could be something as simple as flipping a coin or rolling a dice. Y ou are recording the results of the experiment. We are both in different rooms, but connected through a phone line. I run the experiment and receive a result, and communicate that result to you via the phone.",
    "e line. I run the experiment and receive a result, and communicate that result to you via the phone. Y ou record that result in a notebook, where you pick some binary string representation of that result as what you write down. As a scribe, you are necessary in this situation—I may run hundreds of trials and my memory is limited, so I cannot remember the results of all of my trials.",
    "n hundreds of trials and my memory is limited, so I cannot remember the results of all of my trials. For example, if I roll a dice and neither of us knows anything about the fairness of the dice, you could denote the outcome one as “0, ” two as “1, ” three as “10, ” four as “11, ” five as “100, ” and six as “101. ” Whenever I communicate a result of the experiment to you, you add that result’s corresponding string representation to the end of the string consisting of all results so far.",
    "ult’s corresponding string representation to the end of the string consisting of all results so far. If I were to roll a one, followed by two twos, and finally a one, using the encoding scheme defined so far you would have written down “0110. ” After all runs of the experiment have ended, I have a meeting with you and try to decipher this string “0110” into a sequence of outcomes for use in my research.",
    "with you and try to decipher this string “0110” into a sequence of outcomes for use in my research. However, as the researcher, I am puzzled by this string—does it represent a one, followed by two twos, and finally a one? Or does it represent a one, followed by a two, followed by a three? Or even a one, followed by a four, followed by a one? It seems that there are at least a few possible translations of this string into outcomes using the encoding scheme.",
    "ere are at least a few possible translations of this string into outcomes using the encoding scheme. Entropy, Cross Entropy, and KL Divergence | 29 --- Page 46 --- To prevent this situation from ever occurring again, we decide to enforce some limi‐ tations on the binary strings you can use to represent outcomes. We use what is called a prefix code , which disallows binary string representations of different outcomes from being prefixes of each other.",
    "ich disallows binary string representations of different outcomes from being prefixes of each other. It’s not too difficult to see why this would result in a unique translation of string to outcomes. Let’s say we have a binary string, some prefix of which we have been able to successfully decode into a series of outcomes. To decode the rest of the string, or the suffix, we must first find the next outcome in the series.",
    "To decode the rest of the string, or the suffix, we must first find the next outcome in the series. When we find a prefix of this suffix that translates to an outcome, we already know that, by definition, there is no smaller prefix that translates to a valid outcome. We now have a larger prefix of the binary string that has been successfully translated to a series of outcomes. We then recursively use this logic until we have reached the end of the string.",
    "series of outcomes. We then recursively use this logic until we have reached the end of the string. Now that we have some guidelines on string representations for outcomes, we redo the original experiment with one as “0, ” two as “10, ” three as “110, ” four as “1110, ” five as “11110, ” and six as “111110. ” However, as noted earlier, I may carry out hundreds of trials, and as the scribe you probably want to limit the amount of writing you have to do.",
    "ndreds of trials, and as the scribe you probably want to limit the amount of writing you have to do. With no information about the dice, we can’t do too much better than this. Assuming each outcome shows up with probability 1 6, the expected number of letters you’ d need to write down per trial is 3.5. We could get down to 3 if we set one as “000, ” two as “001, ” three as “010, ” four as “011, ” five as “100, ” and six as “101, ” for example. But what if we knew information about the dice?",
    "1, ” five as “100, ” and six as “101, ” for example. But what if we knew information about the dice? For example, what if it were a weighted dice that showed up six almost all of the time? In that case, you probably want to assign a shorter binary string to six, for example “0” (instead of assigning “0” to one) so you can limit the expected amount of writing you have to do.",
    "0” (instead of assigning “0” to one) so you can limit the expected amount of writing you have to do. It makes intuitive sense that, as the result of any single trial becomes more and more certain, the expected number of characters you’ d need to write becomes lower by assigning the shortest binary strings to the most likely outcomes.",
    "d need to write becomes lower by assigning the shortest binary strings to the most likely outcomes. This raises the question: given a probability distribution over outcomes, what is the optimal encoding scheme, where optimal is defined as the fewest expected number of characters you’ d need to write per trial? Although this whole situation may feel a bit contrived, it provides us with a slightly different lens through which we can understand the uncertainty within a probability distribution.",
    "ly different lens through which we can understand the uncertainty within a probability distribution. As we noted, as the result of an experiment becomes more and more certain, the optimal encoding scheme would allow the scribe to write fewer and fewer characters in expectation per trial. For example, in the extreme case where we already knew beforehand that a six would always show up, the scribe wouldn’t need to write anything down.",
    "dy knew beforehand that a six would always show up, the scribe wouldn’t need to write anything down. 30 | Chapter 2: Fundamentals of Probability --- Page 47 --- It turns out that, although we won’t show it here, the best you can do is assign a binary string of length log21 pxi to each possible outcome xi, where pxi is its probability.",
    "ssign a binary string of length log21 pxi to each possible outcome xi, where pxi is its probability. The expected string length of any given trial would then be: Epxlog21 px= ∑xipxilog21 pxi = − ∑xipxilog2pxi This expression is defined as the entropy of a probability distribution. In the case where we are completely certain of the final outcome (e.g., the dice always lands up six), we can evaluate the expression for entropy and see that we get a result of 0.",
    "always lands up six), we can evaluate the expression for entropy and see that we get a result of 0. In the case where we are completely certain of the final outcome (e.g., the dice always lands up six), we can evaluate the expression for entropy and see that we get a result of 0. Additionally, the probability distribution that has the highest entropy is the one that places equal probability over all possible outcomes.",
    "on that has the highest entropy is the one that places equal probability over all possible outcomes. This is because, for any given trial, we are no more certain that a particular outcome will appear as opposed to any other outcome. As a result, we cannot use the strategy of assigning a shorter string to any single outcome. Now that we have defined entropy, we can discuss cross entropy, which provides us a way of measuring the distinctness of two distributions. Equation 2-1.",
    "s entropy, which provides us a way of measuring the distinctness of two distributions. Equation 2-1. Cross entropy CEpq=Epxlog21 qx= ∑xpxlog21 qx= − ∑xpxlog2qx Note that cross entropy has a log1 qx term, which can be interpreted as the optimal binary string length assigned to each outcome, assuming outcomes appear according to probability distribution q(x). However, note that this is an expectation with respect to p(x), so how do we interpret this entire expression?",
    "ote that this is an expectation with respect to p(x), so how do we interpret this entire expression? Well, we can understand the cross entropy to mean the expected string length for any trial given we have optimized for the encoding scheme for distribution q(x) while, in reality, all of the outcomes are appearing according to the distribution p(x).",
    "bution q(x) while, in reality, all of the outcomes are appearing according to the distribution p(x). This can definitely happen in an experiment where we have only limited a priori information about the experiment, so we assume some distribution q(x) to optimize our encoding scheme, but as we carry out trials, we learn more information that gets us closer to the true distribution p(x). The KL divergence takes this logic a bit further.",
    "that gets us closer to the true distribution p(x). The KL divergence takes this logic a bit further. If we take the cross entropy, which tells us the expected number of bits per trial given we have optimized our encoding for the incorrect distribution q(x), and subtract from that the entropy, which tells Entropy, Cross Entropy, and KL Divergence | 31 --- Page 48 --- us the expected number of bits per trial given we have optimized for the correct distribution p(x), we get the expected number of extra bits required to represent a trial when using q(x) compared to p(x).",
    "et the expected number of extra bits required to represent a trial when using q(x) compared to p(x). Here is the expression for the KL divergence: KLpq=Epxlog21 qx− log21 px=Epxlog2px qx At the unique global minimum q(x) = p(x), the KL divergence is exactly zero. Why this is the unique minimum is a bit beyond the scope of this text, so we leave that as an exercise for you.",
    "the unique minimum is a bit beyond the scope of this text, so we leave that as an exercise for you. In practice, when trying to match the true distribution p(x) with a learned distribu‐ tion q(x), KL divergence is often minimized as an objective function.",
    "p(x) with a learned distribu‐ tion q(x), KL divergence is often minimized as an objective function. Most models will actually minimize the cross entropy in place of the KL divergence, which is effectively the same optimization problem due to the KL being a difference between the cross entropy and the entropy of p(x), where the entropy of p(x) is a constant and has no dependence on the weights that parameterize q(x).",
    "where the entropy of p(x) is a constant and has no dependence on the weights that parameterize q(x). Thus, the gradient with respect to the weights that parameterize q(x) when using either objective is the same. One common example where cross-entropy/KL divergence is optimized is in the standard training of a neural network classifier.",
    "cross-entropy/KL divergence is optimized is in the standard training of a neural network classifier. The neural network’s objective is to learn a distribution over target classes such that, for any given example xi, pθyx=xi matches the true distribution p(y|x = xi), which has all of its probabil‐ ity mass placed over the true label yi and zero probability over all other classes.",
    "of its probabil‐ ity mass placed over the true label yi and zero probability over all other classes. Minimizing the sum of cross entropies between the learned distribution and the true distribution over all examples is actually the exact same as minimizing the negative log likelihood of the data. Both are valid interpretations of how neural networks are trained, and lead to the same objective function. We encourage you to try writing out both expressions independently to see this.",
    "objective function. We encourage you to try writing out both expressions independently to see this. Continuous Probability Distributions So far, we have looked at probability distributions through the lens of discrete out‐ comes and events. However, as it turns out, probability distributions aren’t just for sets of discrete outcomes like the CIFAR-10 target classes or the MNIST digits. We can define probability distributions over sample spaces of infinite size, such as all the real numbers.",
    "define probability distributions over sample spaces of infinite size, such as all the real numbers. In this section, we will extend principles covered in the previous sections to the continuous realm. In the continuous realm, probability distributions are often referred to as probability density functions , or PDFs. PDFs are nonnegative functions over a sample space, such as all the reals, that integrate to one.",
    "s. PDFs are nonnegative functions over a sample space, such as all the reals, that integrate to one. Recall from calculus that the integration of a function is the area of the region underneath the function, bounded by the x-axis. PDFs follow the basic tenets introduced in the first section, but instead of adding 32 | Chapter 2: Fundamentals of Probability --- Page 49 --- the probability of outcomes to get the probability of an event, we use integration.",
    "--- Page 49 --- the probability of outcomes to get the probability of an event, we use integration. For example, say X is a continuous random variable that is defined over all the real numbers. If we’ d like to know the probability of the event PX≤ 2, all we’ d need to do is integrate the PDF of X from negative infinity to 2. But how about the probability of any individual outcome, say P(X = 2)?",
    "from negative infinity to 2. But how about the probability of any individual outcome, say P(X = 2)? Since we use integration to find probabilities in the continuous space, the probability of any individual outcome is actually zero due to the width of the region being infinitesimal. We instead use the term likelihood to distinguish between the probability of events and the value that the PDF evaluates to when we input a setting of X.",
    "ween the probability of events and the value that the PDF evaluates to when we input a setting of X. Likelihoods are still valuable, as they tell us what individual outcomes we are most likely to see when performing an experiment over a continuous space. Going forward, when con‐ sidering continuous probability distributions, we will only refer to events as having probability, rather than individual outcomes.",
    "distributions, we will only refer to events as having probability, rather than individual outcomes. One famous example of a continuous probability distribution is the uniform distri‐ bution over some interval on the real line. Under the uniform distribution, the likelihood of each outcome is the same, meaning that no outcome is any more likely to appear than another.",
    "hood of each outcome is the same, meaning that no outcome is any more likely to appear than another. Thus, the uniform distribution looks like a rectangle, where the base of the rectangle is the interval constituting its domain, and the height, or the likelihood for each outcome, is the value that makes the area of the rectangle equal to one. Figure 2-4 shows the uniform distribution over the interval [0,0.5]. Figure 2-4.",
    "angle equal to one. Figure 2-4 shows the uniform distribution over the interval [0,0.5]. Figure 2-4. The uniform distribution has uniform height over its entire area, which shows that each value in the domain of the distribution has equal likelihood. This example was chosen specifically to show a concrete difference between likeli‐ hoods and probabilities in the continuous realm.",
    "cally to show a concrete difference between likeli‐ hoods and probabilities in the continuous realm. The height of the rectangle being Continuous Probability Distributions | 33 --- Page 50 --- 2 was no error—there is no constraint on the magnitude of the likelihood in continu‐ ous distributions, unlike probabilities, which must be less than or equal to 1.",
    "kelihood in continu‐ ous distributions, unlike probabilities, which must be less than or equal to 1. Another famous example of a continuous probability distribution is the Gaussian distribution , which is one of the more common ways in which data presents itself in the real world. The Gaussian distribution is defined by two parameters: its mean μ and its standard deviation σ.",
    "ld. The Gaussian distribution is defined by two parameters: its mean μ and its standard deviation σ. The PDF of a Gaussian distribution is: fx;μ,σ=1 σ2πe−1 2x−μ σ2 Why this function integrates to 1 over the real domain is beyond the scope of this chapter, but one important characteristic of a Gaussian distribution is that its mean is also its unique mode. In other words, the outcome with the highest likelihood is also, uniquely, the mean outcome. This is not the case for all distributions.",
    "highest likelihood is also, uniquely, the mean outcome. This is not the case for all distributions. For example, Figure 2-4 does not have this property. The graph of a standard Gaussian, which has mean zero and unit variance, is shown in Figure 2-5 (the PDF asymptotically reaches zero in the limit in both directions). Figure 2-5.",
    "own in Figure 2-5 (the PDF asymptotically reaches zero in the limit in both directions). Figure 2-5. The Gaussian distribution has a bell shape, with highest likelihood in the center and dropping exponentially as the value in question gets farther and farther from the center. Why is the Gaussian distribution so prevalent in real-world data? One reason for this is a theorem called the Central Limit Theorem (CLT).",
    "evalent in real-world data? One reason for this is a theorem called the Central Limit Theorem (CLT). This theorem states that sums of independent random variables converge to a Gaussian distribution as the number of variables in the sum goes to infinity, even if each variable is not distributed as 34 | Chapter 2: Fundamentals of Probability --- Page 51 --- a Gaussian. One example is the number of masked neurons after a dropout layer is applied.",
    "age 51 --- a Gaussian. One example is the number of masked neurons after a dropout layer is applied. As the number of neurons from the previous layer goes to infinity, the number of masked neurons (which is a sum of independent Bernoulli random vari‐ ables, as discussed in “Random Variables” on page 22), when standardized correctly, is approximately distributed as a standard Gaussian distribution.",
    "22), when standardized correctly, is approximately distributed as a standard Gaussian distribution. We won’t cover CLT in much depth here, but it has more recently been extended to weakly dependent variables under certain special conditions. Many real-world datasets can be seen as approximately sums of many random vari‐ ables.",
    "l conditions. Many real-world datasets can be seen as approximately sums of many random vari‐ ables. For example, the distribution of a disease prevalence within a given population, similarly to the number of masked neurons after applying dropout, is the sum of many Bernoulli random variables (where each person is a Bernoulli random variable that has a value of 1 if they have the disease and a value of 0 if they do not)—although likely dependent.",
    "as a value of 1 if they have the disease and a value of 0 if they do not)—although likely dependent. Continuous random variables are still functions, just as we defined discrete random variables. The only difference is that the range of this function is a continuous space.",
    "rete random variables. The only difference is that the range of this function is a continuous space. To compute the expectation and variance of a continuous random variable, all we need to do is replace our summations with integrations, as follows: EX= ∫xx*fX=xdx VarX= ∫xx−EX2*fX=xdx As an example, let’s evaluate the expectation for our uniform random variable defined earlier.",
    "X=xdx As an example, let’s evaluate the expectation for our uniform random variable defined earlier. But first, confirm that it makes intuitive sense that the expectation should be 0.25, since the endpoints of the interval are 0 and 0.5 and all values in between are of equal likelihood. Now, let’s evaluate the integral and see if the computation matches our intuition: ∫00 . 5x*fxdx= ∫00 . 52xdx =x2 00 . 5 = 0 .",
    "tegral and see if the computation matches our intuition: ∫00 . 5x*fxdx= ∫00 . 52xdx =x2 00 . 5 = 0 . 25 Where the superscript and the subscript of the | symbol represent the values at which we will evaluate the preceding function, which we will then difference to get the value of the integral. We see that the expectation comes out to the same value as our intuition, which is a great sanity check. Bayes’ Theorem also holds for continuous variables.",
    "as our intuition, which is a great sanity check. Bayes’ Theorem also holds for continuous variables. The only major difference is when marginalizing out a subset of variables, you will need to integrate over the entire domain of the marginalized subset rather than taking a discrete sum over Continuous Probability Distributions | 35 --- Page 52 --- all possible configurations of the marginalized subset.",
    "obability Distributions | 35 --- Page 52 --- all possible configurations of the marginalized subset. Again, this is an example of extending the tenets of probability to the continuous space by replacing summations with integrations.",
    "tending the tenets of probability to the continuous space by replacing summations with integrations. Here is Bayes’ Theorem for continuous probability distributions, following the notation from “Bayes’ Theorem” on page 27 : PB=bqueryA=PAB=bqueryPB=bquery PA=PAB=bqueryPB=bquery ∫bPA,B=bdb And finally, we have our discussion on entropy, cross entropy, and KL divergence. All three of these extend nicely to the continuous space as well.",
    "cross entropy, and KL divergence. All three of these extend nicely to the continuous space as well. We replace our summations with integrations and note that the properties introduced in the previous section still hold. For example, over a given domain, the distribution with the highest entropy is the uniform distribution, and the KL divergence between two distributions is zero if and only if the two distributions are the exact same.",
    "ivergence between two distributions is zero if and only if the two distributions are the exact same. Here are the definitions in their continuous form, following Equation 2-1 : Hfx= − ∫xfxlog2fxdx KLfxgx= ∫xfxlog2fx gxdx CEfxgx= − ∫xfxlog2gxdx Our extension of these concepts to the continuous space will come in handy in Chapter 10 , where we model many distributions as Gaussians.",
    "continuous space will come in handy in Chapter 10 , where we model many distributions as Gaussians. Additionally, we use the KL divergence/cross-entropy terms as a regularization procedure on the complex‐ ity of one of our learned distributions. Since KL divergence is only zero when the query distribution matches the target distribution, setting the target distribution to a Gaussian forces the learned distribution to approximate a Gaussian.",
    "ing the target distribution to a Gaussian forces the learned distribution to approximate a Gaussian. Summary In this chapter we covered the fundamentals of probability, first building the intuition behind the basics of probability distributions and then moving to relevant applica‐ tions of probability, such as conditional probability, random variables, expectation, and variance.",
    "tions of probability, such as conditional probability, random variables, expectation, and variance. We saw the applications of probability in deep learning, such as how a neural net parametrizes a probability distribution during classification tasks, and how we can quantify the mathematical properties of dropout, a regularization technique in neural nets.",
    "w we can quantify the mathematical properties of dropout, a regularization technique in neural nets. Finally, we discussed measurements of uncertainty in probability dis‐ tributions such as entropy, and generalized these concepts to the continuous realm. 36 | Chapter 2: Fundamentals of Probability --- Page 53 --- Probability is a field that affects the choices in our everyday lives, and it’s key to understand the meaning behind the numbers.",
    "ffects the choices in our everyday lives, and it’s key to understand the meaning behind the numbers. Additionally, we hope that this intro‐ duction puts the rest of the book in perspective and allows you to more rigorously understand future concepts. In the next chapter, we will discuss the structure of neural networks, and the motivations behind their design. Summary | 37 --- Page 55 --- 1Kuhn, Deanna, et al. Handbook of Child Psychology. Vol. 2, Cognition, Perception, and Language .",
    "-- 1Kuhn, Deanna, et al. Handbook of Child Psychology. Vol. 2, Cognition, Perception, and Language . Wiley, 1998.CHAPTER 3 The Neural Network Building Intelligent Machines The brain is the most incredible organ in the human body. It dictates the way we perceive every sight, sound, smell, taste, and touch. It enables us to store memories, experience emotions, and even dream. Without it, we would be primitive organisms, incapable of anything other than the simplest of reflexes.",
    "hout it, we would be primitive organisms, incapable of anything other than the simplest of reflexes. The brain is, inherently, what makes us intelligent. The infant brain weighs only a single pound, but somehow it solves problems that even our biggest, most powerful supercomputers find impossible. Within a matter of months after birth, infants can recognize the faces of their parents, discern discrete objects from their backgrounds, and even tell voices apart.",
    "faces of their parents, discern discrete objects from their backgrounds, and even tell voices apart. Within a year, they’ve already developed an intuition for natural physics, can track objects even when they become partially or completely blocked, and can associate sounds with specific mean‐ ings.",
    "when they become partially or completely blocked, and can associate sounds with specific mean‐ ings. And by early childhood, they have a sophisticated understanding of grammar and thousands of words in their vocabularies.1 For decades, we’ve dreamed of building intelligent machines with brains like ours— robotic assistants to clean our homes, cars that drive themselves, microscopes that automatically detect diseases.",
    "ants to clean our homes, cars that drive themselves, microscopes that automatically detect diseases. But building these artificially intelligent machines requires us to solve some of the most complex computational problems we have ever grappled with; problems that our brains can already solve in a manner of micro‐ seconds. To tackle these problems, we’ll have to develop a radically different way of programming a computer using techniques largely developed over the past decade.",
    "lly different way of programming a computer using techniques largely developed over the past decade. This is an extremely active field of artificial computer intelligence often referred to as deep learning . 39 --- Page 56 --- 2Y . LeCun, L. Bottou, Y . Bengio, and P . Haffner. “Gradient-Based Learning Applied to Document Recognition.",
    "n, L. Bottou, Y . Bengio, and P . Haffner. “Gradient-Based Learning Applied to Document Recognition. ” Proceedings of the IEEE , 86(11):2278-2324, November 1998.The Limits of Traditional Computer Programs Why exactly are certain problems so difficult for computers to solve? Well, it turns out that traditional computer programs are designed to be very good at two things: (1) performing arithmetic really fast and (2) explicitly following a list of instructions.",
    "o things: (1) performing arithmetic really fast and (2) explicitly following a list of instructions. So if you want to do some heavy financial number crunching, you’re in luck. Tradi‐ tional computer programs can do the trick. But let’s say we want to do something slightly more interesting, like write a program to automatically read someone’s hand‐ writing. Figure 3-1 will serve as a starting point. Figure 3-1.",
    "o automatically read someone’s hand‐ writing. Figure 3-1 will serve as a starting point. Figure 3-1. Image from MNIST handwritten digit dataset2 Although every digit in Figure 3-1 is written in a slightly different way, we can easily recognize every digit in the first row as a zero, every digit in the second row as a one, etc. Let’s try to write a computer program to crack this task. What rules could we use to tell one digit from another? Well, we can start simple!",
    "crack this task. What rules could we use to tell one digit from another? Well, we can start simple! For example, we might state that we have a zero if our image has only a single, closed loop. All the examples in Figure 3-1 seem to fit this bill, but this isn’t really a sufficient condition. What if someone doesn’t perfectly close the loop on their zero? And, as in Figure 3-2 , how do you distinguish a messy zero from a six? 40 | Chapter 3: The Neural Network --- Page 57 --- Figure 3-2.",
    "distinguish a messy zero from a six? 40 | Chapter 3: The Neural Network --- Page 57 --- Figure 3-2. A zero that’s algorithmically difficult to distinguish from a six Y ou could potentially establish some sort of cutoff for the distance between the starting point of the loop and the ending point, but it’s not exactly clear where we should be drawing the line. But this dilemma is only the beginning of our worries. How do we distinguish between threes and fives? Or between fours and nines?",
    "eginning of our worries. How do we distinguish between threes and fives? Or between fours and nines? We can add more and more rules, or features , through careful observation and months of trial and error, but it’s quite clear that this isn’t going to be an easy process. Many other classes of problems fall into this same category: object recognition, speech comprehension, automated translation, etc. We don’t know what program to write because we don’t know how it’s done by our brains.",
    "slation, etc. We don’t know what program to write because we don’t know how it’s done by our brains. And even if we did know how to do it, the program might be horrendously complicated. The Mechanics of Machine Learning To tackle these classes of problems, we’ll have to use a different kind of approach. A lot of the things we learn in school growing up have much in common with traditional computer programs.",
    "of the things we learn in school growing up have much in common with traditional computer programs. We learn how to multiply numbers, solve equations, and take derivatives by internalizing a set of instructions. But the things we learn at an extremely early age, the things we find most natural, are learned by example, not by formula. For instance, when we were two years old, our parents didn’t teach us how to recog‐ nize a dog by measuring the shape of its nose or the contours of its body.",
    "’t teach us how to recog‐ nize a dog by measuring the shape of its nose or the contours of its body. We learned to recognize a dog by being shown multiple examples and being corrected when we made the wrong guess. When we were born, our brains provided us with a model that described how we would be able to see the world. As we grew up, that model would take in our sensory inputs and make a guess about what we were experiencing.",
    "ew up, that model would take in our sensory inputs and make a guess about what we were experiencing. If that guess was confirmed by our parents, our model would be reinforced. If our parents said we were wrong, we’ d modify our model to incorporate this new information. Over our lifetime, our model becomes more and more accurate as we assimilate more and more examples. Obviously all of this happens subconsciously, but we can use this to our advantage.",
    "d more examples. Obviously all of this happens subconsciously, but we can use this to our advantage. Deep learning is a subset of a more general field of AI called machine learning , which is predicated on this idea of learning from example.",
    "ral field of AI called machine learning , which is predicated on this idea of learning from example. In machine learning, instead of teaching a computer a massive list of rules to solve the problem, we give it a model The Mechanics of Machine Learning | 41 --- Page 58 --- with which it can evaluate examples, and a small set of instructions to modify the model when it makes a mistake. We expect that, over time, a well-suited model would be able to solve the problem extremely accurately.",
    "expect that, over time, a well-suited model would be able to solve the problem extremely accurately. Let’s be a little bit more rigorous about what this means so we can formulate this idea mathematically. Let’s define our model to be a function ℎx,θ. The input x is an example expressed in vector form. For example, if x were a grayscale image, the vector’s components would be pixel intensities at each position, as shown in Figure 3-3 . Figure 3-3.",
    "ector’s components would be pixel intensities at each position, as shown in Figure 3-3 . Figure 3-3. The process of vectorizing an image for a machine learning algorithm 42 | Chapter 3: The Neural Network --- Page 59 --- 3Rosenblatt, Frank. “The perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. ” Psychological Review 65.6 (1958): 386.The input θ is a vector of the parameters that our model uses.",
    "Psychological Review 65.6 (1958): 386.The input θ is a vector of the parameters that our model uses. Our machine learning program tries to perfect the values of these parameters as it is exposed to more and more examples. We’ll see this in action and in more detail in Chapter 4 . To develop a more intuitive understanding for machine learning models, let’s walk through a quick example.",
    "elop a more intuitive understanding for machine learning models, let’s walk through a quick example. Let’s say we wanted to determine how to predict exam performance based on the number of hours of sleep we get and the number of hours we study the previous day. We collect a lot of data, and for each data point x=x1x2T, we record the number of hours of sleep we got ( x1), the number of hours we spent studying ( x2), and whether we performed above or below the class average.",
    "number of hours we spent studying ( x2), and whether we performed above or below the class average. Our goal, then, might be to learn a model ℎx,θ with parameter vector θ=θ0θ1θ2T such that: ℎx,θ=−1 ifxT·θ1 θ2+θ0< 0 1 ifxT·θ1 θ2+θ0≥ 0 So we guess that the blueprint for our model ℎx,θ is as described (geometrically, this particular blueprint describes a linear classifier that divides the coordinate plane into two halves).",
    "rticular blueprint describes a linear classifier that divides the coordinate plane into two halves). Then, we want to learn a parameter vector θ such that our model makes the right predictions ( −1 if we perform below average, and 1 otherwise) given an input example x. This model is called a linear perceptron , and it’s a model that’s been used since the 1950s.3 Let’s assume our data is as shown in Figure 3-4 . The Mechanics of Machine Learning | 43 --- Page 60 --- Figure 3-4.",
    "data is as shown in Figure 3-4 . The Mechanics of Machine Learning | 43 --- Page 60 --- Figure 3-4. Sample data for our exam predictor algorithm and a potential classifier Then it turns out that by selecting θ=−24 3 4T, our machine learning model makes the correct prediction on every data point: ℎx,θ=−1 if 3x1+ 4x2− 24 < 0 1 if 3x1+ 4x2− 24 ≥ 0 An optimal parameter vector θ positions the classifier so that we make as many correct predictions as possible.",
    "parameter vector θ positions the classifier so that we make as many correct predictions as possible. In most cases, there are many (or even infinitely many) possible choices for θ that are optimal. Fortunately for us, most of the time these alternatives are so close to one another that the difference is negligible. If this is not the case, we may want to collect more data to narrow our choice of θ. While the setup seems reasonable, there are still some pretty significant questions that remain.",
    "θ. While the setup seems reasonable, there are still some pretty significant questions that remain. First off, how do we even come up with an optimal value for the parameter vector θ in the first place? Solving this problem requires a technique commonly known as optimization . An optimizer aims to maximize the performance of a machine learning model by iteratively tweaking its parameters until the error is minimized.",
    "nce of a machine learning model by iteratively tweaking its parameters until the error is minimized. We’ll begin to tackle this question of learning parameter vectors in 44 | Chapter 3: The Neural Network --- Page 61 --- 4Bubeck, Sébastien. “Convex Optimization: Algorithms and Complexity. ” Foundations and Trends® in Machine Learning . 8.3-4 (2015): 231-357. 5Restak, Richard M. and David Grubin. The Secret Life of the Brain .",
    "arning . 8.3-4 (2015): 231-357. 5Restak, Richard M. and David Grubin. The Secret Life of the Brain . Joseph Henry Press, 2001.more detail in Chapter 4 , when we describe the process of gradient descent .4 In later chapters, we’ll try to find ways to make this process even more efficient. Second, it’s quite clear that this particular model (the linear perceptron model) is quite limited in the relationships it can learn.",
    "s particular model (the linear perceptron model) is quite limited in the relationships it can learn. For example, the distributions of data shown in Figure 3-5 cannot be described well by a linear perceptron. Figure 3-5. As our data takes on more complex forms, we need more complex models to describe them But these situations are only the tip of the iceberg.",
    ", we need more complex models to describe them But these situations are only the tip of the iceberg. As we move on to much more complex problems, such as object recognition and text analysis, our data becomes extremely high dimensional, and the relationships we want to capture become highly nonlinear. To accommodate this complexity, recent research in machine learning has attempted to build models that resemble the structures utilized by our brains.",
    "machine learning has attempted to build models that resemble the structures utilized by our brains. It’s essentially this body of research, commonly referred to as deep learning , that has had spectacular success in tackling problems in computer vision and natural language processing. These algorithms not only far surpass other kinds of machine learning algorithms, but also rival (or even exceed) the accuracies achieved by humans.",
    "s of machine learning algorithms, but also rival (or even exceed) the accuracies achieved by humans. The Neuron The foundational unit of the human brain is the neuron. A tiny piece of the brain, about the size of grain of rice, contains over 10,000 neurons, each of which forms an average of 6,000 connections with other neurons.5 It’s this massive biological network that enables us to experience the world around us.",
    "er neurons.5 It’s this massive biological network that enables us to experience the world around us. Our goal in this section is to use this natural structure to build machine learning models that solve problems in an analogous way. The Neuron | 45 --- Page 62 --- 6McCulloch, Warren S., and Walter Pitts. “ A Logical Calculus of the Ideas Immanent in Nervous Activity. ” The Bulletin of Mathematical Biophysics .",
    "ical Calculus of the Ideas Immanent in Nervous Activity. ” The Bulletin of Mathematical Biophysics . 5.4 (1943): 115-133.At its core, the neuron is optimized to receive information from other neurons, process this information in a unique way, and send its result to other cells. This process is summarized in Figure 3-6 . The neuron receives its inputs along antennae- like structures called dendrites .",
    "ed in Figure 3-6 . The neuron receives its inputs along antennae- like structures called dendrites . Each of these incoming connections is dynamically strengthened or weakened based on how often it is used (this is how we learn new concepts), and it’s the strength of each connection that determines the contribution of the input to the neuron’s output. After being weighted by the strength of their respective connections, the inputs are summed together in the cell body .",
    "d by the strength of their respective connections, the inputs are summed together in the cell body . This sum is then transformed into a new signal that’s propagated along the cell’s axon and sent off to other neurons. Figure 3-6. A functional description of a biological neuron’s structure We can translate this functional understanding of the neurons in our brain into an artificial model that we can represent on our computer.",
    "standing of the neurons in our brain into an artificial model that we can represent on our computer. Such a model is described in Figure 3-7 , leveraging the approach first pioneered in 1943 by Warren S. McCul‐ loch and Walter H. Pitts.6 Just as in biological neurons, our artificial neuron takes in some number of inputs, x1,x2, ...,xn, each of which is multiplied by a specific weight, w1,w2, ...,wn. These weighted inputs are, as before, summed to produce the logit of the neuron, z= ∑i= 0nwixi.",
    ",wn. These weighted inputs are, as before, summed to produce the logit of the neuron, z= ∑i= 0nwixi. In many cases, the logit also includes a bias, which is a constant (not shown in the figure). The logit is then passed through a function f to produce the output y=fz. This output can be transmitted to other neurons. 46 | Chapter 3: The Neural Network --- Page 63 --- Figure 3-7.",
    "can be transmitted to other neurons. 46 | Chapter 3: The Neural Network --- Page 63 --- Figure 3-7. Schematic for a neuron in an artificial neural net We’ll conclude our mathematical discussion of the artificial neuron by re-expressing its functionality in vector form. Let’s reformulate the inputs as a vector x = [x1 x2 … xn] and the weights of the neuron as w = [w1 w2 … wn]. Then we can re-express the output of the neuron as y=fx·w+b, where b is the bias term.",
    "[w1 w2 … wn]. Then we can re-express the output of the neuron as y=fx·w+b, where b is the bias term. We can compute the output by performing the dot product of the input and weight vectors, adding in the bias term to produce the logit, and then applying the transformation function. While this seems like a trivial reformulation, thinking about neurons as a series of vector manipulations will be crucial to how we implement them in software later in this book.",
    "ies of vector manipulations will be crucial to how we implement them in software later in this book. Expressing Linear Perceptrons as Neurons In “The Mechanics of Machine Learning” on page 41, we talked about using machine learning models to capture the relationship between success on exams and time spent studying and sleeping.",
    "ng models to capture the relationship between success on exams and time spent studying and sleeping. To tackle this problem, we constructed a linear perceptron classifier that divided the Cartesian coordinate plane into two halves: ℎx,θ=−1 if 3x1+ 4x2− 24 < 0 1 if 3x1+ 4x2− 24 ≥ 0 As shown in Figure 3-4 , this is an optimal choice for θ because it correctly classifies every sample in our dataset. Here, we show that our model h is easily using a neuron. Consider the neuron depicted in Figure 3-8 .",
    "ere, we show that our model h is easily using a neuron. Consider the neuron depicted in Figure 3-8 . The neuron has two inputs, a bias, and uses the function: fz=−1 ifz< 0 1 ifz≥ 0 It’s easy to show that our linear perceptron and the neuronal model are perfectly equivalent. And in general, it’s quite simple to show that singular neurons are strictly more expressive than linear perceptrons.",
    "t’s quite simple to show that singular neurons are strictly more expressive than linear perceptrons. Every linear perceptron can be expressed as a single neuron, but single neurons can also express models that cannot be expressed by any linear perceptron. Expressing Linear Perceptrons as Neurons | 47 --- Page 64 --- 7Mountcastle, Vernon B. “Modality and Topographic Properties of Single Neurons of Cat’s Somatic Sensory Cortex. ” Journal of Neurophysiology 20.4 (1957): 408-434. Figure 3-8.",
    "rons of Cat’s Somatic Sensory Cortex. ” Journal of Neurophysiology 20.4 (1957): 408-434. Figure 3-8. Expressing our exam performance perceptron as a neuron Feed-Forward Neural Networks Although single neurons are more powerful than linear perceptrons, they’re not nearly expressive enough to solve complicated learning problems. There’s a reason our brain is made of more than one neuron. For example, it is impossible for a single neuron to differentiate handwritten digits.",
    "n one neuron. For example, it is impossible for a single neuron to differentiate handwritten digits. So to tackle much more complicated tasks, we’ll have to take our machine learning model even further. The neurons in the human brain are organized in layers. In fact, the human cerebral cortex (the structure responsible for most of human intelligence) is made up of six layers.7 Information flows from one layer to another until sensory input is converted into conceptual understanding.",
    "tion flows from one layer to another until sensory input is converted into conceptual understanding. For example, the bottommost layer of the visual cortex receives raw visual data from the eyes. This information is processed by each layer and passed on to the next until, in the sixth layer, we conclude whether we are looking at a cat, or a soda can, or an airplane. Borrowing from these concepts, we can construct an artificial neural network .",
    "can, or an airplane. Borrowing from these concepts, we can construct an artificial neural network . A neural network comes about when we start hooking up neurons to each other, the input data, and to the output nodes, which correspond to the network’s answer to a learning problem. Figure 3-9 demonstrates a simple example of an artificial neural network, similar to the architecture described in McCulloch and Pitt’s work in 1943. 48 | Chapter 3: The Neural Network --- Page 65 --- Figure 3-9.",
    "in McCulloch and Pitt’s work in 1943. 48 | Chapter 3: The Neural Network --- Page 65 --- Figure 3-9. A feed-forward neural network with three layers (input, one hidden, and output) and three neurons per layer The bottom layer of the network pulls in the input data. The top layer of neurons (output nodes) computes our final answer.",
    "network pulls in the input data. The top layer of neurons (output nodes) computes our final answer. The middle layer(s) of neurons are called the hidden layers , and we let wi,jk be the weight of the connection between the itℎ neuron in the ktℎ layer with the jtℎ neuron in the k+ 1st layer. These weights constitute our parameter vector, θ, and just as before, our ability to solve problems with neural networks depends on finding the optimal values to plug into θ.",
    "ability to solve problems with neural networks depends on finding the optimal values to plug into θ. We note that in this example, connections traverse only from a lower layer to a higher layer. There are no connections between neurons in the same layer, and there are no connections that transmit data from a higher layer to a lower layer. These neural networks are called feed-forward networks, and we start by discussing these networks because they are the simplest to analyze.",
    "orward networks, and we start by discussing these networks because they are the simplest to analyze. We present this analysis Feed-Forward Neural Networks | 49 --- Page 66 --- (specifically, the process of selecting the optimal values for the weights) in Chapter 4 . More complicated connectivities will be addressed in later chapters.",
    "for the weights) in Chapter 4 . More complicated connectivities will be addressed in later chapters. We’ll discuss the major types of layers that are utilized in feed-forward neural net‐ works, but before we proceed, here’s a couple of important notes to keep in mind: 1.As we mentioned, the layers of neurons that lie sandwiched between the first layer of neurons (input layer) and the last layer of neurons (output layer) are called the hidden layers.",
    "of neurons (input layer) and the last layer of neurons (output layer) are called the hidden layers. This is where most of the magic is happening when the neural net tries to solve problems. Whereas (as in the handwritten digit example) we would previously have to spend a lot of time identifying useful features, the hidden layers automate this process for us.",
    "to spend a lot of time identifying useful features, the hidden layers automate this process for us. Oftentimes, taking a look at the activities of hidden layers can tell you a lot about the features the network has automatically learned to extract from the data. 2.Although in Figure 3-9 every layer has the same number of neurons, this is neither necessary nor recommended.",
    "in Figure 3-9 every layer has the same number of neurons, this is neither necessary nor recommended. More often than not, hidden layers have fewer neurons than the input layer to force the network to learn compressed representations of the original input. For example, while our eyes obtain raw pixel values from our surroundings, our brain thinks in terms of edges and contours.",
    "eyes obtain raw pixel values from our surroundings, our brain thinks in terms of edges and contours. This is because the hidden layers of biological neurons in our brain, force us to come up with better representations for everything we perceive. 3.It is not required that every neuron has its output connected to the inputs of all neurons in the next layer. In fact, selecting which neurons to connect to which other neurons in the next layer is an art that comes from experience.",
    "ch neurons to connect to which other neurons in the next layer is an art that comes from experience. We’ll discuss this issue in more depth as we work through various examples of neural networks. 4.The inputs and outputs are vectorized representations. For example, you might imagine a neural network where the inputs are the individual pixel RGB values in an image represented as a vector (refer to Figure 3-3 ).",
    "puts are the individual pixel RGB values in an image represented as a vector (refer to Figure 3-3 ). The last layer might have two neurons that correspond to the answer to our problem: 1, 0 if the image contains a dog, 0, 1 if the image contains a cat, 1, 1 if it contains both, and 0, 0 if it contains neither. We’ll also observe that, similarly to our reformulation for the neuron, we can also mathematically express a neural network as a series of vector and matrix operations.",
    "on, we can also mathematically express a neural network as a series of vector and matrix operations. Let’s consider the input to the itℎ layer of the network to be a vector x = [x1 x2 … xn]. We’ d like to find the vector y = [y1 y2 … ym] produced by propagating the input through the neurons. We can express this as a simple matrix multiply if we construct a weight matrix W of size n×m and a bias vector of size m.",
    "a simple matrix multiply if we construct a weight matrix W of size n×m and a bias vector of size m. In this matrix, each column corresponds to a neuron, where the jtℎ element of the column corresponds to the weight of the connection pulling in the jtℎ element of the input. In other words, y = ƒ( WTx + b), where the transformation function is applied to the vector 50 | Chapter 3: The Neural Network --- Page 67 --- element-wise.",
    "n function is applied to the vector 50 | Chapter 3: The Neural Network --- Page 67 --- element-wise. This reformulation will become all the more critical as we begin to implement these networks in software. Linear Neurons and Their Limitations Most neuron types are defined by the function f they apply to their logit z. Let’s first consider layers of neurons that use a linear function in the form of fz=az+b.",
    "r logit z. Let’s first consider layers of neurons that use a linear function in the form of fz=az+b. For example, a neuron that attempts to estimate a cost of a meal in a fast-food restaurant would use a linear neuron where a= 1 and b= 0. Using fz=z and weights equal to the price of each item, the linear neuron in Figure 3-10 would take in some ordered triple of servings of burgers, fries, and sodas, and output the price of the combination. Figure 3-10.",
    "iple of servings of burgers, fries, and sodas, and output the price of the combination. Figure 3-10. An example of a linear neuron Linear neurons are easy to compute with, but they run into serious limitations. In fact, it can be shown that any feed-forward neural network consisting of only linear neurons can be expressed as a network with no hidden layers. This is problematic because, as we discussed, hidden layers are what enable us to learn important features from the input data.",
    ", as we discussed, hidden layers are what enable us to learn important features from the input data. In other words, to learn complex relationships, we need to use neurons that employ some sort of nonlinearity. Sigmoid, Tanh, and ReLU Neurons Three major types of neurons are used in practice that introduce nonlinearities in their computations.",
    "ree major types of neurons are used in practice that introduce nonlinearities in their computations. The first of these is the sigmoid neuron , which uses the function: fz=1 1 +e−z Intuitively, this means that when the logit is very small, the output of a logistic neuron is close to 0. When the logit is very large, the output of the logistic neuron is Linear Neurons and Their Limitations | 51 --- Page 68 --- 8Nair, Vinod, and Geoffrey E. Hinton.",
    "n is Linear Neurons and Their Limitations | 51 --- Page 68 --- 8Nair, Vinod, and Geoffrey E. Hinton. “Rectified Linear Units Improve Restricted Boltzmann Machines. ” Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010.close to 1. In-between these two extremes, the neuron assumes an S-shape, as shown in Figure 3-11 . Figure 3-11.",
    "In-between these two extremes, the neuron assumes an S-shape, as shown in Figure 3-11 . Figure 3-11. The output of a sigmoid neuron as z varies Tanh neurons use a similar kind of S-shaped nonlinearity, but instead of ranging from 0 to 1, the output of tanh neurons ranges from −1 to 1. As you would expect, they use fz= tanhz. The resulting relationship between the output y and the logit z is depicted in Figure 3-12 .",
    "tanhz. The resulting relationship between the output y and the logit z is depicted in Figure 3-12 . When S-shaped nonlinearities are used, the tanh neuron is often preferred over the sigmoid neuron because it is zero-centered. A different kind of nonlinearity is used by the Rectified Linear Unit (ReLU) neuron . It uses the function fz= max 0,z, resulting in a characteristic hockey-stick-shaped response, as shown in Figure 3-13 .",
    "n fz= max 0,z, resulting in a characteristic hockey-stick-shaped response, as shown in Figure 3-13 . The ReLU has recently become the neuron of choice for many tasks (especially in computer vision) for a number of reasons, despite some drawbacks.8 We’ll discuss these reasons in Chapter 7 , as well as strategies to combat the potential pitfalls. 52 | Chapter 3: The Neural Network --- Page 69 --- Figure 3-12. The output of a tanh neuron as z varies Figure 3-13.",
    "The Neural Network --- Page 69 --- Figure 3-12. The output of a tanh neuron as z varies Figure 3-13. The output of a ReLU neuron as z varies Sigmoid, Tanh, and ReLU Neurons | 53 --- Page 70 --- Softmax Output Layers Oftentimes, we want our output vector to be a probability distribution over a set of mutually exclusive labels. For example, let’s say we want to build a neural network to recognize handwritten digits from the MNIST dataset.",
    "let’s say we want to build a neural network to recognize handwritten digits from the MNIST dataset. Each label (0 through 9) is mutually exclusive, but it’s unlikely that we will be able to recognize digits with 100% confidence. Using a probability distribution gives us a better idea of how confident we are in our predictions. As a result, the desired output vector is of the following form, where ∑i= 09pi= 1: p0p1p2p3...p9 This is achieved by using a special output layer called a softmax layer .",
    "∑i= 09pi= 1: p0p1p2p3...p9 This is achieved by using a special output layer called a softmax layer . Unlike in other kinds of layers, the output of a neuron in a softmax layer depends on the outputs of all the other neurons in its layer. This is because we require the sum of all the outputs to be equal to 1.",
    "other neurons in its layer. This is because we require the sum of all the outputs to be equal to 1. Letting zi be the logit of the itℎ softmax neuron, we can achieve this normalization by setting its output to: yi=ezi ∑jezj A strong prediction would have a single entry in the vector close to 1, while the remaining entries would be close to 0. A weak prediction would have multiple possible labels that are more or less equally likely.",
    "se to 0. A weak prediction would have multiple possible labels that are more or less equally likely. Summary In this chapter, we’ve built a basic intuition for machine learning and neural net‐ works. We’ve talked about the basic structure of a neuron, how feed-forward neural networks work, and the importance of nonlinearity in tackling complex learning problems. In the next chapter, we will begin to build the mathematical background necessary to train a neural network to solve problems.",
    "ll begin to build the mathematical background necessary to train a neural network to solve problems. Specifically, we will talk about finding optimal parameter vectors, best practices while training neural networks, and major challenges. In later chapters, we will take these foundational ideas to build more specialized neural architectures.",
    "ater chapters, we will take these foundational ideas to build more specialized neural architectures. 54 | Chapter 3: The Neural Network --- Page 71 --- CHAPTER 4 Training Feed-Forward Neural Networks The Fast-Food Problem We’re beginning to understand how we can tackle some interesting problems using deep learning, but one big question still remains: how exactly do we figure out what the parameter vectors (the weights for all of the connections in our neural network) should be?",
    "what the parameter vectors (the weights for all of the connections in our neural network) should be? This is accomplished by a process commonly referred to as training (see Figure 4-1 ). During training, we show the neural net a large number of training examples and iteratively modify the weights to minimize the errors we make on the training examples. After enough examples, we expect that our neural network will be quite effective at solving the task it’s been trained to do. Figure 4-1.",
    "our neural network will be quite effective at solving the task it’s been trained to do. Figure 4-1. This is the neuron we want to train for the fast-food problem Let’s continue with an example from Chapter 3 involving a linear neuron: every single day, we purchase a restaurant meal consisting of burgers, fries, and sodas. We buy some number of servings for each item. We want to be able to predict how much a meal is going to cost us, but the items don’t have price tags.",
    "want to be able to predict how much a meal is going to cost us, but the items don’t have price tags. The only thing the cashier 55 --- Page 72 --- will tell us is the total price of the meal. We want to train a single linear neuron to solve this problem. How do we do it? One idea is to be intelligent about picking our training cases.",
    "solve this problem. How do we do it? One idea is to be intelligent about picking our training cases. For one meal we could buy only a single serving of burgers, for another we could buy only a single serving of fries, and then for our last meal we could buy a single serving of soda. In general, intelligently selecting training examples is a good idea. Lots of research shows that by engineering a clever training set, you can make your neural network a lot more effective.",
    "ws that by engineering a clever training set, you can make your neural network a lot more effective. The issue with using this approach alone is that in real situations, it rarely ever gets you close to the solution. For example, there’s no clear analog of this strategy in image recognition. It’s just not a practical solution. Instead, we try to motivate a solution that works well in general. Let’s say we have a large set of training examples.",
    "motivate a solution that works well in general. Let’s say we have a large set of training examples. Then we can calculate what the neural network will output on the itℎ training example using the simple formula in the diagram. We want to train the neuron so that we pick the most optimal weights possible—the weights that minimize the errors we make on the training examples. In this case, let’s say we want to minimize the square error over all of the training examples that we encounter.",
    "let’s say we want to minimize the square error over all of the training examples that we encounter. More formally, if we know that ti is the true answer for the itℎ training example, and yi is the value computed by the neural network, we want to minimize the value of the error function E: E=1 2∑iti−yi2 The squared error is zero when our model makes a perfectly correct prediction on every training example. Moreover, the closer E is to 0, the better our model is.",
    "rrect prediction on every training example. Moreover, the closer E is to 0, the better our model is. As a result, our goal is to select our parameter vector θ (the values for all the weights in our model) such that E is as close to 0 as possible. Now at this point you might be wondering why we need to bother ourselves with error functions when we can treat this problem as a system of equations.",
    "ed to bother ourselves with error functions when we can treat this problem as a system of equations. After all, we have a bunch of unknowns (weights) and we have a set of equations (one for each training example). That would automatically give us an error of 0, assuming that we have a consistent set of training examples. That’s a smart observation, but the insight unfortunately doesn’t generalize well.",
    "raining examples. That’s a smart observation, but the insight unfortunately doesn’t generalize well. Remember that although we’re using a linear neuron here, linear neurons aren’t used very much in practice because they’re constrained in what they can learn. And the moment we start using nonlinear neurons like the sigmoidal, tanh, or ReLU neurons we talked about at the end of Chapter 3 , we can no longer set up a system of linear equations.",
    "rons we talked about at the end of Chapter 3 , we can no longer set up a system of linear equations. Clearly, we need a better strategy to tackle the training process. 56 | Chapter 4: Training Feed-Forward Neural Networks --- Page 73 --- Gradient Descent Let’s visualize how we might minimize the squared error over all of the training examples by simplifying the problem. Say our linear neuron has only two inputs (and thus only two weights, w1 and w2).",
    "fying the problem. Say our linear neuron has only two inputs (and thus only two weights, w1 and w2). Then we can imagine a 3D space where the horizontal dimensions correspond to the weights w1 and w2, and the vertical dimension corresponds to the value of the error function E. In this space, points in the horizontal plane correspond to different settings of the weights, and the height at those points corresponds to the incurred error.",
    "different settings of the weights, and the height at those points corresponds to the incurred error. If we consider the errors we make over all possible weights, we get a surface in this 3D space, in particular, a quadratic bowl as shown in Figure 4-2 . Figure 4-2. The quadratic error surface for a linear neuron We can also conveniently visualize this surface as a set of elliptical contours, where the minimum error is at the center of the ellipses.",
    "s surface as a set of elliptical contours, where the minimum error is at the center of the ellipses. In this setup, we are working in a 2D plane where the dimensions correspond to the two weights. Contours correspond to settings of w1 and w2 that evaluate to the same value of E. The closer the contours are to each other, the steeper the slope. In fact, it turns out that the direction of the steepest descent is always perpendicular to the contours.",
    "ct, it turns out that the direction of the steepest descent is always perpendicular to the contours. This direction is expressed as a vector known as the gradient . Now we can develop a high-level strategy for how to find the values of the weights that minimizes the error function. Suppose we randomly initialize the weights of our network so we find ourselves somewhere on the horizontal plane.",
    "ndomly initialize the weights of our network so we find ourselves somewhere on the horizontal plane. By evaluating the gradient at our current position, we can find the direction of steepest descent, and we can take a step in that direction. Then we’ll find ourselves at a new position that’s closer to the minimum than we were before.",
    "ction. Then we’ll find ourselves at a new position that’s closer to the minimum than we were before. We can reevaluate the direction of steepest descent by taking the gradient at this new position and taking a step in this Gradient Descent | 57 --- Page 74 --- 1Rosenbloom, P . “The Method of Steepest Descent. ” Proceedings of Symposia in Applied Mathematics . Vol. 6. 1956.new direction.",
    "of Steepest Descent. ” Proceedings of Symposia in Applied Mathematics . Vol. 6. 1956.new direction. It’s easy to see that, as shown in Figure 4-3 , following this strategy will eventually get us to the point of minimum error. This algorithm is known as gradient descent , and we’ll use it to tackle the problem of training individual neurons and the more general challenge of training entire networks.1 Figure 4-3.",
    "training individual neurons and the more general challenge of training entire networks.1 Figure 4-3. Visualizing the error surface as a set of contours The Delta Rule and Learning Rates Before we derive the exact algorithm for training our fast-food neuron, we have a quick note on hyperparameters . In addition to the weight parameters defined in our neural network, learning algorithms also require a couple of additional parameters to carry out the training process.",
    "earning algorithms also require a couple of additional parameters to carry out the training process. One of these so-called hyperparameters is the learning rate. In practice, at each step of moving perpendicular to the contour, we need to deter‐ mine how far we want to walk before recalculating our new direction. This distance needs to depend on the steepness of the surface. Why? The closer we are to the mini‐ mum, the shorter we want to step forward.",
    "epness of the surface. Why? The closer we are to the mini‐ mum, the shorter we want to step forward. We know we are close to the minimum because the surface is a lot flatter, so we can use the steepness as an indicator of how close we are to the minimum. However, if our error surface is rather mellow, training can potentially take a large amount of time.",
    "owever, if our error surface is rather mellow, training can potentially take a large amount of time. As a result, we often multiply the 58 | Chapter 4: Training Feed-Forward Neural Networks --- Page 75 --- gradient by a factor ϵ, the learning rate. Picking the learning rate is a hard problem (Figure 4-4 ). As we just discussed, if we pick a learning rate that’s too small, we risk taking too long during the training process.",
    "d, if we pick a learning rate that’s too small, we risk taking too long during the training process. But if we pick a learning rate that’s too big, we’ll mostly likely start diverging away from the minimum. In Chapter 5 , we’ll learn about various optimization techniques that utilize adaptive learning rates to automate the process of selecting learning rates. Figure 4-4.",
    "hat utilize adaptive learning rates to automate the process of selecting learning rates. Figure 4-4. Convergence is difficult when our learning rate is too large Now, we are finally ready to derive the delta rule for training our linear neuron. In order to calculate how to change each weight, we evaluate the gradient, which is essentially the partial derivative of the error function with respect to each of the weights.",
    "ich is essentially the partial derivative of the error function with respect to each of the weights. In other words, we want: Δwk= −ϵ∂E ∂wk = −ϵ∂ ∂wk1 2∑iti−yi2 = ∑iϵti−yi∂yi ∂wk = ∑iϵxkiti−yi The Delta Rule and Learning Rates | 59 --- Page 76 --- Applying this method of changing the weights at every iteration, we are finally able to utilize gradient descent.",
    "method of changing the weights at every iteration, we are finally able to utilize gradient descent. Gradient Descent with Sigmoidal Neurons In this section and the next, we will deal with training neurons and neural networks that utilize nonlinearities. We use the sigmoidal neuron as a model, and leave the derivations for other nonlinear neurons as an exercise for you. For simplicity, we assume that the neurons do not use a bias term, although our analysis easily extends to this case.",
    "e assume that the neurons do not use a bias term, although our analysis easily extends to this case. We merely need to assume that the bias is a weight on an incoming connection whose input value is always one. Let’s recall the mechanism by which logistic neurons compute their output value from their inputs: z= ∑kwkxk y=1 1 +e−z The neuron computes the weighted sum of its inputs, the logit z. It then feeds its logit into the input function to compute y, its final output.",
    "inputs, the logit z. It then feeds its logit into the input function to compute y, its final output. Fortunately for us, these functions have nice derivatives, which makes learning easy! For learning, we want to compute the gradient of the error function with respect to the weights.",
    "sy! For learning, we want to compute the gradient of the error function with respect to the weights. To do so, we start by taking the derivative of the logit with respect to the inputs and the weights: ∂z ∂wk=xk ∂z ∂xk=wk Also, quite surprisingly, the derivative of the output with respect to the logit is quite simple if you express it in terms of the output: dy dz=e−z 1 +e−z2 =1 1 +e−ze−z 1 +e−z =1 1 +e−z1 −1 1 +e−z =y1 −y 60 | Chapter 4: Training Feed-Forward Neural Networks --- Page 77 --- 2Rumelhart, David E., Geoffrey E.",
    "| Chapter 4: Training Feed-Forward Neural Networks --- Page 77 --- 2Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. “Learning Representations by Back- Propagating Errors.",
    ", Geoffrey E. Hinton, and Ronald J. Williams. “Learning Representations by Back- Propagating Errors. ” Cognitive Modeling 5.3 (1988): 1.We then use the chain rule to get the derivative of the output with respect to each weight: ∂y ∂wk=dy dz∂z ∂wk=xky1 −y Putting all of this together, we can now compute the derivative of the error function with respect to each weight: ∂E ∂wk= ∑i∂E ∂yi∂yi ∂wk= − ∑ixkiyi1 −yiti−yi Thus, the final rule for modifying the weights becomes: Δwk= ∑iϵxkiyi1 −yiti−yi As you may notice, the new modification rule is just like the delta rule, except with extra multiplicative terms included to account for the logistic component of the sigmoidal neuron.",
    "h extra multiplicative terms included to account for the logistic component of the sigmoidal neuron. The Backpropagation Algorithm Now we’re finally ready to tackle the problem of training multilayer neural networks (instead of just single neurons). To accomplish this task, we’ll use an approach known as backpropagation , pioneered by David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams in 1986.2 So what’s the idea behind backpropagation?",
    "art, Geoffrey E. Hinton, and Ronald J. Williams in 1986.2 So what’s the idea behind backpropagation? We don’t know what the hidden units ought to be doing, but what we can do is compute how fast the error changes as we change a hidden activity. From there, we can figure out how fast the error changes when we change the weight of an individual connection. Essentially, we’ll be trying to find the path of steepest descent.",
    "ight of an individual connection. Essentially, we’ll be trying to find the path of steepest descent. The only catch is that we’re going to be working in an extremely high-dimensional space. We start by calculating the error derivatives with respect to a single training example. Each hidden unit can affect many output units. Thus, we’ll have to combine many separate effects on the error in an informative way. Our strategy will be one of dynamic programming.",
    "eparate effects on the error in an informative way. Our strategy will be one of dynamic programming. Once we have the error derivatives for one layer of hidden units, we’ll use them to compute the error derivatives for the activities of the layer below. And once we find the error derivatives for the activities of the hidden units, it’s The Backpropagation Algorithm | 61 --- Page 78 --- quite easy to get the error derivatives for the weights leading into a hidden unit.",
    "--- Page 78 --- quite easy to get the error derivatives for the weights leading into a hidden unit. We’ll redefine some notation for ease of discussion and refer to Figure 4-5 . Figure 4-5. Reference diagram for the derivation of the backpropagation algorithm The subscript we use will refer to the layer of the neuron. The symbol y will refer to the activity of a neuron, as usual. Similarly, the symbol z will refer to the logit of the neuron.",
    "o the activity of a neuron, as usual. Similarly, the symbol z will refer to the logit of the neuron. We start by taking a look at the base case of the dynamic programming problem. Specifically, we calculate the error function derivatives at the output layer: E=1 2∑j ∈ output tj−yj2 ∂E ∂yj= −tj−yj Now we tackle the inductive step. Let’s presume we have the error derivatives for layer j. We next aim to calculate the error derivatives for the layer below it, layer i.",
    "vatives for layer j. We next aim to calculate the error derivatives for the layer below it, layer i. To do so, we must accumulate information about how the output of a neuron in layer i affects the logits of every neuron in layer j.",
    "formation about how the output of a neuron in layer i affects the logits of every neuron in layer j. This can be done as follows, using the fact that the partial derivative of the logit with respect to the incoming output data from the layer beneath is merely the weight of the connection wij: ∂E ∂yi= ∑j∂E ∂zjdzj dyi= ∑jwij∂E ∂zj 62 | Chapter 4: Training Feed-Forward Neural Networks --- Page 79 --- Furthermore, we observe the following: ∂E ∂zj=∂E ∂yjdyj dzj=yj1 −yj∂E ∂yj Combining these two, we can finally express the error derivatives of layer i in terms of the error derivatives of layer j: ∂E ∂yi= ∑jwijyj1 −yj∂E ∂yj Once we’ve gone through the whole dynamic programming routine, having filled up the table appropriately with all of our partial derivatives (of the error function with respect to the hidden unit activities), we can then determine how the error changes with respect to the weights.",
    "he hidden unit activities), we can then determine how the error changes with respect to the weights. This gives us a way to modify the weights after each training example: ∂E ∂wij=∂zj ∂wij∂E ∂zj=yiyj1 −yj∂E ∂yj Finally, to complete the algorithm, just as before, we merely sum up the partial derivatives over all the training examples in our dataset.",
    "t as before, we merely sum up the partial derivatives over all the training examples in our dataset. This gives us the following modification formula: Δwij= − ∑k ∈ dataset ϵyikyjk1 −yjk∂Ek ∂yjk This completes our description of the backpropagation algorithm. Stochastic and Minibatch Gradient Descent In the algorithms we described in “The Backpropagation Algorithm” on page 61, we used a version of gradient descent known as batch gradient descent .",
    "ation Algorithm” on page 61, we used a version of gradient descent known as batch gradient descent . The idea behind batch gradient descent is that we use our entire dataset to compute the error surface and then follow the gradient to take the path of steepest descent. For a simple quadratic error surface, this works quite well. But in most cases, our error surface may be a lot more complicated. Let’s consider the scenario in Figure 4-6 .",
    "cases, our error surface may be a lot more complicated. Let’s consider the scenario in Figure 4-6 . Stochastic and Minibatch Gradient Descent | 63 --- Page 80 --- Figure 4-6. Batch gradient descent is sensitive to saddle points, which can lead to premature convergence We have only a single weight, and we use random initialization and batch gradient descent to find its optimal setting.",
    "gle weight, and we use random initialization and batch gradient descent to find its optimal setting. The error surface, however, has a flat region (also known as saddle point in high-dimensional spaces), and if we get unlucky, we might find ourselves getting stuck while performing gradient descent. Another potential approach is stochastic gradient descent (SGD), where at each iter‐ ation, our error surface is estimated with respect to only a single example.",
    "D), where at each iter‐ ation, our error surface is estimated with respect to only a single example. This approach is illustrated by Figure 4-7 , where instead of a single static error surface, our error surface is dynamic. As a result, descending on this stochastic surface significantly improves our ability to navigate flat regions. Figure 4-7.",
    "on this stochastic surface significantly improves our ability to navigate flat regions. Figure 4-7. The stochastic error surface fluctuates with respect to the batch error surface, enabling saddle point avoidance 64 | Chapter 4: Training Feed-Forward Neural Networks --- Page 81 --- The major pitfall of SGD, however, is that looking at the error incurred one example at a time may not be a good enough approximation of the error surface.",
    "he error incurred one example at a time may not be a good enough approximation of the error surface. This, in turn, could potentially make gradient descent take a significant amount of time. One way to combat this problem is using minibatch gradient descent . In minibatch gradient descent, at every iteration we compute the error surface with respect to some subset of the total dataset (instead of just a single example).",
    "e error surface with respect to some subset of the total dataset (instead of just a single example). This subset is called a minibatch , and in addition to the learning rate, minibatch size is another hyperparameter. Minibatches strike a balance between the efficiency of batch gradient descent and the local-minima avoidance afforded by stochastic gradient descent.",
    "cy of batch gradient descent and the local-minima avoidance afforded by stochastic gradient descent. In the context of backpropagation, our weight update step becomes: Δwij= − ∑k ∈ minibatc ℎϵyikyjk1 −yjk∂Ek ∂yjk This is identical to what we derived in the previous section, but instead of summing over all the examples in the dataset, we sum over the examples in the current minibatch.",
    "of summing over all the examples in the dataset, we sum over the examples in the current minibatch. For a more theoretical discussion of why SGD and minibatch gradient descent result in an unbiased estimate of the gradient over the total dataset, please refer to “Neural Net Learning Theory” on page 74 . Test Sets, Validation Sets, and Overfitting One of the major issues with artificial neural networks is that the models are quite complicated.",
    "ng One of the major issues with artificial neural networks is that the models are quite complicated. For example, let’s consider a neural network that pulls data from an image from the MNIST database (28 × 28 pixels), feeds into two hidden layers with 30 neurons, and finally reaches a softmax layer of 10 neurons. The total number of parameters in the network is nearly 25,000. This can be quite problematic, and to understand why, let’s consider a new toy example, illustrated in Figure 4-8 .",
    "te problematic, and to understand why, let’s consider a new toy example, illustrated in Figure 4-8 . Test Sets, Validation Sets, and Overfitting | 65 --- Page 82 --- Figure 4-8. Two potential models that might describe our dataset: a linear model versus a degree 12 polynomial We are given a bunch of data points on a flat plane, and our goal is to find a curve that best describes this dataset (i.e., will allow us to predict the y coordinate of a new point given its x coordinate).",
    "his dataset (i.e., will allow us to predict the y coordinate of a new point given its x coordinate). Using the data, we train two different models: a linear model and a degree 12 polynomial. Which curve should we trust? The line that gets almost no training example correct? Or the complicated curve that hits every single point in the dataset? At this point we might trust the linear fit because it seems much less contrived. But just to be sure, let’s add more data to our dataset.",
    "r fit because it seems much less contrived. But just to be sure, let’s add more data to our dataset. The result is shown in Figure 4-9 . Now the verdict is clear: the linear model is not only better subjectively but also quantitatively (measured using the squared error metric). This leads to an interesting point about training and evaluating machine learning models.",
    "r metric). This leads to an interesting point about training and evaluating machine learning models. By building a very complex model, it’s quite easy to perfectly fit our training dataset because we give our model enough degrees of freedom to contort itself to fit the observations in the training set. But when we evaluate such a complex model on new data, it performs poorly. In other words, the model does not generalize well.",
    "a complex model on new data, it performs poorly. In other words, the model does not generalize well. This is a phenomenon called overfitting , and it is one of the biggest challenges that a machine learning engineer must combat. This becomes an even more significant issue in deep learning, where our neural networks have large numbers of layers containing many neurons.",
    "ue in deep learning, where our neural networks have large numbers of layers containing many neurons. 66 | Chapter 4: Training Feed-Forward Neural Networks --- Page 83 --- The number of connections in these models is astronomical, reaching the millions. As a result, overfitting is commonplace. Figure 4-9. Evaluating our model on new data indicates that the linear fit is a much better model than the degree 12 polynomial Let’s see how this looks in the context of a neural network.",
    "ter model than the degree 12 polynomial Let’s see how this looks in the context of a neural network. Say we have a neural network with two inputs, a softmax output of size 2, and a hidden layer with 3, 6, or 20 neurons. We train these networks using minibatch gradient descent (batch size 10), and the results, visualized using ConvNetJS , are shown in Figure 4-10 . Figure 4-10.",
    "atch size 10), and the results, visualized using ConvNetJS , are shown in Figure 4-10 . Figure 4-10. A visualization of neural networks with 3, 6, and 20 neurons (in that order) in their hidden layer Test Sets, Validation Sets, and Overfitting | 67 --- Page 84 --- It’s already quite apparent from these images that as the number of connections in our network increases, so does our propensity to overfit to the data.",
    "s the number of connections in our network increases, so does our propensity to overfit to the data. We can similarly see the phenomenon of overfitting as we make our neural networks deep. These results are shown in Figure 4-11 . Figure 4-11. Neural networks with one, two, and four hidden layers (in that order) of three neurons each This leads to three major observations. First, the machine learning engineer is always working with a direct trade-off between overfitting and model complexity.",
    "earning engineer is always working with a direct trade-off between overfitting and model complexity. If the model isn’t complex enough, it may not be powerful enough to capture all of the useful information necessary to solve a problem. However, if our model is very complex (especially if we have a limited amount of data at our disposal), we run the risk of overfitting.",
    "ex (especially if we have a limited amount of data at our disposal), we run the risk of overfitting. Deep learning takes the approach of solving complex problems with complex models and taking additional countermeasures to prevent overfitting. We’ll see a lot of these measures in this and later chapters. Second, it is misleading to evaluate a model using the data we used to train it.",
    "and later chapters. Second, it is misleading to evaluate a model using the data we used to train it. Using the example in Figure 4-8 , this would falsely suggest that the degree 12 polynomial model is preferable to a linear fit. As a result, we almost never train our model on the entire dataset. Instead, we split up our data into a training set and a test set (Figure 4-12 ).",
    "the entire dataset. Instead, we split up our data into a training set and a test set (Figure 4-12 ). This enables us to make a fair evaluation of our model by directly measuring how well it generalizes on new data it has not yet seen. In the real world, large datasets are hard to come by, so it might seem like a waste to not use all of the data at our disposal during the training process. Consequently, it may be tempting to reuse training data for testing or cut corners while compiling test data.",
    "tly, it may be tempting to reuse training data for testing or cut corners while compiling test data. Be forewarned: if the test set isn’t well constructed, we won’t be able draw any meaningful conclusions about our model. 68 | Chapter 4: Training Feed-Forward Neural Networks --- Page 85 --- Figure 4-12.",
    "about our model. 68 | Chapter 4: Training Feed-Forward Neural Networks --- Page 85 --- Figure 4-12. Nonoverlapping training and test sets Third, it’s quite likely that while we’re training our data, there’s a point in time when instead of learning useful features, we start overfitting to the training set. To avoid that, we want to be able to stop the training process as soon as we start overfitting to prevent poor generalization. To do this, we divide our training process into epochs .",
    "overfitting to prevent poor generalization. To do this, we divide our training process into epochs . An epoch is a single iteration over the entire training set. If we have a training set of size d and we are doing minibatch gradient descent with batch size b, then an epoch would be equivalent to d b model updates. At the end of each epoch, we want to measure how well our model is generalizing. To do this, we use an additional validation set , which is shown in Figure 4-13 . Figure 4-13.",
    "zing. To do this, we use an additional validation set , which is shown in Figure 4-13 . Figure 4-13. A validation set to prevent overfitting during the training process At the end of an epoch, the validation set will tell us how the model does on data it has yet to see. If the accuracy on the training set continues to increase while the accuracy on the validation set stays the same (or decreases), it’s a good sign that it’s time to stop training because we’re overfitting.",
    "the same (or decreases), it’s a good sign that it’s time to stop training because we’re overfitting. The validation set is also helpful as a proxy measure of accuracy during the process of hyperparameter optimization . We’ve covered several hyperparameters so far (learning rate, minibatch size, etc.), but we have yet to develop a framework for how to find the optimal values for these hyperparameters.",
    "but we have yet to develop a framework for how to find the optimal values for these hyperparameters. One potential way to find the optimal setting of hyperparameters is by applying a grid search, where we pick a value for each hyperparameter from a finite set of options (e.g., ϵ ∈ 0 . 001, 0 . 01, 0 . 1 , batch size ∈16, 64, 128 , ...), and train the model with every possible permutation of hyperparameter choices.",
    "∈16, 64, 128 , ...), and train the model with every possible permutation of hyperparameter choices. We elect the combination Test Sets, Validation Sets, and Overfitting | 69 --- Page 86 --- 3Nelder, John A., and Roger Mead. “ A Simplex Method for Function Minimization.",
    "| 69 --- Page 86 --- 3Nelder, John A., and Roger Mead. “ A Simplex Method for Function Minimization. ” The Computer Journal 7.4 (1965): 308-313.of hyperparameters with the best performance on the validation set and report the accuracy of the model trained with the best combination on the test set.3 With this in mind, before we jump into describing the various ways to directly combat overfitting, let’s outline the workflow we use when building and training deep learning models.",
    "mbat overfitting, let’s outline the workflow we use when building and training deep learning models. The workflow is described in detail in Figure 4-14 . It is a tad intricate, but it’s critical to understand the pipeline to ensure that we’re properly training our neural networks. Figure 4-14. Detailed workflow for training and evaluating a deep learning model First, we define our problem rigorously.",
    "workflow for training and evaluating a deep learning model First, we define our problem rigorously. This involves determining our inputs, the potential outputs, and the vectorized representations of both. For instance, let’s say our goal was to train a deep learning model to identify cancer. Our input would be an RBG image, which can be represented as a vector of pixel values.",
    "ntify cancer. Our input would be an RBG image, which can be represented as a vector of pixel values. Our output would be a probability distribution over three mutually exclusive possibilities: (1) normal, 70 | Chapter 4: Training Feed-Forward Neural Networks --- Page 87 --- (2) benign tumor (a cancer that has yet to metastasize), or (3) malignant tumor (a cancer that has already metastasized to other organs). After we define our problem, we need to build a neural network architecture to solve it.",
    "er organs). After we define our problem, we need to build a neural network architecture to solve it. Our input layer would have to be of appropriate size to accept the raw data from the image, and our output layer would have to be a softmax of size 3. We will also have to define the internal architecture of the network (number of hidden layers, the connectivities, etc.).",
    "define the internal architecture of the network (number of hidden layers, the connectivities, etc.). We’ll further discuss the architecture of image recognition models when we talk about convolutional neural networks in Chapter 6 . At this point, we also want to collect a significant amount of data for training or modeling. This data would probably be in the form of uniformly sized pathological images that have been labeled by a medical expert.",
    "ly be in the form of uniformly sized pathological images that have been labeled by a medical expert. We shuffle and divide this data up into separate training, validation, and test sets. Finally, we’re ready to begin gradient descent. We train the model on our training set for an epoch at a time. At the end of each epoch, we ensure that our error on the training set and validation set is decreasing.",
    "he end of each epoch, we ensure that our error on the training set and validation set is decreasing. When one of these stops improving, we terminate and make sure we’re happy with the model’s performance on the test data. If we’re unsatisfied, we need to rethink our architecture or reconsider whether the data we collect has the information required to make the prediction we’re interested in making.",
    "the data we collect has the information required to make the prediction we’re interested in making. If our training set error stopped improving, we probably need to do a better job of capturing the important features in our data. If our validation set error stopped improving, we probably need to take measures to prevent overfitting.",
    "ur validation set error stopped improving, we probably need to take measures to prevent overfitting. If, however, we are happy with the performance of our model on the training data, then we can measure its performance on the test data, which the model has never seen before this point. If it is unsatisfactory, we need more data in our dataset because the test set seems to consist of example types that weren’t well represented in the training set. Otherwise, we are finished!",
    "sist of example types that weren’t well represented in the training set. Otherwise, we are finished! Preventing Overfitting in Deep Neural Networks Several techniques have been proposed to prevent overfitting during the training process. In this section, we’ll discuss these techniques in detail. One method of combatting overfitting is called regularization . Regularization modi‐ fies the objective function that we minimize by adding additional terms that penalize large weights.",
    "fies the objective function that we minimize by adding additional terms that penalize large weights. We change the objective function so that it becomes Error +λfθ, where fθ grows larger as the components of θ grow larger, and λ is the regulariza‐ tion strength (another hyperparameter). The value we choose for λ determines how much we want to protect against overfitting. A λ= 0 implies that we do not take any measures against the possibility of overfitting.",
    "overfitting. A λ= 0 implies that we do not take any measures against the possibility of overfitting. If λ is too large, then our model will prioritize keeping θ as small as possible over trying to find the parameter values that Preventing Overfitting in Deep Neural Networks | 71 --- Page 88 --- 4Tikhonov, Andrei Nikolaevich, and Vladlen Borisovich Glasko. “Use of the Regularization Method in Non- Linear Problems.",
    "kolaevich, and Vladlen Borisovich Glasko. “Use of the Regularization Method in Non- Linear Problems. ” USSR Computational Mathematics and Mathematical Physics 5.3 (1965): 93-107.perform well on our training set. As a result, choosing λ is a very important task and can require some trial and error. The most common type of regularization in machine learning is L2 regularization .4 It can be implemented by augmenting the error function with the squared magnitude of all weights in the neural network.",
    "ed by augmenting the error function with the squared magnitude of all weights in the neural network. In other words, for every weight w in the neural network, we add 1 2λw2 to the error function. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. This has the appealing property of encouraging the network to use all of its inputs a little rather than using only some of its inputs a lot.",
    "aging the network to use all of its inputs a little rather than using only some of its inputs a lot. Of particular note is that during the gradient descent update, using the L2 regularization ultimately means that every weight is decayed linearly to zero. Because of this phenomenon, L2 regularization is also commonly referred to as weight decay . We can visualize the effects of L2 regularization using ConvNetJS.",
    "nly referred to as weight decay . We can visualize the effects of L2 regularization using ConvNetJS. Similar to Figures 2-10 and 2-11 , we use a neural network with 2 inputs, a softmax output of size 2, and a hidden layer with 20 neurons. We train the networks using minibatch gradient descent (batch size 10) and regularization strengths of 0.01, 0.1, and 1. The results can be seen in Figure 4-15 . Figure 4-15.",
    "regularization strengths of 0.01, 0.1, and 1. The results can be seen in Figure 4-15 . Figure 4-15. A visualization of neural networks trained with regularization strengths of 0.01, 0.1, and 1 (in that order) Another common type of regularization is L1 regularization. Here, we add the term λw for every weight w in the neural network. The L1 regularization has the intrigu‐ ing property that it leads the weight vectors to become sparse during optimization (i.e., close to exactly zero).",
    "that it leads the weight vectors to become sparse during optimization (i.e., close to exactly zero). Neurons with L1 regularization end up using only a small subset of their most important inputs and become quite resistant to noise in the inputs. In comparison, weight vectors from L2 regularization are usually diffuse, small numbers.",
    "the inputs. In comparison, weight vectors from L2 regularization are usually diffuse, small numbers. L1 regularization is useful when you want to understand exactly 72 | Chapter 4: Training Feed-Forward Neural Networks --- Page 89 --- 5Srebro, Nathan, Jason DM Rennie, and Tommi S. Jaakkola. “Maximum-Margin Matrix Factorization. ” NIPS , Vol. 17, 2004. 6Srivastava, Nitish, et al. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.",
    "004. 6Srivastava, Nitish, et al. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting. ” Journal of Machine Learning Research 15.1 (2014): 1929-1958.which features are contributing to a decision. If this level of feature analysis isn’t necessary, we prefer to use L2 regularization because it empirically performs better.",
    "analysis isn’t necessary, we prefer to use L2 regularization because it empirically performs better. Max norm constraints have a similar goal of attempting to restrict θ from becoming too large, but they do this more directly.5 Max norm constraints enforce an absolute upper bound on the magnitude of the incoming weight vector for every neuron and use projected gradient descent to enforce the constraint.",
    "ncoming weight vector for every neuron and use projected gradient descent to enforce the constraint. So any time a gradient descent step moves the incoming weight vector such that w2>c, we project the vector back onto the ball (centered at the origin) with radius c. Typical values of c are 3 and 4. One of the nice properties is that the parameter vector cannot grow out of control (even if the learning rates are too high) because the updates to the weights are always bounded.",
    "rol (even if the learning rates are too high) because the updates to the weights are always bounded. Dropout is a different kind of method for preventing overfitting that has become one of the most favored methods of preventing overfitting in deep neural networks.6 While training, dropout is implemented by only keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise.",
    "keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise. Intuitively, this forces the network to be accurate even in the absence of certain information. It prevents the network from becoming too dependent on any one neuron (or any small combination of neurons). Expressed more mathematically, it prevents overfitting by providing a way of approximately combining exponentially many different neural network architectures efficiently.",
    "ay of approximately combining exponentially many different neural network architectures efficiently. The process of dropout is expressed in Figure 4-16 . Dropout is pretty intuitive, but there are some important intricacies to consider. First, we’ d like the outputs of neurons during test time to be equivalent to their expected outputs at training time. We could fix this naively by scaling the output at test time. For example, if p= 0 .",
    "training time. We could fix this naively by scaling the output at test time. For example, if p= 0 . 5 , neurons must halve their outputs at test time in order to have the same (expected) output they would have during training. This is easy to see because a neuron’s output is set to 0 with probability 1 −p. This means that if a neuron’s output prior to dropout was x, then after dropout, the expected output would be Eoutput =px+1 −p· 0 =px.",
    "ut prior to dropout was x, then after dropout, the expected output would be Eoutput =px+1 −p· 0 =px. This naive implementation of dropout is undesirable, however, because it requires scaling of neuron outputs at test time. Test-time performance is extremely critical to model evaluation, so it’s always preferable to use inverted dropout , where the scaling occurs at training time instead of at test time.",
    "ferable to use inverted dropout , where the scaling occurs at training time instead of at test time. In inverted dropout, any neuron whose activation hasn’t been silenced has its output divided by p before the value is propagated to the next layer. With this fix, Eoutput =p·x p+1 −p· 0 =x, and we can avoid arbitrarily scaling neuronal output at test time. Preventing Overfitting in Deep Neural Networks | 73 --- Page 90 --- Figure 4-16.",
    "utput at test time. Preventing Overfitting in Deep Neural Networks | 73 --- Page 90 --- Figure 4-16. Dropout sets each neuron in the network as inactive with some random probability during each minibatch of training Neural Net Learning Theory Let’s cover some of the theory underpinning SGD and minibatch gradient descent. We noted some of the empirical gains from SGD and minibatch gradient descent in this chapter.",
    "scent. We noted some of the empirical gains from SGD and minibatch gradient descent in this chapter. Here, we try to understand why it is even “OK, ” in theory, to use these learning algorithms as a substitute for the gradient update over the entire dataset, which can often be intractable with many training examples.",
    "gradient update over the entire dataset, which can often be intractable with many training examples. The intractability arises from having to take the partial derivative with respect to every weight in the neural network for every training example in any given iteration, which is prohibitive for large datasets and neural networks of even moderate size.",
    "given iteration, which is prohibitive for large datasets and neural networks of even moderate size. Here, for simplicity, we will refer to X as the data matrix , which is a matrix of dimension nbyd, where n refers to the number of inputs in the entire dataset and d refers to the number of features associated with any given input. In other words, each row of X corresponds to a single training example.",
    "ciated with any given input. In other words, each row of X corresponds to a single training example. In addition to X, we have y, a vector of labels, or desired outputs, associated with each input of the data matrix. As can be inferred, y is of dimension nby 1 . We refer to f· ,θ as the function defined by the neural network, and θ as the weights (which we intend to learn through the training process) that parametrize it.",
    "k, and θ as the weights (which we intend to learn through the training process) that parametrize it. Finally, we refer to Ly,y as the error, or loss, function when comparing the predicted output y with the true output y. 74 | Chapter 4: Training Feed-Forward Neural Networks --- Page 91 --- Let’s start with some basic assumptions. Where do X and y even come from? We assume that there is some underlying data-generating process from which X and y are produced.",
    "om? We assume that there is some underlying data-generating process from which X and y are produced. That is, there exists a joint distribution over inputs and labels px,y from which the dataset we observe has been randomly sampled. In classical learning theory, we would like our learned function, or neural network f· ,θ, to minimize the objective: Epx,yLfx,θ,y or the expected loss with respect to the true distribution px,y. We term this objective the population risk.",
    "xpected loss with respect to the true distribution px,y. We term this objective the population risk. Why minimize the population risk? Intuitively, the more popular any given pairing of input datapoint xi and output label yi under the true distribution, the more we would like weight the output of fxi,θ resembling yi. Lfxi,θ,yi quantifies how well (or in this case, equivalently, how poorly given L is an error function) the output of fxi,θ resembles yi.",
    "this case, equivalently, how poorly given L is an error function) the output of fxi,θ resembles yi. The optimal weight is simply pxi,yi, which quantifies the popularity of the pairing. Putting this logic together over all possible pairings, we arrive at the population risk objective. Note that this logic holds regardless of whether x and y are discrete or continuous. Unfortunately, we have no access to the true distribution.",
    "ether x and y are discrete or continuous. Unfortunately, we have no access to the true distribution. If we did, our problem would already be solved (more accurately, is instantly solved when y is discrete). The best we can do is approximate px,y with the data we see. We call this empirical distribution pDx,y, which, in the limit of infinite data, converges to px,y, making any gradient update based on the empirical distribution unbiased.",
    "te data, converges to px,y, making any gradient update based on the empirical distribution unbiased. The domain of the empirical distribution is our training set. Instead of minimizing the population risk, we minimize the empirical risk as a proxy: EpDx,yLfx;θ,y=1 n∑i= 1nLfxi;θ,yi Now, we would like to find a set of weights θ, that minimize the empirical risk.",
    "=1 n∑i= 1nLfxi;θ,yi Now, we would like to find a set of weights θ, that minimize the empirical risk. We do that by taking the gradient of the objective with respect to θ: ∇θEpDx,yLfx;θ,y =∇θ1 n∑i= 1nLfxi;θ,yi =1 n∑i= 1n∇θLfxi;θ,yi Preventing Overfitting in Deep Neural Networks | 75 --- Page 92 --- We now see that the newly introduced empirical risk minimization is the theoretical motivation for the gradient update over the entire dataset presented in “The Backpro‐ pagation Algorithm” on page 61 .",
    "gradient update over the entire dataset presented in “The Backpro‐ pagation Algorithm” on page 61 . The only, subtle difference is the factor 1 n, which is a constant that can be rolled into the learning rate.",
    "subtle difference is the factor 1 n, which is a constant that can be rolled into the learning rate. Extending the last equality, we have: 1 n∑i= 1n∇θLfxi;θ,yi=EpDx,y∇θLfx;θ,y We can do something that uses almost the exact same logic, which allows us to use the empirical risk as a proxy for population risk: approximate the expectation with respect to the empirical distribution via sampling. Again, this is unbiased since we achieve the empirical distribution in the limit of infinite samples.",
    "gain, this is unbiased since we achieve the empirical distribution in the limit of infinite samples. The number of samples is left as a hyperparameter, where using a single sample has been popularly termed SGD; using a relatively small number of samples has been popularly termed minibatch gradient descent. Summary In this chapter, we’ve learned all of the basics involved in training feed-forward neural networks.",
    "In this chapter, we’ve learned all of the basics involved in training feed-forward neural networks. We’ve talked about gradient descent, the backpropagation algorithm, as well as various methods we can use to prevent overfitting. In the next chapter, we’ll put these lessons into practice when we use the PyTorch library to efficiently implement our first neural networks.",
    "ns into practice when we use the PyTorch library to efficiently implement our first neural networks. Then in Chapter 6 , we’ll return to the problem of optimizing objective functions for training neural networks and design algorithms to significantly improve performance. These improvements will enable us to process much more data, which means we’ll be able to build more comprehensive models.",
    "l enable us to process much more data, which means we’ll be able to build more comprehensive models. 76 | Chapter 4: Training Feed-Forward Neural Networks --- Page 93 --- CHAPTER 5 Implementing Neural Networks in PyTorch Introduction to PyTorch In this chapter, you will learn the basics of PyTorch, one of the most popular deep learning frameworks in use today.",
    "you will learn the basics of PyTorch, one of the most popular deep learning frameworks in use today. PyTorch was introduced by Facebook’s AI Research Lab in 2016 and gained users rapidly, both in industry and in research, through the following years. One reason for PyTorch’s widespread adoption was its intuitive, Pythonic feel, which fit naturally into the preexisting workstreams and coding para‐ digms followed by deep learning practitioners.",
    "lly into the preexisting workstreams and coding para‐ digms followed by deep learning practitioners. In particular, this chapter will discuss the data structures utilized by PyTorch, how to define neural models in PyTorch, and how to connect data with models for training and testing. Finally, we implement a practical example in PyTorch—a classifier for the MNIST digits dataset, complete with code for training and testing the classifier.",
    "classifier for the MNIST digits dataset, complete with code for training and testing the classifier. Installing PyTorch Installing a CPU-compatible version of PyTorch is relatively simple. The PyTorch docs recommend using conda, a package management system. Within conda, you can create multiple environments, where an environment is a context that encap‐ sulates all of your package installs.",
    "le environments, where an environment is a context that encap‐ sulates all of your package installs. Access to a package does not transfer across environments—this allows the user to have a clean separation between different contexts by downloading packages within individual environments. We recommend that you create a conda environment for deep learning purposes that you can switch into whenever necessary.",
    "u create a conda environment for deep learning purposes that you can switch into whenever necessary. We refer you to the conda docs for guidance on how to download conda and further notes on environments.",
    "refer you to the conda docs for guidance on how to download conda and further notes on environments. 77 --- Page 94 --- Once you have installed conda, created your deep learning environment, and switched into it, the PyTorch docs recommend running the following code from your command line to download a CPU-compatible version of PyTorch on macOS: conda install pytorch torchvision torchaudio -c pytorch Note that with this install come torchvision and torchaudio, which are specialized packages for working with image data and audio data, respectively.",
    "torchaudio, which are specialized packages for working with image data and audio data, respectively. If you are on a Linux system, the docs recommend running the following code from your command line: conda install pytorch torchvision torchaudio cpuonly -c pytorch Now, you can navigate to a Python shell (still in your deep learning environment), and the following command should run with no issues: import torch It is important to get this command running with no errors in your Python shell before moving on to running the code in the following sections, as they all require the ability to import the PyTorch package.",
    "g the code in the following sections, as they all require the ability to import the PyTorch package. PyTorch Tensors Tensors are the primary data structure by which PyTorch stores and manipulates numerical information. Tensors can be seen as a generalization of arrays and matri‐ ces, which we covered in detail in our introduction to linear algebra in Chapter 1 .",
    "rays and matri‐ ces, which we covered in detail in our introduction to linear algebra in Chapter 1 . Specifically, tensors, as a generalization of 2D matrices and 1D arrays, can store mul‐ tidimensional data such as batches of three-channel images. Note that this requires 4D data storage, since each image is 3D (including the channel dimension), and a fourth dimension that is required to index each individual image.",
    "ding the channel dimension), and a fourth dimension that is required to index each individual image. Tensors can even represent dimensionalities beyond the 4D space, although the usage of such tensors in practice is uncommon. In PyTorch, tensors are utilized universally. They are used to represent the inputs to models, the weight layers within the models themselves, and the outputs of models.",
    "ent the inputs to models, the weight layers within the models themselves, and the outputs of models. The standard linear algebra operations of transposition, addition, multiplication, inversion, etc., can all be run on tensors. Tensor Init How do we initialize tensors? We can initialize a tensor from a variety of data types.",
    ". Tensor Init How do we initialize tensors? We can initialize a tensor from a variety of data types. Some examples are Python lists and Python numerical primitives: arr = [1,2] tensor = torch.tensor(arr) val = 2.0 tensor = torch.tensor(val) 78 | Chapter 5: Implementing Neural Networks in PyTorch --- Page 95 --- Tensors can also be initialized from NumPy arrays, allowing PyTorch to be integrated easily into existing data science and machine learning workflows: import numpy as np np_arr = np.array([1,2]) x_t = torch.from_numpy(np_arr) Additionally, tensors can be formed via some common PyTorch API endpoints: zeros_t = torch.zeros((2,3)) # Returns 2x3 tensor of zeros ones_t = torch.ones((2,3)) # Returns 2x3 tensor of ones rand_t = torch.randn((2,3)) # Returns 2x3 tensor of random numbers Tensor Attributes In the examples we just saw, we passed a tuple as the argument to each function call.",
    "sor Attributes In the examples we just saw, we passed a tuple as the argument to each function call. The number of indices in the tuple is the dimensionality of the tensor to be created, while the number at each index represents the desired size of that particular dimension.",
    "be created, while the number at each index represents the desired size of that particular dimension. To access the dimensionality of a tensor, we can call its shape attribute: zeros_t.shape # Returns torch.Size([2, 3]) Calling the shape attribute on any of the previous examples should return the same tuple as the input argument, assuming the tensor has not been significantly modified in-between. What are some other attributes of tensors?",
    "he tensor has not been significantly modified in-between. What are some other attributes of tensors? In addition to dimension, tensors also store information on the type of data being stored: floating point, complex, integer, and boolean. There exist subtypes within each of these categories, but we won’t go into the differences between each subtype here.",
    "pes within each of these categories, but we won’t go into the differences between each subtype here. It’s also important to note that a tensor cannot contain a mix and match of various data types—all data within a single tensor must be of the same data type. To access the data type of a tensor, we can call its dtype attribute: x_t = torch.tensor(2.0) x_t.dtype # Returns torch.float32 Additionally, although we haven’t shown this yet, we can set the data type of a tensor during initialization.",
    "lly, although we haven’t shown this yet, we can set the data type of a tensor during initialization. Extending one of our previous examples: arr = [1,2] x_t = torch.tensor(arr, dtype=torch.float32) In addition to the data type and shape of a tensor, we can also learn the device on which the tensor is allocated.",
    "the data type and shape of a tensor, we can also learn the device on which the tensor is allocated. These devices include the famous CPU, which is standard with any computer and is the default storage for any tensor, and the GPU, or graphics processing unit, which is a specialized data processing unit often used in the image space.",
    "graphics processing unit, which is a specialized data processing unit often used in the image space. GPUs massively speed up many common tensor operations such as multiplication via parallel processing over hundreds of small, specialized cores, thus PyTorch Tensors | 79 --- Page 96 --- making them immensely useful for most deep learning applications.",
    "Torch Tensors | 79 --- Page 96 --- making them immensely useful for most deep learning applications. To access the tensor device, we can call its device attribute: x_t.device # Returns device(type='cpu') by default Similarly to data type, we can set the device of a tensor upon initialization: # PyTorch will use GPU if it's available device = 'cuda' if torch.cuda.is_available() else 'cpu' arr = [1,2] x_t = torch.tensor(arr, dtype=torch.float32, device=device) This is a common approach to checking whether a GPU is available via code and using a GPU if it is available.",
    "common approach to checking whether a GPU is available via code and using a GPU if it is available. If the GPU is not available, it will use a CPU without error.",
    "de and using a GPU if it is available. If the GPU is not available, it will use a CPU without error. If you have defined a tensor with a certain set of attributes and would like to modify these attributes, you can use the to function: x_t = x_t.to(device, dtype=torch.int) And finally, as we’ll cover in “Gradients in PyTorch” on page 83, PyTorch tensors can be initialized with the argument requires_grad , which when set to True , stores the tensor’s gradient in an attribute called grad .",
    "requires_grad , which when set to True , stores the tensor’s gradient in an attribute called grad . Tensor Operations The PyTorch API provides us with many possible tensor operations, ranging from tensor arithmetic to tensor indexing. In this section we will cover some of the more useful tensor operations—ones that you will likely use often in your deep learning applications. One of the most basic operations is multiplying a tensor by some scalar c.",
    "ep learning applications. One of the most basic operations is multiplying a tensor by some scalar c. This can be achieved via the code: c = 10 x_t = x_t*c This results in an element-wise product of the scalar with the entries of the tensor. Another one of the most basic tensor operations is tensor addition and subtraction. To do this, we can simply add tensors via +.",
    "c tensor operations is tensor addition and subtraction. To do this, we can simply add tensors via +. Subtraction follows directly from being able to do addition and multiplying the second tensor by the scalar –1: x1_t = torch.zeros((1,2)) x2_t = torch.ones((1,2)) x1_t + x2_t # returns tensor([[1., 1.]]) The result is an element-wise sum of the two tensors. This can be seen as a direct generalization of matrix addition for any dimensionality.",
    "two tensors. This can be seen as a direct generalization of matrix addition for any dimensionality. Note that this direct gener‐ alization implicitly assumes the same constraint we discussed for matrix addition a 80 | Chapter 5: Implementing Neural Networks in PyTorch --- Page 97 --- while ago: that the two tensors being summed are of the same dimension.",
    "s in PyTorch --- Page 97 --- while ago: that the two tensors being summed are of the same dimension. PyTorch, similarly, will accept any two broadcastable inputs with no issues, where broadcasting is a procedure by which the two inputs are resolved to a common shape, and broad‐ castable refers to whether it is even possible for the two inputs to be resolved to a common shape. If the two tensors are already of the same shape, no broadcasting is necessary.",
    "d to a common shape. If the two tensors are already of the same shape, no broadcasting is necessary. We refer you to the PyTorch documentation for more information on how the API determines if the two inputs are broadcastable, and how broadcasting is performed in such cases. Tensor multiplication is another useful operation to become familiar with. Tensor multiplication works the same as matrix and vector multiplication when the dimen‐ sionality of each tensor is less than or equal to 2.",
    "atrix and vector multiplication when the dimen‐ sionality of each tensor is less than or equal to 2. However, tensor multiplication also works on tensors of arbitrarily high dimensionality, given the two tensors are compatible. We can think of tensor multiplication in high dimensions as batched matrix multiplications: imagine we have two tensors, the first is of shape (2,1,2) and the second is of shape (2,2,2).",
    "ions: imagine we have two tensors, the first is of shape (2,1,2) and the second is of shape (2,2,2). We can further represent the first tensor as a length-two list of 1 × 2 matrices, while the second is a length-two list of 2 × 2 matrices. Their product is a length-two list, where index i of the product is the matrix product of index i of the first tensor and index i of the second tensor, as shown in Figure 5-1 . Figure 5-1.",
    "f index i of the first tensor and index i of the second tensor, as shown in Figure 5-1 . Figure 5-1. To help visualize the general tensor multiplication method, this figure shows the matrix multiplication that occurs before restacking. Restacking the resultant list into a 3D tensor, we see that the product is of shape (2,1,2).",
    "acking. Restacking the resultant list into a 3D tensor, we see that the product is of shape (2,1,2). Now, we can generalize this to four dimensions, where instead of imagining we have a list of matrices, we represent each 4D tensor as a grid of matrices and the (i,j)-th index of the product is the matrix product of the (i,j)-th indices of the two 4D input tensors.",
    ")-th index of the product is the matrix product of the (i,j)-th indices of the two 4D input tensors. We represent this mathematically: Pi,j,x,z= ∑yAi,j,x,y*Bi,j,y,z This procedure is generalizable to any dimensionality, assuming that the two input tensors follow the constraints of matrix multiplication. As with tensor addition, there are exceptions that involve broadcasting, though we won’t cover those in detail here.",
    "ddition, there are exceptions that involve broadcasting, though we won’t cover those in detail here. PyTorch Tensors | 81 --- Page 98 --- We refer you to the PyTorch documentation for detailed information on broadcast‐ ing.",
    "-- Page 98 --- We refer you to the PyTorch documentation for detailed information on broadcast‐ ing. To multiply two tensors in PyTorch, you can use the torch matmul function: x1_t = torch.tensor([[1,2],[3,4]]) x2_t = torch.tensor([[1,2,3],[4,5,6]]) torch.matmul(x1_t, x2_t) # Returns tensor([[9,12,15],[19,26,33]]) In addition to arithmetic operations on tensors, we can also index and slice tensors.",
    "],[19,26,33]]) In addition to arithmetic operations on tensors, we can also index and slice tensors. If you have prior experience with NumPy, you’ll notice that PyTorch indexing is very similar and is based on linear algebra fundamentals.",
    "Py, you’ll notice that PyTorch indexing is very similar and is based on linear algebra fundamentals. If you have a 3D tensor, you can access the value at position (i,j,k) via the following code: i,j,k = 0,1,1 x3_t = torch.tensor([[[3,7,9],[2,4,5]],[[8,6,2],[3,9,1]]]) print(x3_t) # out: # tensor([[[3, 7, 9], # [2, 4, 5]], # [[8, 6, 2], # [3, 9, 1]]]) x3_t[i,j,k] # out: # tensor(4) To access larger slices of the tensor, say the matrix at position 0 in a 3D tensor, you can use the following code: x3_t[0] # Returns the matrix at position 0 in tensor x3_t[0,:,:] # Also returns the matrix at position 0 in tensor!",
    "ns the matrix at position 0 in tensor x3_t[0,:,:] # Also returns the matrix at position 0 in tensor! # out: # tensor([[3, 7, 9], # [2, 4, 1]]) where the two lines of code are interpreted to be equivalent by the PyTorch API. This is because using a single indexer, such as x3_t[0] , implicitly assumes that the user would like to access all indices (i,j,k) that satisfy the condition i = 0 (i.e., the top matrix in the stack of matrices that is the original 3D tensor).",
    "the condition i = 0 (i.e., the top matrix in the stack of matrices that is the original 3D tensor). Usage of the : symbol makes this implicit assumption clear by telling PyTorch directly that the user would not like to subset the data at that dimension.",
    "clear by telling PyTorch directly that the user would not like to subset the data at that dimension. We can also use the : symbol to subset the data, for example: x3_t[0,1:3,:] # returns tensor([[2, 4, 5]]) where the last line of code is interpreted as: find all indices (i,j,k) such that i = 0, j≥ 1, and j< 3 (: follows the standard Python list indexing convention of being inclusive at the start of the defined range and exclusive at the end).",
    "indexing convention of being inclusive at the start of the defined range and exclusive at the end). In plain English, we want to access the second and third rows of the top matrix in the stack of matrices that is the 82 | Chapter 5: Implementing Neural Networks in PyTorch --- Page 99 --- original 3D tensor. Note that this usage of : is consistent with standard Python list indexing. In addition to accessing indices or slices of a tensor, we can also set those indices and slices to new values.",
    "to accessing indices or slices of a tensor, we can also set those indices and slices to new values. In the single index case, this is as simple as: x3_t[0,1,2] = 1 # out: # tensor([[[3, 7, 9], # [2, 4, 1]], # [[8, 6, 2], # [3, 9, 1]]]) To set larger slices of the tensor, the most straightforward way is to define a tensor that is of the same dimensionality as the slice, and use the following code: x_t = torch.randn(2,3,4) sub_tensor = torch.randn(2,4) x_t[0,1:3,:] = sub_tensor Additionally, via broadcasting, we can do things like: x_t[0,1:3,:] = 1 sub_tensor = torch.randn(1,4) x_t[0,1:3,:] = sub_tensor The first line sets the entirety of those two rows to 1, and the second sets both rows of the slice to the single row passed in as sub_tensor .",
    "wo rows to 1, and the second sets both rows of the slice to the single row passed in as sub_tensor . In the next section, we will show how to compute the gradients of a function in PyTorch, and how to access the values of those gradients. Gradients in PyTorch Just as a recap, let’s recall derivatives and partial derivatives from calculus.",
    "radients in PyTorch Just as a recap, let’s recall derivatives and partial derivatives from calculus. The partial derivative of a function, which could be as simple as a polynomial function of a few variables to something as complex as a neural network, with respect to one of the function’s inputs represents the rate of change of the output of the function as that input’s value changes slightly.",
    "represents the rate of change of the output of the function as that input’s value changes slightly. So, large magnitude derivatives indicate that the output is very volatile with small changes in the input (think fx=x10 when x is of moderate size), while small magnitude derivatives indicate that the output is relatively stable with small changes in the input (think fx=x 10).",
    "tives indicate that the output is relatively stable with small changes in the input (think fx=x 10). If the function takes in more than one input, the gradient is the vector that is composed of all of these partial derivatives: fx,y,z=x2+y2+z2 Gradients in PyTorch | 83 --- Page 100 --- ∂f ∂x=∇xfx,y,z= 2x ∇f=2x2y2z Continuing from this example, how would we represent this in PyTorch?",
    "- ∂f ∂x=∇xfx,y,z= 2x ∇f=2x2y2z Continuing from this example, how would we represent this in PyTorch? We can use the following code: x = torch.tensor(2.0, requires_grad=True) y = torch.tensor(3.0, requires_grad=True) z = torch.tensor(1.5, requires_grad=True) f = x**2+y**2+z**2 f.backward() x.grad, y.grad, z.grad # out: # (tensor(4.), tensor(6.), tensor(3.)) The call to backward() computes the partial derivative of the output f with respect to each of the input variables.",
    "kward() computes the partial derivative of the output f with respect to each of the input variables. We should expect the values for x.grad , y.grad , and z.grad to be 4.0, 6.0, and 3.0, respectively. In the case of neural networks, we can represent the neural network as fx,θ, where f is the neural network, x is some vector representing the input, and θ is the parameters of f.",
    "here f is the neural network, x is some vector representing the input, and θ is the parameters of f. Instead of computing the gradient of the output of f with respect to x as done in the previous example, we compute the gradient of the loss of the output of f with respect to θ. Adjusting θ via the gradient will eventually lead to a setting of θ that results in a small loss for the training data and one that hopefully generalizes to data that f hasn’t seen before.",
    "all loss for the training data and one that hopefully generalizes to data that f hasn’t seen before. In the next section, we will introduce the building blocks of neural networks. The PyTorch nn Module The PyTorch nn module provides all of the baseline functionality necessary for defining, training, and testing a model. To import the nn module, all you need to do is run the following line of code: import torch.nn as nn In this section, we will cover some of the most common uses of the nn module.",
    "import torch.nn as nn In this section, we will cover some of the most common uses of the nn module. For example, to initialize a weight matrix needed for a feed-forward neural network, you can use the following code: in_dim, out_dim = 256, 10 vec = torch.randn(256) layer = nn.Linear(in_dim, out_dim, bias=True) out = layer(vec) This defines a single layer with bias in a feed-forward neural network, which is a matrix of weights that takes as input a vector of dimension 256 and outputs a vector of dimension 10.",
    "atrix of weights that takes as input a vector of dimension 256 and outputs a vector of dimension 10. The last line of code demonstrates how we can easily apply this layer 84 | Chapter 5: Implementing Neural Networks in PyTorch --- Page 101 --- to an input vector and store the output in a new tensor.",
    "Neural Networks in PyTorch --- Page 101 --- to an input vector and store the output in a new tensor. If we wanted to do the same thing using only our knowledge from prior sections, we would need to manually define a weight matrix W and bias vector b via torch .tensor and explicitly compute: W = torch.rand(10,256) b = torch.zeros(10,1) out = torch.matmul(W, vec) + b The nn module’s Linear layer allows us to abstract away these manual operations so we can write clean, concise code.",
    "Linear layer allows us to abstract away these manual operations so we can write clean, concise code. A feed-forward neural network can be thought of as simply a composition of such layers, for example: in_dim, feature_dim, out_dim = 784, 256, 10 vec = torch.randn(784) layer1 = nn.Linear(in_dim, feature_dim, bias=True) layer2 = nn.Linear(feature_dim, out_dim, bias=True) out = layer2(layer1(vec)) This code represents a neural network that is the function composition layer2(layer1(vec)) , or mathematically: W2W1*x+b1+b2.",
    "ural network that is the function composition layer2(layer1(vec)) , or mathematically: W2W1*x+b1+b2. To represent more complex, nonlinear functions, the nn module additionally provides nonlinearities such as ReLU, which can be accessed via nn.ReLU , and tanh, which can be accessed via nn.Tanh . These nonlinearities are applied in between layers, as follows: relu = nn.ReLU() out = layer2(relu(layer1(vec))) We’ve gone over almost everything necessary to define a model in PyTorch.",
    "layer2(relu(layer1(vec))) We’ve gone over almost everything necessary to define a model in PyTorch. The last thing to cover is the nn.Module class—the base class from which all neural networks are subclassed in PyTorch. The nn.Module class has one important method that your specific model’s subclass will override. This method is the forward method, and it defines how the layers initialized in your model’s constructor interact with the input to generate the model’s output.",
    "yers initialized in your model’s constructor interact with the input to generate the model’s output. Here is an example of some code that can be used to encapsulate the simple two-layer neural network we just defined: class BaseClassifier(nn.Module): def __init__(self, in_dim, feature_dim, out_dim): super(BaseClassifier, self).__init__() self.layer1 = nn.Linear(in_dim, feature_dim, bias=True) self.layer2 = nn.Linear(feature_dim, out_dim, bias=True) self.relu = nn.ReLU() def forward(self, x): x = self.layer1(x) x = self.relu(x) out = self.layer2(x) return out The PyTorch nn Module | 85 --- Page 102 --- We’ve written our first neural network in PyTorch!",
    "n out The PyTorch nn Module | 85 --- Page 102 --- We’ve written our first neural network in PyTorch! BaseClassifier is a bug-free model class that can be instantiated after defining in_dim , feature_dim , and out_dim . The constructor takes in these three variables as arguments in the constructor, which makes the model flexible in terms of layer size.",
    "ee variables as arguments in the constructor, which makes the model flexible in terms of layer size. This is the sort of model that can be used effectively as a first-pass classifier for datasets such as MNIST, as we will demonstrate in “Building the MNIST Classifier in PyTorch” on page 89.",
    "sets such as MNIST, as we will demonstrate in “Building the MNIST Classifier in PyTorch” on page 89. To generate the output of a model on some input, we can use the model as follows: no_examples = 10 in_dim, feature_dim, out_dim = 784, 256, 10 x = torch.randn((no_examples, in_dim)) classifier = BaseClassifier(in_dim, feature_dim, out_dim) out = classifier(x) Note that we implicitly call the forward function when using the classifier model as a function in the final line.",
    "mplicitly call the forward function when using the classifier model as a function in the final line. Comparing this to the initial approach of manually defining each layer’s parameters as a torch tensor and computing the output via matmul operations, this is a much more clean, modular, and reusable approach to defining neural networks. In addition to being able to define the model, instantiate it, and run data through it, we must be able to train and test the model.",
    "ine the model, instantiate it, and run data through it, we must be able to train and test the model. To train (and test) the model, we need a loss metric to evaluate the model. During training, once we calculate this loss metric, we can use our knowledge from the previous section and call backward() on the computed loss. This will store the gradient in each parameter p’s grad attribute.",
    "backward() on the computed loss. This will store the gradient in each parameter p’s grad attribute. Since we have defined a classifier model, we can use the cross-entropy loss metric from PyTorch nn: loss = nn.CrossEntropyLoss() target = torch.tensor([0,3,2,8,2,9,3,7,1,6]) computed_loss = loss(out, target) computed_loss.backward() In the preceding code, target is a tensor of shape ( no_examples ), and each index represents the ground truth class of the input corresponding with that index.",
    "ples ), and each index represents the ground truth class of the input corresponding with that index. Now that we’ve computed the gradient of the loss of the minibatch of examples with respect to all of the parameters in the classifier, we can perform the gradient descent step. When defining a neural network as a subclass of nn.Module , we can access all of its parameters via the parameters() function—another convenience provided by the PyTorch API.",
    "all of its parameters via the parameters() function—another convenience provided by the PyTorch API. To view the shape of each parameter in the neural network, you can run the code: for p in classifier.parameters(): print(p.shape) # out: # torch.Size([256, 784]) # torch.Size([256]) 86 | Chapter 5: Implementing Neural Networks in PyTorch --- Page 103 --- # torch.Size([10, 256]) # torch.Size([10]) As we can see, the first layer has 256 × 784 weights and a bias vector of length 256.",
    "rch.Size([10]) As we can see, the first layer has 256 × 784 weights and a bias vector of length 256. The last layer has 10 × 256 weights and a bias vector of length 10. During gradient descent, we need to adjust the parameters based on their gradients. We could do this manually, but PyTorch has abstracted away this functionality into the torch.optim module.",
    "ld do this manually, but PyTorch has abstracted away this functionality into the torch.optim module. This module provides functionality for determining the optimizer, which may be more complex than classic gradient descent, and updating the parameters of the model.",
    ", which may be more complex than classic gradient descent, and updating the parameters of the model. Y ou can define the optimizer as follows: from torch import optim lr = 1e-3 optimizer = optim.SGD(classifier.parameters(), lr=lr) This code creates an optimizer that will update the parameters of the classifier via SGD at the end of each minibatch.",
    "an optimizer that will update the parameters of the classifier via SGD at the end of each minibatch. To actually perform this update, you can use the following code: optimizer.step() # Updates parameters via SGD optimizer.zero_grad() # Zeroes out gradients between minibatches In the simple case of a feed-forward network as defined in BaseClassifier , the testing mode of such a network is the same as the training mode—we can just call classifier(test_x) on any minibatch in the test set to evaluate the model.",
    "ing mode—we can just call classifier(test_x) on any minibatch in the test set to evaluate the model. However, as we’ll discuss later, this is not true for all neural architectures. This code works for a single minibatch—performing training over the entire dataset would require manually shuffling the dataset at each epoch and splitting the dataset into minibatches that can be iterated through.",
    "g the dataset at each epoch and splitting the dataset into minibatches that can be iterated through. Thankfully, PyTorch has also abstrac‐ ted this process out into what are called PyTorch datasets and dataloaders. In the next section, we will cover these modules in detail. PyTorch Datasets and Dataloaders The PyTorch Dataset is a base class that can be used to access your specific data.",
    "s and Dataloaders The PyTorch Dataset is a base class that can be used to access your specific data. In practice, you would subclass the Dataset class by overriding two important methods: __len__() and __getitem__() . The first method, as you can probably tell from its name, refers to the length of the dataset—i.e., the number of examples that the model will be trained or tested on.",
    "the length of the dataset—i.e., the number of examples that the model will be trained or tested on. If we think of the dataset as a list of examples, the second method takes as input an index and returns the example at that index. Each example consists of both the data point (e.g., image) and label (e.g., value from 0 to 9 in the case of MNIST).",
    "sists of both the data point (e.g., image) and label (e.g., value from 0 to 9 in the case of MNIST). Here is some example code for a dataset: import os from PIL import Image PyTorch Datasets and Dataloaders | 87 --- Page 104 --- from torchvision import transforms class ImageDataset(Dataset): def __init__(self, img_dir, label_file): super(ImageDataset, self).__init__() self.img_dir = img_dir self.labels = torch.tensor(np.load(label_file, allow_pickle=True)) self.transforms = transforms.ToTensor() def __getitem__(self, idx): img_pth = os.path.join(self.img_dir, \"img_{}.jpg\".format(idx)) img = Image.open(img_pth) img = self.transforms(img).flatten() label = self.labels[idx] return {\"data\":img, \"label\":label} def __len__(self): return len(self.labels) In this example, we assume that the directory containing our dataset consists of images that follow the naming convention img-idx.png , where idx refers to the index of the image.",
    "f images that follow the naming convention img-idx.png , where idx refers to the index of the image. Additionally, we assume that our ground-truth labels are stored in a saved NumPy array, which can be loaded and indexed using idx to find each image’s corresponding label. The DataLoader class in PyTorch takes as input a dataset instantiation, and abstracts away all of the heavy lifting required to load in the dataset by the minibatch and shuffle the dataset between epochs.",
    "avy lifting required to load in the dataset by the minibatch and shuffle the dataset between epochs. Although we won’t go behind the scenes in too much depth, the DataLoader class does make use of Python’s multiprocessing built-in module to efficiently load minibatches in parallel.",
    "es make use of Python’s multiprocessing built-in module to efficiently load minibatches in parallel. Here is some example code that puts everything together: train_dataset = ImageDataset(img_dir='./data/train/', label_file='./data/train/labels.npy') train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True) To iterate through these dataloaders, use the following code as a template: for minibatch in train_loader: data, labels = minibatch['data'], minibatch['label'] print(data) print(labels) The data returned is a tensor of shape (64,784) and the labels returned are of shape (64,).",
    "(labels) The data returned is a tensor of shape (64,784) and the labels returned are of shape (64,). As you can tell, the dataloader also does the work of stacking all of the examples into a single tensor that can simply be run through the network: 88 | Chapter 5: Implementing Neural Networks in PyTorch --- Page 105 --- for minibatch in train_loader: data, labels = minibatch['data'], minibatch['label'] out = classifier(data) # to be completed in the next section!",
    "minibatch['data'], minibatch['label'] out = classifier(data) # to be completed in the next section! where out is of shape (64,10) in the case of MNIST. In the next section, we will put together all of our learnings to build a neural architecture that can be trained and tested on the MNIST dataset, provide code samples for training and testing the model by building off of work in this section, and show example training and testing loss curves.",
    "he model by building off of work in this section, and show example training and testing loss curves. Building the MNIST Classifier in PyTorch It’s time to build an MNIST classifier in PyTorch.",
    "curves. Building the MNIST Classifier in PyTorch It’s time to build an MNIST classifier in PyTorch. For the most part, we can reuse a lot of the code presented and explained earlier: import matplotlib.pyplot as plt import torch from torch import optim import torch.nn as nn from torch.utils.data import Dataset, DataLoader from torchvision.datasets import MNIST from torchvision.transforms import ToTensor class BaseClassifier(nn.Module): def __init__(self, in_dim, feature_dim, out_dim): super(BaseClassifier, self).__init__() self.classifier = nn.Sequential( nn.Linear(in_dim, feature_dim, bias=True), nn.ReLU(), nn.Linear(feature_dim, out_dim, bias=True) ) def forward(self, x): return self.classifier(x) # Load in MNIST dataset from PyTorch train_dataset = MNIST(\".\", train=True, download=True, transform=ToTensor()) test_dataset = MNIST(\".\", train=False, download=True, transform=ToTensor()) train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) Note that, by default, the minibatch tensors and model parameters are on CPU, so there was no need to call the to function on each of these to change the device.",
    "ters are on CPU, so there was no need to call the to function on each of these to change the device. Also, the MNIST dataset provided by PyTorch unfortunately does not come with a validation set, so we’ll do our best to use insights solely from the training loss curve to inform our final hyperparameter decision for the test set: Building the MNIST Classifier in PyTorch | 89 --- Page 106 --- # Instantiate model, optimizer, and hyperparameter(s) in_dim, feature_dim, out_dim = 784, 256, 10 lr=1e-3 loss_fn = nn.CrossEntropyLoss() epochs=40 classifier = BaseClassifier(in_dim, feature_dim, out_dim) optimizer = optim.SGD(classifier.parameters(), lr=lr) def train(classifier=classifier, optimizer=optimizer, epochs=epochs, loss_fn=loss_fn): classifier.train() loss_lt = [] for epoch in range(epochs): running_loss = 0.0 for minibatch in train_loader: data, target = minibatch data = data.flatten(start_dim=1) out = classifier(data) computed_loss = loss_fn(out, target) computed_loss.backward() optimizer.step() optimizer.zero_grad() # Keep track of sum of loss of each minibatch running_loss += computed_loss.item() loss_lt.append(running_loss/len(train_loader)) print(\"Epoch: {} train loss: {}\".format(epoch+1, running_loss/len(train_loader))) plt.plot([i for i in range(1,epochs+1)], loss_lt) plt.xlabel(\"Epoch\") plt.ylabel(\"Training Loss\") plt.title( \"MNIST Training Loss: optimizer {}, lr {}\".format(\"SGD\", lr)) plt.show() # Save state to file as checkpoint torch.save(classifier.state_dict(), 'mnist.pt') def test(classifier=classifier, loss_fn = loss_fn): classifier.eval() accuracy = 0.0 computed_loss = 0.0 with torch.no_grad(): for data, target in test_loader: data = data.flatten(start_dim=1) out = classifier(data) _, preds = out.max(dim=1) 90 | Chapter 5: Implementing Neural Networks in PyTorch --- Page 107 --- # Get loss and accuracy computed_loss += loss_fn(out, target) accuracy += torch.sum(preds==target) print(\"Test loss: {}, test accuracy: {}\".format( computed_loss.item()/(len(test_loader)*64), accuracy*100.0/(len(test_loader)*64))) Additionally, note that we call classifier.train() and classifier.eval() at the beginning of the training and test functions, respectively.",
    "ier.train() and classifier.eval() at the beginning of the training and test functions, respectively. The calls to these functions communicate to the PyTorch backend whether the model is in training mode or inference mode. Y ou might be wondering why we need to call classifier.train() and classifier.eval() if there is no difference between the behavior of the neural network at train and test time.",
    ".eval() if there is no difference between the behavior of the neural network at train and test time. Although this is true in our first-pass example, the training and testing modes for other neural architectures are not necessarily the same. For example, if dropout layers are added to the model architecture, the dropout layers need to be ignored during the testing phase. We add in the calls to train() and eval() here since it is generally considered good practice to do so.",
    "add in the calls to train() and eval() here since it is generally considered good practice to do so. As a first step, we need to set some starting hyperparameters for model training. We start with a slightly conservative learning rate in 1e-4 and inspect the training loss curve and testing accuracy after 40 epochs, or iterations through the entire dataset. Figure 5-2 shows a graph of the training loss curve through the epochs. Figure 5-2.",
    "entire dataset. Figure 5-2 shows a graph of the training loss curve through the epochs. Figure 5-2. We see signs of underfitting as the model performance on the training set is failing to level out, meaning we have not yet settled into a local optimum Building the MNIST Classifier in PyTorch | 91 --- Page 108 --- We can see that this loss curve is not particularly close to leveling out near the end of training, which we’ d hope to start seeing for a model training at a sufficient learning rate.",
    "nd of training, which we’ d hope to start seeing for a model training at a sufficient learning rate. And although we don’t have a validation set to confirm our suspicions, we have strong reason to suspect that a higher learning rate would help. After setting the learning rate to a slightly more aggressive 1e-3 , we observe a training loss curve that is much more in line with what we’ d hope to see ( Figure 5-3 ). Figure 5-3.",
    "raining loss curve that is much more in line with what we’ d hope to see ( Figure 5-3 ). Figure 5-3. This leveling out of the loss curve is more like what we’ d expect to see with an appropriate learning rate for the problem The loss curve starts to level out only near the end of training. This trend indicates that the model is likely in the sweet spot between underfitting to the training data, like our previous attempt, and overfitting to the training data.",
    "underfitting to the training data, like our previous attempt, and overfitting to the training data. Evaluating the trained model at 40 epochs on the test set achieves an accuracy of 91%! Although this is nowhere close to the top performers on MNIST today, which primarily use convolu‐ tional neural classifiers, it is a great start. We recommend you try some extensions to the code, such as increasing the number of hidden layers and substituting in a more sophisticated optimizer.",
    ", such as increasing the number of hidden layers and substituting in a more sophisticated optimizer. 92 | Chapter 5: Implementing Neural Networks in PyTorch --- Page 109 --- Summary In this chapter, we covered the basics of PyTorch and its functionality. Specifically, we learned the concept of tensors in PyTorch, and how these tensors store numerical information.",
    "ly, we learned the concept of tensors in PyTorch, and how these tensors store numerical information. Additionally, we learned how to manipulate tensors via tensor opera‐ tions, access the data within a tensor, and set a few important attributes. We also discussed gradients in PyTorch and how they can be stored within a tensor. We built our first neural network via standard nn functionality in the the PyTorch nn module section.",
    "e built our first neural network via standard nn functionality in the the PyTorch nn module section. Comparing the nn-based approach with an approach that used PyTorch ten‐ sors solely out of the box showed much of the effective abstraction that the nn module provides, lending to its ease of use. And finally, we put all of our learnings together in the final section, where we trained and tested an MNIST digits feed-forward neural classifier to 91% accuracy on the PyTorch-provided test set.",
    "ted an MNIST digits feed-forward neural classifier to 91% accuracy on the PyTorch-provided test set. Although we covered much of the fundamentals and have equipped you with the knowledge you need to get your hands dirty, we have only scratched the surface of all that the PyTorch API has to offer—we encourage you to explore further and improve upon the models we built in this section.",
    "s to offer—we encourage you to explore further and improve upon the models we built in this section. We recommend that you visit the PyTorch documentation to learn more and build your own neural nets, including trying other architectures, on a variety of online datasets, such as the CIFAR-10 image recognition datasets. In the next section, we will cover neural network implementation, one of the other most popular deep learning frameworks in use today.",
    "neural network implementation, one of the other most popular deep learning frameworks in use today. Summary | 93 --- Page 111 --- 1Bengio, Y oshua, et al. “Greedy Layer-Wise Training of Deep Networks.",
    "Summary | 93 --- Page 111 --- 1Bengio, Y oshua, et al. “Greedy Layer-Wise Training of Deep Networks. ” Advances in Neural Information Processing Systems 19 (2007): 153.CHAPTER 6 Beyond Gradient Descent The Challenges with Gradient Descent The fundamental ideas behind neural networks have existed for decades, but it wasn’t until recently that neural network-based learning models have become mainstream.",
    "ades, but it wasn’t until recently that neural network-based learning models have become mainstream. Our fascination with neural networks has everything to do with their expressiveness, a quality we’ve unlocked by creating networks with many layers. As we have discussed in previous chapters, deep neural networks are able to crack problems that were previously deemed intractable.",
    "s chapters, deep neural networks are able to crack problems that were previously deemed intractable. Training deep neural networks end to end, however, is fraught with difficult challenges that took many technological innovations to unravel, including massive labeled datasets (ImageNet, CIFAR-10, etc.), better hardware in the form of GPU acceleration, and several algorithmic discoveries.",
    "FAR-10, etc.), better hardware in the form of GPU acceleration, and several algorithmic discoveries. For several years, researchers resorted to layer-wise greedy pretraining to grapple with the complex error surfaces presented by deep learning models.1 These time- intensive strategies would try to find more accurate initializations for the model’s parameters one layer at a time before using minibatch gradient descent to converge to the optimal parameter settings.",
    "yer at a time before using minibatch gradient descent to converge to the optimal parameter settings. More recently, however, breakthroughs in optimi‐ zation methods have enabled us to train models directly in an end-to-end fashion. In this chapter, we will discuss several of these breakthroughs. The next couple of sections will focus primarily on local minima and whether they pose hurdles for successfully training deep models.",
    "focus primarily on local minima and whether they pose hurdles for successfully training deep models. Then we will further explore the nonconvex error surfaces induced by deep models, why vanilla minibatch gradient descent falls short, and how modern nonconvex optimizers overcome these pitfalls.",
    "minibatch gradient descent falls short, and how modern nonconvex optimizers overcome these pitfalls. 95 --- Page 112 --- Local Minima in the Error Surfaces of Deep Networks The primary challenge in optimizing deep learning models is that we are forced to use minimal local information to infer the global structure of the error surface. This is difficult because there is usually very little correspondence between local and global structure. Take the following analogy as an example.",
    "little correspondence between local and global structure. Take the following analogy as an example. Let’s assume you’re an insect on the continental United States. Y ou’re dropped ran‐ domly on the map, and your goal is to find the lowest point on this surface. How do you do it? If all you can observe is your immediate surroundings, this seems like an intractable problem.",
    "o it? If all you can observe is your immediate surroundings, this seems like an intractable problem. If the surface of the US were bowl-shaped (or mathematically speaking, convex) and we were smart about our learning rate, we could use the gradient descent algorithm to eventually find the bottom of the bowl.",
    "earning rate, we could use the gradient descent algorithm to eventually find the bottom of the bowl. But the surface of the US is extremely complex, that is to say, is a nonconvex surface, which means that even if we find a valley (a local minimum), we have no idea if it’s the lowest valley on the map (the global minimum).",
    "valley (a local minimum), we have no idea if it’s the lowest valley on the map (the global minimum). In Chapter 4 , we talked about how a minibatch version of gradient descent can help navigate a troublesome error surface when there are spurious regions of magnitude zero gradients. But as we can see in Figure 6-1 , even a stochastic error surface won’t save us from a deep local minimum. Figure 6-1.",
    "in Figure 6-1 , even a stochastic error surface won’t save us from a deep local minimum. Figure 6-1. Minibatch gradient descent may aid in escaping shallow local minima, but often fails when dealing with deep local minima, as shown Now comes the critical question. Theoretically, local minima pose a significant issue. But in practice, how common are local minima in the error surfaces of deep net‐ works? And in which scenarios are they actually problematic for training?",
    "rror surfaces of deep net‐ works? And in which scenarios are they actually problematic for training? In the following two sections, we’ll pick apart common misconceptions about local minima. 96 | Chapter 6: Beyond Gradient Descent --- Page 113 --- Model Identifiability The first source of local minima is tied to a concept commonly referred to as model identifiability .",
    "he first source of local minima is tied to a concept commonly referred to as model identifiability . One observation about deep neural networks is that their error surfaces are guaranteed to have a large—and in some cases, an infinite—number of local minima. There are two major reasons this observation is true. The first is that within a layer of a fully connected feed-forward neural network, any rearrangement of neurons will still give you the same final output at the end of the network.",
    "k, any rearrangement of neurons will still give you the same final output at the end of the network. We illustrate this using a simple three-neuron layer in Figure 6-2 . As a result, within a layer with n neurons, there are n! ways to rearrange parameters. And for a deep network with l layers, each with n neurons, we have a total of n!l equiva‐ lent configurations. Figure 6-2.",
    "with l layers, each with n neurons, we have a total of n!l equiva‐ lent configurations. Figure 6-2. Rearranging neurons in a layer of a neural network results in equivalent configurations due to symmetry In addition to the symmetries of neuron rearrangements, nonidentifiability is present in other forms in certain kinds of neural networks. For example, there is an infinite number of equivalent configurations that for an individual ReLU neuron result in equivalent networks.",
    "umber of equivalent configurations that for an individual ReLU neuron result in equivalent networks. Because an ReLU uses a piecewise linear function, we are free to multiply all of the incoming weights by any nonzero constant k while scaling all of Model Identifiability | 97 --- Page 114 --- 2Goodfellow, Ian J., Oriol Vinyals, and Andrew M. Saxe. “Qualitatively characterizing neural network optimi‐ zation problems.",
    "l Vinyals, and Andrew M. Saxe. “Qualitatively characterizing neural network optimi‐ zation problems. ” arXiv preprint arXiv :1412.6544 (2014).the outgoing weights by 1 k without changing the behavior of the network. We leave the justification for this statement as an exercise for you. Ultimately, however, local minima that arise because of the nonidentifiability of deep neural networks are not inherently problematic.",
    "that arise because of the nonidentifiability of deep neural networks are not inherently problematic. This is because all nonidentifiable configurations behave in an indistinguishable fashion no matter what input values they are fed. This means they will achieve the same error on the training, validation, and testing datasets. In other words, all of these models will have learned equally from the training data and will have identical behavior during generalization to unseen examples.",
    "ly from the training data and will have identical behavior during generalization to unseen examples. Instead, local minima are only problematic when they are spurious . A spurious local minimum corresponds to a configuration of weights in a neural network that incurs a higher error than the configuration at the global minimum.",
    "weights in a neural network that incurs a higher error than the configuration at the global minimum. If these kinds of local minima are common, we quickly run into significant problems while using gradient-based optimization methods because we can take only local structure into account. How Pesky Are Spurious Local Minima in Deep Networks? For many years, deep learning practitioners blamed all of their troubles in training deep networks on spurious local minima, albeit with little evidence.",
    "l of their troubles in training deep networks on spurious local minima, albeit with little evidence. Today, it remains an open question whether spurious local minima with a high error rate relative to the global minimum are common in practical deep networks. However, many recent studies seem to indicate that most local minima have error rates and generalization characteristics that are very similar to global minima.",
    "l minima have error rates and generalization characteristics that are very similar to global minima. One way we might try to naively tackle this problem is by plotting the value of the error function over time as we train a deep neural network. This strategy, however, doesn’t give us enough information about the error surface because it is difficult to tell whether the error surface is “bumpy, ” or whether we merely have a difficult time figuring out which direction we should be moving in.",
    "y, ” or whether we merely have a difficult time figuring out which direction we should be moving in. To more effectively analyze this problem, Goodfellow et al.",
    "which direction we should be moving in. To more effectively analyze this problem, Goodfellow et al. (a team of researchers collaborating between Google and Stanford) published a paper in 2014 that attempted to separate these two potential confounding factors.2 Instead of analyzing the error function over time, they cleverly investigated what happens on the error surface between a randomly initialized parameter vector and a successful final solution by using linear interpolation.",
    "randomly initialized parameter vector and a successful final solution by using linear interpolation. So, given a randomly initialized parameter vector θi and 98 | Chapter 6: Beyond Gradient Descent --- Page 115 --- stochastic gradient descent (SGD) solution θf, we aim to compute the error function at every point along the linear interpolation θα=α·θf+1 −α·θi. They wanted to investigate whether local minima would hinder our gradient-based search method even if we knew which direction to move in.",
    "cal minima would hinder our gradient-based search method even if we knew which direction to move in. They showed that for a wide variety of practical networks with different types of neurons, the direct path between a randomly initialized point in the parameter space and a stochastic gradient descent solution isn’t plagued with troublesome local minima. We can even demonstrate this ourselves using the feed-forward ReLU network we built in Chapter 5 .",
    ". We can even demonstrate this ourselves using the feed-forward ReLU network we built in Chapter 5 . Using a checkpoint file that we saved while training our original feed-forward network, we can reinstantiate the model using load_state_dict and torch.load : # Load checkpoint from SGD training IN_DIM, FEATURE_DIM, OUT_DIM = 784, 256, 10 model = Net(IN_DIM, FEATURE_DIM, OUT_DIM) model.load_state_dict(torch.load('mnist.pt')) In PyTorch, we cannot access a model’s parameters directly since the model.parame ters() method returns a generator that provides only a copy of the parameters.",
    "ince the model.parame ters() method returns a generator that provides only a copy of the parameters. To modify a model’s parameters, we use torch.load to read the state dictionary containing the parameter values from the file, and then use load_state_dict to set the model’s parameters with these values.",
    "values from the file, and then use load_state_dict to set the model’s parameters with these values. Instead of using torch.load to load the state dictionary from a file, we can also access the state dictionary from a model itself using the state_dict method: import copy # Access parameters with state_dict opt_state_dict = copy.deepcopy(model.state_dict()) for param_tensor in opt_state_dict: print(param_tensor, \"\\t\", opt_state_dict[param_tensor].size()) # outputs: # classifier.1.weight torch.Size([256, 784]) # classifier.1.bias torch.Size([256]) # classifier.3.weight torch.Size([256, 256]) # classifier.3.bias torch.Size([256]) # classifier.5.weight torch.Size([10, 256]) # classifier.5.bias torch.Size([10]) Note that we need to use the copy.deepcopy method to copy a dictionary with its values.",
    "h.Size([10]) Note that we need to use the copy.deepcopy method to copy a dictionary with its values. Just setting opt_state_dict = model.state_dict() would result in a How Pesky Are Spurious Local Minima in Deep Networks? | 99 --- Page 116 --- shallow copy, and opt_state_dict would be changed when we load our model with interpolated parameters later.",
    "copy, and opt_state_dict would be changed when we load our model with interpolated parameters later. Next, we instantiate a new model with randomly initialized parameters and save those parameters as rand_state_dict : # Create randomly initialized network model_rand = Net(IN_DIM, FEATURE_DIM, OUT_DIM) rand_state_dict = copy.deepcopy(model_rand.state_dict()) With these two networks appropriately initialized, we can now construct the linear interpolation using the mixing parameters alpha and beta : # Create a new state_dict for interpolated parameters test_model = Net(IN_DIM, FEATURE_DIM, OUT_DIM) test_state_dict = copy.deepcopy(test_model.state_dict()) alpha = 0.2 beta = 1.0 - alpha for p in opt_state_dict: test_state_dict[p] = (opt_state_dict[p] * beta + rand_state_dict[p] * alpha) Next, we will compute the average loss over the entire test dataset using the model with the interpolated parameters.",
    "pute the average loss over the entire test dataset using the model with the interpolated parameters. For convenience, let’s create a function for inference: def inference(testloader, model, loss_fn): running_loss = 0.0 with torch.no_grad(): for inputs, labels in testloader: outputs = model(inputs) loss = loss_fn(outputs, labels) running_loss += loss running_loss /= len(testloader) return running_loss Finally, we can vary the value of alpha to understand how the error surface changes as we traverse the line between the randomly initialized point and the final SGD solution: results = [] for alpha in torch.arange(-2, 2, 0.05): beta = 1.0 - alpha # Compute interpolated parameters for p in opt_state_dict: test_state_dict[p] = (opt_state_dict[p] * beta + rand_state_dict[p] * alpha) # Load interpolated parameters into test model model.load_state_dict(test_state_dict) 100 | Chapter 6: Beyond Gradient Descent --- Page 117 --- # Compute loss given interpolated parameters loss = inference(trainloader, model, loss_fn) results.append(loss.item()) This creates Figure 6-3 , which we can inspect ourselves.",
    "odel, loss_fn) results.append(loss.item()) This creates Figure 6-3 , which we can inspect ourselves. In fact, if we run this experiment over and over again, we find that there are no truly troublesome local minima that would get us stuck. It seems that the true struggle of gradient descent isn’t the existence of troublesome local minima, but instead is that we have a tough time finding the appropriate direction to move in. We’ll return to this thought a little later. Figure 6-3.",
    "nding the appropriate direction to move in. We’ll return to this thought a little later. Figure 6-3. The cost function of a three-layer feed-forward network as we linearly interpolate on the line connecting a randomly initialized parameter vector and an SGD solution Flat Regions in the Error Surface Although it seems that our analysis is devoid of troublesome local minimum, we do notice a peculiar flat region where the gradient approaches zero when we get to approximately alpha=1 .",
    "ice a peculiar flat region where the gradient approaches zero when we get to approximately alpha=1 . This point is not a local minima, so it is unlikely to get us completely stuck, but it seems like the zero gradient might slow down learning if we are unlucky enough to encounter it. Flat Regions in the Error Surface | 101 --- Page 118 --- More generally, given an arbitrary function, a point at which the gradient is the zero vector is called a critical point .",
    "an arbitrary function, a point at which the gradient is the zero vector is called a critical point . Critical points come in various flavors. We’ve already talked about local minima. It’s also not hard to imagine their counterparts, the local maxima , which don’t really pose much of an issue for SGD. But then there are these strange critical points that lie somewhere in between. These “flat” regions that are potentially pesky but not necessarily deadly are called saddle points .",
    "hese “flat” regions that are potentially pesky but not necessarily deadly are called saddle points . It turns out that as our function has more and more dimensions (i.e., we have more and more parameters in our model), saddle points are exponentially more likely than local minima. Let’s try to intuit why. For a 1D cost function, a critical point can take one of three forms, as shown in Figure 6-4 . Loosely, let’s assume each of these three configurations is equally likely.",
    "as shown in Figure 6-4 . Loosely, let’s assume each of these three configurations is equally likely. This means that given a random critical point in a random 1D function, it has one-third probability of being a local minimum. This means that if we have a total of k critical points, we can expect to have a total of k 3 local minima. Figure 6-4. Analyzing a critical point along a single dimension We can also extend this to higher dimensional functions.",
    "g a critical point along a single dimension We can also extend this to higher dimensional functions. Consider a cost function operating in a d-dimensional space. Let’s take an arbitrary critical point. It turns out that figuring out if this point is a local minimum, local maximum, or a saddle point is a little bit trickier than in the one-dimensional case. Consider the error surface in Figure 6-5 .",
    "s a little bit trickier than in the one-dimensional case. Consider the error surface in Figure 6-5 . Depending on how you slice the surface (from A to B or from C to D), the critical point looks like either a minimum or a maximum. In reality, it’s neither. It’s a more complex type of saddle point. 102 | Chapter 6: Beyond Gradient Descent --- Page 119 --- 3Dauphin, Y ann N., et al. “Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.",
    "al. “Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. ” Advances in Neural Information Processing Systems . 2014. Figure 6-5. A saddle point over a 2D error surface In general, in a d-dimensional parameter space, we can slice through a critical point on d different axes. A critical point can be a local minimum only if it appears as a local minimum in every single one of the d 1D subspaces.",
    "be a local minimum only if it appears as a local minimum in every single one of the d 1D subspaces. Using the fact that a critical point can come in one of three different flavors in a one-dimensional subspace, we realize that the probability that a random critical point is in a random function is 1 3d. This means that a random function with k critical points has an expected number of k 3d local minima.",
    "his means that a random function with k critical points has an expected number of k 3d local minima. In other words, as the dimensionality of our parameter space increases, local minima become exponentially more rare. A more rigorous treatment of this topic is outside the scope of this book, but was explored more extensively by Dauphin et al. in 2014.3 So what does this mean for optimizing deep learning models? For stochastic gradient descent, it’s still unclear.",
    "this mean for optimizing deep learning models? For stochastic gradient descent, it’s still unclear. It seems like these flat segments of the error surface are pesky but ultimately don’t prevent stochastic gradient descent from converging to a good answer. However, it does pose serious problems for methods that attempt to directly solve for a point where the gradient is zero.",
    "serious problems for methods that attempt to directly solve for a point where the gradient is zero. This has been a major hindrance to the usefulness of certain second-order optimization methods for deep learning models, which we will discuss later.",
    "of certain second-order optimization methods for deep learning models, which we will discuss later. Flat Regions in the Error Surface | 103 --- Page 120 --- When the Gradient Points in the Wrong Direction Upon analyzing the error surfaces of deep networks, it seems like the most critical challenge to optimizing deep networks is finding the correct trajectory to move in.",
    "he most critical challenge to optimizing deep networks is finding the correct trajectory to move in. It’s no surprise, however, that this is a major challenge when we look at what happens to the error surface around a local minimum. As an example, we consider an error surface defined over a 2D parameter space, as shown in Figure 6-6 . Figure 6-6.",
    "we consider an error surface defined over a 2D parameter space, as shown in Figure 6-6 . Figure 6-6. Local information encoded by the gradient usually does not corroborate the global structure of the error surface Revisiting the contour diagrams we explored in Chapter 4 , notice that the gradient isn’t usually a very good indicator of the good trajectory. Specifically, only when the contours are perfectly circular does the gradient always point in the direction of the local minimum.",
    "ontours are perfectly circular does the gradient always point in the direction of the local minimum. However, if the contours are extremely elliptical (as is usually the case for the error surfaces of deep networks), the gradient can be as inaccurate as 90 degrees away from the correct direction. We extend this analysis to an arbitrary number of dimensions using some mathemat‐ ical formalism.",
    "n. We extend this analysis to an arbitrary number of dimensions using some mathemat‐ ical formalism. For every weight wi in the parameter space, the gradient computes the value of ∂E ∂wi, or how the value of the error changes as we change the value of wi. Taken together over all weights in the parameter space, the gradient gives us the direction of steepest descent.",
    "er over all weights in the parameter space, the gradient gives us the direction of steepest descent. The general problem with taking a significant step in this direction, however, is that the gradient could be changing under our feet as we move! We demonstrate this simple fact in Figure 6-7 .",
    "radient could be changing under our feet as we move! We demonstrate this simple fact in Figure 6-7 . Going back to the 2D example, 104 | Chapter 6: Beyond Gradient Descent --- Page 121 --- if our contours are perfectly circular and we take a big step in the direction of the steepest descent, the gradient doesn’t change direction as we move. However, this is not the case for highly elliptical contours. Figure 6-7.",
    "ange direction as we move. However, this is not the case for highly elliptical contours. Figure 6-7. The direction of the gradient changes as we move along the direction of steep‐ est descent, as determined from a starting point; the gradient vectors are normalized to identical length to emphasize the change in direction of the gradient vector More generally, we can quantify how the gradient changes under our feet as we move in a certain direction by computing second derivatives.",
    "e gradient changes under our feet as we move in a certain direction by computing second derivatives. Specifically, we want to measure ∂∂E/∂wj ∂wi, which tells us how the gradient component for wj changes as we change the value of wi. We can compile this information into a special matrix known as the Hessian matrix (H). And when describing an error surface where the gradient changes underneath our feet as we move in the direction of steepest descent, this matrix is said to be ill-conditioned .",
    "ur feet as we move in the direction of steepest descent, this matrix is said to be ill-conditioned . Hessian Limits Optimization Certain properties of the Hessian matrix (specifically that it is real and symmetric) allow us to efficiently determine the second derivative (which approximates the curvature of a surface) as we move in a specific direction. Specifically, if we have a unit vector d, the second derivative in that direction is given by dHd.",
    ". Specifically, if we have a unit vector d, the second derivative in that direction is given by dHd. We can now use a second-order approximation via Taylor series to understand what happens to the error function as we step from the current parameter vector x(i) to a new parameter vector x along gradient vector g evaluated at x(i): Ex≈Exi+x−xi⊤g+1 2x−xi⊤Hx−xi When the Gradient Points in the Wrong Direction | 105 --- Page 122 --- If we go further to state that we will be moving ϵ units in the opposite direction of the gradient, we can simplify our expression even more: Exi−ϵg≈Exi−ϵg⊤g+1 2ϵ2g⊤Hg This expression consists of three terms: (1) the value of the error function at the original parameter vector, (2) the improvement in error afforded by the magnitude of the gradient, and (3) a correction term that incorporates the curvature of the surface as represented by the Hessian matrix.",
    "correction term that incorporates the curvature of the surface as represented by the Hessian matrix. In general, we should be able to use this information to design better optimization algorithms. For instance, we can even naively take the second-order approximation of the error function to determine the learning rate at each step that maximizes the reduction in the error function. It turns out, however, that computing the Hes‐ sian matrix exactly is a difficult task.",
    "or function. It turns out, however, that computing the Hes‐ sian matrix exactly is a difficult task. In the next several sections, we’ll describe optimization breakthroughs that tackle ill-conditioning without directly computing the Hessian matrix. Momentum-Based Optimization Fundamentally, the problem of an ill-conditioned Hessian matrix manifests itself in the form of gradients that fluctuate wildly.",
    "f an ill-conditioned Hessian matrix manifests itself in the form of gradients that fluctuate wildly. As a result, one popular mechanism for dealing with ill-conditioning bypasses the computation of the Hessian, and instead, focuses on how to cancel out these fluctuations over the duration of training. One way to think about how we might tackle this problem is by investigating how a ball rolls down a hilly surface.",
    "nk about how we might tackle this problem is by investigating how a ball rolls down a hilly surface. Driven by gravity, the ball eventually settles into a minimum on the surface, but for some reason, it doesn’t suffer from the wild fluctuations and divergences that happen during gradient descent. Why is this the case? Unlike in stochastic gradient descent (which uses only the gradient), there are two major components that determine how a ball rolls down an error surface.",
    "the gradient), there are two major components that determine how a ball rolls down an error surface. The first, which we already model in SGD as the gradient, is what we commonly refer to as acceleration. But acceleration does not single-handedly determine the ball’s movements. Instead, its motion is more directly determined by its velocity. Acceleration indirectly changes the ball’s position only by modifying its velocity.",
    "by its velocity. Acceleration indirectly changes the ball’s position only by modifying its velocity. Velocity-driven motion is desirable because it counteracts the effects of a wildly fluctuating gradient by smoothing the ball’s trajectory over its history. Velocity serves as a form of memory, and this allows us to more effectively accumulate movement in the direction of the minimum while canceling out oscillating accelerations in orthogonal directions.",
    "the direction of the minimum while canceling out oscillating accelerations in orthogonal directions. Our goal, then, is to somehow generate an analog for velocity in our optimization algorithm. We can do this by keeping track of an exponentially weighted decay of past gradients. The premise is simple: every update is computed by 106 | Chapter 6: Beyond Gradient Descent --- Page 123 --- 4Polyak, Boris T. “Some methods of speeding up the convergence of iteration methods.",
    "-- Page 123 --- 4Polyak, Boris T. “Some methods of speeding up the convergence of iteration methods. ” USSR Computational Mathematics and Mathematical Physics 4.5 (1964): 1-17.combining the update in the last iteration with the current gradient.",
    "tical Physics 4.5 (1964): 1-17.combining the update in the last iteration with the current gradient. Concretely, we compute the change in the parameter vector as follows: vi=mvi− 1−ϵgi θi=θi− 1+vi We use the momentum hyperparameter m to determine what fraction of the previous velocity to retain in the new update, and add this “memory” of past gradients to our current gradient.",
    "locity to retain in the new update, and add this “memory” of past gradients to our current gradient. This approach is commonly referred to as momentum .4 Because the momentum term increases the step size we take, using momentum may require a reduced learning rate compared to vanilla stochastic gradient descent. To better visualize how momentum works, we’ll explore a toy example. Specifically, we’ll investigate how momentum affects updates during a random walk .",
    "e a toy example. Specifically, we’ll investigate how momentum affects updates during a random walk . A random walk is a succession of randomly chosen steps. In our example, we’ll imagine a particle on a line that, at every time interval, randomly picks a step size between –10 and 10 and takes a moves in that direction.",
    "ry time interval, randomly picks a step size between –10 and 10 and takes a moves in that direction. This is simply expressed as: rand_walk = [torch.randint(-10, 10, (1,1)) for x in range(100)] We’ll then simulate what happens when we use a slight modification of momentum (i.e., the standard exponentially weighted moving average algorithm) to smooth our choice of step at every time interval.",
    "xponentially weighted moving average algorithm) to smooth our choice of step at every time interval. Again, we can concisely express this as: momentum = 0.1 momentum_rand_walk = \\ [torch.randint(-10, 10, (1,1)) for x in range(100)] for i in range(1, len(rand_walk) - 1): prev = momentum_rand_walk[i-1] rand_choice = torch.randint(-10, 10, (1,1)).item() new_step = momentum * prev + (1 - momentum) * rand_choice momentum_rand_walk[i] = new_step The results, as we vary the momentum from 0 to 1, are quite staggering.",
    "ntum_rand_walk[i] = new_step The results, as we vary the momentum from 0 to 1, are quite staggering. Momentum significantly reduces the volatility of updates. The larger the momentum, the less responsive we are to new updates (e.g., a large inaccuracy on the first estimation of trajectory propagates for a significant period of time). We summarize the results of our toy experiment in Figure 6-8 . Momentum-Based Optimization | 107 --- Page 124 --- Figure 6-8.",
    "of our toy experiment in Figure 6-8 . Momentum-Based Optimization | 107 --- Page 124 --- Figure 6-8. Momentum smooths volatility in the step sizes during a random walk using an exponentially weighted moving average To investigate how momentum actually affects the training of feed-forward neural networks, we can retrain our trusty MNIST feed-forward network with a PyTorch momentum optimizer.",
    "al networks, we can retrain our trusty MNIST feed-forward network with a PyTorch momentum optimizer. In this case, we can get away with using the same learning rate (0.01) with a typical momentum of 0.9: optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9) optimizer.step() Notice that when we create a PyTorch optimizer, we need to pass in model.param eters() . The resulting speedup is staggering.",
    "e a PyTorch optimizer, we need to pass in model.param eters() . The resulting speedup is staggering. We display how the cost function changes over time by comparing the visualizations in Figure 6-9 . The figure demon‐ strates that to achieve a cost of 0.1 without momentum (right) requires nearly 18,000 steps (minibatches), whereas with momentum (left), we require just over 2,000. 108 | Chapter 6: Beyond Gradient Descent --- Page 125 --- 5Sutskever, Ilya, et al.",
    "just over 2,000. 108 | Chapter 6: Beyond Gradient Descent --- Page 125 --- 5Sutskever, Ilya, et al. “On the importance of initialization and momentum in deep learning. ” ICML (3) 28 (2013): 1139-1147. Figure 6-9. Comparing training a feed-forward network with (right) and without (left) momentum demonstrates a massive decrease in training time Recently, more work has explored how the classical momentum technique can be improved. Sutskever et al.",
    "ently, more work has explored how the classical momentum technique can be improved. Sutskever et al. in 2013 proposed an alternative called Nesterov momen‐ tum, which computes the gradient on the error surface at θ+vi− 1 during the velocity update instead of at θ.5 This subtle difference seems to allow Nesterov momentum to change its velocity in a more responsive way.",
    "subtle difference seems to allow Nesterov momentum to change its velocity in a more responsive way. It’s been shown that this method has clear benefits in batch gradient descent (convergence guarantees and the ability to use a higher momentum for a given learning rate as compared to classical momentum), but it’s not entirely clear whether this is true for the more stochastic minibatch gradient descent used in most deep learning optimization approaches.",
    "r the more stochastic minibatch gradient descent used in most deep learning optimization approaches. Nerestov momentum is supported in PyTorch out-of-the-box by setting the nesterov argument: optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9, nesterov = True) A Brief View of Second-Order Methods As we discussed, computing the Hessian is a computationally difficult task, and momentum afforded us significant speedup without having to worry about it alto‐ gether.",
    "lt task, and momentum afforded us significant speedup without having to worry about it alto‐ gether. Several second-order methods, however, have been researched over the past several years that attempt to approximate the Hessian directly. For completeness, we give a broad overview of these methods, but a detailed treatment is beyond the scope of this text. The first is conjugate gradient descent , which arises out of attempting to improve on a naive method of steepest descent.",
    "gradient descent , which arises out of attempting to improve on a naive method of steepest descent. In steepest descent, we compute the direction A Brief View of Second-Order Methods | 109 --- Page 126 --- 6Møller, Martin Fodslette. “ A Scaled Conjugate Gradient Algorithm for Fast Supervised Learning. ” Neural Networks 6.4 (1993): 525-533. 7Broyden, C. G. “ A New Method of Solving Nonlinear Simultaneous Equations. ” The Computer Journal 12.1 (1969): 94-99. 8Bonnans, Joseph-Frédéric, et al.",
    "Simultaneous Equations. ” The Computer Journal 12.1 (1969): 94-99. 8Bonnans, Joseph-Frédéric, et al. Numerical Optimization: Theoretical and Practical Aspects . Springer Science & Business Media, 2006.of the gradient and then line search to find the minimum along that direction. We jump to the minimum and then recompute the gradient to determine the direction of the next line search.",
    "p to the minimum and then recompute the gradient to determine the direction of the next line search. It turns out that this method ends up zigzagging a significant amount, as shown in Figure 6-10 , because each time we move in the direction of steepest descent, we undo a little bit of progress in another direction. A remedy to this problem is moving in a conjugate direction relative to the previous choice instead of the direction of steepest descent.",
    "a conjugate direction relative to the previous choice instead of the direction of steepest descent. The conjugate direction is chosen by using an indirect approximation of the Hessian to linearly combine the gradient and our previous direction. With a slight modification, this method generalizes to the nonconvex error surfaces we find in deep networks.6 Figure 6-10.",
    "ion, this method generalizes to the nonconvex error surfaces we find in deep networks.6 Figure 6-10. The method of steepest descent often zigzags; conjugate descent attempts to remedy this issue An alternative optimization algorithm known as the Broyden–Fletcher–Goldfarb– Shanno (BFGS) algorithm attempts to compute the inverse of the Hessian matrix iteratively and use the inverse Hessian to more effectively optimize the parameter vector.7 In its original form, BFGS has a significant memory footprint, but recent work has produced a more memory-efficient version known as L-BFGS .8 In general, while these methods hold some promise, second-order methods are still an area of active research and are unpopular among practitioners.",
    "se, second-order methods are still an area of active research and are unpopular among practitioners. PyTorch does, however, support L-BFGS as well as other second-order methods, such as Averaged Stochastic Gradient Descent, for your own experimentation. 110 | Chapter 6: Beyond Gradient Descent --- Page 127 --- 9Duchi, John, Elad Hazan, and Y oram Singer. “ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.",
    ", and Y oram Singer. “ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. ” Journal of Machine Learning Research 12.Jul (2011): 2121-2159.Learning Rate Adaptation As we have discussed previously, another major challenge for training deep networks is appropriately selecting the learning rate. Choosing the correct learning rate has long been one of the most troublesome aspects of training deep networks because it has a major impact on a network’s performance.",
    "ublesome aspects of training deep networks because it has a major impact on a network’s performance. A learning rate that is too small doesn’t learn quickly enough, but a learning rate that is too large may have difficulty converging as we approach a local minimum or region that is ill-conditioned. One of the major breakthroughs in modern deep network optimization was the advent of learning rate adaption.",
    "he major breakthroughs in modern deep network optimization was the advent of learning rate adaption. The basic concept behind learning rate adaptation is that the optimal learning rate is appropriately modified over the span of learning to achieve good convergence properties. Over the next several sections, we’ll discuss AdaGrad, RMSProp, and Adam, three of the most popular adaptive learning rate algorithms.",
    "’ll discuss AdaGrad, RMSProp, and Adam, three of the most popular adaptive learning rate algorithms. AdaGrad—Accumulating Historical Gradients The first algorithm we’ll discuss is AdaGrad, which attempts to adapt the global learning rate over time using an accumulation of the historical gradients, first pro‐ posed by Duchi et al. in 2011.9 Specifically, we keep track of a learning rate for each parameter.",
    "‐ posed by Duchi et al. in 2011.9 Specifically, we keep track of a learning rate for each parameter. This learning rate is inversely scaled with respect to the square root of the sum of the squares (root mean square) of all the parameter’s historical gradients. We can express this mathematically. We initialize a gradient accumulation vec‐ tor r0=0.",
    "gradients. We can express this mathematically. We initialize a gradient accumulation vec‐ tor r0=0. At every step, we accumulate the square of all the gradient parameters as follows (where the ⊙ operation is element-wise tensor multiplication): ri=ri− 1+gi⊙gi Then we compute the update as usual, except our global learning rate ϵ is divided by the square root of the gradient accumulation vector: θi=θi− 1−ϵ δ ⊕ri⊙g Note that we add a tiny number δ (~10−7) to the denominator to prevent division by zero.",
    "1−ϵ δ ⊕ri⊙g Note that we add a tiny number δ (~10−7) to the denominator to prevent division by zero. Also, the division and addition operations are broadcast to the size of the gradient accumulation vector and applied element-wise. In PyTorch, a built-in optimizer allows for easily utilizing AdaGrad as a learning algorithm: Learning Rate Adaptation | 111 --- Page 128 --- 10Tieleman, Tijmen, and Geoffrey Hinton.",
    "algorithm: Learning Rate Adaptation | 111 --- Page 128 --- 10Tieleman, Tijmen, and Geoffrey Hinton. “Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude. ” COURSERA: Neural Networks for Machine Learning 4.2 (2012).optimizer = optim.Adagrad(model.parameters(), lr = 0.01, weight_decay = 0, initial_accumulator_value = 0) The only hitch is that in PyTorch, the δ and initial gradient accumulation vector are rolled together into the initial_accumulator_value argument.",
    "nitial gradient accumulation vector are rolled together into the initial_accumulator_value argument. On a functional level, this update mechanism means that the parameters with the largest gradients experience a rapid decrease in their learning rates, while parameters with smaller gradients observe only a small decrease in their learning rates.",
    "ates, while parameters with smaller gradients observe only a small decrease in their learning rates. The ulti‐ mate effect is that AdaGrad forces more progress in the more gently sloped directions on the error surface, which can help overcome ill-conditioned surfaces. This results in some good theoretical properties, but in practice, training deep learning models with AdaGrad can be somewhat problematic.",
    "properties, but in practice, training deep learning models with AdaGrad can be somewhat problematic. Empirically, AdaGrad has a tendency to cause a premature drop in learning rate, and as a result doesn’t work particularly well for some deep models. In the next section, we’ll describe RMSProp, which attempts to remedy this shortcoming.",
    "deep models. In the next section, we’ll describe RMSProp, which attempts to remedy this shortcoming. RMSProp—Exponentially Weighted Moving Average of Gradients While AdaGrad works well for simple convex functions, it isn’t designed to navigate the complex error surfaces of deep networks. Flat regions may force AdaGrad to decrease the learning rate before it reaches a minimum. The conclusion is that simply using a naive accumulation of gradients isn’t sufficient.",
    "s a minimum. The conclusion is that simply using a naive accumulation of gradients isn’t sufficient. Our solution is to bring back a concept we introduced earlier while discussing momentum to dampen fluctuations in the gradient. Compared to naive accumula‐ tion, exponentially weighted moving averages also enables us to “toss out” measure‐ ments that we made a long time ago.",
    "weighted moving averages also enables us to “toss out” measure‐ ments that we made a long time ago. More specifically, our update to the gradient accumulation vector is now as follows: ri=ρri− 1+1 −ρgi⊙gi The decay factor ρ determines how long we keep old gradients. The smaller the decay factor, the shorter the effective window.",
    "ines how long we keep old gradients. The smaller the decay factor, the shorter the effective window. Plugging this modification into AdaGrad gives rise to the RMSProp learning algorithm, first proposed by Geoffrey Hinton.10 112 | Chapter 6: Beyond Gradient Descent --- Page 129 --- 11Kingma, Diederik, and Jimmy Ba. “ Adam: A Method for Stochastic Optimization. ” arXiv preprint arXiv :1412.6980 (2014).In PyTorch, we can instantiate the RMSProp optimizer with the following code.",
    "rXiv :1412.6980 (2014).In PyTorch, we can instantiate the RMSProp optimizer with the following code. Note that in this case, unlike in AdaGrad, we pass in δ separately as the epsilon argument to the constructor: optimizer = optim.RMSprop(model.parameters(), lr = 0.01, alpha = 0.99, eps = 1e-8, weight_decay = 0, momentum = 0) As the template suggests, we can utilize RMSProp with momentum (specifically Nerestov momentum).",
    "0) As the template suggests, we can utilize RMSProp with momentum (specifically Nerestov momentum). Overall, RMSProp has been shown to be a highly effective optimizer for deep neural networks, and is a default choice for many seasoned practitioners. Adam—Combining Momentum and RMSProp Before concluding our discussion of modern optimizers, we discuss one final algo‐ rithm—Adam.11 Spiritually, we can think about Adam as a variant combination of RMSProp and momentum. The basic idea is as follows.",
    "can think about Adam as a variant combination of RMSProp and momentum. The basic idea is as follows. We want to keep track of an exponentially weighted mov‐ ing average of the gradient (essentially the concept of velocity in classical momen‐ tum), which we can express as follows: mi = β1mi – 1 + (1 – β1)gi This is our approximation of what we call the first moment of the gradient, or Egi.",
    "– 1 + (1 – β1)gi This is our approximation of what we call the first moment of the gradient, or Egi. And similarly to RMSProp, we can maintain an exponentially weighted moving average of the historical gradients. This is our estimation of what we call the second moment of the gradient, or Egi⊙gi: vi=β2vi− 1+1 −β2gi⊙gi However, it turns out these estimations are biased relative to the real moments because we start off by initializing both vectors to the zero vector.",
    "d relative to the real moments because we start off by initializing both vectors to the zero vector. In order to remedy this bias, we derive a correction factor for both estimations. Here, we describe the derivation for the estimation of the second moment. The derivation for the first moment, which is analogous to the derivation here, is left as an exercise for you.",
    "ion for the first moment, which is analogous to the derivation here, is left as an exercise for you. Learning Rate Adaptation | 113 --- Page 130 --- We begin by expressing the estimation of the second moment in terms of all past gradients. This is done by simply expanding the recurrence relationship: vi=β2vi− 1+1 −β2gi⊙gi vi=β2i− 11 −β2g1⊙g 1+β2i− 21 −β2g2⊙g 2+ ...",
    "nding the recurrence relationship: vi=β2vi− 1+1 −β2gi⊙gi vi=β2i− 11 −β2g1⊙g 1+β2i− 21 −β2g2⊙g 2+ ... + 1 −β2gi⊙gi vi=1 −β2∑k= 1iβi−kgk⊙gk We can then take the expected value of both sides to determine how our estimation Evi compares to the real value of Egi⊙gi: Evi=E1 −β2∑k= 1iβi−kgk⊙gk We can also assume that Egk⊙gk≈Egi≈gi because even if the second moment of the gradient has changed since a historical value, β2 should be chosen so that the old second moments of the gradients are essentially decayed out of relevancy.",
    "be chosen so that the old second moments of the gradients are essentially decayed out of relevancy. As a result, we can make the following simplification: Evi≈Egi⊙gi1 −β2∑k= 1iβi−k Evi≈Egi⊙gi1 −β2i Note that we make the final simplification using the elementary algebraic iden‐ tity 1 −xn=1 −x1 +x+ ... +xn− 1.",
    "make the final simplification using the elementary algebraic iden‐ tity 1 −xn=1 −x1 +x+ ... +xn− 1. The results of this derivation and the analo‐ gous derivation for the first moment are the following correction schemes to account for the initialization bias: m̃i = mi 1 −β1i vi=vi 1 −β2i We can then use these corrected moments to update the parameter vector, resulting in the final Adam update: θi=θi− 1−ϵ δ ⊕vim̃i 114 | Chapter 6: Beyond Gradient Descent --- Page 131 --- Recently, Adam has gained popularity because of its corrective measures against the zero initialization bias (a weakness of RMSProp) and its ability to combine the core concepts behind RMSProp with momentum more effectively.",
    "RMSProp) and its ability to combine the core concepts behind RMSProp with momentum more effectively. PyTorch exposes the Adam optimizer through the following constructor: optimizer = optim.Adam(model.parameters(), lr = 0.001, betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0, amsgrad = False) The default hyperparameter settings for Adam for PyTorch generally perform quite well, but Adam is also generally robust to choices in hyperparameters.",
    "Torch generally perform quite well, but Adam is also generally robust to choices in hyperparameters. The only exception is that the learning rate may need to be modified in certain cases from the default value of 0.001. The Philosophy Behind Optimizer Selection In this chapter, we’ve discussed several strategies that are used to make navigating the complex error surfaces of deep networks more tractable.",
    "ategies that are used to make navigating the complex error surfaces of deep networks more tractable. These strategies have culminated in several optimization algorithms, each with its own benefits and short‐ comings. While it would be awfully nice to know when to use which algorithm, there is very little consensus among expert practitioners.",
    "nice to know when to use which algorithm, there is very little consensus among expert practitioners. Currently, the most popular algorithms are minibatch gradient descent, minibatch gradient with momentum, RMSProp, RMSProp with momentum, Adam, and AdaDelta (which we haven’t discussed here, but is also supported by PyTorch). We encourage you to experiment with these optimization algorithms on the feed-forward network model we built.",
    "age you to experiment with these optimization algorithms on the feed-forward network model we built. One important point, however, is that for most deep learning practitioners, the best way to push the cutting edge of deep learning is not by building more advanced optimizers. Instead, the vast majority of breakthroughs in deep learning over the past several decades have been obtained by discovering architectures that are easier to train instead of trying to wrangle with nasty error surfaces.",
    "ering architectures that are easier to train instead of trying to wrangle with nasty error surfaces. We’ll begin focusing on how to leverage architecture to more effectively train neural networks in the rest of this book. The Philosophy Behind Optimizer Selection | 115 --- Page 132 --- Summary In this chapter, we discussed several challenges that arise when trying to train deep networks with complex error surfaces.",
    "cussed several challenges that arise when trying to train deep networks with complex error surfaces. We discussed how while the challenges of spu‐ rious local minima are likely exaggerated, saddle points and ill-conditioning do pose a serious threat to the success of vanilla minibatch gradient descent. We described how momentum can be used to overcome ill-conditioning, and briefly discussed recent research in second-order methods to approximate the Hessian matrix.",
    "ng, and briefly discussed recent research in second-order methods to approximate the Hessian matrix. We also described the evolution of adaptive learning rate optimizers, which tune the learning rate during the training process for better convergence. Next, we’ll begin tackling the larger issue of network architecture and design. We’ll explore computer vision and how we might design deep networks that learn effec‐ tively from complex images.",
    "computer vision and how we might design deep networks that learn effec‐ tively from complex images. 116 | Chapter 6: Beyond Gradient Descent --- Page 133 --- 1Hubel, David H., and Torsten N. Wiesel. “Receptive Fields and Functional Architecture of Monkey Striate Cortex. ” The Journal of Physiology 195.1 (1968): 215-243. 2Cohen, Adolph I. “Rods and Cones. ” Physiology of Photoreceptor Organs . Springer Berlin Heidelberg, 1972.",
    "Adolph I. “Rods and Cones. ” Physiology of Photoreceptor Organs . Springer Berlin Heidelberg, 1972. 63-110.CHAPTER 7 Convolutional Neural Networks Neurons in Human Vision The human sense of vision is unbelievably advanced. Within fractions of seconds, we can identify objects within our field of view, without thought or hesitation.",
    "actions of seconds, we can identify objects within our field of view, without thought or hesitation. Not only can we name objects we are looking at, we can also perceive their depth, perfectly distinguish their contours, and separate the objects from their backgrounds.",
    "their depth, perfectly distinguish their contours, and separate the objects from their backgrounds. Somehow our eyes take in raw voxels of color data, but our brain transforms that information into more meaningful primitives—lines, curves, and shapes—that might indicate, for example, that we’re looking at a house cat.1 Foundational to the human sense of vision is the neuron.",
    "xample, that we’re looking at a house cat.1 Foundational to the human sense of vision is the neuron. Specialized neurons are responsible for capturing light information in the human eye.2 This light information is then preprocessed, transported to the visual cortex of the brain, and then finally analyzed to completion. Neurons are single-handedly responsible for all of these functions.",
    "finally analyzed to completion. Neurons are single-handedly responsible for all of these functions. As a result, intuitively, it would make a lot of sense to extend our neural network models to build better computer vision systems. In this chapter, we will use our understanding of human vision to build effective deep learning models for image problems. But before we jump in, let’s take a look at more traditional approaches to image analysis and why they fall short.",
    "jump in, let’s take a look at more traditional approaches to image analysis and why they fall short. 117 --- Page 134 --- 3Viola, Paul, and Michael Jones. “Rapid Object Detection using a Boosted Cascade of Simple Features. ” Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on. Vol. 1. IEEE, 2001.The Shortcomings of Feature Selection Let’s begin by considering a simple computer vision problem.",
    "1.The Shortcomings of Feature Selection Let’s begin by considering a simple computer vision problem. I give you a randomly selected image, such as the one in Figure 7-1 . Y our task is to tell me if there is a human face in this picture. This is exactly the problem that Paul Viola and Michael Jones tackled in their seminal paper published in 2001.3 Figure 7-1.",
    "lem that Paul Viola and Michael Jones tackled in their seminal paper published in 2001.3 Figure 7-1. A hypothetical face-recognition algorithm should detect a face in this photo‐ graph of former US President Barack Obama For a human like you or me, this task is completely trivial. For a computer, however, this is a difficult problem. How do we teach a computer that an image contains a face?",
    "ter, however, this is a difficult problem. How do we teach a computer that an image contains a face? We could try to train a traditional machine learning algorithm (like the one we described in Chapter 3 ) by giving it the raw pixel values of the image and hoping it can find an appropriate classifier. Turns out this doesn’t work well at all because the signal-to-noise ratio is much too low for any useful learning to occur. We need an alternative.",
    "the signal-to-noise ratio is much too low for any useful learning to occur. We need an alternative. The compromise that was eventually reached was essentially a trade-off between the traditional computer program, where the human defined all of the logic, and a pure 118 | Chapter 7: Convolutional Neural Networks --- Page 135 --- machine learning approach, where the computer did all of the heavy lifting.",
    "etworks --- Page 135 --- machine learning approach, where the computer did all of the heavy lifting. In this compromise, a human would choose the features (perhaps hundreds or thousands) that they believed were important in making a classification decision. In doing so, the human would be producing a lower-dimensional representation of the same learning problem. The machine learning algorithm would then use these new feature vectors to make classification decisions.",
    "achine learning algorithm would then use these new feature vectors to make classification decisions. Because the feature extraction process improves the signal-to-noise ratio (assuming the appropriate features are picked), this approach had quite a bit of success compared to the state-of-the-art at the time. Viola and Jones had the insight that faces had certain patterns of light and dark patches that they could exploit.",
    "s had the insight that faces had certain patterns of light and dark patches that they could exploit. For example, there is a difference in light intensity between the eye region and the upper cheeks. There is also a difference in light intensity between the nose bridge and the two eyes on either side. These detectors are shown in Figure 7-2 . Figure 7-2. Viola-Jones intensity detectors By themselves, each of these features is not very effective at identifying a face.",
    "tensity detectors By themselves, each of these features is not very effective at identifying a face. But when used together (through a classic machine learning algorithm known as boosting, described in the original manuscript ), their combined effectiveness drasti‐ cally increases. On a dataset of 130 images and 507 faces, the algorithm achieves a 91.4% detection rate with 50 false positives. The performance was unparalleled at the time, but there are fundamental limitations of the algorithm.",
    "he performance was unparalleled at the time, but there are fundamental limitations of the algorithm. If a face is partially covered with shade, the light intensity comparisons no longer work. Moreover, if the algorithm is looking at a face on a crumpled flier or the face of a cartoon character, it would most likely fail. The Shortcomings of Feature Selection | 119 --- Page 136 --- 4Deng, Jia, et al. “ImageNet: A Large-Scale Hierarchical Image Database.",
    "tion | 119 --- Page 136 --- 4Deng, Jia, et al. “ImageNet: A Large-Scale Hierarchical Image Database. ” Computer Vision and Pattern Recognition , 2009. CVPR 2009. IEEE Conference. IEEE, 2009. 5Perronnin, Florent, Jorge Sénchez, and Y an Liu Xerox. “Large-Scale Image Categorization with Explicit Data Embedding. ” Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference. IEEE, 2010. 6Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton.",
    "(CVPR), 2010 IEEE Conference. IEEE, 2010. 6Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “ImageNet Classification with Deep Convolutional Neural Networks. ” Advances in Neural Information Processing Systems . 2012.The problem is the algorithm hasn’t really learned that much about what it means to “see” a face.",
    "12.The problem is the algorithm hasn’t really learned that much about what it means to “see” a face. Beyond differences in light intensity, our brain uses a vast number of visual cues to realize that our field of view contains a human face, including contours, relative positioning of facial features, and color.",
    "view contains a human face, including contours, relative positioning of facial features, and color. And even if there are slight discrepancies in one of our visual cues (for example, if parts of the face are blocked from view or if shade modifies light intensities), our visual cortex can still reliably identify faces. To use traditional machine learning techniques to teach a computer to “see, ” we need to provide our program with a lot more features to make accurate decisions.",
    "puter to “see, ” we need to provide our program with a lot more features to make accurate decisions. Before the advent of deep learning, huge teams of computer vision researchers would take years to debate about the usefulness of different features. As the recognition problems became more and more intricate, researchers had a difficult time coping with the increase in complexity.",
    "me more and more intricate, researchers had a difficult time coping with the increase in complexity. To illustrate the power of deep learning, consider the ImageNet challenge, one of the most prestigious benchmarks in computer vision (sometimes even referred to as the Olympics of computer vision).4 Every year, researchers attempt to classify images into one of 200 possible classes given a training dataset of approximately 450,000 images.",
    "fy images into one of 200 possible classes given a training dataset of approximately 450,000 images. The algorithm is given five guesses to get the right answer before it moves onto the next image in the test dataset. The goal of the competition is to push the state-of-the-art in computer vision to rival the accuracy of human vision itself (approximately 95% to 96%).",
    "-the-art in computer vision to rival the accuracy of human vision itself (approximately 95% to 96%). In 2011, the winner of the ImageNet benchmark had an error rate of 25.7%, making a mistake on one out of every four images.5 Definitely a huge improvement over random guessing, but not good enough for any sort of commercial application. Then in 2012, Alex Krizhevsky from Geoffrey Hinton’s lab at the University of Toronto did the unthinkable.",
    "n 2012, Alex Krizhevsky from Geoffrey Hinton’s lab at the University of Toronto did the unthinkable. Pioneering a deep learning architecture known as a convolutional neural network for the first time on a challenge of this size and complexity, he blew the competition out of the water. The runner-up in the competition scored a commendable 26.1% error rate.",
    "ompetition out of the water. The runner-up in the competition scored a commendable 26.1% error rate. But AlexNet, over the course of just a few months of work, completely crushed 50 years of traditional computer vision research with an error rate of approximately 16%.6 It would be no understatement to say that AlexNet single-handedly put deep learning on the map for computer vision and completely revolutionized the field.",
    "e-handedly put deep learning on the map for computer vision and completely revolutionized the field. 120 | Chapter 7: Convolutional Neural Networks --- Page 137 --- Vanilla Deep Neural Networks Don’t Scale The fundamental goal in applying deep learning to computer vision is to remove the cumbersome, and ultimately limiting, feature selection process.",
    "to computer vision is to remove the cumbersome, and ultimately limiting, feature selection process. As we discussed in Chapter 3 , deep neural networks are perfect for this process because each layer of a neural network is responsible for learning and building up features to represent the input data that it receives.",
    "k is responsible for learning and building up features to represent the input data that it receives. A naive approach might be for us to use a vanilla deep neural network using the network layer primitive we designed in Chapter 5 for the MNIST dataset to achieve the image classification task. If we attempt to tackle the image classification problem in this way, however, we’ll quickly face a pretty daunting challenge, visually demonstrated in Figure 7-3 .",
    "way, however, we’ll quickly face a pretty daunting challenge, visually demonstrated in Figure 7-3 . In MNIST, our images were only 28 × 28 pixels and were black and white. As a result, a neuron in a fully connected hidden layer would have 784 incoming weights. This seems pretty tractable for the MNIST task, and our vanilla neural net performed quite well. This technique, however, does not scale well as our images grow larger.",
    "al net performed quite well. This technique, however, does not scale well as our images grow larger. For example, for a full-color 200 × 200 pixel image, our input layer would have 200 × 200 × 3 = 120,000 weights. And we’re going to want to have lots of these neurons over multiple layers, so these parameters add up quite quickly. Clearly, this full connectivity is not only wasteful, but also means that we’re much more likely to overfit to the training dataset. Figure 7-3.",
    "wasteful, but also means that we’re much more likely to overfit to the training dataset. Figure 7-3. The density of connections between layers increases intractably as the size of the image increases The convolutional network takes advantage of the fact that we’re analyzing images, and sensibly constrains the architecture of the deep network so that we drastically reduce the number of parameters in our model.",
    "rchitecture of the deep network so that we drastically reduce the number of parameters in our model. Inspired by how human vision Vanilla Deep Neural Networks Don’t Scale | 121 --- Page 138 --- 7LeCun, Y ann, et al. “Handwritten Digit Recognition with a Back-Propagation Network. ” Advances in Neural Information Processing Systems .",
    "t Recognition with a Back-Propagation Network. ” Advances in Neural Information Processing Systems . 1990.works, layers of a convolutional network have neurons arranged in three dimensions, so layers have a width, height, and depth, as shown in Figure 7-4 .7 As we’ll see, the neurons in a convolutional layer are connected to only a small, local region of the preceding layer, so we avoid the wastefulness of fully connected neurons.",
    "small, local region of the preceding layer, so we avoid the wastefulness of fully connected neurons. A convolutional layer’s function can be expressed simply: it processes a three-dimensional volume of information to produce a new three-dimensional vol‐ ume of information. We’ll take a closer look at how this works in the next section. Figure 7-4.",
    "vol‐ ume of information. We’ll take a closer look at how this works in the next section. Figure 7-4. Convolutional layers arrange neurons in three dimensions, so layers have width, height, and depth Filters and Feature Maps In order to motivate the primitives of the convolutional layer, let’s build an intuition for how the human brain pieces together raw visual information into an understand‐ ing of the world around us.",
    "e human brain pieces together raw visual information into an understand‐ ing of the world around us. One of the most influential studies in this space came from David Hubel and Torsten Wiesel, who discovered that parts of the visual cortex are responsible for detecting edges. In 1959, they inserted electrodes into the brain of a cat and projected black-and-white patterns on the screen.",
    "ey inserted electrodes into the brain of a cat and projected black-and-white patterns on the screen. They found that some 122 | Chapter 7: Convolutional Neural Networks --- Page 139 --- 8Hubel, David H., and Torsten N. Wiesel. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.",
    ", David H., and Torsten N. Wiesel. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex. ” The Journal of Physiology 148.3 (1959): 574-591.neurons fired only when there were vertical lines, others when there were horizontal lines, and still others when the lines were at particular angles.8 Further work determined that the visual cortex was organized in layers.",
    "were at particular angles.8 Further work determined that the visual cortex was organized in layers. Each layer is responsible for building on the features detected in the previous layers—from lines, to contours, to shapes, to entire objects. Furthermore, within a layer of the visual cortex, the same feature detectors were replicated over the whole area in order to detect features in all parts of an image. These ideas significantly impacted the design of convolutional neural nets.",
    "n all parts of an image. These ideas significantly impacted the design of convolutional neural nets. The first concept that arose was that of a filter , and it turns out that here, Viola and Jones were actually pretty close. A filter is essentially a feature detector, and to understand how it works, let’s consider the toy image in Figure 7-5 . Figure 7-5. We’ll analyze this simple black-and-white image as a toy example Let’s say that we want to detect vertical and horizontal lines in the image.",
    "image as a toy example Let’s say that we want to detect vertical and horizontal lines in the image. One approach would be to use an appropriate feature detector, as shown in Figure 7-6 . For example, to detect vertical lines, we would use the feature detector on the top, slide it across the entirety of the image, and at every step check if we have a match. We keep track of our answers in the matrix in the top right. If there’s a match, we shade the appropriate box black.",
    "our answers in the matrix in the top right. If there’s a match, we shade the appropriate box black. If there isn’t, we leave it white. This result is our feature map , and it indicates where we’ve found the feature we’re looking for in the original image. We can do the same for the horizontal line detector (bottom), resulting in the feature map in the bottom-right corner. Filters and Feature Maps | 123 --- Page 140 --- Figure 7-6.",
    "feature map in the bottom-right corner. Filters and Feature Maps | 123 --- Page 140 --- Figure 7-6. Applying filters that detect vertical and horizontal lines on our toy example This operation is called a convolution. We take a filter and we multiply it over the entire area of an input image. Using the following scheme, let’s try to express this operation as neurons in a network. In this scheme, layers of neurons in a feed-forward neural net represent either the original image or a feature map.",
    "layers of neurons in a feed-forward neural net represent either the original image or a feature map. Filters represent combinations of connections (one such combination is highlighted in Fig‐ ure 7-7 ) that get replicated across the entirety of the input. In Figure 7-7 , connections of the same color are restricted to always have the same weight.",
    "input. In Figure 7-7 , connections of the same color are restricted to always have the same weight. We can achieve this by initializing all the connections in a group with identical weights and by always averaging the weight updates of a group before applying them at the end of each iteration of backpropagation. The output layer is the feature map generated by this filter.",
    "of each iteration of backpropagation. The output layer is the feature map generated by this filter. A neuron in the feature map is activated if the filter contributing to its activity detected an appropriate feature at the corresponding position in the previous layer. 124 | Chapter 7: Convolutional Neural Networks --- Page 141 --- Figure 7-7. Representing filters and feature maps as neurons in a convolutional layer Let’s denote the ktℎ feature map in layer m as mk.",
    "feature maps as neurons in a convolutional layer Let’s denote the ktℎ feature map in layer m as mk. Moreover, let’s denote the corre‐ sponding filter by the values of its weights W.",
    "in layer m as mk. Moreover, let’s denote the corre‐ sponding filter by the values of its weights W. Then, assuming the neurons in the feature map have bias bk (note that the bias is kept identical for all of the neurons in a feature map), we can mathematically express the feature map as follows: mijk=fW*xij+bk This mathematical description is simple and succinct, but it doesn’t completely describe filters as they are used in convolutional neural networks.",
    "cinct, but it doesn’t completely describe filters as they are used in convolutional neural networks. Specifically, filters don’t just operate on a single feature map. They operate on the entire volume of feature maps that have been generated at a particular layer. For example, consider a situation in which we would like to detect a face at a particular layer of a convo‐ lutional net. And we have accumulated three feature maps, one for eyes, one for noses, and one for mouths.",
    "al net. And we have accumulated three feature maps, one for eyes, one for noses, and one for mouths. We know that a particular location contains a face if the corresponding locations in the primitive feature maps contain the appropriate features (two eyes, a nose, and a mouth). In other words, to make decisions about the existence of a face, we must combine evidence over multiple feature maps. This is equally necessary for an input image that is of full color.",
    "ence over multiple feature maps. This is equally necessary for an input image that is of full color. These images have pixels represented as RGB values, so we require three slices in the input volume (one slice for each color). As a result, feature maps must be able to operate over volumes, not just areas. This is shown in Figure 7-8 . Each cell in the input volume is a neuron.",
    "er volumes, not just areas. This is shown in Figure 7-8 . Each cell in the input volume is a neuron. A local portion is multiplied with a filter (corresponding to weights in the convolutional layer) to produce a neuron in a filter map in the following volumetric layer of neurons. Filters and Feature Maps | 125 --- Page 142 --- Figure 7-8.",
    "e following volumetric layer of neurons. Filters and Feature Maps | 125 --- Page 142 --- Figure 7-8. A full-color RGB image as a volume and applying a volumetric convolutional filter As we discussed in the previous section, a convolutional layer (which consists of a set of filters) converts one volume of values into another volume of values. The depth of the filter corresponds to the depth of the input volume.",
    "into another volume of values. The depth of the filter corresponds to the depth of the input volume. This is so that the filter can combine information from all the features that have been learned. The depth of the output volume of a convolutional layer is equivalent to the number of filters in that layer, because each filter produces its own slice. We visualize these relationships in Figure 7-9 . Figure 7-9.",
    "use each filter produces its own slice. We visualize these relationships in Figure 7-9 . Figure 7-9. A three-dimensional visualization of a convolutional layer, where each filter corresponds to a slice in the resulting output volume 126 | Chapter 7: Convolutional Neural Networks --- Page 143 --- In the next section, we will use these concepts and fill in some of the gaps to create a full description of a convolutional layer.",
    "e these concepts and fill in some of the gaps to create a full description of a convolutional layer. Full Description of the Convolutional Layer Let’s use the concepts we’ve developed so far to complete the description of the convolutional layer. First, a convolutional layer takes in an input volume.",
    "e the description of the convolutional layer. First, a convolutional layer takes in an input volume. This input volume has the following characteristics: •Its width win •Its height ℎin •Its depth din •Its zero padding p This volume is processed by a total of k filters, which represent the weights and connections in the convolutional network. These filters have a number of hyperpara‐ meters, which are described as follows: •Their spatial extent e, which is equal to the filter’s height and width.",
    "are described as follows: •Their spatial extent e, which is equal to the filter’s height and width. •Their stride s, or the distance between consecutive applications of the filter on the input volume. If we use a stride of 1, we get the full convolution described in the previous section. We illustrate this in Figure 7-10 . •The bias b (a parameter learned like the values in the filter), which is added to each component of the convolution. Figure 7-10.",
    "ed like the values in the filter), which is added to each component of the convolution. Figure 7-10. A filter’s stride hyperparameter Full Description of the Convolutional Layer | 127 --- Page 144 --- This results in an output volume with the following characteristics: •Its function f, which is applied to the incoming logit of each neuron in the output volume to determine its final value •Its width wout=win−e+ 2p s+ 1 •Its height ℎout=ℎin−e+ 2p s+ 1 •Its depth dout=k The mtℎ “depth slice” of the output volume, where 1 ≤m≤k, corresponds to the function f applied to the sum of the mtℎ filter convoluted over the input volume and the bias bm.",
    "he function f applied to the sum of the mtℎ filter convoluted over the input volume and the bias bm. Moreover, this means that per filter, we have dine2 parameters. In total, that means the layer has kdine2 parameters and k biases. To demonstrate this in action, we provide an example of a convolutional layer in Figures 7-11 and 7-12 with a 5 × 5 × 3 input volume with zero padding p= 1. We’ll use two 3 × 3 × 3 filters (spatial extent ) with a stride s= 2.",
    "volume with zero padding p= 1. We’ll use two 3 × 3 × 3 filters (spatial extent ) with a stride s= 2. We’ll use a linear function to produce the output volume, which will be of size 3 × 3 × 2. We apply the first convolutional filter to the upper-leftmost 3 × 3 piece of the input volume to generate the upper-leftmost entry of the first depth slice. Generally, it’s wise to keep filter sizes small (size 3 × 3 or 5 × 5).",
    "try of the first depth slice. Generally, it’s wise to keep filter sizes small (size 3 × 3 or 5 × 5). Less commonly, larger sizes are used (7 × 7) but only in the first convolutional layer. Having more small filters is an easy way to achieve high representational power while also incur‐ ring a smaller number of parameters.",
    "sy way to achieve high representational power while also incur‐ ring a smaller number of parameters. It’s also suggested to use a stride of 1 to capture all useful information in the feature maps, and a zero padding that keeps the output volume’s height and width equivalent to the input volume’s height and width. 128 | Chapter 7: Convolutional Neural Networks --- Page 145 --- Figure 7-11.",
    "ume’s height and width. 128 | Chapter 7: Convolutional Neural Networks --- Page 145 --- Figure 7-11. A convolutional layer with an input volume of width 5, height 5, depth 3, zero padding 1, and 2 filters (with spatial extent 3 and applied with a stride of 2) results in an output volume of 3 × 3 × 2 Full Description of the Convolutional Layer | 129 --- Page 146 --- Figure 7-12.",
    "volume of 3 × 3 × 2 Full Description of the Convolutional Layer | 129 --- Page 146 --- Figure 7-12. Using the same setup as Figure 7-11 , we generate the next value in the first depth slice of the output volume PyTorch provides us with a convenient operation to easily perform a 2D convolution on a minibatch of input volumes: import torch.nn as nn layer = nn.Conv2d(in_channels = 3, out_channels = 64, 130 | Chapter 7: Convolutional Neural Networks --- Page 147 --- kernel_size = (5, 5), stride = 2, padding = 1 ) Here, in_channels represents the depth, din, or number of input planes.",
    "), stride = 2, padding = 1 ) Here, in_channels represents the depth, din, or number of input planes. For color images, the number of input channels often equals three, representing the RGB channels. The nn.Conv2d layer will accept as an input a four-dimensional tensor of size, bin*din*ℎin*win, where bin is the number of examples in our minibatch. The out_channels argument represents the number of output planes or feature maps.",
    "in our minibatch. The out_channels argument represents the number of output planes or feature maps. The kernel_size argument determines the filter size or spatial extent, e, while the stride and padding arguments determine the stride size, s, and zero padding size, p, respectively. Note that you can pass in equal dimension settings with a single value as shown here with stride and padding.",
    "you can pass in equal dimension settings with a single value as shown here with stride and padding. Max Pooling To aggressively reduce dimensionality of feature maps and sharpen the located features, we sometimes insert a max pooling layer after a convolutional layer. The essential idea behind max pooling is to break up each feature map into equally sized tiles. Then we create a condensed feature map.",
    "ng is to break up each feature map into equally sized tiles. Then we create a condensed feature map. Specifically, we create a cell for each tile, compute the maximum value in the tile, and propagate this maximum value into the corresponding cell of the condensed feature map. This process is illustrated in Figure 7-13 . Figure 7-13. Max pooling significantly reduces parameters as we move up the network Max Pooling | 131 --- Page 148 --- 9Graham, Benjamin. “Fractional Max-Pooling.",
    "e move up the network Max Pooling | 131 --- Page 148 --- 9Graham, Benjamin. “Fractional Max-Pooling. ” arXiv Preprint arXiv :1412.6071 (2014).More rigorously, we can describe a pooling layer with two parameters: •Its spatial extent e •Its stride s It’s important to note that only two major variations of the pooling layer are used. The first is the nonoverlapping pooling layer with e= 2,s= 2. The second is the overlapping pooling layer with e= 3,s= 2.",
    "verlapping pooling layer with e= 2,s= 2. The second is the overlapping pooling layer with e= 3,s= 2. The resulting dimensions of each feature map are as follows: •Its width wout=win−e s+ 1 •Its height ℎout=ℎin−e s+ 1 One interesting property of max pooling is that it is locally invariant . This means that even if the inputs shift around a little bit, the output of the max pooling layer stays constant. This has important implications for visual algorithms.",
    "tput of the max pooling layer stays constant. This has important implications for visual algorithms. Local invariance is a useful property if we care more about whether some feature is present than exactly where it is. However, enforcing large amounts of local invariance can destroy our network’s ability to carry important information. As a result, we usually keep the spatial extent of our pooling layers quite small.",
    "tant information. As a result, we usually keep the spatial extent of our pooling layers quite small. Some recent work along this line has come out of the University of Warwick from Graham,9 who proposes a concept called fractional max pooling . In fractional max pooling, a pseudorandom number generator is used to generate tilings with nonin‐ teger lengths for pooling. Here, fractional max pooling functions as a strong regular‐ izer, helping prevent overfitting in convolutional networks.",
    "pooling functions as a strong regular‐ izer, helping prevent overfitting in convolutional networks. Full Architectural Description of Convolution Networks Now that we’ve described the building blocks of convolutional networks, we start putting them together. Figure 7-14 depicts several architectures that might be of practical use. One theme we notice as we build deeper networks is that we reduce the number of pooling layers and instead stack multiple convolutional layers in tandem.",
    "at we reduce the number of pooling layers and instead stack multiple convolutional layers in tandem. This is generally helpful because pooling operations are inherently destructive. Stacking several convolutional layers before each pooling layer allows us to achieve richer representations. 132 | Chapter 7: Convolutional Neural Networks --- Page 149 --- 10Simonyan, Karen, and Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recog‐ nition.",
    "Karen, and Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recog‐ nition. ” arXiv Preprint arXiv :1409.1556 (2014). Figure 7-14. Various convolutional network architectures of various complexities As a practical note, deep convolutional networks can take up a significant amount of space, and most casual practitioners are usually bottlenecked by the memory capacity on their GPU.",
    "f space, and most casual practitioners are usually bottlenecked by the memory capacity on their GPU. The VGGNet architecture, for example, takes approximately 90 MB of memory on the forward pass per image, and more than 180 MB of memory on the backward pass to update the parameters.10 Many deep networks make a compromise by using strides and spatial extents in the first convolutional layer that reduce the amount of information that needs to be propagated up the network.",
    "onvolutional layer that reduce the amount of information that needs to be propagated up the network. Full Architectural Description of Convolution Networks | 133 --- Page 150 --- Closing the Loop on MNIST with Convolutional Networks Now that we have a better understanding of how to build networks that effectively analyze images, we’ll revisit the MNIST challenge we’ve tackled over the past several chapters. Here, we’ll use a convolutional network to learn how to recognize handwrit‐ ten digits.",
    "al chapters. Here, we’ll use a convolutional network to learn how to recognize handwrit‐ ten digits. Our feed-forward network was able to achieve a 98.2% accuracy. Our goal will be to push the envelope on this result. To tackle this challenge, we’ll build a convolutional network with a pretty stan‐ dard architecture (modeled after the second network in Figure 7-14 ): two convolu‐ tional/ReLU/maxpooling stacks, followed by a fully connected layer with dropout and a terminal fully connected layer.",
    "oling stacks, followed by a fully connected layer with dropout and a terminal fully connected layer. Building the network is easy in PyTorch using the built-in nn classes, as shown in the following code: class MNISTConvNet(nn.Module): def __init__(self): super(MNISTConvNet, self).__init__() self.conv1 = nn.Sequential( nn.Conv2d(1, 32, 5, padding='same'), nn.ReLU(), nn.MaxPool2d(2) ) self.conv2 = nn.Sequential( nn.Conv2d(32, 64, 5, padding='same'), nn.ReLU(), nn.MaxPool2d(2) ) self.fc1 = nn.Sequential( nn.Flatten(), nn.Linear(7*7*64, 1024), nn.Dropout(0.5), nn.Linear(1024, 10) ) def forward(self, x): x = self.conv1(x) x = self.conv2(x) return self.fc1(x) The __init__ method generates two Conv2d/ReLU/MaxPool blocks followed by a block containing two fully connected layers.",
    "generates two Conv2d/ReLU/MaxPool blocks followed by a block containing two fully connected layers. The convolutional layers are created with a particular shape. By default, the stride is set to be 1, while the padding is set to same to keep the width and height constant between input and output tensors. By default, each nn.Conv2d constructor automatically initializes the weights. The max pooling layers consist of nonoverlapping windows of size k.",
    "tically initializes the weights. The max pooling layers consist of nonoverlapping windows of size k. The default, as recommended, is k=2, and we’ll use this default in our MNIST convolutional network. 134 | Chapter 7: Convolutional Neural Networks --- Page 151 --- The forward method defines how our layers and blocks are connected together to perform the forward pass or inference. The code here is quite easy to follow.",
    "connected together to perform the forward pass or inference. The code here is quite easy to follow. The input is expected to be a tensor of size N× 1 × 28 × 28 , where N is the number of examples in a minibatch, 28 is the width and height of each image, and 1 is the depth (because the images are black and white; if the images were in RGB color, the depth would instead be 3 to represent each color map).",
    "d white; if the images were in RGB color, the depth would instead be 3 to represent each color map). The first block, conv1 , builds a convolutional layer with 32 filters that have spatial extent 5. This results in taking an input volume of depth 1 and emitting an output tensor of depth 32. This is then passed through a max pooling layer that compresses the information.",
    "tensor of depth 32. This is then passed through a max pooling layer that compresses the information. The second block, conv2 , then builds a second convolutional layer with 64 filters, again with spatial extent 5, taking an input tensor of depth 32 and emitting an output tensor of depth 64. This, again, is passed through a max pooling layer to compress information. We then prepare to pass the output of the max pooling layer into a fully connected layer. To do this, we flatten the tensor.",
    "the output of the max pooling layer into a fully connected layer. To do this, we flatten the tensor. We can do this by computing the full size of each “subtensor” in the minibatch. We have 64 filters, which corresponds to the depth of 64. We now have to determine the height and width after passing through two max pooling layers. Using the formulas we found in the previous section, it’s easy to confirm that each feature map has a height and width of 7.",
    "und in the previous section, it’s easy to confirm that each feature map has a height and width of 7. Confirming this is left as an exercise for you. We use a fully connected layer to compress the flattened representation into a hidden state of size 1,024. We use a dropout probability in this layer of 0.5 during training and 1 during model evaluation (standard procedure for employing dropout).",
    "yer of 0.5 during training and 1 during model evaluation (standard procedure for employing dropout). Finally, we send this hidden state into a output layer with 10 bins (the softmax is, as usual, performed in the loss constructor for better performance). Finally, we train our network using the Adam optimizer.",
    "he loss constructor for better performance). Finally, we train our network using the Adam optimizer. After several epochs over the dataset, we achieve an accuracy of 99.4%, which isn’t state-of-the-art (approximately 99.7 to 99.8%), but is respectable: lr = 1e-4 num_epochs = 40 model = MNISTConvNet() loss_fn = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=lr) for epochs in range(num_epochs): running_loss = 0.0 num_correct = 0 for inputs, labels in trainloader: optimizer.zero_grad() outputs = model(inputs) Closing the Loop on MNIST with Convolutional Networks | 135 --- Page 152 --- loss = loss_fn(outputs, labels) loss.backward() running_loss += loss.item() optimizer.step() _, idx = outputs.max(dim=1) num_correct += (idx == labels).sum().item() print('Loss: {} Accuracy: {}'.format(running_loss/len(trainloader), num_correct/len(trainloader))) Image Preprocessing Pipelines Enable More Robust Models So far we’ve been dealing with rather tame datasets.",
    "eprocessing Pipelines Enable More Robust Models So far we’ve been dealing with rather tame datasets. Why is MNIST a tame dataset? Well, fundamentally, MNIST has already been preprocessed so that all the images in the dataset resemble each other. The handwritten digits are perfectly cropped in just the same way; there are no color aberrations because MNIST is black and white; and so on. Natural images, however, are an entirely different beast.",
    "cause MNIST is black and white; and so on. Natural images, however, are an entirely different beast. Natural images are messy, and as a result, there are a number of preprocessing operations that we can utilize in order to make training slightly easier. Fortunately, PyTorch offers a package called Torchvision that includes many commonly used transforms for image processing. One technique that is supported out of the box in PyTorch is image whitening.",
    "for image processing. One technique that is supported out of the box in PyTorch is image whitening. The basic idea behind whitening is to zero-center every pixel in an image by subtracting out the mean of the dataset and normalizing to unit 1 variance. This helps us correct for potential differences in dynamic range between images.",
    "to unit 1 variance. This helps us correct for potential differences in dynamic range between images. In PyTorch, we can achieve this using the Normalize transform: from torchvision import transforms transform = transforms.Normalize(mean = (0.1307,), std = (0.3081,) ) The magic numbers for mean , 0.1307, and std, 0.3081, were computed over the entire MNIST dataset, and this technique is called dataset normalization.",
    "81, were computed over the entire MNIST dataset, and this technique is called dataset normalization. We can also expand our dataset artificially by randomly cropping the image, flipping the image, modifying saturation, modifying brightness, etc: 136 | Chapter 7: Convolutional Neural Networks --- Page 153 --- 11S. Ioffe, C. Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covari‐ ate Shift. ” arXiv Preprint arXiv :1502.03167.",
    "ng Deep Network Training by Reducing Internal Covari‐ ate Shift. ” arXiv Preprint arXiv :1502.03167. 2015.transform = transforms.Compose([ transforms.RandomCrop(224), transforms.RandomHorizontalFlip(), transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0), transforms.ToTensor(), transforms.Normalize(mean = (0.1307,), std = (0.3081,) ) ]) Here, we use the Compose transform to create a sequence of transforms from a list.",
    "= (0.3081,) ) ]) Here, we use the Compose transform to create a sequence of transforms from a list. After applying random cropping, flipping, and color adjustments, we convert the image data to a PyTorch tensor and normalize the data. PyTorch models require the data to be in tensor format, and these last two steps are common practice in using PyTorch for deep learning.",
    "e in tensor format, and these last two steps are common practice in using PyTorch for deep learning. Applying these transformations helps us build networks that are robust to the differ‐ ent kinds of variations that are present in natural images, and make predictions with high fidelity in spite of potential distortions.",
    "resent in natural images, and make predictions with high fidelity in spite of potential distortions. Accelerating Training with Batch Normalization In 2015, researchers from Google devised an exciting way to even further accelerate the training of feed-forward and convolutional neural networks using a technique called batch normaliza tion.11 We can think of the intuition behind batch normaliza‐ tion like a tower of blocks, as shown in Figure 7-15 .",
    "hink of the intuition behind batch normaliza‐ tion like a tower of blocks, as shown in Figure 7-15 . When a tower of blocks is stacked together neatly, the structure is stable. However, if we randomly shift the blocks, we could force the tower into configurations that are increasingly unstable. Eventually the tower falls apart. A similar phenomenon can happen during the training of neural networks. Imagine a two-layer neural network.",
    "ar phenomenon can happen during the training of neural networks. Imagine a two-layer neural network. In the process of training the weights of the network, the output distribution of the neurons in the bottom layer begins to shift. The result of the changing distribution of outputs from the bottom layer means that the top layer not only has to learn how to make the appropriate predictions, but it also needs to somehow modify itself to accommodate the shifts in incoming distribution.",
    "ions, but it also needs to somehow modify itself to accommodate the shifts in incoming distribution. This significantly slows down training, and the magnitude of the problem compounds the more layers we have in our networks. Accelerating Training with Batch Normalization | 137 --- Page 154 --- Figure 7-15. Batch normalization reduces shifts in the distribution of inputs of layers Normalization of image inputs helps out the training process by making it more robust to variations.",
    "Normalization of image inputs helps out the training process by making it more robust to variations. Batch normalization takes this a step further by normalizing inputs to every layer in our neural network. Specifically, we modify the architecture of our network to include operations that: 1.Grab the vector of logits incoming to a layer before they pass through the nonlinearity.",
    "ons that: 1.Grab the vector of logits incoming to a layer before they pass through the nonlinearity. 2.Normalize each component of the vector of logits across all examples of the minibatch by subtracting the mean and dividing by the standard deviation (we keep track of the moments using an exponentially weighted moving average). 3.Given normalized inputs x̂, use an affine transform to restore representational power with two vectors of (trainable) parameters: γ x̂ + β.",
    "ne transform to restore representational power with two vectors of (trainable) parameters: γ x̂ + β. PyTorch provides a BatchNorm2d class to perform batch normalization for a convolu‐ tional layer: layer = nn.BatchNorm2d(num_features=32, eps=1e-05, momentum=0.1, affine = True, track_running_stats = True) Here, the num_features argument represents the depth, or number of channels, of the inputs to the batch normalization layer.",
    "rgument represents the depth, or number of channels, of the inputs to the batch normalization layer. Hence, batch normalization is performed over the channel dimension, computing the mean and variance of each minibatch of 2D channels. The num_features is the only required argument. All other arguments are set to their defaults.",
    "nels. The num_features is the only required argument. All other arguments are set to their defaults. 138 | Chapter 7: Convolutional Neural Networks --- Page 155 --- The BatchNorm2d layer performs the following affine transformation: y=x−Ex Varx+ϵ*γ+β The parameters γ and β are learnable parameters and will be trained during the training process if affine = True . Otherwise, the mean is subtracted from the inputs and divided by standard deviation to be normalized.",
    "therwise, the mean is subtracted from the inputs and divided by standard deviation to be normalized. The ϵ argument is only used for mathematical stability. When track_running_stats = True , this layer will keep track of the running mean and variance for use in evaluation mode. The running mean and variance are upda‐ ted using the momentum value. We can also express batch normalization for nonconvolutional feed-forward layers by using the BatchNorm1d constructor.",
    "s batch normalization for nonconvolutional feed-forward layers by using the BatchNorm1d constructor. Here, we set only num_features = 32 and use the defaults for other arguments: layer = nn.BatchNorm1d(num_features=32) In addition to speeding up training by preventing significant shifts in the distribution of inputs to each layer, batch normalization also allows us to significantly increase the learning rate.",
    "nputs to each layer, batch normalization also allows us to significantly increase the learning rate. Moreover, batch normalization acts as a regularizer and removes the need for dropout and (when used) L2 regularization. Although we don’t leverage it here, the authors also claim that batch regularization largely removes the need for photometric distortions, and we can expose the network to more “real” images during the training process.",
    "metric distortions, and we can expose the network to more “real” images during the training process. In the next section, we will motivate and discuss a variant of normalization across the feature axis, rather than the batch. Group Normalization for Memory Constrained Learning Tasks Various forms of normalization in image processing have been studied and utilized in the last decade. The most famous of these is batch normalization.",
    "have been studied and utilized in the last decade. The most famous of these is batch normalization. Just to recap from the previous section, this technique computes the channel-wise mean and variance of the output of each convolutional layer, normalizes each channel using the computed statistics, and then feeds the normalized output to the next convolutional layer. Thus, any given channel in the normalized output will have the same mean and variance (zero and one, respectively) across batches.",
    "normalized output will have the same mean and variance (zero and one, respectively) across batches. In practice, the model will also learn a mean parameter β and a standard deviation parameter γ, which are then applied to the normalized output such that it has mean β and standard deviation γ before being fed into the subsequent layer. This process is used to reduce the shift in distribution of any given channel from one batch to the next.",
    "process is used to reduce the shift in distribution of any given channel from one batch to the next. Note that this is only a reduction of the shift and not a complete removal of it, since the channel distribution might still Group Normalization for Memory Constrained Learning Tasks | 139 --- Page 156 --- 12Wu et. al. “Group Normalization. ” 2018. https://arxiv.org/abs/1803.08494 .look completely different from one batch to the next even though they have the same mean and variance.",
    "ok completely different from one batch to the next even though they have the same mean and variance. In theory, and as has been observed empirically, reducing this internal covariate shift stabilizes training and results in strong performance gains. However, in cases where the batch size is large, the channel-wise mean and variance computations lead to large memory costs.",
    "the batch size is large, the channel-wise mean and variance computations lead to large memory costs. Additionally, the size of the batch itself is very important for batch normalization, as smaller batch sizes degrade performance significantly due to noisy mean and variance estimates.",
    ", as smaller batch sizes degrade performance significantly due to noisy mean and variance estimates. To avoid the issues that come with computations along the batch dimension, group normalization was intro‐ duced.12 Instead of performing a normalization along the batch dimension, group normalization is performed along the channel dimension and is thus unaffected by the aforementioned issues.",
    "zation is performed along the channel dimension and is thus unaffected by the aforementioned issues. Group normalization predefines a number of groups of channels and, for each instance, computes the mean μ and variance σ for each group of channels in each instance of the batch. Each set of computed β and γ parameters is used to normalize the set of entries from which they were computed.",
    "f computed β and γ parameters is used to normalize the set of entries from which they were computed. Additionally, similarly to batch normalization, an offset/mean parameter β and a scale/standard deviation parameter γ are separately learned for each entry set. This is similar to another popular technique known as layer normalization , which is effectively batch normalization but across the full length of the channel dimension rather than the full length of the batch dimension.",
    "across the full length of the channel dimension rather than the full length of the batch dimension. Note that layer normalization is also just a special case of group normalization, where the number of groups of channels is set to one. Figure 7-16 compares batch normalization with group normal‐ ization and layer normalization. The blocked-off section in each cube demonstrates the dimension along which normalization occurs and the group of entries that are normalized together.",
    "he dimension along which normalization occurs and the group of entries that are normalized together. Note that we condense the standard 4D representation into 3D for visualization purposes. Figure 7-16. Comparison of batch normalization, layer normalization, and group normalization 140 | Chapter 7: Convolutional Neural Networks --- Page 157 --- 13Krizhevsky, Alex, and Geoffrey Hinton. “Learning Multiple Layers of Features from Tiny Images.",
    "--- 13Krizhevsky, Alex, and Geoffrey Hinton. “Learning Multiple Layers of Features from Tiny Images. ” University of Toronto (2009).Y ou may be wondering why techniques like group normalization and layer normal‐ ization are even effective. After all, it seems as though batch normalization is only useful due to forcing each feature (or channels in our case) to have the same mean and variance.",
    "nly useful due to forcing each feature (or channels in our case) to have the same mean and variance. For some insight, the initial paper on layer normalization states that the reason for normalizing the features for each instance separately is that “changes in the output of one layer will tend to cause highly correlated changes in the summed input to the next layer.",
    "put of one layer will tend to cause highly correlated changes in the summed input to the next layer. ” In summary, the neurons that make up every subsequent layer in the feed-forward network will see the same statistics from one training example to the next with layer normalization. Furthermore, why group normalization over layer normalization?",
    "to the next with layer normalization. Furthermore, why group normalization over layer normalization? In Wu et al., the idea behind using group normalization is that it is less restrictive than layer normal‐ ization—a different distribution can be learned for each group of features, signifying the ability to learn potentially different levels of contribution and importance for different groups.",
    "e ability to learn potentially different levels of contribution and importance for different groups. Now that we have sufficiently covered group normalization as a concept, its connec‐ tion to prior work, and motivation for using group normalization in practice, we can now dive into some PyTorch code for implementing group normalization.",
    "malization in practice, we can now dive into some PyTorch code for implementing group normalization. PyTorch provides a torch.nn.GroupNorm class to create group normalization layers: layer = nn.GroupNorm(num_groups=1, num_channels=32) We need to specify only the number of groups and number of channels. Now that we’ve developed an enhanced toolkit for analyzing natural images with convolutional networks, we’ll build a classifier for tackling the CIFAR-10 challenge.",
    "al images with convolutional networks, we’ll build a classifier for tackling the CIFAR-10 challenge. Building a Convolutional Network for CIFAR-10 The CIFAR-10 challenge consists of 32 × 32 color images that belong to one of 10 possible classes.13 This is a surprisingly hard challenge because it can be difficult for even a human to figure out what is in a picture. An example is shown in Figure 7-17 . Building a Convolutional Network for CIFAR-10 | 141 --- Page 158 --- Figure 7-17.",
    "n in Figure 7-17 . Building a Convolutional Network for CIFAR-10 | 141 --- Page 158 --- Figure 7-17. A dog from the CIFAR-10 dataset In this section, we’ll build networks both with and without batch normalization as a basis of comparison. We increase the learning rate by 10-fold for the batch normalization network to take full advantage of its benefits. We’ll display code for only the batch normalization network here because building the vanilla convolutional network is similar.",
    "the batch normalization network here because building the vanilla convolutional network is similar. We distort random 24 × 24 crops of the input images to feed into our network for training. We use the example code provided by Google to do this. We’ll jump right into the network architecture. To start, let’s take a look at how we integrate batch normalization into the convolutional and fully connected layers.",
    "ke a look at how we integrate batch normalization into the convolutional and fully connected layers. As expected, batch normalization happens to the logits before they’re fed into a nonlinearity: 142 | Chapter 7: Convolutional Neural Networks --- Page 159 --- class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.block1 = nn.Sequential( nn.Conv2d(1, 32, 3, 1), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.Conv2d(32, 64, 3, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.25), ) self.block2 = nn.Sequential( nn.Flatten(), nn.Linear(9216, 128), nn.BatchNorm1d(128), nn.ReLU(inplace=True), nn.Dropout(0.5), nn.Linear(128,10), nn.BatchNorm1d(10) ) def forward(self, x): x = self.block1(x) return self.block2(x) Finally, we use the Adam optimizer to train our convolutional networks.",
    "ck1(x) return self.block2(x) Finally, we use the Adam optimizer to train our convolutional networks. After some amount of time training, our networks are able to achieve an impressive 92.3% accuracy on the CIFAR-10 task without batch normalization and 96.7% accuracy with batch normalization. This result actually matches (and potentially exceeds) current state-of-the-art research on this task. In the next section, we’ll take a closer look at learning and visualize how our networks perform.",
    "k. In the next section, we’ll take a closer look at learning and visualize how our networks perform. Visualizing Learning in Convolutional Networks On a high level, the simplest thing that we can do to visualize training is plot the cost function and validation errors over time as training progresses. We can clearly demonstrate the benefits of batch normalization by comparing the rates of convergence between our two networks.",
    "the benefits of batch normalization by comparing the rates of convergence between our two networks. Plots taken in the middle of the training process are shown in Figure 7-18 . Visualizing Learning in Convolutional Networks | 143 --- Page 160 --- Figure 7-18. Training a convolutional network without batch normalization (left) versus with batch normalization (right) Without batch normalization, cracking the 90% accuracy threshold requires over 80,000 minibatches.",
    ") Without batch normalization, cracking the 90% accuracy threshold requires over 80,000 minibatches. On the other hand, with batch normalization, crossing the same threshold requires only slightly over 14,000 minibatches. We can also inspect the filters that our convolutional network learns in order to understand what the network finds important to its classification decisions.",
    "work learns in order to understand what the network finds important to its classification decisions. Con‐ volutional layers learn hierarchical representations, so we’ d hope that the first con‐ volutional layer learns basic features (edges, simple curves, etc.), and the second convolutional layer will learn more complex features. Unfortunately, the second 144 | Chapter 7: Convolutional Neural Networks --- Page 161 --- 14Maaten, Laurens van der, and Geoffrey Hinton. “Visualizing Data Using t-SNE.",
    "orks --- Page 161 --- 14Maaten, Laurens van der, and Geoffrey Hinton. “Visualizing Data Using t-SNE. ” Journal of Machine Learning Research 9. Nov (2008): 2579-2605.convolutional layer is difficult to interpret even if we decided to visualize it, so we only include the first layer filters in Figure 7-19 . Figure 7-19.",
    "we decided to visualize it, so we only include the first layer filters in Figure 7-19 . Figure 7-19. A subset of the learned filters in the first convolutional layer of our network We can make out a number of interesting features in our filters: vertical, horizontal, and diagonal edges, in addition to small dots or splotches of one color surrounded by another. We can be confident that our network is learning relevant features because the filters are not just noise.",
    "be confident that our network is learning relevant features because the filters are not just noise. We can also try to visualize how our network has learned to cluster various kinds of images pictorially. To illustrate this, we take a large network that has been trained on the ImageNet challenge and then grab the hidden state of the fully connected layer just before the softmax for each image.",
    "and then grab the hidden state of the fully connected layer just before the softmax for each image. We then take this high-dimensional representation for each image and use an algorithm known as t-Distributed Stochastic Neighbor Embedding , or t-SNE , to compress it to a 2D representation that we can vis‐ ualize.14 We don’t cover the details of t-SNE here, but there are a number of publicly Visualizing Learning in Convolutional Networks | 145 --- Page 162 --- 15Image credit: Andrej Karpathy.",
    "sualizing Learning in Convolutional Networks | 145 --- Page 162 --- 15Image credit: Andrej Karpathy. http://cs.stanford.edu/people/karpathy/cnnembed .available software tools that will do it for us, including the script . We visualize the embeddings in Figure 7-20 , and the results are quite spectacular. Figure 7-20.",
    "t . We visualize the embeddings in Figure 7-20 , and the results are quite spectacular. Figure 7-20. The t-SNE embedding (center) surrounded by zoomed-in subsegments of the embedding (periphery)15 At first, on a high level, it seems that images that are similarly colored are closer together. This is interesting, but what’s even more striking is when we zoom into parts of the visualization, we realize that it’s more than just color.",
    "striking is when we zoom into parts of the visualization, we realize that it’s more than just color. We realize that all pictures of boats are in one place, all pictures of humans are in another place, and all pictures of butterflies are in yet another location in the visualization. Quite clearly, convolutional networks have spectacular learning capabilities. 146 | Chapter 7: Convolutional Neural Networks --- Page 163 --- 16He et. al. “Deep Residual Learning for Image Recognition.",
    "utional Neural Networks --- Page 163 --- 16He et. al. “Deep Residual Learning for Image Recognition. ” arXiv Preprint arXiv :1512.03385. 2015.Residual Learning and Skip Connections for Very Deep Networks We have made great progress in the field of computer vision over the past decade, and in this section we introduce one of the more recent advancements. Earlier, we discussed AlexNet, which was a breakthrough in neural methods applied to image classification.",
    "r, we discussed AlexNet, which was a breakthrough in neural methods applied to image classification. Since then, researchers have pushed toward deeper and deeper architectures in the hope of solving image classification. However, since AlexNet’s breakthrough, at least a few reputable studies tended to see decreases in training accuracy when naively stacking layers as compared to their shallower counterparts.",
    "eases in training accuracy when naively stacking layers as compared to their shallower counterparts. It’s particularly interesting that the problem isn’t even overfitting (as is suggested by a low training accuracy and a high validation accuracy), which would be understand‐ able for a network with such a large number of parameters.",
    "ion accuracy), which would be understand‐ able for a network with such a large number of parameters. Additionally, we can easily construct a deep network by ourselves that has the exact same performance as its shallow counterpart: take the trained shallow network layers and simply stack layers that perform the identity operation. The fact that we do worse via a specialized optimization algorithm compared to our naive construction is quite astounding.",
    "rse via a specialized optimization algorithm compared to our naive construction is quite astounding. The problem is that training stalls for some inexplicable reason, settling in a local minimum that we can’t get out of. Unfortunately, the theoretical justification for this is still a bit hazy. In 2015, He et al.16 introduced the ResNet34 architecture, a deep architecture that surpassed all of its peers in major image classification competitions.",
    "ure, a deep architecture that surpassed all of its peers in major image classification competitions. With a version that consisted of over 30 trainable layers, He et al. redefined how we train deep computer vision architectures. In particular, their contribution was the introduction of what we now call skip connections , which add the feature vector obtained from a layer to the feature vector obtained one or two layers after the current layer.",
    "ctor obtained from a layer to the feature vector obtained one or two layers after the current layer. More precisely, let’s say we are midway through the network so far and our original input x has been converted to some intermediate representation x’. The skip connection would take x’ and add it to the result of the next layer, F(x') , before passing the representation on to the following layer G. So instead of seeing F(x') , G sees F(x') + x’ .",
    "ing the representation on to the following layer G. So instead of seeing F(x') , G sees F(x') + x’ . Note that the skip connection does not need to add the current representation to the result of F. As represented in Figure 7-21 , we could also add x’ to the result of G, so the next layer H sees G(F(x')) + x’ instead of just G(F(x')). Residual Learning and Skip Connections for Very Deep Networks | 147 --- Page 164 --- Figure 7-21.",
    "). Residual Learning and Skip Connections for Very Deep Networks | 147 --- Page 164 --- Figure 7-21. The skip connection here skips F and G, summing the input to F with the output of G, which comprises the input to H These skip connections are just the identity operation, so they add no additional parameters to train.",
    "ese skip connections are just the identity operation, so they add no additional parameters to train. Additionally, since the skip connection is the identity operation, it must be the case that x’ and G(F(x')) , in the example where the skip connection skips two layers, must be the same dimension. If this were not the case, we would not be able to add the two feature vectors.",
    "the same dimension. If this were not the case, we would not be able to add the two feature vectors. This does place a constraint on the network architecture, but we hope to construct a deep network anyway, and this approach lends itself well to such networks since we wouldn’t want the dimensionality to decrease too rapidly (recall the discussion on padding). It’s natural to ask why skip connections work so well.",
    "o rapidly (recall the discussion on padding). It’s natural to ask why skip connections work so well. After all, it does seem like a pretty simple modification to the plain deep network architecture. Let’s think back to the original motivation: through experimentation, researchers had noticed a degradation in performance as networks got deeper and deeper.",
    "rimentation, researchers had noticed a degradation in performance as networks got deeper and deeper. However, it must be the case that deeper networks are able to perform at least as well as their shallower counterparts, since we can construct a naive solution where the additional layers are the identity mapping.",
    "parts, since we can construct a naive solution where the additional layers are the identity mapping. It’s also important to note that the representations learned by shallower counterparts such as AlexNet are quite good, as they achieved state-of- the-art performance just a couple of years prior.",
    "AlexNet are quite good, as they achieved state-of- the-art performance just a couple of years prior. If we make the assumption that representations at downstream layers in deep networks are only going to be slightly different from one layer to the next, which is reasonable due to the fact that shallower networks still can learn very good representations, it would instead make sense to optimize the difference between representations (which should be close to zero for all weights) rather than attempt to achieve something close to the identity operation, which is a very specific and imbalanced weight setting.",
    "e something close to the identity operation, which is a very specific and imbalanced weight setting. That’s where residual connections come in. The downstream layers of the neural net‐ work, such as F and G, are learning precisely this difference between representations and then adding the difference back to the incoming representation x’ to achieve an only slightly different representation G(F(x')) + x’ .",
    "the incoming representation x’ to achieve an only slightly different representation G(F(x')) + x’ . This is in contrast with the traditional feed-forward neural network paradigm, which would attempt to learn a 148 | Chapter 7: Convolutional Neural Networks --- Page 165 --- weight setting that is approximately close to identity for F and G, which seems like a much harder problem. In the next section, we will put our knowledge together to build a residual network.",
    "harder problem. In the next section, we will put our knowledge together to build a residual network. Building a Residual Network with Superhuman Vision In the previous section, we discussed residual connections and how they allow for improved gradient flow through deep neural networks. In this section, we will repli‐ cate the implementation of a neural network with residual connections, specifically the ResNet34 architecture from He et al. ’s original.",
    "etwork with residual connections, specifically the ResNet34 architecture from He et al. ’s original. PyTorch’s Torchvision library provides constructors for many commonly used resnets. We can use it to create a ResNet34 model: from torchvision.models import resnet34 model = resnet34() Let’s see how resnet34 creates a residual network.",
    "hvision.models import resnet34 model = resnet34() Let’s see how resnet34 creates a residual network. Most versions of residual networks consist of the following structure: •Convolutional block (CONV->BN->ReLU->MAXPOOL) •Four residual layers •A classifier block with average pooling and a linear layer Each residual layer consists of one or more residual blocks. For example, the layers F and G from Figure 7-21 form a residual block.",
    "one or more residual blocks. For example, the layers F and G from Figure 7-21 form a residual block. Here is the PyTorch code for a simplified implementation of a residual block for ResNet34: class ResidualBlock(nn.Module): def __init__(self, in_layers, out_layers, downsample=None): super(ResidualBlock, self).__init__() self.conv1 = nn.Conv2d(in_layers, out_layers, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(out_layers) self.conv2 = nn.Conv2d(out_layers, out_layers, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(out_layers) self.downsample = downsample self.relu = nn.ReLU(inplace=True) def forward(self, inp): # Residual block out = self.conv1(inp) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) Building a Residual Network with Superhuman Vision | 149 --- Page 166 --- if self.downsample: inp = self.downsample(inp) # Shortcut connection out += inp return out Similarly to the previous section, each residual block in the ResNet34 architecture consists of two convolutional layers.",
    "ious section, each residual block in the ResNet34 architecture consists of two convolutional layers. The downsample argument allows for an optional downsampler function. The purpose of downsampling is to match the dimensions of the input with the output of the residual block, if the two are of different dimensions. The following is an example of a downsampler that matches the number of channels of the input to that of the output of the residual block.",
    "ampler that matches the number of channels of the input to that of the output of the residual block. Note that this downsampler does not change the size of each feature map given the kernel_size is 1 and the stride is also only 1, and affects the dimensions only by increasing the number of feature maps from 64 to 128: downsample = nn.Sequential( nn.Conv2d(64, 128, kernel_size=1, stride=1, bias=False), nn.BatchNorm2d(128) ) The number of residual blocks for each of the four residual layers in ResNet34 is defined as [3, 4, 6, 3], respectively.",
    "al blocks for each of the four residual layers in ResNet34 is defined as [3, 4, 6, 3], respectively. The ResNet34 architecture is named this way because it has 33 convolutional layers and 1 fully connected layer at the end, which serves as the predictor portion of the network. The 33 convolutional layers are arranged in four sections that have 3, 4, 6, and 3 residual blocks, in that order.",
    "tional layers are arranged in four sections that have 3, 4, 6, and 3 residual blocks, in that order. To get to the total of 33, there is a single convolutional layer at the beginning that operates on the original image input, which is assumed to have 3 channels. The following PyTorch code initializes each of these components, closely modeled after the official PyTorch implementation of the various versions presented in the original paper.",
    "d after the official PyTorch implementation of the various versions presented in the original paper. The first component, up to the max pool, operates on the original input, and each of the following components requires downsampling only between components. This is because, within each component, the input and output of each ResidualBlock are of the same dimension.",
    "ecause, within each component, the input and output of each ResidualBlock are of the same dimension. Although we won’t show it explicitly in this section, the combination of a kernel_size of 3, stride of 1, and padding of 1 ensures that the size of each feature map stays constant from beginning to end.",
    "1, and padding of 1 ensures that the size of each feature map stays constant from beginning to end. Addi‐ tionally, given the number of feature maps stays constant within each component, all dimensions end up remaining the same: class ResNet34(nn.Module): def __init__(self): super(ResNet34, self).__init__() self.conv1 = nn.Sequential( 150 | Chapter 7: Convolutional Neural Networks --- Page 167 --- nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1) ) # Note that each ResidualBlock has 2 conv layers # 3 blocks in a row, 6 conv layers self.comp1 = nn.Sequential( ResidualBlock(64, 64), ResidualBlock(64, 64), ResidualBlock(64, 64) ) # 4 blocks in a row, 8 conv layers downsample1 = nn.Sequential( nn.Conv2d(64, 128, kernel_size=1, stride=1, bias=False), nn.BatchNorm2d(128) ) self.comp2 = nn.Sequential( ResidualBlock(64, 128, downsample=downsample1), ResidualBlock(128, 128), ResidualBlock(128, 128), ResidualBlock(128, 128) ) # 6 blocks in a row, 12 conv layers downsample2 = nn.Sequential( nn.Conv2d(128, 256, kernel_size=1, stride=1, bias=False), nn.BatchNorm2d(256) ) self.comp3 = nn.Sequential( ResidualBlock(128, 256, downsample=downsample2), ResidualBlock(256, 256), ResidualBlock(256, 256), ResidualBlock(256, 256), ResidualBlock(256, 256), ResidualBlock(256, 256), ) # 3 blocks in a row, 6 conv layers downsample3 = nn.Sequential( nn.Conv2d(256, 512, kernel_size=1, stride=1, bias=False), nn.BatchNorm2d(512) ) self.comp4 = nn.Sequential( ResidualBlock(256, 512, downsample=downsample3), ResidualBlock(512, 512), ResidualBlock(512, 512) Building a Residual Network with Superhuman Vision | 151 --- Page 168 --- 17Gatys, Leon A., Alexander S.",
    "ding a Residual Network with Superhuman Vision | 151 --- Page 168 --- 17Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “ A Neural Algorithm of Artistic Style. ” arXiv Preprint arXiv :1508.06576 (2015).",
    "Matthias Bethge. “ A Neural Algorithm of Artistic Style. ” arXiv Preprint arXiv :1508.06576 (2015). ) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # ImageNet classifier: 1000 classes self.fc = nn.Linear(512, 1000) def forward(self, inp): out = self.conv1(inp) out = self.comp1(out) out = self.comp2(out) out = self.comp3(out) out = self.comp4(out) out = self.avgpool(out) out = torch.flatten(out, 1) out = self.fc(out) return out In the next section, we will present some of the latest advancements in computer vision regarding neural style transfer.",
    "we will present some of the latest advancements in computer vision regarding neural style transfer. Leveraging Convolutional Filters to Replicate Artistic Styles Over the past couple of years, we’ve also developed algorithms that leverage convo‐ lutional networks in much more creative ways. One of these algorithms is called neural style .17 The goal of neural style is to be able to take an arbitrary photograph and render it as if it were painted in the style of a famous artist.",
    "to take an arbitrary photograph and render it as if it were painted in the style of a famous artist. This seems like a daunting task, and it’s not exactly clear how we might approach this problem if we didn’t have a convolutional network. However, it turns out that clever manipulation of convolutional filters can produce spectacular results on this problem. Let’s take a pretrained convolutional network. We’re dealing with three images.",
    "lts on this problem. Let’s take a pretrained convolutional network. We’re dealing with three images. The first two are the source of content p and the source of style a. The third image is the generated image x. Our goal is to derive an error function that we can backpropagate that, when minimized, will perfectly combine the content of the desired photograph and the style of the desired artwork. We start with content first.",
    "content of the desired photograph and the style of the desired artwork. We start with content first. If a layer in the network has kl filters, then it produces a total of kl feature maps. Let’s call the size of each feature map ml, the height times the width of the feature map. This means that the activations in all the feature maps of this layer can be stored in a matrix F(l) of size kl×ml.",
    "the activations in all the feature maps of this layer can be stored in a matrix F(l) of size kl×ml. We can also represent all the 152 | Chapter 7: Convolutional Neural Networks --- Page 169 --- activations of the photograph in a matrix P(l) and all the activations of the generated image in the matrix X(l). We use the relu4_2 of the original VGGNet: Econtent (p, x) = ∑ij(Pij(l) – Xij(l))2 Now we can try tackling style.",
    "lu4_2 of the original VGGNet: Econtent (p, x) = ∑ij(Pij(l) – Xij(l))2 Now we can try tackling style. To do this we construct a matrix known as the Gram matrix , which represents correlations between feature maps in a given layer. The correlations represent the texture and feel that is common among all features, irrespective of which features we’re looking at.",
    "texture and feel that is common among all features, irrespective of which features we’re looking at. Constructing the Gram matrix, which is of size kl×kl, for a given image, is done as follows: G(l) ij = ∑c = 0ml F(l) ic F(l) jc We can compute the Gram matrices for both the artwork in matrix A(l) and the generated image in G(l).",
    "e can compute the Gram matrices for both the artwork in matrix A(l) and the generated image in G(l). We can then represent the error function as: Estylea,x=1 4kl2ml2∑l= 1L∑ij1 LAijl−Gijl2 Here, we weight each squared difference equally (dividing by the number of layers we want to include in our style reconstruction). Specifically, we use the relu1_1 , relu2_1 , relu3_1 , relu4_1 , and relu5_1 layers of the original VGGNet.",
    "cally, we use the relu1_1 , relu2_1 , relu3_1 , relu4_1 , and relu5_1 layers of the original VGGNet. We omit a full discussion of the TensorFlow code for brevity, but the results, as shown in Figure 7-22 , are again quite spectacular. We mix a photograph of the iconic MIT dome and Leonid Afremov’s Rain Princess . Leveraging Convolutional Filters to Replicate Artistic Styles | 153 --- Page 170 --- 18Image credit: Anish Athalye. 19Karpathy, Andrej, et al.",
    "ate Artistic Styles | 153 --- Page 170 --- 18Image credit: Anish Athalye. 19Karpathy, Andrej, et al. “Large-scale Video Classification with Convolutional Neural Networks. ” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 2014. 20Abdel-Hamid, Ossama, et al. “ Applying Convolutional Neural Networks Concepts to Hybrid NN-HMM Model for Speech Recognition. ” IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Kyoto, 2012, pp. 4277-4280.",
    "ational Conference on Acoustics, Speech, and Signal Processing (ICASSP), Kyoto, 2012, pp. 4277-4280. Figure 7-22. The result of mixing the Rain Princess with a photograph of the MIT dome18 Learning Convolutional Filters for Other Problem Domains Although our examples in this chapter focus on image recognition, there are sev‐ eral other problem domains in which convolutional networks are useful. A natural extension of image analysis is video analysis.",
    "in which convolutional networks are useful. A natural extension of image analysis is video analysis. In fact, using five-dimensional tensors (including time as a dimension) and applying three-dimensional convolutions is an easy way to extend the convolutional paradigm to video.19 Convolutional filters have also been successfully used to analyze audiograms.20 In these applications, a convolutional network slides over an audiogram input to predict phonemes on the other side.",
    "tions, a convolutional network slides over an audiogram input to predict phonemes on the other side. Less intuitively, convolutional networks have also found some use in natural language processing. We’ll see some examples of this in later chapters. More exotic uses of con‐ volutional networks include teaching algorithms to play board games, and analyzing biological molecules for drug discovery. We’ll also discuss both of these examples in later chapters of this book.",
    "ecules for drug discovery. We’ll also discuss both of these examples in later chapters of this book. 154 | Chapter 7: Convolutional Neural Networks --- Page 171 --- Summary In this chapter, we learned how to build neural networks that analyze images. We developed the concept of a convolution, and leveraged this idea to create tractable networks that can analyze both simple and more complex natural images.",
    "this idea to create tractable networks that can analyze both simple and more complex natural images. We built several of these convolutional networks in TensorFlow and leveraged various image processing pipelines and batch normalization to make training our networks faster and more robust. Finally, we visualized the learning of convolutional networks and explored other interesting applications of the technology.",
    "he learning of convolutional networks and explored other interesting applications of the technology. Images were easy to analyze because we were able to come up with effective ways to represent them as tensors. In other situations (e.g., natural language), it’s less clear how one might represent our input data as tensors. To tackle this problem as a stepping stone to new deep learning models, we’ll develop some key concepts in vector embeddings and representation learning in the next chapter.",
    "e’ll develop some key concepts in vector embeddings and representation learning in the next chapter. Summary | 155 --- Page 173 --- CHAPTER 8 Embedding and Representation Learning Learning Lower-Dimensional Representations In the previous chapter, we motivated the convolutional architecture using a simple argument. The larger our input vector, the larger our model. Large models with lots of parameters are expressive, but they’re also increasingly data hungry.",
    "del. Large models with lots of parameters are expressive, but they’re also increasingly data hungry. This means that without sufficiently large volumes of training data, we will likely overfit. Convo‐ lutional architectures help us cope with the curse of dimensionality by reducing the number of parameters in our models without necessarily diminishing expressiveness. Regardless, convolutional networks still require large amounts of labeled training data.",
    "essiveness. Regardless, convolutional networks still require large amounts of labeled training data. And for many problems, labeled data is scarce and expensive to generate. Our goal in this chapter will be to develop effective learning models in situations where labeled data is scarce, but wild, unlabeled data is plentiful. We’ll approach this problem by learning embeddings , or low-dimensional representations, in an unsupervised fashion.",
    "his problem by learning embeddings , or low-dimensional representations, in an unsupervised fashion. Because these unsupervised models allow us to offload all of the heavy lifting of automated feature selection, we can use the generated embeddings to solve learning problems using smaller models that require less data. This process is summarized in Figure 8-1 .",
    "ing problems using smaller models that require less data. This process is summarized in Figure 8-1 . In the process of developing algorithms that learn good embeddings, we’ll also explore other applications of learning lower-dimensional representations, such as visualization and semantic hashing. We’ll start by considering situations where all of the important information is already contained within the original input vector itself.",
    "where all of the important information is already contained within the original input vector itself. In this case, learning embeddings is equivalent to developing an effective compression algorithm. 157 --- Page 174 --- Figure 8-1. Using embeddings to automate feature selection in the face of scarce labeled data In the next section, we’ll introduce principal component analysis (PCA), a classic method for dimensionality reduction.",
    ", we’ll introduce principal component analysis (PCA), a classic method for dimensionality reduction. In subsequent sections, we’ll explore more powerful neural methods for learning compressive embeddings. Principal Component Analysis The basic concept behind PCA is to find a set of axes that communicates the most information about our dataset.",
    "oncept behind PCA is to find a set of axes that communicates the most information about our dataset. More specifically, if we have d-dimensional data, we’ d like to find a new set of m<d dimensions that conserves as much valuable informa‐ tion from the original dataset as possible. For simplicity, let’s choose d= 2,m= 1. Assuming that variance corresponds to information, we can perform this transforma‐ tion through an iterative process.",
    "iance corresponds to information, we can perform this transforma‐ tion through an iterative process. First, we find a unit vector along which the dataset has maximum variance. Because this direction contains the most information, we select this direction as our first axis. Then from the set of vectors orthogonal to this first choice, we pick a new unit vector along which the dataset has maximum variance. This is our second axis.",
    "ce, we pick a new unit vector along which the dataset has maximum variance. This is our second axis. We continue this process until we have found a total of d new vectors that represent new axes. We project our data onto this new set of axes. We then decide a good value for m and toss out all but the first m axes (the principal components, which store the most information). The result is shown in Figure 8-2 . Figure 8-2.",
    "cipal components, which store the most information). The result is shown in Figure 8-2 . Figure 8-2. An illustration of PCA for dimensionality reduction to capture the dimension with the most information (as proxied by variance) 158 | Chapter 8: Embedding and Representation Learning --- Page 175 --- For the mathematically inclined, we can view this operation as a projection onto the vector space spanned by the top m eigenvectors of the dataset’s correlation matrix, which is equivalent to the dataset’s covariance matrix when the dataset has been z-score normalized (zero-mean and unit-variance per input dimension).",
    "trix when the dataset has been z-score normalized (zero-mean and unit-variance per input dimension). Let us repre‐ sent the dataset as a matrix X with dimensions n×d (i.e., n inputs of d dimensions). We’ d like to create an embedding matrix T with dimensions n×m. We can compute the matrix using the relationship T = X, where each column of W corresponds to an eigenvector of the matrix 1 nXΤX.",
    "g the relationship T = X, where each column of W corresponds to an eigenvector of the matrix 1 nXΤX. Those with linear algebra background or core data science experience may be seeing a striking parallel between PCA and the singular value decomposition (SVD), which we cover in more depth in “Theory: PCA and SVD” on page 187 . While PCA has been used for decades for dimensionality reduction, it spectacularly fails to capture important relationships that are piecewise linear or nonlinear.",
    "n, it spectacularly fails to capture important relationships that are piecewise linear or nonlinear. Take, for instance, the example illustrated in Figure 8-3 . The example shows data points selected at random from two concentric circles. We hope that PCA will transform this dataset so that we can pick a single new axis that allows us to easily separate the dots.",
    "sform this dataset so that we can pick a single new axis that allows us to easily separate the dots. Unfortunately for us, there is no linear direction that contains more information here than another (we have equal variance in all directions). Instead, as human beings, we notice that information is being encoded in a nonlinear way, in terms of how far points are from the origin.",
    "hat information is being encoded in a nonlinear way, in terms of how far points are from the origin. With this information in mind, we notice that the polar transformation (expressing points as their distance from the origin, as the new horizontal axis, and their angle bearing from the original x-axis, as the new vertical axis) does just the trick. Figure 8-3 highlights the shortcomings of an approach like PCA in capturing impor‐ tant relationships in complex datasets.",
    "the shortcomings of an approach like PCA in capturing impor‐ tant relationships in complex datasets. Because most of the datasets we are likely to encounter in the wild (images, text, etc.) are characterized by nonlinear relationships, we must develop a theory that will perform nonlinear dimensionality reduction. Deep learning practitioners have closed this gap using neural models, which we’ll cover in the next section. Principal Component Analysis | 159 --- Page 176 --- Figure 8-3.",
    "ich we’ll cover in the next section. Principal Component Analysis | 159 --- Page 176 --- Figure 8-3. A situation in which PCA fails to optimally transform the data for dimen‐ sionality reduction Motivating the Autoencoder Architecture When we talked about feed-forward networks, we discussed how each layer learned progressively more relevant representations of the input.",
    "works, we discussed how each layer learned progressively more relevant representations of the input. In fact, in Chapter 7 , we took the output of the final convolutional layer and used that as a lower-dimensional representation of the input image. Putting aside the fact that we want to generate these low-dimensional representations in an unsupervised fashion, there are funda‐ mental problems with these approaches in general.",
    "tions in an unsupervised fashion, there are funda‐ mental problems with these approaches in general. Specifically, while the selected layer does contain information from the input, the network has been trained to pay attention to the aspects of the input that are critical to solving the task at hand.",
    "trained to pay attention to the aspects of the input that are critical to solving the task at hand. As a result, there’s a significant amount of information loss with respect to elements of the input that may be important for other classification tasks, but potentially less important than the one immediately at hand. However, the fundamental intuition here still applies. We define a new network architecture that we call the autoencoder .",
    "al intuition here still applies. We define a new network architecture that we call the autoencoder . We first take the input and compress it into a low-dimensional vector. This part of the network is called the encoder because it is responsible for producing the low-dimensional embedding or code .",
    "s called the encoder because it is responsible for producing the low-dimensional embedding or code . The second part of the network, instead of mapping the embedding to an arbitrary label as we would 160 | Chapter 8: Embedding and Representation Learning --- Page 177 --- 1Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. “Reducing the Dimensionality of Data with Neural Networks.",
    "Geoffrey E., and Ruslan R. Salakhutdinov. “Reducing the Dimensionality of Data with Neural Networks. ” Science 313.5786 (2006): 504-507.in a feed-forward network, tries to invert the computation of the first half of the network and reconstruct the original input. This piece is known as the decoder . The overall architecture is illustrated in Figure 8-4 . Figure 8-4.",
    "piece is known as the decoder . The overall architecture is illustrated in Figure 8-4 . Figure 8-4. The autoencoder architecture attempts to construct a high-dimensional input into a low-dimensional embedding and then uses that low-dimensional embedding to reconstruct the input To demonstrate the surprising effectiveness of autoencoders, we’ll build and visualize the autoencoder architecture in Figure 8-4 .",
    "ffectiveness of autoencoders, we’ll build and visualize the autoencoder architecture in Figure 8-4 . Specifically, we will highlight its superior ability to separate MNIST digits as compared to PCA.",
    ". Specifically, we will highlight its superior ability to separate MNIST digits as compared to PCA. Implementing an Autoencoder in PyTorch The seminal paper “Reducing the Dimensionality of Data with Neural Networks, ” which describes the autoencoder, was written by Hinton and Salakhutdinov in 2006.1 Their hypothesis was that the nonlinear complexities afforded by a neural model would allow them to capture structure that linear methods, such as PCA, would miss.",
    "y a neural model would allow them to capture structure that linear methods, such as PCA, would miss. To demonstrate this point, they ran an experiment on MNIST using both an autoencoder and PCA to reduce the dataset into 2D data points. In this section, we will recreate their experimental setup to validate this hypothesis and further explore the architecture and properties of feed-forward autoencoders.",
    "te this hypothesis and further explore the architecture and properties of feed-forward autoencoders. The setup shown in Figure 8-5 is built with the same principle, but the 2D embedding is now treated as the input, and the network attempts to reconstruct the original image. Because we are essentially applying an inverse operation, we architect the decoder network so that the autoencoder has the shape of an hourglass.",
    "e operation, we architect the decoder network so that the autoencoder has the shape of an hourglass. The output of the decoder network is a 784-dimensional vector that can be reconstructed into a 28 × 28 image: class Decoder(nn.Module): def __init__(self, n_in, n_hidden_1, n_hidden_2, n_hidden_3, n_out): super(Decoder, self).__init__() self.layer1 = nn.Sequential( nn.Linear(n_in, n_hidden_1, bias=True), nn.BatchNorm1d(n_hidden_1), nn.Sigmoid()) self.layer2 = nn.Sequential( Implementing an Autoencoder in PyTorch | 161 --- Page 178 --- nn.Linear(n_hidden_1, n_hidden_2, bias=True), nn.BatchNorm1d(n_hidden_2), nn.Sigmoid()) self.layer3 = nn.Sequential( nn.Linear(n_hidden_2, n_hidden_3, bias=True), nn.BatchNorm1d(n_hidden_3), nn.Sigmoid()) n_size = math.floor(math.sqrt(n_out)) self.layer4 = nn.Sequential( nn.Linear(n_hidden_3, n_out, bias=True), nn.BatchNorm1d(n_out), nn.Sigmoid(), nn.Unflatten(1, torch.Size([1, n_size,n_size]))) def forward(self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return self.layer4(x) Figure 8-5.",
    "self, x): x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return self.layer4(x) Figure 8-5. The experimental setup for dimensionality reduction of the MNIST dataset employed by Hinton and Salakhutdinov, 2006 162 | Chapter 8: Embedding and Representation Learning --- Page 179 --- In order to accelerate training, we’ll reuse the batch normalization strategy we employed in Chapter 7 .",
    "rder to accelerate training, we’ll reuse the batch normalization strategy we employed in Chapter 7 . Also, because we’ d like to visualize the results, we’ll avoid introducing sharp transitions in our neurons. In this example, we’ll use sigmoidal neurons instead of our usual ReLU neurons: decoder = Decoder(2,250,500,1000,784) Finally, we need to construct a measure (or objective function) that describes how well our model functions.",
    "we need to construct a measure (or objective function) that describes how well our model functions. Specifically, we want to measure how close the reconstruc‐ tion is to the original image. We can measure this simply by computing the distance between the original 784-dimensional input and the reconstructed 784-dimensional output.",
    "he distance between the original 784-dimensional input and the reconstructed 784-dimensional output. More specifically, given an input vector I and a reconstruction O, we’ d like to minimize the value of ∥ I−O ∥ =∑iIi−Oi2, also known as the L2 norm of the difference between the two vectors. We average this function over the whole minibatch to generate our final objective function.",
    "vectors. We average this function over the whole minibatch to generate our final objective function. Finally, we’ll train the network using the Adam optimizer, logging a scalar summary of the error incurred at every minibatch using torch.utils.tensorboard.SummaryWriter .",
    "calar summary of the error incurred at every minibatch using torch.utils.tensorboard.SummaryWriter . In PyTorch, we can concisely express the loss and training operations as follows: loss_fn = nn.MSELoss() optimizer = optim.Adam(decoder.parameters(), lr = 0.001, betas=(0.9,0.999), eps=1e-08) trainset = datasets.MNIST('.', train=True, transform=transforms.ToTensor(), download=True) trainloader = DataLoader(trainset, batch_size=32, shuffle=True) # Training Loop NUM_EPOCHS = 5 for epoch in range(NUM_EPOCHS): for input, labels in trainloader: optimizer.zero_grad() code = encoder(input) output = decoder(code) #print(input.shape, output.shape) loss = loss_fn(output, input) optimizer.step() print(f\"Epoch: {epoch} Loss: {loss}\") Finally, we’ll need a method to evaluate the generalizability of our model.",
    ": {epoch} Loss: {loss}\") Finally, we’ll need a method to evaluate the generalizability of our model. As usual, we’ll use a validation dataset and compute the same L2 norm measurement for model evaluation.",
    "usual, we’ll use a validation dataset and compute the same L2 norm measurement for model evaluation. In addition, we’ll collect image summaries so that we can compare both the input images and the reconstructions: Implementing an Autoencoder in PyTorch | 163 --- Page 180 --- i = 0 with torch.no_grad(): for images, labels in trainloader: if i == 3: break grid = utils.make_grid(images) plt.figure() plt.imshow(grid.permute(1,2,0)) code = encoder(images) output = decoder(code) grid = utils.make_grid(output) plt.figure() plt.imshow(grid.permute(1,2,0)) i += 1 We can visualize the model graph, the training and validation costs, and the image summaries using TensorBoard.",
    "alize the model graph, the training and validation costs, and the image summaries using TensorBoard. Simply run the following command: $ tensorboard --logdir ~/path/to/mnist_autoencoder_hidden=2_logs Then navigate your browser to http://localhost:6006/ . The results of the “Graph” tab are shown in Figure 8-6 . Thanks to how we’ve namespaced the components of our model graph, our model is nicely organized.",
    "6 . Thanks to how we’ve namespaced the components of our model graph, our model is nicely organized. We can easily click through the components and delve deeper, tracing how data flows up through the various layers of the encoder and through the decoder, how the optimizer reads the output of our training module, and how gradients in turn affect all of the components of the model.",
    "output of our training module, and how gradients in turn affect all of the components of the model. We also visualize both the training (after each minibatch) and validation costs (after each epoch), closely monitoring the curves for potential overfitting. The TensorBoard visualizations of the costs over the span of training are shown in Figure 8-7 . As we would expect for a successful model, both the training and validation curves decrease until they flatten off asymptotically.",
    "ssful model, both the training and validation curves decrease until they flatten off asymptotically. After approximately 200 epochs, we attain a validation cost of 4.78. While the curves look promising, it’s difficult, upon first glance, to understand whether we’ve reached a plateau at a “good” cost, or whether our model is still doing a poor job of reconstructing the original inputs. 164 | Chapter 8: Embedding and Representation Learning --- Page 181 --- Figure 8-6.",
    "original inputs. 164 | Chapter 8: Embedding and Representation Learning --- Page 181 --- Figure 8-6. TensorBoard allows us to neatly view the high-level components and data flow of our computation graph (top) and also click through to more closely inspect the data flows of individual subcomponents (bottom) Implementing an Autoencoder in PyTorch | 165 --- Page 182 --- Figure 8-7.",
    "ual subcomponents (bottom) Implementing an Autoencoder in PyTorch | 165 --- Page 182 --- Figure 8-7. The cost incurred on the training set (logged after each minibatch) and on the validation set (logged after each epoch) To get a sense of what that means, let’s explore the MNIST dataset. We pick an arbitrary image of a 1 from the dataset and call it X. In Figure 8-8 , we compare the image to all other images in the dataset.",
    "the dataset and call it X. In Figure 8-8 , we compare the image to all other images in the dataset. Specifically, for each digit class, we compute the average of the L2 costs, comparing X to each instance of the digit class. As a visual aid, we also include the average of all of the instances for each digit class. 166 | Chapter 8: Embedding and Representation Learning --- Page 183 --- Figure 8-8.",
    "ach digit class. 166 | Chapter 8: Embedding and Representation Learning --- Page 183 --- Figure 8-8. The image of the 1 on the left is compared to all of the other digits in the MNIST dataset; each digit class is represented visually with the average of all of its members and labeled with the average of the L2 costs, comparing the 1 on the left with all of the class members On average, X is 5.75 units away from other 1s in MNIST.",
    "1 on the left with all of the class members On average, X is 5.75 units away from other 1s in MNIST. In terms of L2 distance, the non-1 digits closest to the X are the 7s (8.94 units) and the digits farthest are the 0s (11.05 units). Given these measurements, it’s quite apparent that with an average cost of 4.78, our autoencoder is producing high-quality reconstructions.",
    "parent that with an average cost of 4.78, our autoencoder is producing high-quality reconstructions. Because we are collecting image summaries, we can confirm this hypothesis directly by inspecting the input images and reconstructions directly. The reconstructions for three randomly chosen samples from the test set are shown in Figure 8-9 . Figure 8-9.",
    "structions for three randomly chosen samples from the test set are shown in Figure 8-9 . Figure 8-9. A side-by-side comparison of the original inputs (from the validation set) and reconstructions after 5, 100, and 200 epochs of training Implementing an Autoencoder in PyTorch | 167 --- Page 184 --- After five epochs, we can start to make out some of the critical strokes of the original image that are being picked by the autoencoder, but for the most part, the reconstructions are still hazy mixtures of closely related digits.",
    "coder, but for the most part, the reconstructions are still hazy mixtures of closely related digits. By 100 epochs, the 0 and 4 are reconstructed with strong strokes, but it looks like the autoencoder is still having trouble differentiating between 5s, 3s, and possibly 8s. However, by 200 epochs, it’s clear that even this more difficult ambiguity is clarified, and all of the digits are crisply reconstructed.",
    "at even this more difficult ambiguity is clarified, and all of the digits are crisply reconstructed. Finally, we’ll complete the section by exploring the 2D codes produced by traditional PCA and autoencoders. We’ll want to show that autoencoders produce better visuali‐ zations. In particular, we’ll want to show that autoencoders do a much better job of visually separating instances of different digit classes than PCA.",
    "oencoders do a much better job of visually separating instances of different digit classes than PCA. We’ll start by quickly covering the code we use to produce 2D PCA codes: from sklearn import decomposition import input_data mnist = input_data.read_data_sets(\"data/\", one_hot=False) pca = decomposition.PCA(n_components=2) pca.fit(mnist.train.images) pca_codes = pca.transform(mnist.test.images) We first pull up the MNIST dataset.",
    "mnist.train.images) pca_codes = pca.transform(mnist.test.images) We first pull up the MNIST dataset. We’ve set the flag one_hot=False because we’ d like the labels to be provided as integers instead of one-hot vectors (as a quick reminder, a one-hot vector representing an MNIST label would be a vector of size 10 with the itℎ component set to one to represent digit i and the rest of the components set to zero).",
    "with the itℎ component set to one to represent digit i and the rest of the components set to zero). We use the commonly used machine learning library scikit-learn to perform the PCA, setting the n_components=2 flat so that scikit-learn knows to generate 2D codes.",
    "to perform the PCA, setting the n_components=2 flat so that scikit-learn knows to generate 2D codes. We can also reconstruct the original images from the 2D codes and visualize the reconstructions: from matplotlib import pyplot as plt pca_recon = pca.inverse_transform(pca_codes[:1]) plt.imshow(pca_recon[0].reshape((28,28)), cmap=plt.cm.gray) plt.show() 168 | Chapter 8: Embedding and Representation Learning --- Page 185 --- The code snippet shows how to visualize the first image in the test dataset, but we can easily modify the code to visualize any arbitrary subset of the dataset.",
    "he test dataset, but we can easily modify the code to visualize any arbitrary subset of the dataset. Comparing the PCA reconstructions to the autoencoder reconstructions in Figure 8-10 , it’s quite clear that the autoencoder vastly outperforms PCA with 2D codes. In fact, the PCA ’s performance is somewhat reminiscent of the autoencoder only five epochs into training. It has trouble distinguishing 5s from 3s and 8s, 0s from 8s, and 4s from 9s.",
    "e epochs into training. It has trouble distinguishing 5s from 3s and 8s, 0s from 8s, and 4s from 9s. Repeating the same experiment with 30-dimensional codes provides significant improvement to the PCA reconstructions, but they are still significantly worse than the 30-dimensional autoencoder. Figure 8-10.",
    "structions, but they are still significantly worse than the 30-dimensional autoencoder. Figure 8-10. Comparing the reconstructions by both PCA and autoencoder side by side Now, to complete the experiment, we must load up a saved PyTorch model, retrieve the 2D codes, and plot both the PCA and autoencoder codes. We’re careful to rebuild the PyTorch graph exactly how we set it up during training.",
    "oencoder codes. We’re careful to rebuild the PyTorch graph exactly how we set it up during training. We pass the path to the model checkpoint we saved during training as a command-line argument to the script. Finally, we use a custom plotting function to generate a legend and appropri‐ ately color data points of different digit classes.",
    "ting function to generate a legend and appropri‐ ately color data points of different digit classes. In the resulting visualization in Figure 8-11 , it is extremely difficult to make out sepa‐ rable clusters in the 2D PCA codes; the autoencoder has clearly done a spectacular job at clustering codes of different digit classes.",
    "; the autoencoder has clearly done a spectacular job at clustering codes of different digit classes. This means that a simple machine learning model is going to be able to much more effectively classify data points consisting of autoencoder embeddings as compared to PCA embeddings. Implementing an Autoencoder in PyTorch | 169 --- Page 186 --- Figure 8-11.",
    "mpared to PCA embeddings. Implementing an Autoencoder in PyTorch | 169 --- Page 186 --- Figure 8-11. 2D embeddings produced by PCA (top) and by an autoencoder (bottom) In this section, we successfully set up and trained a feed-forward autoencoder and demonstrated that the resulting embeddings were superior to PCA, a classical dimen‐ sionality reduction method.",
    "d that the resulting embeddings were superior to PCA, a classical dimen‐ sionality reduction method. In the next section, we’ll explore a concept known as denoising, which acts as a form of regularization by making our embeddings more robust. 170 | Chapter 8: Embedding and Representation Learning --- Page 187 --- 2Vincent, Pascal, et al. “Extracting and Composing Robust Features with Denoising Autoencoders. ” Proceedings of the 25th International Conference on Machine Learning .",
    "ith Denoising Autoencoders. ” Proceedings of the 25th International Conference on Machine Learning . ACM, 2008.Denoising to Force Robust Representations Denoising improves the ability of the autoencoder to generate embeddings that are resistant to noise. The human ability for perception is surprisingly resistant to noise. Take Figure 8-12 , for example. Despite the fact that I’ve corrupted half of the pixels in each image, you still have no problem making out the digit.",
    "hat I’ve corrupted half of the pixels in each image, you still have no problem making out the digit. In fact, even easily confused digits (like the 2 and the 7) are still distinguishable. Figure 8-12. Human perception allows us to identify even obscured digits One way to look at this phenomenon is probabilistically.",
    "allows us to identify even obscured digits One way to look at this phenomenon is probabilistically. Even if we’re exposed to a random sampling of pixels from an image, if we have enough information, our brain is still capable of concluding the ground truth of what the pixels represent with maximal probability. Our mind is able to, quite literally, fill in the blanks to draw a conclusion.",
    "maximal probability. Our mind is able to, quite literally, fill in the blanks to draw a conclusion. Even though only a corrupted version of a digit hits our retina, our brain is still able to reproduce the set of activations (i.e., the code or embedding) that we normally would use to represent the image of that digit. This is a property we might hope to enforce in our embedding algorithm, and it was first explored by Vincent et al.",
    "rty we might hope to enforce in our embedding algorithm, and it was first explored by Vincent et al. in 2008, when they introduced the denoising autoencoder .2 The basic principles behind denoising are quite simple. We corrupt some fixed percentage of the pixels in the input image by setting them to zero. Given an original input X, let’s call the corrupted version CX.",
    "input image by setting them to zero. Given an original input X, let’s call the corrupted version CX. The denoising autoencoder is identical to the vanilla autoencoder except for one detail: the input to the encoder network is the corrupted CX instead of X. In other words, the autoencoder is forced to learn a code for each input that is resistant to the corruption mechanism and is able Denoising to Force Robust Representations | 171 --- Page 188 --- 3Bengio, Y oshua, et al.",
    "nd is able Denoising to Force Robust Representations | 171 --- Page 188 --- 3Bengio, Y oshua, et al. “Generalized Denoising Auto-Encoders as Generative Models. ” Advances in Neural Information Processing Systems . 2013.to interpolate through the missing information to recreate the original, uncorrupted image. We can also think about this process more geometrically. Let’s say we had a 2D dataset with various labels.",
    "also think about this process more geometrically. Let’s say we had a 2D dataset with various labels. Let’s take all of the data points in a particular category (i.e., with some fixed label), and call this subset of data points S. While any arbitrary sampling of points could end up taking any form while visualized, we presume that for real-life categories, there is some underlying structure that unifies all of the points in S. This underlying, unifying geometric structure is known as a manifold .",
    "ifies all of the points in S. This underlying, unifying geometric structure is known as a manifold . The manifold is the shape that we want to capture when we reduce the dimensionality of our data; and as Bengio et al.",
    "he shape that we want to capture when we reduce the dimensionality of our data; and as Bengio et al. described in 2013, our autoencoder is implicitly learning this manifold as it learns how to reconstruct data after pushing it through a bottleneck (the code layer).3 The autoencoder must figure out whether a point belongs to one manifold or another when trying to generate a reconstruction of an instance with potentially different labels.",
    "r another when trying to generate a reconstruction of an instance with potentially different labels. As an illustration, let’s consider the scenario in Figure 8-13 , where the points in S are a simple low-dimensional manifold (a solid circle in the diagram). In part A, we see our data points in S (black xs) and the manifold that best describes them. We also observe an approximation of our corruption operation.",
    "the manifold that best describes them. We also observe an approximation of our corruption operation. Specifically, the arrow and nonconcentric circle demonstrate all the ways in which the corruption could possibly move or modify a data point.",
    "circle demonstrate all the ways in which the corruption could possibly move or modify a data point. Given that we are applying this corruption operation to every data point (i.e., along the entire manifold), this corruption operation artificially expands the dataset to not only include the manifold but also all of the points in space around the manifold, up to a maximum margin of error.",
    "e manifold but also all of the points in space around the manifold, up to a maximum margin of error. This margin is demonstrated by the dashed circles in A, and the dataset expansion is illustrated by the x’s in part B. Finally the autoencoder is forced to learn to collapse all of the data points in this space back to the manifold.",
    "utoencoder is forced to learn to collapse all of the data points in this space back to the manifold. In other words, by learning which aspects of a data point are generalizable, broad strokes, and which aspects are “noise, ” the denoising autoencoder learns to approximate the underlying manifold of S. 172 | Chapter 8: Embedding and Representation Learning --- Page 189 --- Figure 8-13.",
    "manifold of S. 172 | Chapter 8: Embedding and Representation Learning --- Page 189 --- Figure 8-13. The denoising objective enables our model to learn the manifold (dark circle) by learning to map corrupted data (light x’s in B and C) to uncorrupted data (dark x’s) by minimizing the error (arrows in C) between their representations With the philosophical motivations of denoising in mind, we can now make a small modification to our autoencoder script to build a denoising autoencoder: def corrupt_input(x): corrupting_matrix = 2.0*torch.rand_like(x) return x * corrupting_matrix # x = mnist data image of shape 28*28=784 x = torch.rand((28,28)) corrupt = 1.0 # set to 1.0 to corrupt input c_x = (corrupt_input(x) * corrupt) + (x * (1 - corrupt)) This code snippet corrupts the input if the corrupt variable is equal to 1, and it refrains from corrupting the input if the corrupt variable is equal to 0.",
    "able is equal to 1, and it refrains from corrupting the input if the corrupt variable is equal to 0. After making this modification, we can rerun our autoencoder, resulting in the reconstructions shown in Figure 8-14 . It’s quite apparent that the denoising autoencoder has faithfully replicated our incredible human ability to fill in the missing pixels. Denoising to Force Robust Representations | 173 --- Page 190 --- Figure 8-14.",
    "in the missing pixels. Denoising to Force Robust Representations | 173 --- Page 190 --- Figure 8-14. We apply a corruption operation to the dataset and train a denoising autoencoder to reconstruct the original, uncorrupted images Sparsity in Autoencoders One of the most difficult aspects of deep learning is a problem known as interpreta‐ bility . Interpretability is a property of a machine learning model that measures how easy it is to inspect and explain its process and/or output.",
    "achine learning model that measures how easy it is to inspect and explain its process and/or output. Deep models are generally difficult to interpret because of the nonlinearities and massive numbers of parameters that make up a model. While deep models are generally more accurate, a lack of interpretability often hinders their adoption in highly valuable, but highly risky, applications.",
    "of interpretability often hinders their adoption in highly valuable, but highly risky, applications. For example, if a machine learning model is predicting that a patient has or does not have cancer, the doctor will likely want an explanation to confirm the model’s conclusion. We can address one aspect of interpretability by exploring the characteristics of the output of an autoencoder.",
    "ess one aspect of interpretability by exploring the characteristics of the output of an autoencoder. In general, an autoencoder’s representations are dense, and this has implications with respect to how the representation changes as we make coherent modifications to the input. Consider the situation in Figure 8-15 . 174 | Chapter 8: Embedding and Representation Learning --- Page 191 --- Figure 8-15.",
    "n Figure 8-15 . 174 | Chapter 8: Embedding and Representation Learning --- Page 191 --- Figure 8-15. The activations of a dense representation combine and overlay information from multiple features in ways that are difficult to interpret The autoencoder produces a dense representation, that is, the representation of the original image is highly compressed.",
    "uces a dense representation, that is, the representation of the original image is highly compressed. Because we have only so many dimensions to work with in the representation, the activations of the representation combine information from multiple features in ways that are extremely difficult to disentangle. The result is that as we add components or remove components, the output represen‐ tation changes in unexpected ways.",
    "t as we add components or remove components, the output represen‐ tation changes in unexpected ways. It’s virtually impossible to interpret how and why the representation is generated in the way it is. The ideal outcome for us is if we can build a representation where there is a 1-to-1 correspondence, or close to a 1-to-1 correspondence, between high-level features and individual components in the code.",
    "close to a 1-to-1 correspondence, between high-level features and individual components in the code. When we are able to achieve this, we get very close to the system described in Figure 8-16 , which shows how the representation changes as we add and remove components. The representation is the sum of the individual strokes in the image. With the right combination of space and sparsity, a representation is more interpretable. Figure 8-16.",
    "th the right combination of space and sparsity, a representation is more interpretable. Figure 8-16. How activations in the representation change with the addition and removal of strokes Sparsity in Autoencoders | 175 --- Page 192 --- 4Ranzato, Marc’ Aurelio, et al. “Efficient Learning of Sparse Representations with an Energy-Based Model. ” Proceedings of the 19th International Conference on Neural Information Processing Systems . MIT Press, 2006. 5Ranzato, Marc’ Aurelio, and Martin Szummer.",
    "eural Information Processing Systems . MIT Press, 2006. 5Ranzato, Marc’ Aurelio, and Martin Szummer. “Semi-supervised Learning of Compact Document Representa‐ tions with Deep Networks. ” Proceedings of the 25th International Conference on Machine Learning . ACM, 2008. 6Makhzani, Alireza, and Brendan Frey. “k-Sparse Autoencoders.",
    "rence on Machine Learning . ACM, 2008. 6Makhzani, Alireza, and Brendan Frey. “k-Sparse Autoencoders. ” arXiv preprint arXiv :1312.5663 (2013).While this is the ideal outcome, we’ll have to think through what mechanisms we can leverage to enable this interpretability in the representation. The issue here is clearly the bottlenecked capacity of the code layer; but unfortunately, increasing the capacity of the code layer alone is not sufficient.",
    "he code layer; but unfortunately, increasing the capacity of the code layer alone is not sufficient. In the medium case, while we can increase the size of the code layer, there is no mechanism that prevents each individual feature picked up by the autoencoder from affecting a large fraction of the components with smaller magnitudes.",
    "ked up by the autoencoder from affecting a large fraction of the components with smaller magnitudes. In the more extreme case, where the features that are picked up are more complex and therefore more bountiful, the capacity of the code layer may be even larger than the dimensionality of the input. In this case, the code layer has so much capacity that the model could quite literally perform a “copy” operation where the code layer learns no useful representation.",
    "uld quite literally perform a “copy” operation where the code layer learns no useful representation. What we really want is to force the autoencoder to utilize as few components of the representation vector as possible, while still effectively reconstructing the input. This is similar to the rationale behind using regularization to prevent overfitting in simple neural networks, as we discussed in Chapter 4 , except we want as many components to be zero (or extremely close to zero) as possible.",
    "n Chapter 4 , except we want as many components to be zero (or extremely close to zero) as possible. As in Chapter 4 , we’ll achieve this by modifying the objective function with a sparsity penalty, which increases the cost of any representation that has a large number of nonzero components: ESparse =E+β· SparsityPenalty The value of β determines how strongly we favor sparsity at the expense of generating better reconstructions.",
    "of β determines how strongly we favor sparsity at the expense of generating better reconstructions. For the mathematically inclined, you would do this by treating the values of each of the components of every representation as the outcome of a random variable with an unknown mean. We would then employ a measure of diver‐ gence comparing the distribution of observations of this random variable (the values of each component) and the distribution of a random variable whose mean is known to be 0.",
    "the values of each component) and the distribution of a random variable whose mean is known to be 0. A measure that is often used to this end is the Kullback-Leibler (often referred to as KL) divergence. Further discussion on sparsity in autoencoders is beyond the scope of this text, but they are covered by Ranzato et al. (20074 and 20085).",
    "encoders is beyond the scope of this text, but they are covered by Ranzato et al. (20074 and 20085). More recently, the theoretical properties and empirical effectiveness of introducing an intermediate function before the code layer that zeroes out all but k of the maximum activations in the representation were investigated by Makhzani and Frey (2014).6 These k-Sparse autoencoders were shown to be just as effective as other mechanisms 176 | Chapter 8: Embedding and Representation Learning --- Page 193 --- of sparsity despite being shockingly simple to implement and understand (as well as computationally more efficient).",
    "ite being shockingly simple to implement and understand (as well as computationally more efficient). This concludes our discussion of autoencoders. We’ve explored how we can use autoencoders to find strong representations of data points by summarizing their con‐ tent. This mechanism of dimensionality reduction works well when the independent data points are rich and contain all of the relevant information pertaining to their structure in their original representation.",
    "tain all of the relevant information pertaining to their structure in their original representation. In the next section, we’ll explore strategies that we can use when the main source of information is in the context of the data point instead of the data point itself. When Context Is More Informative than the Input Vector So far, we’ve mostly focused on the concept of dimensionality reduction.",
    "ative than the Input Vector So far, we’ve mostly focused on the concept of dimensionality reduction. In dimen‐ sionality reduction, we generally have rich inputs that contain lots of noise on top of the core, structural information that we care about. In these situations, we want to extract this underlying information while ignoring the variations and noise that are extraneous to this fundamental understanding of the data.",
    "ignoring the variations and noise that are extraneous to this fundamental understanding of the data. In other situations, we have input representations that say very little at all about the content that we are trying to capture. In these situations, our goal is not to extract information but rather to gather information from context to build useful representations. All of this probably sounds too abstract to be useful at this point, so let’s concretize these ideas with a real example.",
    "sounds too abstract to be useful at this point, so let’s concretize these ideas with a real example. Building models for language is a tricky business. The first problem we have to over‐ come when building language models is finding a good way to represent individual words. At first glance, it’s not entirely clear how one builds a good representation. Let’s start with the naive approach, considering Figure 8-17 . Figure 8-17.",
    "s a good representation. Let’s start with the naive approach, considering Figure 8-17 . Figure 8-17. Generating one-hot vector representations for words using a simple document When Context Is More Informative than the Input Vector | 177 --- Page 194 --- If a document has a vocabulary V with V words, we can represent the words with one-hot vectors. We have V-dimensional representation vectors, and we associate each unique word with an index in this vector.",
    "-dimensional representation vectors, and we associate each unique word with an index in this vector. To represent unique word wi, we set the itℎ component of the vector to be 1, and zero out all of the other components. However, this representation scheme seems rather arbitrary. This vectorization does not make similar words into similar vectors. This is problematic, because we’ d like our models to know that the words “jump” and “leap” have similar meanings.",
    "matic, because we’ d like our models to know that the words “jump” and “leap” have similar meanings. Simi‐ larly, we’ d like our models to know when words are verbs or nouns or prepositions. The naive one-hot encoding of words to vectors does not capture any of these characteristics. To address this challenge, we’ll need to find some way of discovering these relationships and encoding this information into a vector.",
    "eed to find some way of discovering these relationships and encoding this information into a vector. It turns out that one way to discover relationships between words is by analyzing their surrounding context. For example, synonyms such as “jump” and “leap” can be used interchangeably in their respective contexts. In addition, both words generally appear when a subject is performing the action over a direct object.",
    "addition, both words generally appear when a subject is performing the action over a direct object. We use this principle all the time when we run across new vocabulary while reading. For example, if we read the sentence “The warmonger argued with the crowd, ” we can immediately draw conclusions about the word “warmonger” even if we don’t already know the dictionary definition.",
    "draw conclusions about the word “warmonger” even if we don’t already know the dictionary definition. In this context, “warmonger” precedes a word we know to be a verb, which makes it likely that “warmonger” is a noun and the subject of this sentence. Also, the “warmonger” is “arguing, ” which might imply that a “warmonger” is generally a combative or argumentative individual.",
    "rguing, ” which might imply that a “warmonger” is generally a combative or argumentative individual. Overall, as illustrated in Fig‐ ure 8-18 , by analyzing the context (i.e., a fixed window of words surrounding a target word), we can quickly surmise the meaning of the word. Figure 8-18. Analyzing context to determine a word’s meaning It turns out we can use the same principles we used when building the autoencoder to build a network that builds strong, distributed representations.",
    "ed when building the autoencoder to build a network that builds strong, distributed representations. Two strategies are shown in Figure 8-19 . One possible method (shown in A) passes the target through an encoder network to create an embedding. Then we have a decoder network take this embedding; but instead of trying to reconstruct the original input as we did with 178 | Chapter 8: Embedding and Representation Learning --- Page 195 --- 7Mikolov, Tomas, et al.",
    "with 178 | Chapter 8: Embedding and Representation Learning --- Page 195 --- 7Mikolov, Tomas, et al. “Distributed Representations of Words and Phrases and their Compositionality. ” Advances in Neural Information Processing Systems . 2013. 8Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. “Efficient Estimation of Word Representations in Vector Space. ” ICLR Workshop , 2013.the autoencoder, the decoder attempts to construct a word from the context.",
    ". ” ICLR Workshop , 2013.the autoencoder, the decoder attempts to construct a word from the context. The second possible method (shown in B) does exactly the reverse: the encoder takes a word from the context as input, producing the target. Figure 8-19.",
    "the reverse: the encoder takes a word from the context as input, producing the target. Figure 8-19. General architectures for designing encoders and decoders that generate embeddings by mapping words to their respective contexts (A) or vice versa (B) In the next section, we’ll describe how we use this strategy (along with some slight modifications for performance) to produce word embeddings in practice.",
    "ategy (along with some slight modifications for performance) to produce word embeddings in practice. The Word2Vec Framework Word2Vec, a framework for generating word embeddings, was pioneered by Mikolov et al. The original paper detailed two strategies for generating embeddings, similar to the two strategies for encoding context we discussed in the previous section. The first flavor of Word2Vec that Mikolov et al.",
    "oding context we discussed in the previous section. The first flavor of Word2Vec that Mikolov et al. introduced was the Continuous Bag of Words (CBOW) model.7 This model is much like strategy B from Figure 8-19 . The CBOW model used the encoder to create an embedding from the full context (treated as one input) and predict the target word. It turns out this strategy works best for smaller datasets, an attribute that is further discussed in the original paper.",
    "ategy works best for smaller datasets, an attribute that is further discussed in the original paper. The second flavor of Word2Vec is the Skip-Gram model , introduced by Mikolov et al.8 The Skip-Gram model does the inverse of CBOW , taking the target word as an input, and then attempting to predict one of the words in the context. Let’s walk through a toy example to explore what the dataset for a Skip-Gram model looks like. Consider the sentence “the boy went to the bank.",
    "what the dataset for a Skip-Gram model looks like. Consider the sentence “the boy went to the bank. ” If we broke this sentence down into a sequence of (context, target) pairs, we would obtain [([the, went], boy), ([boy, to], went), ([went, the], to), ([to, bank], the)]. Taking this a step further, we have to split each (context, target) pair into (input, output) pairs where the input is the target and The Word2Vec Framework | 179 --- Page 196 --- the output is one of the words from the context.",
    "t and The Word2Vec Framework | 179 --- Page 196 --- the output is one of the words from the context. From the first pair ([the, went], boy), we would generate the two pairs (boy, the) and (boy, went). We continue to apply this operation to every (context, target) pair to build our dataset. Finally, we replace each word with its unique index i ∈ 0, 1, ...,V− 1 corresponding to its index in the vocabulary. The structure of the encoder is surprisingly simple.",
    "1 corresponding to its index in the vocabulary. The structure of the encoder is surprisingly simple. It is essentially a lookup table with V rows, where the itℎ row is the embedding corresponding to the itℎ vocabulary word. All the encoder has to do is take the index of the input word and output the appropriate row in the lookup table.",
    "er has to do is take the index of the input word and output the appropriate row in the lookup table. This an efficient operation because on a GPU, this operation can be represented as a product of the transpose of the lookup table and the one-hot vector representing the input word.",
    "s a product of the transpose of the lookup table and the one-hot vector representing the input word. We can implement this simply in PyTorch with the following PyTorch function: emb = nn.Embedding(10, 100) x = torch.tensor([0]) out = emb(x) Where out is the embedding matrix, and x is a tensor of indices we want to look up. For information on optional parameters, we refer you to the PyTorch API documentation . The decoder is slightly trickier because we make some modifications for perfor‐ mance.",
    "cumentation . The decoder is slightly trickier because we make some modifications for perfor‐ mance. The naive way to construct the decoder would be to attempt to reconstruct the one-hot encoding vector for the output, which we could implement with a run-of-the-mill feed-forward layer coupled with a softmax. The only concern is that it’s inefficient because we have to produce a probability distribution over the whole vocabulary space. To reduce the number of parameters, Mikolov et al.",
    "ity distribution over the whole vocabulary space. To reduce the number of parameters, Mikolov et al. used a strategy for implementing the decoder known as noise-contrastive estimation (NCE). The strategy is illustrated in Figure 8-20 . A binary logistic regression compares the embedding of the target with the embedding of a context word and randomly sampled noncontext words.",
    "embedding of the target with the embedding of a context word and randomly sampled noncontext words. We construct a loss function describing how effectively the embeddings enable identifica‐ tion of words in the context of the target versus words outside the context of the target. 180 | Chapter 8: Embedding and Representation Learning --- Page 197 --- Figure 8-20.",
    "of the target. 180 | Chapter 8: Embedding and Representation Learning --- Page 197 --- Figure 8-20. The NCE strategy The NCE strategy uses the lookup table to find the embedding for the output, as well as embeddings for random selections from the vocabulary that are not in the context of the input.",
    "ll as embeddings for random selections from the vocabulary that are not in the context of the input. We then employ a binary logistic regression model that, one at a time, takes the input embedding and the embedding of the output or random selection, and then outputs a value between 0 to 1 corresponding to the probability that the comparison embedding represents a vocabulary word present in the input’s context.",
    "obability that the comparison embedding represents a vocabulary word present in the input’s context. We then take the sum of the probabilities corresponding to the noncontext comparisons and subtract the probability corresponding to the context comparison. This value is the objective function that we want to minimize (in the optimal scenario where the model has perfect performance, the value will be –1). An example of implementing NCE in PyTorch can be found on GitHub .",
    "rformance, the value will be –1). An example of implementing NCE in PyTorch can be found on GitHub . While Word2Vec is admittedly not a deep machine learning model, we discuss it here for many reasons. First, it thematically represents a strategy (finding embeddings The Word2Vec Framework | 181 --- Page 198 --- using context) that generalizes to many deep learning models.",
    "d2Vec Framework | 181 --- Page 198 --- using context) that generalizes to many deep learning models. When we learn about models for sequence analysis in Chapter 9 , we’ll see this strategy employed for gener‐ ating skip-thought vectors to embed sentences. Moreover, when we start building more and more models for language starting in Chapter 9 , we’ll find that using Word2Vec embeddings instead of one-hot vectors to represent words will yield far superior results.",
    "g Word2Vec embeddings instead of one-hot vectors to represent words will yield far superior results. Now that we understand how to architect the Skip-Gram model and its importance, we can start implementing it in PyTorch. Implementing the Skip-Gram Architecture To build the dataset for our Skip-Gram model, we’ll utilize a modified version of the PyTorch Word2Vec data reader in input_word_data.py .",
    "model, we’ll utilize a modified version of the PyTorch Word2Vec data reader in input_word_data.py . We’ll start off by setting a couple of important parameters for training and regularly inspecting our model. Of particular note, we employ a minibatch size of 32 examples and train for 5 epochs (full passes through the dataset). We’ll use embeddings of size 128.",
    "examples and train for 5 epochs (full passes through the dataset). We’ll use embeddings of size 128. We’ll use a context window of five words to the left and to the right of each target word, and sample four context words from this window. Finally, we’ll use 64 randomly chosen noncontext words for NCE. Implementing the embedding layer is not particularly complicated.",
    "y chosen noncontext words for NCE. Implementing the embedding layer is not particularly complicated. We merely have to initialize the lookup table with a matrix of values: vocab_size = 500 emb_vector_len = 128 embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = emb_vector_len) PyTorch does not currently have a built-in NCE loss function. However, there are some implementations on the internet.",
    "urrently have a built-in NCE loss function. However, there are some implementations on the internet. One example is the info-nce-pytorch library: pip install info-nce-pytorch We utilize InfoNCE to compute the NCE cost for each training example, and then compile all of the results in the minibatch into a single measurement: loss = InfoNCE() batch_size, embedding_size = 32, 128 query = embedding(outputs) positive_key = embedding(targets) output = loss(query, positive_key) Now that we have our objective function expressed as a mean of the NCE costs, we set up the training as usual.",
    "e have our objective function expressed as a mean of the NCE costs, we set up the training as usual. Here, we follow in the footsteps of Mikolov et al.",
    "f the NCE costs, we set up the training as usual. Here, we follow in the footsteps of Mikolov et al. and employ stochastic gradient descent with a learning rate of 0.1: 182 | Chapter 8: Embedding and Representation Learning --- Page 199 --- optimizer = optim.SGD(embedding.parameters(), lr = 0.1) def train(inputs, targets, embedding): optimizer.zero_grad() input_emb = embedding(inputs) target_emb = embedding(targets) loss = loss_fn(input_emb, target_emb) loss.backward() optimizer.step() return loss We also inspect the model regularly using a validation function, which normalizes the embeddings in the lookup table and uses cosine similarity to compute distances for a set of validation words from all other words in the vocabulary: cosine_similarity = nn.CosineSimilarity() def evaluate(inputs, targets, embedding): with torch.no_grad(): input_emb = embedding(inputs) target_emb = embedding(targets) norm = torch.sum(input_emb, dim=1) normalized = input_emb/norm score = cosine_similarity(normalized, target_emb) return normalized, score Putting all of these components together, we’re finally ready to run the Skip-Gram model.",
    "zed, score Putting all of these components together, we’re finally ready to run the Skip-Gram model. We skim over this portion of the code because it is very similar to how we constructed models in the past. The only difference is the additional code during the inspection step. We randomly select 20 validation words out of the 500 most common words in our vocabulary of 10,000 words.",
    "domly select 20 validation words out of the 500 most common words in our vocabulary of 10,000 words. For each of these words, we use the cosine similarity function we built to find the nearest neighbors: n_epochs=1 for epoch in range(n_epochs): # Train running_loss = 0.0 for inputs, targets in trainloader: loss = train(inputs, targets) running_loss += loss.item() writer.add_scalar('Train Loss', running_loss/len(trainloader), epoch) #Validate running_score = 0.0 for inputs, targets in valloader: _, score = evaluate(inputs, targets) running_score += score writer.add_scalar('Val Score', running_score/len(valloader), epoch) Implementing the Skip-Gram Architecture | 183 --- Page 200 --- The code starts to run, and we can begin to see how the model evolves over time.",
    "83 --- Page 200 --- The code starts to run, and we can begin to see how the model evolves over time. At the beginning, the model does a poor job of embedding (as is apparent from the inspection step).",
    "At the beginning, the model does a poor job of embedding (as is apparent from the inspection step). However, by the time training completes, the model has clearly found representations that effectively capture the meanings of individual words: ancient: egyptian, cultures, mythology, civilization, etruscan, greek, classical, preserved however: but, argued, necessarily, suggest, certainly, nor, believe, believed type: typical, kind, subset, form, combination, single, description, meant white: yellow, black, red, blue, colors, grey, bright, dark system: operating, systems, unix, component, variant, versions, version, essentially energy: kinetic, amount, heat, gravitational, nucleus, radiation, particles, transfer world: ii, tournament, match, greatest, war, ever, championship, cold y: z, x, n, p, f, variable, mathrm, sum, line: lines, ball, straight, circle, facing, edge, goal, yards, among: amongst, prominent, most, while, famous, particularly, argue, many image: png, jpg, width, images, gallery, aloe, gif, angel kingdom: states, turkey, britain, nations, islands, namely, ireland, rest long: short, narrow, thousand, just, extended, span, length, shorter through: into, passing, behind, capture, across, when, apart, goal i: you, t, know, really, me, want, myself, we source: essential, implementation, important, software, content, genetic, alcohol, application because: thus, while, possibility, consequently, furthermore, but, certainly, moral eight: six, seven, five, nine, one, four, three, b 184 | Chapter 8: Embedding and Representation Learning --- Page 201 --- french: spanish, jacques, pierre, dutch, italian, du, english, belgian written: translated, inspired, poetry, alphabet, hebrew, letters, words, read While not perfect, there are some strikingly meaningful clusters captured here.",
    "letters, words, read While not perfect, there are some strikingly meaningful clusters captured here. Num‐ bers, countries, and cultures are clustered close together. The pronoun “I” is clustered with other pronouns. The word “world” is interestingly close to both “championship” and “war. ” And the word “written” is found to be similar to “translated, ” “poetry, ” “alphabet, ” “letters, ” and “words. ” Finally, we conclude this section by visualizing our word embeddings in Figure 8-21 .",
    "and “words. ” Finally, we conclude this section by visualizing our word embeddings in Figure 8-21 . To display our 128-dimensional embeddings in 2D space, we’ll use a visualization method known as t-SNE. If you’ll recall, we also used t-SNE in Chapter 7 to visualize the relationships between images in ImageNet. Using t-SNE is quite simple, as it has a built-in function in the commonly used machine learning library scikit-learn.",
    "te simple, as it has a built-in function in the commonly used machine learning library scikit-learn. We can construct the visualization using the following code: tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000) plot_embeddings = np.asfarray(final_embeddings[:plot_num,:], dtype='float') low_dim_embs = tsne.fit_transform(plot_embeddings) labels = [reverse_dictionary[i] for i in xrange(plot_only)] data.plot_with_labels(low_dim_embs, labels) In Figure 8-21 , we notice that similar concepts are closer together than disparate concepts, indicating that our embeddings encode meaningful information about the functions and definitions of individual words.",
    "ur embeddings encode meaningful information about the functions and definitions of individual words. Implementing the Skip-Gram Architecture | 185 --- Page 202 --- Figure 8-21. Skip-Gram embeddings using t-SNE For a more detailed exploration of the properties of word embeddings and interesting patterns (verb tenses, countries and capitals, analogy completion, etc.), we refer you to the original Mikolov et al. paper.",
    "ountries and capitals, analogy completion, etc.), we refer you to the original Mikolov et al. paper. 186 | Chapter 8: Embedding and Representation Learning --- Page 203 --- Theory: PCA and SVD Those who have taken any form of applied linear algebra are probably familiar with the SVD, one of the most important matrix factorizations in all of linear algebra.",
    "bly familiar with the SVD, one of the most important matrix factorizations in all of linear algebra. For those uninitiated with the SVD, I will first explain the key concepts behind it (assuming some prior linear algebra knowledge) before jumping into its relationship with PCA. The SVD states that any matrix M with dimension mbyn can be factorized into the form UΣV⊺ (where ⊺ represents the trans‐ pose operation) with Uof dimension mbym, Σ of dimension mbyn, andVof dimension nbyn.",
    "ents the trans‐ pose operation) with Uof dimension mbym, Σ of dimension mbyn, andVof dimension nbyn. The matrices UandV are both orthogonal matri‐ ces. Orthogonal matrices are square matrices made up of orthonormal column vec‐ tors. An important fact about orthogonal matrices is that their transposes are also orthogonal matrices, so V⊺ in the decomposition is still orthogonal. In addition, the transpose of an orthogonal matrix is its inverse, so we have U⊺U=UU⊺=Im and V⊺V=VV⊺=In.",
    "ddition, the transpose of an orthogonal matrix is its inverse, so we have U⊺U=UU⊺=Im and V⊺V=VV⊺=In. Σ is a rectangular diagonal matrix with only nonnegative entries along its diagonal, which are termed the singular values of the M. Although the SVD itself is not unique, the singular values of a matrix are.",
    "ingular values of the M. Although the SVD itself is not unique, the singular values of a matrix are. If we inspect the product Σx, where x is any random vector, we note that Σ simply acts as a scaling factor for each dimension of x due to Σ being a diagonal matrix (and when rectangular diagonal, either adds dimensions with value 0 when tall or removes dimensions when wide).",
    "ectangular diagonal, either adds dimensions with value 0 when tall or removes dimensions when wide). Another important and potentially more nonobvious property of orthogonal matrices is that they preserve the length, or L2 norm, of any vector they are multiplied by (this is left as an exercise for you). Orthogonal matrices can change only a vector’s orientation, and thus, we characterize the action of an orthogonal matrix upon a vector as a rotation.",
    "ientation, and thus, we characterize the action of an orthogonal matrix upon a vector as a rotation. We call norms such as the L2 norm rotationally invariant for this reason. One famous example of an orthogonal matrix you’re already familiar with is the identity matrix I—this matrix maps any vector to itself, so we can think of it as a rotation of 0. To understand the SVD more intuitively, let’s imagine the matrix-vector product Mx decomposed as UΣV⊺x.",
    "understand the SVD more intuitively, let’s imagine the matrix-vector product Mx decomposed as UΣV⊺x. Based on our discussion so far, we can see that the action of the matrix M upon x can be decomposed into a rotation, followed by a scaling, followed by another rotation. Now that we have an intuitive understanding of SVD, let’s connect it back to the PCA algorithm presented in the main text.",
    "tuitive understanding of SVD, let’s connect it back to the PCA algorithm presented in the main text. Let’s assume we have a data matrix X, which is of dimension d by n, where d represents the number of features per datapoint and n represents the number of datapoints. Again, we assume for simplicity that the rows of X have been z-score normalized.",
    "mber of datapoints. Again, we assume for simplicity that the rows of X have been z-score normalized. The PCA algorithm can be reduced to taking the eigendecomposition of the correlation matrix 1 nXX⊺, which we will Implementing the Skip-Gram Architecture | 187 --- Page 204 --- represent as PDP⊺. The matrix of eigenvalues D is a diagonal matrix, while the matrix P is a matrix of the corresponding eigenvectors as columns.",
    "D is a diagonal matrix, while the matrix P is a matrix of the corresponding eigenvectors as columns. Generally, the eigendecomposition of a matrix looks like PDP−1, but here the correlation matrix is symmetric so the eigenvectors are orthogonal (we leave this as an exercise for you). Thus, we can represent P as an orthogonal matrix once the eigenvectors are normalized to unit length, at which point the inverse and transpose are equal.",
    "the eigenvectors are normalized to unit length, at which point the inverse and transpose are equal. Instead of working with the correlation matrix, let’s instead work with the data matrix first and then move to the correlation matrix. We first represent X as UΣV⊺.",
    "rk with the data matrix first and then move to the correlation matrix. We first represent X as UΣV⊺. Now, we express the correlation matrix in terms of components of the SVD: 1 nXX⊺=1 nUΣV⊺VΣ⊺U⊺ =1 nUΣ2U⊺ We already see the obvious parallels to the correlation matrix’s eigendecomposition: U is orthogonal, and the square of the singular value matrix is also a diagonal matrix (this matrix is just the diagonal matrix of the squares of all the singular values).",
    "diagonal matrix (this matrix is just the diagonal matrix of the squares of all the singular values). One can show that U’s columns, also termed the left singular vectors, are the eigenvectors of the correlation matrix. Imagine multiplying 1 nUΣ2U⊺ by Uei, where ei is a vector of all zeroes except for a one at the index corresponding to any single column ( Uei is the ith column of U).",
    "roes except for a one at the index corresponding to any single column ( Uei is the ith column of U). In practice, it is actually ideal to use the SVD of the data matrix rather than take the eigendecomposition of the correlation matrix due to precision issues when calculating the correlation matrix, which is a product of two potentially very large matrices. Summary In this chapter, we explored various methods in representation learning.",
    "ery large matrices. Summary In this chapter, we explored various methods in representation learning. We learned about how we can perform effective dimensionality reduction using autoencoders. We also learned about denoising and sparsity, which augment autoencoders with useful properties. After discussing autoencoders, we shifted our attention to repre‐ sentation learning when context of an input is more informative than the input itself.",
    "ion to repre‐ sentation learning when context of an input is more informative than the input itself. We learned how to generate embeddings for English words using the Skip-Gram model, which will prove useful as we explore deep learning models for understanding language. In the next chapter, we will build on this tangent to analyze language and other sequences using deep learning.",
    "chapter, we will build on this tangent to analyze language and other sequences using deep learning. 188 | Chapter 8: Embedding and Representation Learning --- Page 205 --- CHAPTER 9 Models for Sequence Analysis Surya Bhupatiraju Analyzing Variable-Length Inputs Up until now, we’ve worked only with data with fixed sizes: images from MNIST, CIFAR-10, and ImageNet. These models are incredibly powerful, but there are many situations in which fixed-length models are insufficient.",
    "re incredibly powerful, but there are many situations in which fixed-length models are insufficient. The vast majority of inter‐ actions in our daily lives require a deep understanding of sequences—whether it’s reading the morning newspaper, making a bowl of cereal, listening to the radio, watching a presentation, or deciding to execute a trade on the stock market. To adapt to variable-length inputs, we’ll have to be a little bit more clever about how we approach designing deep learning models.",
    "uts, we’ll have to be a little bit more clever about how we approach designing deep learning models. Figure 9-1 illustrates how our feed-forward neural networks break when analyzing sequences. If the sequence is the same size as the input layer, the model can perform as expected. It’s even possible to deal with smaller inputs by padding zeros to the end of the input until it’s the appropriate length.",
    "deal with smaller inputs by padding zeros to the end of the input until it’s the appropriate length. However, the moment the input exceeds the size of the input layer, naively using the feed-forward network no longer works. Feed-forward networks thrive on fixed input size problems. Zero padding can address the handling of smaller inputs, but when naively utilized, these models break when inputs exceed the fixed input size. 189 --- Page 206 --- Figure 9-1.",
    "ilized, these models break when inputs exceed the fixed input size. 189 --- Page 206 --- Figure 9-1. Broken feed-forward network Not all hope is lost, however. In the next couple of sections, we’ll explore several strategies we can leverage to “hack” feed-forward networks to handle sequences. Later in the chapter, we’ll analyze the limitations of these hacks and discuss new architectures to address them.",
    "chapter, we’ll analyze the limitations of these hacks and discuss new architectures to address them. We will conclude the chapter by discussing some of the most advanced architectures explored to date to tackle some of the most difficult challenges in replicating human-level logical reasoning and cognition over sequences.",
    "most difficult challenges in replicating human-level logical reasoning and cognition over sequences. Tackling seq2seq with Neural N-Grams In this section, we’ll begin exploring a feed-forward neural network architecture that can process a body of text and produce a sequence of part-of-speech (POS) tags. In other words, we want to appropriately label each word in the input text as a noun, verb, preposition, and so on. An example of this is shown in Figure 9-2 .",
    "the input text as a noun, verb, preposition, and so on. An example of this is shown in Figure 9-2 . While it’s not the same complexity as building an AI that can answer questions after reading a story, it’s a solid first step toward developing an algorithm that can understand the meaning of how words are used in a sentence.",
    "toward developing an algorithm that can understand the meaning of how words are used in a sentence. This problem is also interesting because it is an instance of a class of problems known as seq2seq , where the goal is to transform an input sequence into a corresponding output sequence. Other famous seq2seq problems include translating text between languages (which we will tackle later in this chapter), text summarization, and transcribing speech to text.",
    "s (which we will tackle later in this chapter), text summarization, and transcribing speech to text. 190 | Chapter 9: Models for Sequence Analysis --- Page 207 --- Figure 9-2. An example of an accurate POS parse of an English sentence As we discussed, it’s not obvious how we might take a body of text all at once to predict the full sequence of POS tags. Instead, we leverage a trick that is akin to the way we developed distributed vector representations of words in the previous chapter.",
    "is akin to the way we developed distributed vector representations of words in the previous chapter. The key observation is this: it is not necessary to take into account long-term dependencies to predict the POS of any given word . The implication of this observation is that instead of using the whole sequence to predict all of the POS tags simultaneously, we can predict each POS tag one at a time by using a fixed-length subsequence.",
    "tags simultaneously, we can predict each POS tag one at a time by using a fixed-length subsequence. In particular, we utilize the subsequence starting from the word of interest and extending n words into the past. This neural n-gram strategy is depicted in Figure 9-3 . Figure 9-3.",
    "extending n words into the past. This neural n-gram strategy is depicted in Figure 9-3 . Figure 9-3. Using a feed-forward network to perform seq2seq when we can ignore long-term dependencies Specifically, when we predict the POS tag for the itℎ word in the input, we use the the i−n+ 1st,i−n+ 2nd, ...,itℎ words as the input. We’ll refer to this subsequence as the context window . In order to process the entire text, we’ll start by positioning the network at the beginning of the text.",
    "der to process the entire text, we’ll start by positioning the network at the beginning of the text. We’ll then proceed to move the network’s context window one word at a time, predicting the POS tag of the rightmost word, until we reach the end of the input. Leveraging the word embedding strategy from last chapter, we’ll also use condensed representations of the words instead of one-hot vectors. This will allow us to reduce the number of parameters in our model and make learning faster.",
    "ectors. This will allow us to reduce the number of parameters in our model and make learning faster. Tackling seq2seq with Neural N-Grams | 191 --- Page 208 --- Implementing a Part-of-Speech Tagger Now that we have a strong understanding of the POS network architecture, we can dive into the implementation. On a high level, the network consists of an input layer that leverages a three-gram context window.",
    "On a high level, the network consists of an input layer that leverages a three-gram context window. We’ll use word embeddings that are 300-dimensional, resulting in a context window of size 900. The feed-forward net‐ work will have two hidden layers of size 512 neurons and 256 neurons, respectively. Then, the output layer will be a softmax calculating the probability distribution of the POS tag output over a space of 44 possible tags.",
    "max calculating the probability distribution of the POS tag output over a space of 44 possible tags. As usual, we’ll use the Adam optimizer with our default hyperparameter settings, train for a total of 1,000 epochs, and leverage batch-normalization for regularization. The actual network is extremely similar to networks we’ve implemented in the past. Rather, the tricky part of building the POS tagger is in preparing the dataset.",
    "emented in the past. Rather, the tricky part of building the POS tagger is in preparing the dataset. We’ll leverage pretrained word embeddings generated from Google News . It includes vec‐ tors for 3 million words and phrases and was trained on roughly 100 billion words. We can use the gensim Python package to read the dataset. Google Colab already has gensim preinstalled. If you are using another machine, you can use pip to install the package.",
    "y has gensim preinstalled. If you are using another machine, you can use pip to install the package. Y ou will also need to download the Google News data file: $ pip install gensim $ wget https://s3.amazonaws.com/dl4j-distribution/ GoogleNews-vectors-negative300.bin.gz -O googlenews.bin.gz We can subsequently load these vectors into memory using the following command: from gensim.models import KeyedVectors model = KeyedVectors.load_word2vec_format('./googlenews.bin.gz', binary=True) The issue with this operation, however, is that it’s incredibly slow (it can take up to an hour, depending on the specs of your machine).",
    "r, is that it’s incredibly slow (it can take up to an hour, depending on the specs of your machine). To avoid loading the full dataset into memory every single time we run our program, especially while debugging code or experimenting with different hyperparameters, we cache the relevant subset of the vectors to disk using a lightweight database known as LevelDB .",
    "we cache the relevant subset of the vectors to disk using a lightweight database known as LevelDB . To build the appropriate Python bindings (which allow us to interact with a LevelDB instance from Python), we simply use the following command: $ pip install leveldb As we mentioned, the gensim model contains three million words, which is larger than our dataset. For the sake of efficiency, we’ll selectively cache word vectors for words in our dataset and discard everything else.",
    "ficiency, we’ll selectively cache word vectors for words in our dataset and discard everything else. To figure out which words we’ d like to cache, let’s download the POS dataset from the CoNLL-2000 task .",
    "igure out which words we’ d like to cache, let’s download the POS dataset from the CoNLL-2000 task . $ wget http://www.cnts.ua.ac.be/conll2000/chunking/train.txt.gz -O - | gunzip | 192 | Chapter 9: Models for Sequence Analysis --- Page 209 --- cut -f1,2 -d\" \" > pos.train.txt $ wget http://www.cnts.ua.ac.be/conll2000/chunking/test.txt.gz -O - | gunzip | cut -f1,2 -d \" \" > pos.test.txt The dataset consists of contiguous text that is formatted as a sequence of rows, where the first element is a word and the second element is the corresponding part of speech.",
    "rows, where the first element is a word and the second element is the corresponding part of speech. Here are the first several lines of the training dataset: Confidence NN in IN the DT pound NN is VBZ widely RB expected VBN to TO take VB another DT sharp JJ dive NN if IN trade NN figures NNS for IN September NNP , , due JJ for IN release NN tomorrow NN ... To match the formatting of the dataset to the gensim model, we’ll have to do some preprocessing.",
    "... To match the formatting of the dataset to the gensim model, we’ll have to do some preprocessing. For example, the model replaces digits with '#' characters, combines separate words into entities where appropriate (e.g., considering “New_Y ork” as a single token instead of two separate words), and utilizes underscores where the raw data uses dashes.",
    "ingle token instead of two separate words), and utilizes underscores where the raw data uses dashes. We preprocess the dataset to conform to this model schema with the following code (analogous code is used to process the training data): def create_pos_dataset(filein, fileout): dataset = [] with open(filein) as f: dataset_raw = f.readlines() dataset_raw = [e.split() for e in dataset_raw if len(e.split()) > 0] counter = 0 while counter < len(dataset_raw): pair = dataset_raw[counter] if counter < len(dataset_raw) - 1: Implementing a Part-of-Speech Tagger | 193 --- Page 210 --- next_pair = dataset_raw[counter + 1] if (pair[0] + \"_\" + next_pair[0] in model) and \\ (pair[1] == next_pair[1]): dataset.append([pair[0] + \"_\" + next_pair[0], pair[1]]) counter += 2 continue word = re.sub(\"\\d\", \"#\", pair[0]) word = re.sub(\"-\", \"_\", word) if word in model: dataset.append([word, pair[1]]) counter += 1 continue if \"_\" in word: subwords = word.split(\"_\") for subword in subwords: if not (subword.isspace() or len(subword) == 0): dataset.append([subword, pair[1]]) counter += 1 continue dataset.append([word, pair[1]]) counter += 1 with open(fileout, 'w') as processed_file: for item in dataset: processed_file.write(\"%s\\n\" % (item[0] + \" \" + item[1])) return dataset train_pos_dataset = create_pos_dataset('./pos.train.txt', './pos.train.processed.txt') test_pos_dataset = create_pos_dataset('./pos.test.txt', './pos.test.processed.txt') Now that we’ve appropriately processed the datasets for use, we can load the words in LevelDB.",
    "txt') Now that we’ve appropriately processed the datasets for use, we can load the words in LevelDB. If the word or phrase is present in the gensim model, we can cache that in the LevelDB instance.",
    "DB. If the word or phrase is present in the gensim model, we can cache that in the LevelDB instance. If not, we randomly select a vector to represent to the token, and cache it so that we remember to use the same vector in case we encounter it again: import leveldb db = leveldb.LevelDB(\"./word2vecdb\") counter = 0 dataset_vocab = {} tags_to_index = {} index_to_tags = {} index = 0 for pair in train_pos_dataset + test_pos_dataset: 194 | Chapter 9: Models for Sequence Analysis --- Page 211 --- if pair[0] not in dataset_vocab: dataset_vocab[pair[0]] = index index += 1 if pair[1] not in tags_to_index: tags_to_index[pair[1]] = counter index_to_tags[counter] = pair[1] counter += 1 nonmodel_cache = {} counter = 1 total = len(dataset_vocab.keys()) for word in dataset_vocab: if word in model: db.Put(bytes(word,'utf-8'), model[word]) elif word in nonmodel_cache: db.Put(bytes(word,'utf-8'), nonmodel_cache[word]) else: #print(word) nonmodel_cache[word] = np.random.uniform(-0.25, 0.25, 300).astype(np.float32) db.Put(bytes(word,'utf-8'), nonmodel_cache[word]) counter += 1 After running the script for the first time, we can just load our data straight from the database if it already exists: db = leveldb.LevelDB(\"./word2vecdb\") x = db.Get(bytes('Confidence','utf-8')) print(np.frombuffer(x,dtype='float32').shape) # out: (300,) Next, we build dataset objects for both training and test datasets, which we can use to generate minibatches for training and testing purposes.",
    "ining and test datasets, which we can use to generate minibatches for training and testing purposes. Building the dataset object requires access to the LevelDB db, the dataset , a dictionary tags_to_index that maps POS tags to indices in the output vector, and a boolean flat get_all that determines whether getting the minibatch should retrieve the full set by default: from torch.utils.data import Dataset from torch.utils.data import DataLoader class NgramPOSDataset(Dataset): def __init__(self, db, dataset, tags_to_index, n_grams): super(NgramPOSDataset, self).__init__() self.db = db self.dataset = dataset self.tags_to_index = tags_to_index self.n_grams = n_grams Implementing a Part-of-Speech Tagger | 195 --- Page 212 --- def __getitem__(self, index): ngram_vector = np.array([]) for ngram_index in range(index, index + self.n_grams): word, _ = self.dataset[ngram_index] vector_bytes = self.db.Get(bytes(word, 'utf-8')) vector = np.frombuffer(vector_bytes, dtype='float32') ngram_vector = np.append(ngram_vector, vector) _, tag = self.dataset[index + int(np.floor(self.n_grams/2))] label = self.tags_to_index[tag] return torch.tensor(ngram_vector, dtype=torch.float32), label def __len__(self): return (len(self.dataset) - self.n_grams + 1) trainset = NgramPOSDataset(db, train_pos_dataset, tags_to_index, 3) trainloader = DataLoader(trainset, batch_size=4, shuffle=True) Finally, we design our feed-forward network similarly to our approaches in previous chapters.",
    "=True) Finally, we design our feed-forward network similarly to our approaches in previous chapters. We omit a discussion of the code and refer to the file Ch09_01_POS_Tag‐ ger.ipynb in the book’s repository . Every epoch, we manually inspect the model by parsing the sentence: “The woman, after grabbing her umbrella, went to the bank to deposit her cash.",
    "parsing the sentence: “The woman, after grabbing her umbrella, went to the bank to deposit her cash. ” Within 100 epochs of training, the algorithm achieves over 96% accuracy and nearly perfectly parses the validation sentence (it makes the understandable mistake of confusing the possessive pronoun and personal pronoun tags for the first appearance of the word “her”). We’ll conclude this by including the visualizations of our model’s performance using TensorBoard in Figure 9-4 .",
    "de this by including the visualizations of our model’s performance using TensorBoard in Figure 9-4 . The POS tagging model was a great exercise, but it was mostly rinsing and repeating concepts we’ve learned in previous chapters. In the rest of the chapter, we’ll start to think about much more complicated sequence-related learning tasks.",
    "st of the chapter, we’ll start to think about much more complicated sequence-related learning tasks. To tackle these more difficult problems, we’ll need to broach brand-new concepts, develop new architectures, and start to explore the cutting edge of modern deep learning research. We’ll start by tackling the problem of dependency parsing next. 196 | Chapter 9: Models for Sequence Analysis --- Page 213 --- Figure 9-4.",
    "dependency parsing next. 196 | Chapter 9: Models for Sequence Analysis --- Page 213 --- Figure 9-4. TensorBoard visualization of our feed-forward POS tagging model Dependency Parsing and SyntaxNet The framework we used to solve the POS tagging task was rather simple. Sometimes we need to be much more creative about how we tackle seq2seq problems, especially as the complexity of the problem increases.",
    "reative about how we tackle seq2seq problems, especially as the complexity of the problem increases. In this section, we’ll explore strategies that employ creative data structures to tackle difficult seq2seq problems. As an illus‐ trative example, we’ll explore the problem of dependency parsing. Dependency Parsing and SyntaxNet | 197 --- Page 214 --- The idea behind building a dependency parse tree is to map the relationships between words in a sentence.",
    "dea behind building a dependency parse tree is to map the relationships between words in a sentence. Take, for example, the dependency in Figure 9-5 . The words “I” and “taxi” are children of the word “took, ” specifically as the subject and direct object of the verb, respectively. Figure 9-5. An example of a dependency parse, which generates a tree of relationships between words in a sentence One way to express a tree as a sequence is by linearizing it.",
    "ationships between words in a sentence One way to express a tree as a sequence is by linearizing it. Let’s consider the examples in Figure 9-6 . Essentially, if you have a graph with a root R, and children A (connected by edge r_a), B (connected by edge r_b), and C (connected by edge r_c), we can linearize the representation as (R, r_a, A, r_b, B, r_c, C) . We can even represent more complex graphs.",
    "arize the representation as (R, r_a, A, r_b, B, r_c, C) . We can even represent more complex graphs. Let’s assume, for example, that node B actually has two more children named D (connected by edge b_d) and E (connected by edge b_e). We can represent this new graph as (R, r_a, A, r_b, [B, b_d, D, b_e, E], r_c, C). Figure 9-6.",
    "b_e). We can represent this new graph as (R, r_a, A, r_b, [B, b_d, D, b_e, E], r_c, C). Figure 9-6. We linearize two example trees: the diagrams omit edge labels for the sake of visual clarity Using this paradigm, we can take our example dependency parse and linearize it, as shown in Figure 9-7 . 198 | Chapter 9: Models for Sequence Analysis --- Page 215 --- 1Nivre, Joakim. “Incrementality in Deterministic Dependency Parsing.",
    "uence Analysis --- Page 215 --- 1Nivre, Joakim. “Incrementality in Deterministic Dependency Parsing. ” Proceedings of the Workshop on Incre‐ mental Parsing: Bringing Engineering and Cognition Together . Association for Computational Linguistics, 2004; Chen, Danqi, and Christopher D. Manning. “ A Fast and Accurate Dependency Parser Using Neural Networks. ” EMNLP . 2014. Figure 9-7.",
    "Manning. “ A Fast and Accurate Dependency Parser Using Neural Networks. ” EMNLP . 2014. Figure 9-7. Linearization of the dependency parse tree example One interpretation of this seq2seq problem would be to read the input sentence and produce a sequence of tokens as an output that represents the linearization of the input’s dependency parse.",
    "a sequence of tokens as an output that represents the linearization of the input’s dependency parse. It’s not particularly clear, however, how we might port our strategy from the previous section, where there was a clear one-to-one mapping between words and their POS tags. Moreover, we could easily make decisions about a POS tag by looking at the nearby context.",
    "POS tags. Moreover, we could easily make decisions about a POS tag by looking at the nearby context. For dependency parsing, there’s no clear relationship between how words are ordered in the sentence and how tokens in the linearization are ordered. It also seems like dependency parsing tasks us with identifying edges that may span a significantly large number of words.",
    "endency parsing tasks us with identifying edges that may span a significantly large number of words. Therefore, at first glance, it seems like this setup directly violates our assumption that we need not take into account any long-term dependencies. To make the problem more approachable, we instead reconsider the dependency parsing task as finding a sequence of valid “actions” that generates the correct dependency parse.",
    "y parsing task as finding a sequence of valid “actions” that generates the correct dependency parse. This technique, known as the arc-standard system, was first described by Nivre in 2004 and later leveraged in a neural context by Chen and Manning in 2014.1 In the arc-standard system, we start by putting the first two words of the sentence in the stack and maintaining the remaining words in the buffer, as shown in Figure 9-8 . Dependency Parsing and SyntaxNet | 199 --- Page 216 --- Figure 9-8.",
    "buffer, as shown in Figure 9-8 . Dependency Parsing and SyntaxNet | 199 --- Page 216 --- Figure 9-8. Three options in the arc-standard system: shift a word from the buffer to the stack, draw an arc from the right element to the left element (left arc), or draw an arc from the left element to the right element (right arc) At any step, we can take one of three possible classes of actions: Shift Move a word from the buffer to the front of the stack.",
    "e of three possible classes of actions: Shift Move a word from the buffer to the front of the stack. Left arc Combine the two elements at the front of the stack into a single unit where the root of the rightmost element is the parent node and the root of leftmost element is the child node. Right arc Combine the two elements at the front of the stack into a single unit where the root of the left element is the parent node, and the root of right element is the child node.",
    "re the root of the left element is the parent node, and the root of right element is the child node. We note that while there is only one way to perform a shift, the arc actions can be of many flavors, each differentiated by the dependency label assigned to the arc that is generated. That being said, we’ll simplify our discussions and illustrations in this section by considering each decision as a choice among three actions (rather than tens of actions).",
    "section by considering each decision as a choice among three actions (rather than tens of actions). 200 | Chapter 9: Models for Sequence Analysis --- Page 217 --- We terminate this process when the buffer is empty and the stack has one element in it (which represents the full dependency parse). To illustrate this process in its entirety, we illustrate a sequence of actions that generates the dependency parse for our example input sentence in Figure 9-9 . Figure 9-9.",
    "tions that generates the dependency parse for our example input sentence in Figure 9-9 . Figure 9-9. A sequence of actions that results in the correct dependency parse; we omit labels It’s not too difficult to reformulate this decision-making framework as a learning problem.",
    "t labels It’s not too difficult to reformulate this decision-making framework as a learning problem. At every step, we take the current configuration, and we vectorize the configuration by extracting a large number of features that describe the configuration (words in specific locations of the stack/buffer, specific children of the words in these locations, part of speech tags, etc.).",
    "of the stack/buffer, specific children of the words in these locations, part of speech tags, etc.). During train time, we can feed this vector into a feed-forward network and compare its prediction of the next action to take to a gold-standard decision made by a human linguist.",
    "pare its prediction of the next action to take to a gold-standard decision made by a human linguist. To use this model in the wild, we can take the action that the network recommends, apply it to the configuration, and use this new configuration as the starting point for the next step (feature extraction, action prediction, and action application). This process is shown in Figure 9-10 . Dependency Parsing and SyntaxNet | 201 --- Page 218 --- Figure 9-10.",
    "ocess is shown in Figure 9-10 . Dependency Parsing and SyntaxNet | 201 --- Page 218 --- Figure 9-10. A neural framework for arc-standard dependency parsing Taken together, these ideas form the core for Google’s SyntaxNet, the state-of-the-art open source implementation for dependency parsing.",
    "core for Google’s SyntaxNet, the state-of-the-art open source implementation for dependency parsing. Delving into the nitty-gritty aspects of implementation is beyond the scope of this text, but we refer you to the open source repository , which contains an implementation of Parsey McParseface, the most accurate publicly reported English language parser as of the publication of this text.",
    "ace, the most accurate publicly reported English language parser as of the publication of this text. 202 | Chapter 9: Models for Sequence Analysis --- Page 219 --- Beam Search and Global Normalization In the previous section, we described a naive strategy for deploying SyntaxNet in practice. The strategy was purely greedy ; that is, we selected prediction with the highest probability without being concerned that we might potentially paint ourselves into a corner by making an early mistake.",
    "being concerned that we might potentially paint ourselves into a corner by making an early mistake. In the POS example, making an incorrect prediction was largely inconsequential. This is because each prediction could be considered a purely independent subproblem (the results of a given prediction do not affect the inputs of the next step). This assumption no longer holds in SyntaxNet, because our prediction at step n affects the input we use at step n+ 1.",
    "o longer holds in SyntaxNet, because our prediction at step n affects the input we use at step n+ 1. This implies that any mistake we make will influence all later decisions. Moreover, there’s no good way of “going backward” and fixing mistakes when they become apparent. Garden path sentences are an extreme case of where this is important. Consider the following sentence: “The complex houses married and single soldiers and their families. ” The first glance pass-through is confusing.",
    "houses married and single soldiers and their families. ” The first glance pass-through is confusing. Most people interpret “complex” as an adjective “houses” as a noun, and “married” as a past tense verb. This makes little semantic sense though, and starts to break down as the rest of the sentence is read. Instead, we realize that “complex” is a noun (as in a military complex) and that “houses” is a verb.",
    "Instead, we realize that “complex” is a noun (as in a military complex) and that “houses” is a verb. In other words, the sentence implies that the military complex contains soldiers (who may be single or married) and their families. A greedy version of SyntaxNet would fail to correct the early parse mistake of considering “complex” as an adjective describing the “houses, ” and therefore would fail on the full version of the sentence.",
    "an adjective describing the “houses, ” and therefore would fail on the full version of the sentence. To remedy this shortcoming, we use a strategy known as beam search , illustrated in Figure 9-11 . We generally leverage beam searches in situations like SyntaxNet, where the output of our network at a particular step influences the inputs used in future steps.",
    "et, where the output of our network at a particular step influences the inputs used in future steps. The basic idea behind beam search is that instead of greedily selecting the most probable prediction at each step, we maintain a beam of the most likely hypothesis (up to a fixed beam size b ) for the sequence of the first k actions and their associated probabilities. Beam searching can be broken up into two major phases: expansion and pruning.",
    "ociated probabilities. Beam searching can be broken up into two major phases: expansion and pruning. Beam Search and Global Normalization | 203 --- Page 220 --- Figure 9-11. Using beam search (with beam size 2) while deploying a trained SyntaxNet model During the expansion step, we take each hypothesis and consider it as a possible input to SyntaxNet. Assume SyntaxNet produces a probability distribution over a space of A total actions.",
    "to SyntaxNet. Assume SyntaxNet produces a probability distribution over a space of A total actions. We then compute the probability of each of the bA possible hypotheses for the sequence of the first k+ 1 actions. Then, during the pruning step, we keep only the b hypothesis out of the bA total options with the largest probabili‐ ties.",
    "ng step, we keep only the b hypothesis out of the bA total options with the largest probabili‐ ties. As Figure 9-11 illustrates, beam searching enables SyntaxNet to correct incorrect predictions post facto by entertaining less probable hypotheses early that might turn out to be more fruitful later in the sentence. In fact, digging deeper into the illustrated 204 | Chapter 9: Models for Sequence Analysis --- Page 221 --- 2Andor, Daniel, et al. “Globally Normalized Transition-Based Neural Networks.",
    "lysis --- Page 221 --- 2Andor, Daniel, et al. “Globally Normalized Transition-Based Neural Networks. ” arXiv preprint arXiv :1603.06042 (2016). 3Ibid.example, a greedy approach would have suggested that the correct sequence of moves would have been a shift followed by a left arc. In reality, the best (highest probability) option would have been to use a left arc followed by a right arc. Beam searching with beam size 2 surfaces this result.",
    "een to use a left arc followed by a right arc. Beam searching with beam size 2 surfaces this result. The full open source version takes this a full step further and attempts to bring the concept of beam searching to the process of training the network. As Andor et al. described in 2016,2 this process of global normalization provides both strong theoretical guarantees and clear performance gains relative to local normalization in practice.",
    "rong theoretical guarantees and clear performance gains relative to local normalization in practice. In a locally normalized network, our network is tasked with selecting the best action given a configuration. The network outputs a score that is normalized using a softmax layer. This is meant to model a probability distribution over all possible actions, provided the actions performed thus far.",
    "model a probability distribution over all possible actions, provided the actions performed thus far. Our loss function attempts to force the probability distribution to the ideal output (i.e., probability 1 for the correct action and 0 for all other actions). The cross-entropy loss does a spectacular job of ensuring this for us. In a globally normalized network, our interpretation of the scores is slightly different.",
    "is for us. In a globally normalized network, our interpretation of the scores is slightly different. Instead of putting the scores through a softmax to generate a per-action probability distribution, we instead add up all the scores for a hypothesis action sequence. One way of ensuring that we select the correct hypothesis sequence is by computing this sum over all possible hypotheses and then applying a softmax layer to generate a probability distribution.",
    "er all possible hypotheses and then applying a softmax layer to generate a probability distribution. We could theoretically use the same cross-entropy loss func‐ tion as we used in the locally normalized network. The problem with this strategy, however, is that there is an intractably large number of possible hypothesis sequences.",
    "is strategy, however, is that there is an intractably large number of possible hypothesis sequences. Even considering an average sentence length of 10 and a conservative total number of 15 possible actions—1 shift and 7 labels for each of the left and right arcs—this corresponds to 1,000,000,000,000,000 possible hypotheses.",
    "s for each of the left and right arcs—this corresponds to 1,000,000,000,000,000 possible hypotheses. To make this problem tractable, as shown in Figure 9-12 , we apply a beam search, with a fixed beam size, until we either (1) reach the end of the sentence, or (2) the correct sequence of actions is no longer contained on the beam.",
    "the end of the sentence, or (2) the correct sequence of actions is no longer contained on the beam. We then construct a loss function that tries to push the “gold standard” action sequence (highlighted in blue) as high as possible on the beam by maximizing its score relative to the other hypotheses. While we won’t dive into the details of how we might construct this loss function here, we refer you to the original paper by Andor et al.",
    "f how we might construct this loss function here, we refer you to the original paper by Andor et al. in 2016.3 The paper also describes a more sophisticated POS tagger that uses global normalization and beam search to significantly increase accuracy (compared to the POS tagger we built earlier in the chapter). Beam Search and Global Normalization | 205 --- Page 222 --- Figure 9-12.",
    "lt earlier in the chapter). Beam Search and Global Normalization | 205 --- Page 222 --- Figure 9-12. Coupling training and beam search can make global normalization in SyntaxNet tractable A Case for Stateful Deep Learning Models While we’ve explored several tricks to adapt feed-forward networks to sequence analysis, we’ve yet to truly find an elegant solution to sequence analysis. In the POS tagger example, we made the explicit assumption that we can ignore long-term dependencies.",
    "n the POS tagger example, we made the explicit assumption that we can ignore long-term dependencies. We were able to overcome some of the limitations of this assumption by introducing the concepts of beam searching and global normalization, but even still, the problem space was constrained to situations in which there was a one-to-one mapping between elements in the input sequence to elements in the output sequence.",
    "was a one-to-one mapping between elements in the input sequence to elements in the output sequence. For example, even in the dependency parsing model, we had to reformulate the prob‐ lem to discover a one-to-one mapping between a sequence of input configurations while constructing the parse tree and arc-standard actions. Sometimes, however, the task is far more complicated than finding a one-to-one mapping between input and output sequences.",
    "e task is far more complicated than finding a one-to-one mapping between input and output sequences. For example, we might want to develop a model that can consume an entire input sequence at once and then conclude if the sentiment of the entire input was positive or negative. We’ll build a simple model to perform this task later in the chapter. We may want an algorithm that consumes a complex input (such as an image) and generate a sentence, one word at a time, describing the input.",
    "complex input (such as an image) and generate a sentence, one word at a time, describing the input. We may event want to translate sentences from one language to another (e.g., from English to French). In all of these instances, there’s no obvious mapping between input tokens and output tokens. Instead, the process is more like the situation in Figure 9-13 . 206 | Chapter 9: Models for Sequence Analysis --- Page 223 --- Figure 9-13.",
    "tuation in Figure 9-13 . 206 | Chapter 9: Models for Sequence Analysis --- Page 223 --- Figure 9-13. The ideal model for sequence analysis can store information in memory over long periods of time, leading to a coherent “thought” vector that it can use to generate an answer The idea is simple. We want our model to maintain some sort of memory over the span of reading the input sequence.",
    "mple. We want our model to maintain some sort of memory over the span of reading the input sequence. As it reads the input, the model should be able to modify this memory bank, taking into account the information that it observes. By the time it has reached the end of the input sequence, the internal memory contains a “thought” that represents the key pieces of information, that is, the meaning, of the original input.",
    "thought” that represents the key pieces of information, that is, the meaning, of the original input. We should then, as shown in Figure 9-13 , be able to use this thought vector to either produce a label for the original sequence or produce an appropriate output sequence (translation, description, abstractive summary, etc.). The concept here isn’t something we’ve explored in any of the previous chapters. Feed-forward networks are inherently “stateless.",
    "ing we’ve explored in any of the previous chapters. Feed-forward networks are inherently “stateless. ” After it’s been trained, the feed- forward network is a static structure. It isn’t able to maintain memories between inputs, or change how it processes an input based on inputs it has seen in the past. To execute this strategy, we’ll need to reconsider how we construct neural networks to create deep learning models that are “stateful.",
    "ed to reconsider how we construct neural networks to create deep learning models that are “stateful. ” To do this, we’ll have to return to how we think about networks on an individual neuron level. In the next section, we’ll explore how recurrent connections (as opposed to the feed-forward connections we have studied this far) enable models to maintain state as we describe a class of models known as recurrent neural networks (RNNs).",
    "models to maintain state as we describe a class of models known as recurrent neural networks (RNNs). Recurrent Neural Networks RNNs were first introduced in the 1980s, but have regained popularity recently due to several intellectual and hardware breakthroughs that have made them tractable to train. RNNs are different from feed-forward networks because they leverage a special type of neural layer, known as recurrent layers, that enable the network to maintain state between uses of the network.",
    "r, known as recurrent layers, that enable the network to maintain state between uses of the network. Recurrent Neural Networks | 207 --- Page 224 --- Figure 9-14 illustrates the neural architecture of a recurrent layer. All of the neurons have both (1) incoming connections emanating from all of the neurons of the previous layer and (2) outgoing connections leading to all of the neurons to the subsequent layer.",
    "e previous layer and (2) outgoing connections leading to all of the neurons to the subsequent layer. We notice here, however, that these aren’t the only connections that neurons of a recurrent layer have. Unlike a feed-forward layer, recurrent layers also have recurrent connections, which propagate information between neurons of the same layer. A fully connected recurrent layer has information flow from every neuron to every other neuron in its layer (including itself).",
    "layer has information flow from every neuron to every other neuron in its layer (including itself). Thus a recurrent layer with r neurons has a total of r2 recurrent connections. Figure 9-14. A recurrent layer contains recurrent connections, that is to say, connections between neurons that are located in the same layer To better understand how RNNs work, let’s explore how one functions after it’s been appropriately trained.",
    "ter understand how RNNs work, let’s explore how one functions after it’s been appropriately trained. Every time we want to process a new sequence, we create a fresh instance of our model. We can reason about networks that contain recurrent layers by dividing the lifetime of the network instance into discrete time steps. At each time step, we feed the model the next element of the input.",
    "stance into discrete time steps. At each time step, we feed the model the next element of the input. Feed-forward connections represent information flow from one neuron to another where the data being transferred is the computed neuronal activation from the current time step. Recurrent connections, however, represent information flow where the data is the stored neuronal activation from the previous time step.",
    "esent information flow where the data is the stored neuronal activation from the previous time step. Thus, the activations of the neurons in a recurrent network represent the accumulating state of the network instance. The initial activations of neurons in the recurrent layer are parameters of our model, and we determine the optimal values for them just like we determine the optimal values for the weights of each connection during the process of training.",
    "e we determine the optimal values for the weights of each connection during the process of training. 208 | Chapter 9: Models for Sequence Analysis --- Page 225 --- It turns out that, given a fixed lifetime (say t time steps) of an RNN instance, we can actually express the instance as a feed-forward network (albeit irregularly structured). This clever transformation, illustrated in Figure 9-15 , is often referred to as “unrolling” the RNN through time. Let’s consider the example RNN in the figure.",
    "often referred to as “unrolling” the RNN through time. Let’s consider the example RNN in the figure. We’ d like to map a sequence of two inputs (each dimension 1) to a single output (also of dimension 1). We perform the transformation by taking the neurons of the single recurrent layer and replicating them it t times, once for each time step. We similarly replicate the neurons of the input and output layers.",
    "t times, once for each time step. We similarly replicate the neurons of the input and output layers. We redraw the feed-forward connections within each time replica just as they were in the original network. Then we draw the recurrent connections as feed-forward connections from each time replica to the next (since the recurrent connections carry the neuronal activation from the previous time step). Figure 9-15.",
    "e the recurrent connections carry the neuronal activation from the previous time step). Figure 9-15. We can run an RNN through time to express it as a feed-forward network that we can train using backpropagation We can also now train the RNN by computing the gradient based on the unrolled version. This means that all of the backpropagation techniques that we used for feed-forward networks also apply to training RNNs. We do run into one issue, however.",
    "at we used for feed-forward networks also apply to training RNNs. We do run into one issue, however. After every batch of training examples we use, we need to modify the weights based on the error derivatives we calculate. In our unrolled network, we have sets of connections that all correspond to the same connection in the original RNN. The error derivatives calculated for these unrolled connections, however, are not guaranteed to be (and, in practice, probably won’t be) equal.",
    "unrolled connections, however, are not guaranteed to be (and, in practice, probably won’t be) equal. We can circumvent this issue by averaging or summing the error derivatives over all the connections that belong to the same set. This allows us to utilize an error derivative that considers all of the dynamics acting on the weight of a connection as we attempt to force the network to construct an accurate output. Recurrent Neural Networks | 209 --- Page 226 --- 4Kilian, Joe, and Hava T.",
    "truct an accurate output. Recurrent Neural Networks | 209 --- Page 226 --- 4Kilian, Joe, and Hava T. Siegelmann. “The dynamic universality of sigmoidal neural networks. ” Information and computation 128.1 (1996): 48-56.The Challenges with Vanishing Gradients Our motivation for using a stateful network model hinges on this idea of capturing long-term dependencies in the input sequence.",
    "tateful network model hinges on this idea of capturing long-term dependencies in the input sequence. It seems reasonable that an RNN with a large memory bank (i.e., a significantly sized recurrent layer) would be able to summarize these dependencies.",
    "ry bank (i.e., a significantly sized recurrent layer) would be able to summarize these dependencies. In fact, from a theoretical perspective, Kilian and Siegelmann demonstrated in 1996 that the RNN is a universal functional represen‐ tation.4 In other words, with enough neurons and the right parameter settings, an RNN can be used to represent any functional mapping between input and output sequences. The theory is promising, but it doesn’t necessarily translate to practice.",
    "put and output sequences. The theory is promising, but it doesn’t necessarily translate to practice. While it is nice to know that it is possible for an RNN to represent any arbitrary function, it is more useful to know whether it is practical to teach the RNN a realistic functional mapping from scratch by applying gradient descent algorithms. If it turns out to be impractical, we’ll be in hot water, so it will be useful for us to be rigorous in exploring this question.",
    "tical, we’ll be in hot water, so it will be useful for us to be rigorous in exploring this question. Let’s start our investigation by considering the simplest possible RNN, shown in Figure 9-16 , with a single input neuron, a single output neuron, and a fully connected recurrent layer with one neuron. Figure 9-16.",
    "neuron, a single output neuron, and a fully connected recurrent layer with one neuron. Figure 9-16. A single neuron, fully connected recurrent layer (both compressed and unrolled) for the sake of investigating gradient-based learning algorithms 210 | Chapter 9: Models for Sequence Analysis --- Page 227 --- Let’s start off simple.",
    "ng algorithms 210 | Chapter 9: Models for Sequence Analysis --- Page 227 --- Let’s start off simple. Given nonlinearity f, we can express the activation ℎt of the hidden neuron of the recurrent layer at time step t as follows, where it is the incoming logit from the input neuron at time step t: ℎt=fwintit+wrect− 1ℎt− 1 Let’s try to compute how the activation of the hidden neuron changes in response to changes to the input logit from k time steps in the past.",
    "f the hidden neuron changes in response to changes to the input logit from k time steps in the past. In analyzing this component of the backpropagation gradient expressions, we can start to quantify how much “memory” is retained from past inputs.",
    "ation gradient expressions, we can start to quantify how much “memory” is retained from past inputs. We start by taking the partial derivative and apply the chain rule: ∂ℎt ∂it−k=f′wintit+wrect− 1ℎt− 1 ∂ ∂it−kwintit+wrect− 1ℎt− 1 Because the values of the input and recurrent weights are independent of the input logit at time step t−k, we can further simplify this expression: ∂ℎt ∂it−k=f′wintit+wrect− 1ℎt− 1wrect− 1 ∂ℎt− 1 ∂it−k Because we care about the magnitude of this derivative, we can take the absolute value of both sides.",
    "ecause we care about the magnitude of this derivative, we can take the absolute value of both sides. We also know that for all common nonlinearities (the tanh, logistic, and ReLU nonlinearities), the maximum value of f′ is at most 1. This leads to the following recursive inequality: ∂ℎt ∂it−k≤wrect− 1·∂ℎt− 1 ∂it−k We can continue to expand this inequality recursively until we reach the base case, at step t−k: ∂ℎt ∂it−k≤wrect− 1· ...",
    "xpand this inequality recursively until we reach the base case, at step t−k: ∂ℎt ∂it−k≤wrect− 1· ... ·wrect−k·∂ℎt−k ∂it−k We can evaluate this partial derivative similarly to how we proceeded previously: ℎt−k=fwint−kit−k+wrect−k− 1ℎt−k− 1 The Challenges with Vanishing Gradients | 211 --- Page 228 --- ∂ℎt−k ∂it−k=f′wint−kit−k+wrect−k− 1ℎt−k− 1 ∂ ∂it−kwint−kit−k +wrect−k− 1ℎt−k− 1 In this expression, the hidden activation at time t−k− 1 is independent of the value of the input at t−k.",
    "is expression, the hidden activation at time t−k− 1 is independent of the value of the input at t−k. Thus we can rewrite this expression as: ∂ℎt−k ∂it−k=f′wint−kit−k+wrect−k− 1ℎt−k− 1wint−k Finally, taking the absolute value on both sides and again applying the observation about the maximum value of f′, we can write: ∂ℎt−k ∂it−k≤wint−k This results in the final inequality (which we can simplify because we constrain the connections at different time steps to have equal value): ∂ℎt ∂it−k≤wrect− 1· ...",
    "e we constrain the connections at different time steps to have equal value): ∂ℎt ∂it−k≤wrect− 1· ... ·wrect−k·wint−k=wreck·win This relationship places a strong upper bound on how much a change in the input at time t−k can impact the hidden state at time t. Because the weights of our model are initialized to small values at the beginning of training, the value of this derivative approaches zero as k increases.",
    "ll values at the beginning of training, the value of this derivative approaches zero as k increases. In other words, the gradient quickly diminishes when it’s computed with respect to inputs several time steps into the past, severely limiting our model’s ability to learn long-term dependencies. This issue is commonly referred to as the problem of vanishing gradients , and it severely impacts the learning capabilities of vanilla RNNs.",
    "problem of vanishing gradients , and it severely impacts the learning capabilities of vanilla RNNs. In order to address this limitation, we will spend the next section exploring an extraordinarily influential twist on recurrent layers known as long short-term memory.",
    "exploring an extraordinarily influential twist on recurrent layers known as long short-term memory. 212 | Chapter 9: Models for Sequence Analysis --- Page 229 --- Long Short-Term Memory Units To combat the problem of vanishing gradients, Sepp Hochreiter and Jürgen Schmid‐ huber introduced the long short-term memory (LSTM) architecture.",
    "Sepp Hochreiter and Jürgen Schmid‐ huber introduced the long short-term memory (LSTM) architecture. The basic princi‐ ple behind the architecture was that the network would be designed for the purpose of reliably transmitting important information many time steps into the future. The design considerations resulted in the architecture shown in Figure 9-17 . Figure 9-17.",
    "e future. The design considerations resulted in the architecture shown in Figure 9-17 . Figure 9-17. The architecture of an LSTM unit, illustrated at a tensor (designated by arrows) and operation (designated by the inner blocks) level For the purposes of this discussion, we’ll take a step back from the individual neuron level and start talking about the network as collection tensors and operations on ten‐ sors. As the figure indicates, the LSTM unit is composed of several key components.",
    "erations on ten‐ sors. As the figure indicates, the LSTM unit is composed of several key components. One of the core components of the LSTM architecture is the memory cell , a tensor represented by the bolded loop in the center of the figure. The memory cell holds critical information that it has learned over time, and the network is designed to effectively maintain useful information in the memory cell over many time steps.",
    "work is designed to effectively maintain useful information in the memory cell over many time steps. At every time step, the LSTM unit modifies the memory cell with new information with three different phases. First, the unit must determine how much of the previous memory to keep. This is determined by the keep gate , shown in detail in Figure 9-18 . Long Short-Term Memory Units | 213 --- Page 230 --- Figure 9-18.",
    ", shown in detail in Figure 9-18 . Long Short-Term Memory Units | 213 --- Page 230 --- Figure 9-18. Architecture of the keep gate of an LSTM unit The basic idea of the keep gate is simple. The memory state tensor from the previous time step is rich with information, but some of that information may be stale (and therefore might need to be erased).",
    "with information, but some of that information may be stale (and therefore might need to be erased). We figure out which elements in the memory state tensor are still relevant and which elements are irrelevant by trying to compute a bit tensor (a tensor of zeros and ones) that we multiply with the previous state. If a particular location in the bit tensor holds a 1, it means that location in the memory cell is still relevant and ought to be kept.",
    "tensor holds a 1, it means that location in the memory cell is still relevant and ought to be kept. If that particular location instead holds a 0, it means that the location in the memory cell is no longer relevant and ought to be eased. We approximate this bit tensor by concatenating the input of this time step and the LSTM unit’s output from the previous time step and applying a sigmoid layer to the resulting tensor.",
    "LSTM unit’s output from the previous time step and applying a sigmoid layer to the resulting tensor. A sigmoidal neuron, as you may recall, outputs a value that is either very close to 0 or very close to 1 most of the time (the only exception is when the input is close to 0). As a result, the output of the sigmoidal layer is a close approximation of a bit tensor, and we can use this to complete the keep gate.",
    "oidal layer is a close approximation of a bit tensor, and we can use this to complete the keep gate. Once we’ve figured out what information to keep in the old state and what to erase, we’re ready to think about what information we’ d like to write into the memory state. This part of the LSTM unit is known as the write gate , and it’s depicted in Figure 9-19 . This is broken down into two major parts. The first component is figuring out what information we’ d like to write into the state.",
    "ajor parts. The first component is figuring out what information we’ d like to write into the state. This is computed by the tanh layer to create an intermediate tensor. The second component is figuring out which components of this computed tensor we actually want to include into the new state and which we want to toss before writing. We do this by approximating a bit vector of 0’s and 1’s using the same strategy (a sigmoidal layer) as we used in the keep gate.",
    "a bit vector of 0’s and 1’s using the same strategy (a sigmoidal layer) as we used in the keep gate. We multiply the bit vector with our intermediate tensor and then add the result to create the new state vector for the LSTM. 214 | Chapter 9: Models for Sequence Analysis --- Page 231 --- Figure 9-19. Architecture of the write gate of an LSTM unit At every time step, we’ d like the LSTM unit to provide an output.",
    "of the write gate of an LSTM unit At every time step, we’ d like the LSTM unit to provide an output. While we could treat the state vector as the output directly, the LSTM unit is engineered to provide more flexibility by emitting an output tensor that is an “interpretation” or external “communication” of what the state vector represents. The architecture of the output gate is shown in Figure 9-20 .",
    "” of what the state vector represents. The architecture of the output gate is shown in Figure 9-20 . We use a nearly identical structure as the write gate: (1) the tanh layer creates an intermediate tensor from the state vector, (2) the sigmoid layer produces a bit tensor mask using the current input and previous output, and (3) the intermediate tensor is multiplied with the bit tensor to produce the final output. Figure 9-20.",
    "the intermediate tensor is multiplied with the bit tensor to produce the final output. Figure 9-20. Architecture of the output gate of an LSTM unit So why is this better than using a raw RNN unit? The key observation is how information propagates through the network when we unroll the LSTM unit through time. The unrolled architecture is shown in Figure 9-21 . At the top, we can observed the propagation of the state vector, whose interactions are primarily linear through time.",
    "observed the propagation of the state vector, whose interactions are primarily linear through time. The result is that the gradient that relates an input several time steps in the past to the current output does not attenuate as dramatically as in the vanilla RNN Long Short-Term Memory Units | 215 --- Page 232 --- architecture. This means that the LSTM can learn long-term relationships much more effectively than our original formulation of the RNN. Figure 9-21.",
    "long-term relationships much more effectively than our original formulation of the RNN. Figure 9-21. Unrolling an LSTM unit through time 216 | Chapter 9: Models for Sequence Analysis --- Page 233 --- Finally, we want to understand how easy it is to generate arbitrary architectures with LSTM units. How “composable” are LSTMs? Do we need to sacrifice flexibility to use LSTM units instead of a vanilla RNN?",
    "mposable” are LSTMs? Do we need to sacrifice flexibility to use LSTM units instead of a vanilla RNN? Just as we can we can stack RNN layers to create more expressive models with more capacity, we can stack LSTM units, where the input of the second unit is the output of the first unit, the input of the third unit is the output of the second, and so on. Figure 9-22 shows how this works with a multicellular architecture made of two LSTM units.",
    "nd so on. Figure 9-22 shows how this works with a multicellular architecture made of two LSTM units. This means that anywhere we use a vanilla RNN layer, we can easily substitute an LSTM unit. Figure 9-22. Composing LSTM units as one might stack recurrent layers in a neural network Now that we have overcome the issue of vanishing gradients and understand the inner workings of LSTM units, we’re ready to dive into the implementation of our first RNN models.",
    "e inner workings of LSTM units, we’re ready to dive into the implementation of our first RNN models. Long Short-Term Memory Units | 217 --- Page 234 --- PyTorch Primitives for RNN Models PyTorch provides seceral primitives that we can use out of the box in order to build RNN models.",
    "els PyTorch provides seceral primitives that we can use out of the box in order to build RNN models. First, we have torch.nn.RNNCell objects that represent either an RNN layer or an LSTM unit: import torch.nn as nn cell_1 = nn.RNNCell(input_size = 10, hidden_size = 20, nonlinearity='tanh') cell_2 = nn.LSTMCell(input_size = 10, hidden_size = 20) cell_3 = nn.GRUCell(input_size = 10, hidden_size = 20) The RNNCell abstraction represents a vanilla recurrent neuron layer, while the LSTMCell represents an implementation of the LSTM unit.",
    "a vanilla recurrent neuron layer, while the LSTMCell represents an implementation of the LSTM unit. PyTorch also includes a variation of the LSTM unit known as the Gated Recurrent Unit (GRU), proposed in 2014 by Y oshua Bengio’s group. The critical initialization variable for all of these cells is the size of the hidden state vector or hidden_size . In addition to the primitives, PyTorch provides multilayer RNN and LSTM classes for stacking layers.",
    "In addition to the primitives, PyTorch provides multilayer RNN and LSTM classes for stacking layers. If we want to stack recurrent units or layers, we can use the following: multi_layer_rnn = nn.RNN(input_size = 10, hidden_size = 20, num_layers = 2, nonlinearity = 'tanh') multi_layer_lstm = nn.LSTM(input_size = 10, hidden_size = 20, num_layers = 2) We can also use the dropout parameter to apply dropout to the inputs and outputs of an LSTM with specified keep probability.",
    "out parameter to apply dropout to the inputs and outputs of an LSTM with specified keep probability. If the dropout parameter is nonzero, the model introduces a dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout : multi_layer_rnn = nn.RNN(input_size = 10, hidden_size = 20, num_layers = 2, nonlinearity = 'tanh', batch_first = False, dropout = 0.5) multi_layer_lstm = nn.LSTM(input_size = 10, hidden_size = 20, num_layers = 2, 218 | Chapter 9: Models for Sequence Analysis --- Page 235 --- batch_first = False, dropout = 0.5) As shown here, the multilayer RNN and LSTM classes also provide a batch_first parameter.",
    "pout = 0.5) As shown here, the multilayer RNN and LSTM classes also provide a batch_first parameter. If batch_first equals True , then the input and output tensors are pro‐ vided as (batch, seq, feature) instead of (seq, batch, feature) . Note that this does not apply to hidden or cell states. The default value of batch_first is False . See the PyTorch documentation for details.",
    "cell states. The default value of batch_first is False . See the PyTorch documentation for details. Finally, we instantiate an RNN by calling the PyTorch LSTM constructor: input = torch.randn(5, 3, 10) # (time_steps, batch, input_size) h_0 = torch.randn(2, 3, 20) # (n_layers, batch_size, hidden_size) c_0 = torch.randn(2, 3, 20) # (n_layers, batch_size, hidden_size) rnn = nn.LSTM(10, 20, 2) # (input_size, hidden_size, num_layers) output_n, (hn, cn) = rnn(input, (h_0, c_0)) The result of calling rnn is a tensor representing the outputs of the RNN, output_n , along with the final state vectors for each layer.",
    "r representing the outputs of the RNN, output_n , along with the final state vectors for each layer. The first tensor, hn, contains the hidden state vectors for each layer that holds the outputs of the Output Gates at time, n. The second tensor, cn, contains the state vectors for the memory cells of each layer, which is the output of the write gates. Both hn and cn are of size (n_layers, batch_size, hidden_size) .",
    "h is the output of the write gates. Both hn and cn are of size (n_layers, batch_size, hidden_size) . Now that we have an understanding of the tools at our disposal in constructing RNNs in PyTorch, we’ll build our first LSTM in the next section, focused on the task of sentiment analysis. Implementing a Sentiment Analysis Model In this section, we attempt to analyze the sentiment of movie reviews taken from the Large Movie Review Dataset.",
    "ion, we attempt to analyze the sentiment of movie reviews taken from the Large Movie Review Dataset. This dataset consists of 50,000 reviews from IMDb, each of which is labeled as having positive or negative sentiment. We use a simple LSTM model leveraging dropout to learn how to classify the sentiment of movie reviews. The LSTM model will consume the movie review one word at a time.",
    "ify the sentiment of movie reviews. The LSTM model will consume the movie review one word at a time. Once it has consumed the entire review, we’ll use its output as the basis of a binary classification to map the sentiment to be “positive” or “negative. ” Let’s start off by loading the dataset with the PyTorch library Torchtext, which comes preinstalled with Google Colab.",
    "loading the dataset with the PyTorch library Torchtext, which comes preinstalled with Google Colab. If you’re running on another machine, you can install Torchtext by running the following command: $ pip install torchtext Once we’ve installed the package, we can download the dataset and define a token‐ izer.",
    "ll torchtext Once we’ve installed the package, we can download the dataset and define a token‐ izer. Torchtext provides many natural language processing (NLP) datasets and token‐ izers through the torchtext.datasets and torchtext.data.utils submodules, Implementing a Sentiment Analysis Model | 219 --- Page 236 --- respectively . We’ll use the built-in IMDb dataset and standard 'basic_english' tokenizer provided by PyTorch.",
    "ly . We’ll use the built-in IMDb dataset and standard 'basic_english' tokenizer provided by PyTorch. from torchtext.datasets import IMDB from torchtext.data.utils import get_tokenizer # Load dataset train_iter = IMDB(split=('train')) # Define tokenizer and build vocabulary tokenizer = get_tokenizer('basic_english') Until now, we’ve been using map-style datasets from PyTorch. Torchtext returns NLP datasets as iterable-style datasets, which are more appropriate for streaming data.",
    "text returns NLP datasets as iterable-style datasets, which are more appropriate for streaming data. Next, we need to create a vocabulary based on the training dataset and prune the vocabulary to include only the 30,000 most common words. Then, we need to pad each input sequence up to a length of 500 words, and process the labels.",
    "words. Then, we need to pad each input sequence up to a length of 500 words, and process the labels. from torchtext.vocab import build_vocab_from_iterator def yield_tokens(data_iter): for _, text in data_iter: yield tokenizer(text) # build vocab from iterator and add a list of any special tokens text_vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>']) text_vocab.set_default_index(text_vocab['<unk>']) As shown, Torchtext provides a function, build_vocab_from_iterator , to create a vocabulary.",
    "unk>']) As shown, Torchtext provides a function, build_vocab_from_iterator , to create a vocabulary. However, this function expects a list of tokens as input, where next(train_iter) would return a tuple (label_string, review_string) . To satisfy this requirement, we define a function to yield tokens as the dataset is iterated. Finally, we add special tokens for unknown and padding, and set the default.",
    "he dataset is iterated. Finally, we add special tokens for unknown and padding, and set the default. Next, we need to actually prune the vocabulary and pad the review sequences, as well as convert the label strings, 'neg' or 'pos' , to numbers.",
    "ary and pad the review sequences, as well as convert the label strings, 'neg' or 'pos' , to numbers. We accomplish this by defining a pipeline function for both the labels and review strings: def text_pipeline(x, max_size=512): text = tokenizer(x) # reduce vocab size pruned_text = [] for token in text: if text_vocab.get_stoi()[token] >= 30000: token = '<unk>' pruned_text.append(token) # pad sequence or truncate if len(pruned_text) <= max_size: 220 | Chapter 9: Models for Sequence Analysis --- Page 237 --- pruned_text += ['<pad>'] * (max_size - len(pruned_text)) else: pruned_text = pruned_text[0:max_size] return text_vocab(pruned_text) label_pipeline = lambda x: (0 if (x == 'neg') else 1) The text_pipeline function converts the inputs to 500-dimensional vectors.",
    "0 if (x == 'neg') else 1) The text_pipeline function converts the inputs to 500-dimensional vectors. Each vector corresponds to a movie review where the itℎ component of the vector corre‐ sponds to the index of the itℎ word of the review in our global dictionary of 30,000 words. To complete the data preparation, we create a special Python class designed to serve minibatches of a desired size from the underlying dataset.",
    "a special Python class designed to serve minibatches of a desired size from the underlying dataset. We can use the built-in DataLoader class from PyTorch to sample the dataset in batches.",
    "ing dataset. We can use the built-in DataLoader class from PyTorch to sample the dataset in batches. Before we do so, we need to define a function, collate_batch , that will tell the DataLoader how to preprocess each batch: def collate_batch(batch): label_list, text_list = [], [] for label, review in batch: label_list.append(label_pipeline(label)) text_list.append(text_pipeline(review)) return (torch.tensor(label_list, dtype=torch.long), torch.tensor(text_list, dtype=torch.int32)) The collate_batch function simply runs the labels and review strings through each respective pipeline and returns the batch as a tuple of tensors (labels_batch, reviews_batch) .",
    "each respective pipeline and returns the batch as a tuple of tensors (labels_batch, reviews_batch) . Once the collate_fn is defined, we simply load the dataset using the IMDb constructor, and configure the dataloaders using the DataLoader constructor: from torch.utils.data import DataLoader train_iter, val_iter = IMDB(split=('train','test')) trainloader = DataLoader(train_iter, batch_size = 4, shuffle=False, collate_fn=collate_batch) valloader = DataLoader(val_iter, batch_size = 4, shuffle=False, collate_fn=collate_batch) We use the torchtext.datasets.IMDB Python class to serve both the training and validation sets we’ll use while training our sentiment analysis model.",
    "o serve both the training and validation sets we’ll use while training our sentiment analysis model. Now that the data is ready to go, we’ll begin to construct the sentiment analysis model, step by step. First, we’ll want to map each word in the input review to a word vector.",
    "alysis model, step by step. First, we’ll want to map each word in the input review to a word vector. To do this, we’ll utilize an embedding layer, which, as you may recall from Implementing a Sentiment Analysis Model | 221 --- Page 238 --- Chapter 8 , is a simple lookup table that stores an embedding vector that corresponds to each word.",
    "Chapter 8 , is a simple lookup table that stores an embedding vector that corresponds to each word. Unlike in previous examples, where we treated the learning of the word embeddings as a separate problem (i.e., by building a Skip-Gram model), we’ll learn the word embeddings jointly with the sentiment analysis problem by treating the embedding matrix as a matrix of parameters in the full problem.",
    "ent analysis problem by treating the embedding matrix as a matrix of parameters in the full problem. We accomplish this by using the PyTorch primitives for managing embeddings (remember that input represents one full minibatch at a time, not just one movie review vector): import torch.nn as nn embedding = nn.Embedding( num_embeddings=30000, embedding_dim=512, padding_idx=text_vocab.get_stoi()['<pad>']) We then take the result of the embedding layer and build an LSTM with dropout using the primitives we saw in the previous section.",
    "embedding layer and build an LSTM with dropout using the primitives we saw in the previous section. The implementation of the LSTM can be achieved as follows: class TextClassifier(nn.Module): def __init__(self): super(TextClassifier,self).__init__() self.layer_1 = nn.Embedding( num_embeddings=30000, embedding_dim=512, padding_idx=1) self.layer_2 = nn.LSTMCell(input_size=512, hidden_size=512) self.layer_3 = nn.Dropout(p=0.5) self.layer_4 = nn.Sequential( nn.Linear(512, 2), nn.Sigmoid(), nn.BatchNorm1d(2)) def forward(self, x): x = self.layer_1(x) x = x.permute(1,0,2) h = torch.rand(x.shape[1], 512) c = torch.rand(x.shape[1], 512) for t in range(x.shape[0]): h, c = self.layer_2(x[t], (h,c)) h = self.layer_3(h) return self.layer_4(h) We top it all off using a batch-normalized hidden layer, identical to the ones we’ve used time and time again in previous examples.",
    "-normalized hidden layer, identical to the ones we’ve used time and time again in previous examples. Stringing all of these components together, we can build the model by calling TextClassifier : model = TextClassifier() 222 | Chapter 9: Models for Sequence Analysis --- Page 239 --- We omit the other boilerplate involved in setting up summary statistics, saving intermediate snapshots, and creating the session because it’s identical to the other models we’ve built in this book (see the GitHub repository ).",
    "on because it’s identical to the other models we’ve built in this book (see the GitHub repository ). We can then run and visualize the performance of our model using TensorBoard ( Figure 9-23 ). Figure 9-23.",
    "then run and visualize the performance of our model using TensorBoard ( Figure 9-23 ). Figure 9-23. Training cost, validation cost, and accuracy of our movie review sentiment model Implementing a Sentiment Analysis Model | 223 --- Page 240 --- At the beginning of training, the model struggles slightly with stability, and toward the end of the training, the model clearly starts to overfit as training cost and validation cost significantly diverge.",
    "ing, the model clearly starts to overfit as training cost and validation cost significantly diverge. At its optimal performance, however, the model performs rather effectively and generalizes to approximately 86% accuracy on the test set. Congratulations! Y ou’ve built your first RNN. Solving seq2seq Tasks with Recurrent Neural Networks Now that we’ve built a strong understanding of RNNs, we’re ready to revisit the problem of seq2seq.",
    "Now that we’ve built a strong understanding of RNNs, we’re ready to revisit the problem of seq2seq. We started off this chapter with an example of a seq2seq task: mapping a sequence of words in a sentence to a sequence of POS tags. Tackling this problem was tractable because we didn’t need to take into account long-term depen‐ dencies to generate the appropriate tags.",
    "cause we didn’t need to take into account long-term depen‐ dencies to generate the appropriate tags. But there are several seq2seq problems, such as translating between languages or creating a summary for a video, where long-term dependencies are crucial to the success of the model. This is where RNNs come in. The RNN approach to seq2seq looks a lot like the autoencoder we discussed in the previous chapter. The seq2seq model is composed of two separate networks.",
    "ncoder we discussed in the previous chapter. The seq2seq model is composed of two separate networks. The first network is known as the encoder network. The encoder network is a recurrent network (usually one that uses LSTM units) that consumes the entire input sequence. The goal of the encoder network is to generate a condensed understanding of the input and summarize it into a singular thought represented by the final state of the encoder network.",
    "nput and summarize it into a singular thought represented by the final state of the encoder network. Then we use a decoder network, whose starting state is initialized with the final state of the encoder network, to produce the target output sequence token by token. At each step, the decoder network consumes its own output from the previous time step as the current time step’s input. The entire process is visualized in Figure 9-24 .",
    "evious time step as the current time step’s input. The entire process is visualized in Figure 9-24 . 224 | Chapter 9: Models for Sequence Analysis --- Page 241 --- 5Kiros, Ryan, et al. “Skip-Thought Vectors. ” Advances in neural information processing systems . 2015. Figure 9-24. How we use an encoder/decoder recurrent network schema to tackle seq2seq problems In this this setup, we are attempting to translate an English sentence into French.",
    "seq2seq problems In this this setup, we are attempting to translate an English sentence into French. We tokenize the input sentence and use an embedding (similar to our approach in the sentiment analysis model we built in the previous section), one word at a time as an input to the encoder network. At the end of the sentence, we use a special “end-of-sequence” (EOS) token to indicate the end of the input sequence to the encoder network.",
    "cial “end-of-sequence” (EOS) token to indicate the end of the input sequence to the encoder network. Then we take the hidden state of the encoder network and use that as the initialization of the decoder network. The first input to the decoder network is the EOS token, and the output is interpreted as the first word of the predicted French translation. From that point onward, we use the output of the decoder network as the input to itself at the next time step.",
    "point onward, we use the output of the decoder network as the input to itself at the next time step. We continue until the decoder network emits an EOS token as its output, at which point we know that the network has completed producing the translation of the original English sentence. We’ll dissect the practical, open source implementation of this network (with a couple of enhancements and tricks to improve accuracy) later in this chapter.",
    "f this network (with a couple of enhancements and tricks to improve accuracy) later in this chapter. The seq2seq RNN architecture can also be reappropriated for the purpose of learning good embeddings of sequences. For example, Kiros et al. in 2015 invented the notion of a skip-thought vector ,5 which borrowed architectural characteristics from both the autoencoder framework and the Skip-Gram model discussed in Chapter 8 .",
    "characteristics from both the autoencoder framework and the Skip-Gram model discussed in Chapter 8 . The skip- thought vector was generated by dividing a passage into a set of triplets consisting Solving seq2seq Tasks with Recurrent Neural Networks | 225 --- Page 242 --- of consecutive sentences. The authors utilized a single encoder network and two decoder networks, as shown in Figure 9-25 . Figure 9-25.",
    "s utilized a single encoder network and two decoder networks, as shown in Figure 9-25 . Figure 9-25. The skip-thought seq2seq architecture to generate embedding representa‐ tions of entire sentences The encoder network consumed the sentence for which we wanted to generate a condensed representation (which was stored in the final hidden state of the encoder network). Then came the decoding step.",
    "on (which was stored in the final hidden state of the encoder network). Then came the decoding step. The first of the decoder networks would take that representation as the initialization of its own hidden state and attempt to reconstruct the sentence that appeared prior to the input sentence. The second decoder network would instead attempt the sentence that appeared immediately after the input sentence.",
    "coder network would instead attempt the sentence that appeared immediately after the input sentence. The full system was trained end to end on these triplets, and once completed, could be used to generate seemingly cohesive passages of text in addition to improve performance on key sentence-level classification tasks. 226 | Chapter 9: Models for Sequence Analysis --- Page 243 --- Here’s an example of story generation, excerpted from the original paper: she grabbed my hand . \"come on .",
    "an example of story generation, excerpted from the original paper: she grabbed my hand . \"come on . \" she fluttered her back in the air . \"i think we're at your place . I ca n't come get you . \" he locked himself back up \" no . she will . \" kyrian shook his head Now that we’ve developed an understanding of how to leverage RNNs to tackle seq2seq problems, we’re almost ready to try to build our own.",
    "ding of how to leverage RNNs to tackle seq2seq problems, we’re almost ready to try to build our own. Before we get there, however, we’ve got one more major challenge to tackle, and we’ll address it head-on in the next section when we discuss the concept of attentions in seq2seq RNNs. Augmenting Recurrent Networks with Attention Let’s think harder about the translation problem.",
    "RNNs. Augmenting Recurrent Networks with Attention Let’s think harder about the translation problem. If you’ve ever attempted to learn a foreign language, you’ll know that there are several helpful steps when trying to complete a translation. First, it’s helpful to read the full sentence to understand the concept you would like to convey. Then you write out the translation one word at a time, each word following logically from the word you wrote previously.",
    "he translation one word at a time, each word following logically from the word you wrote previously. But one important aspect of translation is that as you compose the new sentence, you often refer back to the original text, focusing on specific parts that are relevant to your current translation. At each step, you are paying attention to the most relevant parts of the original “input” so you can make the best decision about the next word to put on the page. Recall our approach to seq2seq.",
    "u can make the best decision about the next word to put on the page. Recall our approach to seq2seq. By consuming the full input and summarizing it into a “thought” inside its hidden state, the encoder network effectively achieves the first part of the translation process. By using the previous output as its current input, the decoder network achieves the second part of the translation process.",
    "utput as its current input, the decoder network achieves the second part of the translation process. This phenomenon of attention has yet to be captured by our approach to seq2seq, and this is the final building block we’ll need to engineer. Currently, the sole input to the decoder network at a given time step t is its output at time step t− 1.",
    "ently, the sole input to the decoder network at a given time step t is its output at time step t− 1. One way to give the decoder network some vision into the original sentence is by giving the decoder access to all of the outputs from the encoder network (which we previously had completely ignored). These outputs are interesting to us because they represent how the encoder network’s internal state evolves after seeing each new token.",
    "because they represent how the encoder network’s internal state evolves after seeing each new token. A proposed implementation of this strategy is shown in Figure 9-26 . This attempt falls short because it fails to dynamically select the most relevant parts of the input to focus on. Augmenting Recurrent Networks with Attention | 227 --- Page 244 --- 6Bahdanau, Dzmitry, Kyunghyun Cho, and Y oshua Bengio. “Neural Machine Translation by Jointly Learning to Align and Translate.",
    "yun Cho, and Y oshua Bengio. “Neural Machine Translation by Jointly Learning to Align and Translate. ” arXiv preprint arXiv :1409.0473 (2014). Figure 9-26. An attempt at engineering attentional abilities in a seq2seq architecture This approach has a critical flaw, however. The problem here is that at every time step, the decoder considers all of the outputs of the encoder network in the exact same way. However, this is clearly not the case for a human during the translation process.",
    "he exact same way. However, this is clearly not the case for a human during the translation process. We focus on different aspects of the original text when working on different parts of the translation. The key realization here is that it’s not enough to merely give the decoder access to all the outputs. Instead, we must engineer a mechanism by which the decoder network can dynamically pay attention to a specific subset of the encoder’s outputs.",
    "ich the decoder network can dynamically pay attention to a specific subset of the encoder’s outputs. We can fix this problem by changing the inputs to the concatenation operation, using the proposal in Bahdanau et al. 2015 as inspiration.6 Instead of directly using the raw outputs from the encoder network, we perform a weighting operation on the encoder’s outputs. We leverage the decoder network’s state at time t− 1 as the basis for the weighting operation.",
    "puts. We leverage the decoder network’s state at time t− 1 as the basis for the weighting operation. 228 | Chapter 9: Models for Sequence Analysis --- Page 245 --- The weighting operation is illustrated in Figure 9-27 . First we create a scalar (a single number, not a tensor) relevance score for each of the encoder’s outputs. The score is generated by computing the dot product between each encoder output and the decoder’s state at time t− 1.",
    "rated by computing the dot product between each encoder output and the decoder’s state at time t− 1. We then normalize these scores using a softmax operation. Finally, we use these normalized scores to individually scale the encoder’s outputs before plugging them into the concatenation operation. The key here is that the relative scores computed for each encoder output signify how important that particular encoder output is to the decision for the decoder at time step t.",
    "nify how important that particular encoder output is to the decision for the decoder at time step t. In fact, as we’ll see later, we can visualize which parts of the input are most relevant to the translation at each time step by inspecting the output of the softmax. Figure 9-27.",
    "relevant to the translation at each time step by inspecting the output of the softmax. Figure 9-27. A modification to our original proposal that enables a dynamic attentional mechanism based on the hidden state of the decoder network in the previous time step Armed with this strategy for engineering attention into seq2seq architectures, we’re finally ready to get our hands dirty with an RNN model for translating English sentences into French.",
    "inally ready to get our hands dirty with an RNN model for translating English sentences into French. But before we jump in, it’s worth noting that attentions are incredibly applicable in problems that extend beyond language translation. Atten‐ tions can be important in speech-to-text problems, where the algorithm learns to dynamically pay attention to corresponding parts of the audio while transcribing the audio into text.",
    "ynamically pay attention to corresponding parts of the audio while transcribing the audio into text. Similarly, attentions can be used to improve image captioning algorithms by helping the captioning algorithm focus on specific parts of the input image while writing out the caption.",
    "g the captioning algorithm focus on specific parts of the input image while writing out the caption. Anytime particular parts of the input are highly Augmenting Recurrent Networks with Attention | 229 --- Page 246 --- correlated to correctly producing corresponding segments of the output, attentions can dramatically improve performance.",
    "tly producing corresponding segments of the output, attentions can dramatically improve performance. Dissecting a Neural Translation Network State-of-the-art neural translation networks use a number of different techniques and advancements that build on the basic seq2seq encoder-decoder architecture. Atten‐ tion, as detailed in the previous section, is an important and critical architectural improvement.",
    "‐ tion, as detailed in the previous section, is an important and critical architectural improvement. In this section, we will dissect a fully implemented neural machine translation system, complete with the data processing steps, building the model, training it, and eventually using it as a translation system to convert English phrases to French phrases.",
    "ng it, and eventually using it as a translation system to convert English phrases to French phrases. The pipeline used in training and eventually using a neural machine translation system is similar to that of most machine learning pipelines: gather data, prepare the data, construct the model, train the model, evaluate the model’s progress, and eventually use the trained model to predict or infer something useful. We review each of these steps here.",
    "ally use the trained model to predict or infer something useful. We review each of these steps here. We first gather the data from the International Workshop on Spoken Language Translation (IWSLT2016) repository , which houses large corpora used in training translation systems. For our use case, we’ll be using the English-to-French data. Note that if we want to be able to translate to or from different languages, we would have to train a model from scratch with the new data.",
    "slate to or from different languages, we would have to train a model from scratch with the new data. We then preprocess our data into a format that is easily usable by our models during training and inference time. This will involve some amount of cleaning and tokenizing the sentences in each of the English and French phrases. What follows now is a set of techniques used in preparing the data, and later we will present the implementations of the techniques.",
    "hniques used in preparing the data, and later we will present the implementations of the techniques. The first step is to parse sentences and phrases into formats that are more compatible with the model by tokenization . This is the process by which we discretize a partic‐ ular English or French sentence into its constituent tokens. For instance, a simple word-level tokenizer will consume the sentence “I read.",
    "s constituent tokens. For instance, a simple word-level tokenizer will consume the sentence “I read. ” to produce the array [\"I” , “read” , “.\"], or it would consume the French sentence “Je lis. ” to produce the array [\"Je” , “lis” , “.\"]. A character-level tokenizer may break the sentence into individual characters or into pairs of characters like [\"I” , \" “, “r” , “e” , “a” , “d” , “.\"] and [\"I “, “re” , “ad” , “.\"], respectively.",
    "haracters like [\"I” , \" “, “r” , “e” , “a” , “d” , “.\"] and [\"I “, “re” , “ad” , “.\"], respectively. One kind of tokenization may work better than the other, and each has its pros and cons. For instance, a word-level tokenizer will ensure that the model produces words that are from some dictionary, but the size of the dictionary may be too large to efficiently choose from during decoding. This is in fact a known issue and something that we’ll address in the coming discussions.",
    "decoding. This is in fact a known issue and something that we’ll address in the coming discussions. 230 | Chapter 9: Models for Sequence Analysis --- Page 247 --- On the other hand, the decoder using a character-level tokenization may not produce intelligible outputs, but the total dictionary that the decoder must choose from is much smaller, as it is simply the set of all printable ASCII characters.",
    "decoder must choose from is much smaller, as it is simply the set of all printable ASCII characters. In this tutorial, we use a word-level tokenization, but we encourage you to experiment with different tokenizations to observe the effects this has. It is worth noting that we must also add a special EOS character, to the end of all output sequences because we need to provide a definitive way for the decoder to indicate that it has reached the end of its decoding.",
    "to provide a definitive way for the decoder to indicate that it has reached the end of its decoding. We can’t use regular punctuation because we cannot assume that we are translating full sentences. Note that we do not need EOS characters in our source sequences because we are feeding these in preformatted and do not need an EOS character for ourselves to denote the end of our source sequence.",
    "reformatted and do not need an EOS character for ourselves to denote the end of our source sequence. The next optimization involves further modifying how we represent each source and target sequence, and we introduce a concept called bucketing . This is a method employed primarily in sequence-to-sequence tasks, especially machine translation, that helps the model efficiently handle sentences or phrases of different lengths.",
    "hine translation, that helps the model efficiently handle sentences or phrases of different lengths. We first describe the naive method of feeding in training data and illustrate the shortcomings of this approach. Normally, when feeding in encoder and decoder tokens, the length of the source sequence and the target sequence is not always equal between pairs of examples. For example, the source sequence may have length X, and the target sequence may have length Y.",
    "ples. For example, the source sequence may have length X, and the target sequence may have length Y. It may seem that we need different seq2seq networks to accommodate each ( X, Y ) pair, yet this immediately seems wasteful and inefficient. Instead, we can do a little better if we pad each sequence up to a certain length, as shown in Figure 9-28 , assuming we use a word-level tokenization and that we’ve appended EOS tokens to our target sequences. Figure 9-28.",
    "e a word-level tokenization and that we’ve appended EOS tokens to our target sequences. Figure 9-28. Naive strategy for padding sequences This step saves us the trouble of having to construct a different seq2seq model for each pair of source and target lengths. However, this introduces a different issue: if there were a very long sequence, it would mean that we would have to pad every other sequence up to that length .",
    "very long sequence, it would mean that we would have to pad every other sequence up to that length . This would make a short sequence padded to the end take as much computational resources as a long one with few pad tokens, which is wasteful and could introduce a major performance hit to our model.",
    "one with few pad tokens, which is wasteful and could introduce a major performance hit to our model. We could Dissecting a Neural Translation Network | 231 --- Page 248 --- consider breaking up every sentence in the corpus into phrases such that the length of each phrase does not exceed a certain maximum limit, but it’s not clear how to break the corresponding translations. This is where bucketing helps us.",
    "t, but it’s not clear how to break the corresponding translations. This is where bucketing helps us. Bucketing is the idea that we can place encoder and decoder pairs into buckets of similar size, and only pad up to the maximum length of sequences in each respective bucket. For instance, we can denote a set of buckets, [(5, 10), (10, 15), (20, 25), (30, 40)], where each tuple in the list is the maximum length of the source sequence and target sequence, respectively.",
    "ch tuple in the list is the maximum length of the source sequence and target sequence, respectively. Borrowing the preceding example, we can place the pair of sequences ([\"I” , “read” , “.\"], [\"Je” , “lis” , “. ” , “EOS\"]) in the first bucket, as the source sequence is smaller than 5 tokens and the target sequence is smaller than 10 tokens. We would then place the ([\"See” , “you” , “in” , “a” , “little” , “while\"], [\"A ” , “tout” , “a” , “l’heure” , “EOS\"]) in the second bucket, and so on.",
    "” , “little” , “while\"], [\"A ” , “tout” , “a” , “l’heure” , “EOS\"]) in the second bucket, and so on. This technique allows us to compromise between the two extremes, where we need to pad only as much as necessary, as shown in Figure 9-29 . Figure 9-29.",
    "two extremes, where we need to pad only as much as necessary, as shown in Figure 9-29 . Figure 9-29. Padding sequences with buckets Using bucketing shows a considerable speedup during training and test time, and allows developers and frameworks to write very optimized code to leverage the fact that any sequence from a bucket will have the same size and pack the data together in ways that allow even further GPU efficiency.",
    "t will have the same size and pack the data together in ways that allow even further GPU efficiency. With the sequences properly padded, we need to add one additional token to the target sequences: a GO token . This GO token will signal to the decoder that decoding needs to begin, at which point it will take over and begin decoding. The last improvement we make in the data preparation side is to reverse the source sequences.",
    "oding. The last improvement we make in the data preparation side is to reverse the source sequences. Researchers found that doing so improved performance, and this has become a standard trick to try when training neural machine translation models.",
    "rmance, and this has become a standard trick to try when training neural machine translation models. This is a bit of an engineering hack, but consider the fact that our fixed-size neural state can hold only so much information, and information encoded while processing the beginning of the sentence may be overwritten while encoding later parts of the sentence.",
    "cessing the beginning of the sentence may be overwritten while encoding later parts of the sentence. In many language pairs, the beginning of sentences is harder to translate 232 | Chapter 9: Models for Sequence Analysis --- Page 249 --- than the end of sentences, so this hack of reversing the sentence improves translation accuracy by giving the beginning of the sentence the last say on what final state is encoded. With these ideas in place, the final sequences look as they do in Figure 9-30 .",
    "al state is encoded. With these ideas in place, the final sequences look as they do in Figure 9-30 . Figure 9-30. Final padding scheme with buckets, reversing the inputs, and adding the GO token With these techniques described, we can now detail the implementation. First, we load the dataset, then we define our tokenizers and vocabularies. We do not define the word embeddings here, as we will train our model to compute them.",
    "vocabularies. We do not define the word embeddings here, as we will train our model to compute them. PyTorch’s Torchtext library supports IWSLT2016 in torch.text.datasets : from torchtext.datasets import IWSLT2016 train_iter = IWSLT2016(split=('train'), language_pair=('en','fr')) The dataset constructor returns an iterable-style dataset that can retrieve English and French sentence pairs with next(train_iter) .",
    "n iterable-style dataset that can retrieve English and French sentence pairs with next(train_iter) . We’ll use this iterable-style dataset to create bucketed datasets for batching later in our code. For now, let’s also define our tokenizers and vocabularies for each language. PyTorch offers a get_tokenizer function that operates on common tokenizers. Here, we’ll use the spacy tokenizer for each language.",
    "function that operates on common tokenizers. Here, we’ll use the spacy tokenizer for each language. Y ou may need to download the spacy language files first: pip install -U spacy python -m spacy download en_core_web_sm python -m spacy download fr_core_news_sm Once we have the language files, we can create the tokenizers as follows: from torchtext.data.utils import get_tokenizer tokenizer_en = get_tokenizer('spacy',language='en_core_web_sm') tokenizer_fr = get_tokenizer('spacy',language='fr_core_news_sm') Dissecting a Neural Translation Network | 233 --- Page 250 --- Next, we will create our vocabularies for English and French using PyTorch’s build_vocab_from_iterator function.",
    "l create our vocabularies for English and French using PyTorch’s build_vocab_from_iterator function. This function takes tokens from an iterable- style dataset from a single language and creates a vocabulary.",
    "nction takes tokens from an iterable- style dataset from a single language and creates a vocabulary. Since our dataset has both English and French sentences, we create a yield_tokens function to return only the English or French tokens, and pass this into build_vocab_from_iterator : def yield_tokens(data_iter, language): if language == 'en': for data_sample in data_iter: yield tokenizer_en(data_sample[0]) else: for data_sample in data_iter: yield tokenizer_fr(data_sample[1]) UNK_IDX, PAD_IDX, GO_IDX, EOS_IDX = 0, 1, 2, 3 special_symbols = ['<unk>', '<pad>', '<go>', '<eos>'] # Create Vocabs train_iter = IWSLT2016(root='.data', split=('train'), language_pair=('en', 'fr')) vocab_en = build_vocab_from_iterator( yield_tokens(train_iter, 'en'), min_freq=1, specials=special_symbols, special_first=True) train_iter = IWSLT2016(root='.data', split=('train'), language_pair=('en', 'fr')) vocab_fr = build_vocab_from_iterator( yield_tokens(train_iter, 'fr'), min_freq=1, specials=special_symbols, special_first=True) Notice that we need to reload the train_iter before building the French vocabulary to restart the iterable-style dataset.",
    "o reload the train_iter before building the French vocabulary to restart the iterable-style dataset. We also add in the special tokens and their indices. Now that we have the dataset, tokenizers, and vocabularies, we need to create func‐ tions to preprocess the tokens and generate batches of bucketed data.",
    "aries, we need to create func‐ tions to preprocess the tokens and generate batches of bucketed data. First let’s define a process_tokens function to apply the improvements we discussed earlier: def process_tokens(source, target, bucket_sizes): # find bucket_index for i in range(len(bucket_sizes)+2): # truncate if we exhauset list of buckets if i >= len(bucket_sizes): bucket = bucket_sizes[i-1] bucket_id = i-1 if len(source) > bucket[0]: source = source[:bucket[0]] 234 | Chapter 9: Models for Sequence Analysis --- Page 251 --- if len(target) > (bucket[1]-2): target = target[:bucket[1]-2] break bucket = bucket_sizes[i] if (len(source) < bucket[0]) and ((len(target)+1) < bucket[1]): bucket_id = i break source += ((bucket_sizes[bucket_id][0] - len(source)) * ['<pad>']) source = list(reversed(source)) target.insert(0,'<go>') target.append('<eos>') target += (bucket_sizes[bucket_id][1] - len(target)) * ['<pad>'] return vocab_en(source), vocab_fr(target), bucket_id In this function, we pass in the variable size lists of source and target tokens, plus a list of bucket sizes.",
    "nction, we pass in the variable size lists of source and target tokens, plus a list of bucket sizes. First, we decide on the smallest bucket size that will fit both the source and target token lists. Then, we process the source tokens by padding and reversing the sequence as described earlier. For the target tokens, we add a <go> token to the beginning and add an <eos> token to the end, then pad to the bucket size.",
    "we add a <go> token to the beginning and add an <eos> token to the end, then pad to the bucket size. When determining the smallest bucket size, we accounted for the two added tokens <go> and <eos> . Now we have a function that takes lists of source and target tokens and prepares them appropriately. Next, we need to collect a single batch of data for our model and training loop. To do this, we will use the built-in PyTorch Dataset and DataLoader classes.",
    "odel and training loop. To do this, we will use the built-in PyTorch Dataset and DataLoader classes. We are going to separate Dataset and DataLoader for each bucket size. This approach will enable us to use the built-in feature of the DataLoader for random batching and parallel processing. First we create a BucketedDataset class by subclassing PyTorch’s Dataset class.",
    "parallel processing. First we create a BucketedDataset class by subclassing PyTorch’s Dataset class. Since this will be a map-style dataset, we’ll need to define the __getitem__ and __len__ methods for data access: from torch.utils.data import Dataset class BucketedDataset(Dataset): def __init__(self, bucketed_dataset, bucket_size): super(BucketedDataset, self).__init__() self.length = len(bucketed_dataset) self.input_len = bucket_size[0] self.target_len = bucket_size[1] self.bucketed_dataset = bucketed_dataset def __getitem__(self, index): Dissecting a Neural Translation Network | 235 --- Page 252 --- return (torch.tensor(self.bucketed_dataset[index][0], dtype=torch.float32), torch.tensor(self.bucketed_dataset[index][1], dtype=torch.float32)) def __len__(self): return self.length bucketed_datasets = [] for i, dataset in enumerate(datasets): bucketed_datasets.append(BucketedDataset(dataset, bucket_sizes[i])) We create a list of BucketedDataset objects in bucketed_datasets , one for each bucket size.",
    "es[i])) We create a list of BucketedDataset objects in bucketed_datasets , one for each bucket size. The BucketedDataset constructor also converts our vocabulary integers to PyTorch tensors so we can pass them into our model later. Next we use the PyTorch’s DataLoader class to create dataloaders for each dataset in bucketed_datasets .",
    "we use the PyTorch’s DataLoader class to create dataloaders for each dataset in bucketed_datasets . Since we created Dataset objects, we get the batching capabili‐ ties of the DataLoader class without writing any additional code: from torch.utils.data import DataLoader dataloaders = [] for dataset in bucketed_datasets: dataloaders.append(DataLoader(dataset, batch_size=32, shuffle=True)) The dataloaders list hold a dataloader for each bucket size, so when we run our training or test loops, we will select a bucket size (randomly for training) and use the corresponding dataloader to pull a batch of encoders and decoder inputs: for epoch in range(n_epochs): # exhaust all dataloaders randomly # keep track of when we used up all values dataloader_sizes = [] for dataloader in dataloaders: dataloader_sizes.append(len(dataloader)) while np.array(dataloader_sizes).sum() != 0: bucket_id = torch.randint(0,len(bucket_sizes),(1,1)).item() if dataloader_sizes[bucket_id] == 0: continue source, target = next(iter(dataloaders[bucket_id])) dataloader_sizes[bucket_id] -= 1 loss = train(encoder_inputs, decoder_inputs, target_weights, bucket_id) 236 | Chapter 9: Models for Sequence Analysis --- Page 253 --- We measure the loss incurred during prediction time, as well as keep track of other running metrics: loss += step_loss / steps_per_checkpoint current_step += 1 Lastly, every so often, as dictated by a global variable, we will carry out a number of tasks.",
    "+= 1 Lastly, every so often, as dictated by a global variable, we will carry out a number of tasks. First, we print statistics for the previous batch, such as the loss, the learning rate, and the perplexity. If we find that the loss is not decreasing, the model may have fallen into a local optima. To assist the model in escaping this, we anneal the learning rate so that it won’t make large leaps in any particular direction.",
    "ing this, we anneal the learning rate so that it won’t make large leaps in any particular direction. At this point, we also save a copy of the model and its weights and activations to disk. This concludes the high-level details of training and using the models. We have largely abstracted away the fine details of the model itself. For more, see the book’s repository .",
    "largely abstracted away the fine details of the model itself. For more, see the book’s repository . With this, we’ve successfully completed a full tour of the implementation details of a fairly sophisticated neural machine translation system. Production systems have additional tricks that are not as generalizable, and these systems are trained on huge compute servers to ensure that state-of-the-art performance is met.",
    "hese systems are trained on huge compute servers to ensure that state-of-the-art performance is met. For reference, this exact model was trained on eight NVIDIA Telsa M40 GPUs for four days. We show plots for the perplexity in Figures 9-31 and 9-32 , and show the learning rate anneal over time as well. In Figure 9-31 , we see that after 50,000 epochs, the perplexity decreases from about 6 to 4, which is a reasonable score for a neural machine translation system.",
    "ty decreases from about 6 to 4, which is a reasonable score for a neural machine translation system. In Figure 9-32 , we observe that the learning rate almost smoothly declines to 0. This means that by the time we stopped training, the model was approaching a stable state. Figure 9-31. Plot of perplexity on training data over time Dissecting a Neural Translation Network | 237 --- Page 254 --- Figure 9-32.",
    "training data over time Dissecting a Neural Translation Network | 237 --- Page 254 --- Figure 9-32. Plot of learning rate over time To showcase the attentional model more explicitly, we can visualize the attention that the decoder LSTM computes while translating a sentence from English to French. In particular, we know that as the encoder LSTM is updating its cell state in order to compress the sentence into continuous vector representations, it also computes hidden states at every time step.",
    "sentence into continuous vector representations, it also computes hidden states at every time step. We know that the decoder LSTM computes a convex sum over these hidden states, and we can think of this sum as the attention mechanism; when there is more weight on a particular hidden state, we can interpret that as the model is paying more attention to the token inputted at that time step.This is exactly what we visualize in Figure 9-33 .",
    "attention to the token inputted at that time step.This is exactly what we visualize in Figure 9-33 . The English sentence to be translated is on the top row, and the resulting French translation is on the first column. The lighter a square is, the more attention the decoder paid to that particular column when decoding that row element.",
    "re is, the more attention the decoder paid to that particular column when decoding that row element. That is, the (i, j)th element in the attention map shows the amount of attention that was paid to the jth token in the English sentence when translating the ith token in the French sentence. We can immediately see that the attention mechanism seems to be working quite well.",
    "French sentence. We can immediately see that the attention mechanism seems to be working quite well. Large amounts of attention are generally being placed in the right areas, even though there is slight noise in the model’s prediction. It is possible that adding layers to the network would help produce crisper attention.",
    "s prediction. It is possible that adding layers to the network would help produce crisper attention. One impressive aspect is that the phrase “the European Economic” is translated in reverse in French as the “zone économique européenne, ” and as such, the attention weights reflect this flip. These kinds of attention patterns may be even more interesting when translating from English to a different language that does not parse smoothly from left to right.",
    "en translating from English to a different language that does not parse smoothly from left to right. 238 | Chapter 9: Models for Sequence Analysis --- Page 255 --- Figure 9-33. Visualizing the weights of the convex sum when the decoder attends over hidden states in the encoder With one of the most fundamental architectures understood and implemented, we now move forward to study exciting new developments with RNNs and begin a foray into more sophisticated learning.",
    "ard to study exciting new developments with RNNs and begin a foray into more sophisticated learning. Self-Attention and Transformers Earlier, we discussed a form of attention that was first presented in Bahdanau et al. in 2015. Specifically, we used a simple, feed-forward neural network to calculate the alignment score of each encoder hidden state with the decoder state at the current time step.",
    "te the alignment score of each encoder hidden state with the decoder state at the current time step. In this section, we’ll discuss a different form of attention called scaled dot product attention, its use in self-attention, and the transformer, a recent language modeling breakthrough. Transformer-based models have primarily replaced LSTM, and have been proven to be superior in quality for many sequence-to-sequence problems.",
    "eplaced LSTM, and have been proven to be superior in quality for many sequence-to-sequence problems. Dot product attention is really as simple as it sounds—this method calculates align‐ ment scores as the dot product between each encoder hidden state st. These weights are used in the calculation of the context vector, which is a convex sum (via softmax) of the encoder hidden states. Why use the dot product to measure alignment?",
    "convex sum (via softmax) of the encoder hidden states. Why use the dot product to measure alignment? As we learned in Chapter 1 , the dot product of two vectors can be expressed as a product of the norms of the two vectors and the cosine of the angle between them. As the angle between the two vectors goes to zero, the cosine goes to one.",
    "f the angle between them. As the angle between the two vectors goes to zero, the cosine goes to one. Also, recall from trigonometry that cosine has the range 1 to –1 when the input angle is between 0 degrees and 180 degrees, which is the only part of the domain of the angle we need to consider. The dot product has the nice property that, as the angle between two Self-Attention and Transformers | 239 --- Page 256 --- 7Vaswani et. al. “ Attention Is All Y ou Need.",
    "elf-Attention and Transformers | 239 --- Page 256 --- 7Vaswani et. al. “ Attention Is All Y ou Need. ” arXiv Preprint arXiv :1706.03762 2017.vectors gets smaller, the dot product gets larger. This allows us to use the dot product as a natural measure of similarity. In 2017, Vaswani et al.7 introduced a modification to the preexisting dot product attention framework via the inclusion of a scaling factor—the square root of the dimension of the hidden states. Vaswani et al.",
    "inclusion of a scaling factor—the square root of the dimension of the hidden states. Vaswani et al. acknowledge the fact that, as hidden state representations get larger and larger in terms of dimension, we expect to see significantly more instances of high magnitude dot products.",
    "in terms of dimension, we expect to see significantly more instances of high magnitude dot products. To understand the rea‐ soning behind the inclusion of this scaling factor, assume, for the sake of argument, each index of ℎi is drawn independently and identically distributed from a mean zero, unit variance random variable.",
    "is drawn independently and identically distributed from a mean zero, unit variance random variable. Let’s compute the expectation and variance of their dot product: EstTℎi= ∑j= 1kEst,j*ℎi,j =∑j= 1kEst,jEℎi,j = 0 VarstTℎi= ∑j= 1kVarst,j*ℎi,j = ∑j= 1kEst,j2*ℎi,j2−Est,j*ℎi,j2 = ∑j= 1kEst,j2Eℎi,j2 = ∑j= 1k1 =k Let’s review the steps that got us to these conclusions regarding the expectation and variance.",
    "1 =k Let’s review the steps that got us to these conclusions regarding the expectation and variance. The first equality in the expectation is due to the linearity of expectation, since the dot product can be expressed as a sum of the product of each index. The second equality comes from the fact that the two random variables in each expectation are independent, so we can separate the expectation of the product into a product of expectations.",
    "n are independent, so we can separate the expectation of the product into a product of expectations. The final step follows directly from the fact that each of these individual expectations are zero. The first equality in the variance is due to the linearity of variance when the individ‐ ual terms are all independent. The second equality is just the definition of variance.",
    "the individ‐ ual terms are all independent. The second equality is just the definition of variance. The third equality uses a result from our calculation of the expectation of the dot product (we can separate out the square of the expectation into the product of squares of expectations, where each individual expectation is zero).",
    "expectation into the product of squares of expectations, where each individual expectation is zero). Additionally, the expectation of the product of squares can be split up into a product of expectations 240 | Chapter 9: Models for Sequence Analysis --- Page 257 --- of squares, since the square of each random variable is independent of the squares of all other random variables.",
    "ince the square of each random variable is independent of the squares of all other random variables. The second to last equality comes from the fact that the expectation of the square of each random variable is just the variance of the random variable (since the expectation of each random variable is zero). The final equality follows directly. We see that the expectation of the dot product is zero, while its variance is k, the dimension of the hidden representation.",
    "ion of the dot product is zero, while its variance is k, the dimension of the hidden representation. Thus, as the dimension increases, the variance increases—this implies a higher probability of seeing high-magnitude dot products. Unfortunately, with the presence of more high-magnitude dot products comes smaller gradients due to the softmax function.",
    "he presence of more high-magnitude dot products comes smaller gradients due to the softmax function. Although we won’t derive it here, this makes a lot of intuitive sense—think back to the use of softmax in neural networks for classification problems. As the neural network gets more and more confident in a correct prediction (i.e., a high logit value for the true index), the gradient gets smaller and smaller. The scaling factor introduced by Vaswani et al.",
    "true index), the gradient gets smaller and smaller. The scaling factor introduced by Vaswani et al. reduces the magnitude of dot products, leading to larger gradients and better learning. Now that we’ve covered scaled dot product attention, we turn our attention to self-attention.",
    "rning. Now that we’ve covered scaled dot product attention, we turn our attention to self-attention. In the previous sections, we saw attention through the context of machine translation where we are given a training set of sentences that are in English and French, and the goal is to be able to translate unseen English sentences to French. In this specific class of problems, there exists a direct supervision through the target French sentences.",
    "s specific class of problems, there exists a direct supervision through the target French sentences. However, attention can also be used in a completely self-contained manner. The intuition is that, given a sentence in English, we may be able to perform more insightful sentiment analysis, more effective machine reading, and better understanding via learning relationships between tokens within sentences or paragraphs.",
    ", and better understanding via learning relationships between tokens within sentences or paragraphs. The transformer, our final topic for this section, utilizes both scaled dot product attention and self-attention. The transformer architecture (Vaswani et al. 2017) has both encoder and decoder architectures, where there exists self-attention within both the encoder and decoder, as well as standard attention between the encoder and decoder.",
    "within both the encoder and decoder, as well as standard attention between the encoder and decoder. The self-attention layers in the encoders and decoders allow each to attend to all positions prior to the current position in their respective architectures. The standard attention allows the decoder to attend to each encoder hidden state, as described earlier. Self-Attention and Transformers | 241 --- Page 258 --- Summary In this chapter, we’ve delved deep into the world of sequence analysis.",
    "241 --- Page 258 --- Summary In this chapter, we’ve delved deep into the world of sequence analysis. We’ve ana‐ lyzed how we might hack feed-forward networks to process sequences, developed a strong understanding of RNNs, and explored how attentional mechanisms can enable incredible applications ranging from language translation to audio transcription.",
    "hanisms can enable incredible applications ranging from language translation to audio transcription. Sequence analysis is a field that ranges problems not only in natural language, but also topics in finance, such as time-series analysis of returns of financial assets. Any field that involves longitudinal analyses, or analyses across time, could use the appli‐ cations of sequence analysis described in this chapter.",
    "r analyses across time, could use the appli‐ cations of sequence analysis described in this chapter. We advise you to really deepen your understanding of sequence analysis via implementation across different fields and by comparing the results of the techniques presented for natural language with the state-of-the-art in each field.",
    "he results of the techniques presented for natural language with the state-of-the-art in each field. There are also situations in which the techniques presented here may not be the most appropriate modeling choice, and we advise you to think deeply about why the modeling assumptions made here may not apply broadly. Sequence analysis is a powerful tool that has a place in almost all technical applications, not just natural language.",
    "is a powerful tool that has a place in almost all technical applications, not just natural language. 242 | Chapter 9: Models for Sequence Analysis --- Page 259 --- CHAPTER 10 Generative Models Generative models attempt to understand the latent , or underlying, process that produces the data we see.",
    "tive models attempt to understand the latent , or underlying, process that produces the data we see. For example, when breaking down images of digits in the MNIST dataset, we can interpret some attributes of the underlying process generating each image as the digit itself (a discrete variable ranging from zero through nine), the orientation or angle at which it will be drawn, the size of the resulting image, the thickness of the lines, and some noise component (all of which are continuous variables).",
    "image, the thickness of the lines, and some noise component (all of which are continuous variables). So far, we’ve been concerned with discriminative models, either in the regression or classification setting. In the classification setting, discriminative models take as input an example such as an image from the MNIST dataset and attempt to determine the most likely digit category, from zero through nine, that the input belongs to.",
    "empt to determine the most likely digit category, from zero through nine, that the input belongs to. Generative models instead attempt to fully model the data distribution, and in the process may implicitly try to learn some of the features mentioned previ‐ ously to generate images that look as if they were originally from the MNIST dataset.",
    "ntioned previ‐ ously to generate images that look as if they were originally from the MNIST dataset. Note that generative modeling is a harder problem than discriminative modeling, as a discriminative model may, for example, need to learn only a few features well to distinguish between different digits in the MNIST dataset to a satisfactory degree.",
    "features well to distinguish between different digits in the MNIST dataset to a satisfactory degree. Generative models come in many varieties, and in this chapter, we provide a glimpse into a vast research landscape that has begun to blossom only in the past decade. 243 --- Page 260 --- 1Goodfellow et al. “Generative Adversarial Networks. ” arXiv Preprint arXiv :1406.2661.",
    "Page 260 --- 1Goodfellow et al. “Generative Adversarial Networks. ” arXiv Preprint arXiv :1406.2661. 2014.Generative Adversarial Networks Generative Adversarial Networks , or GANs for short, are a form of generative model designed to produce realistic samples of entities, such as images, from noise. They were introduced by Goodfellow et al. in 2014.1 For the remainder of this section, we will assume we are working with an image dataset such as MNIST or CIFAR-10.",
    "nder of this section, we will assume we are working with an image dataset such as MNIST or CIFAR-10. The original GAN architecture is broken down into two neural networks: the discrimina‐ tor and the generator. The generator takes in samples from some noise distribution, such as a multivariate Gaussian distribution, and outputs an image. The discriminator is tasked with pre‐ dicting whether this image was produced by the generator or was sampled from the original dataset.",
    "‐ dicting whether this image was produced by the generator or was sampled from the original dataset. As the generator gets better and better at producing images that look real, the discriminator has a harder time determining whether a given image was produced by the generator or sampled from the dataset. We can think of these two networks as participating in a game, competing against each other to develop.",
    "can think of these two networks as participating in a game, competing against each other to develop. Each network evolves until the generator can eventually produce images that look as if they were drawn directly from the original dataset, and the discriminator cannot distinguish between the two sets of images, i.e., predicts that any image is from the dataset with probability 1 2. More rigorously, we define the data distribution to be pdatax.",
    "rom the dataset with probability 1 2. More rigorously, we define the data distribution to be pdatax. Although we can never really know the true data distribution, in practice we generally think of it as being approximated well enough by the dataset we have on hand ( pdatax is just a uniform distribution over all of the images present in the dataset and zero likelihood associated with all images that are not in the dataset).",
    "present in the dataset and zero likelihood associated with all images that are not in the dataset). We additionally define the distribution parametrized by the generator to be pgx. The random variable x represents an entity such as an image, a collection of pixels that can each be thought of as their own random variables.",
    "such as an image, a collection of pixels that can each be thought of as their own random variables. The generator, which we also refer to as G, defines pgx by mapping samples from the noise distribution, which we will refer to as p(z), to the data space, which consists of all possible images (not just those in the dataset). It is important to keep in mind that G itself is a deterministic function, but implicitly defines a distribution by acting on the noise distribution.",
    "a deterministic function, but implicitly defines a distribution by acting on the noise distribution. Note that this distribution is implicit because we can generate samples from it only via G(z), rather than being an explicit distribution we can work with directly and query an image for its likelihood. Figure 10-1 shows the typical GAN architecture. 244 | Chapter 10: Generative Models --- Page 261 --- Figure 10-1.",
    "hows the typical GAN architecture. 244 | Chapter 10: Generative Models --- Page 261 --- Figure 10-1. The discriminator determines whether any input image was sampled from the dataset or generator. The generator’s goal is to trick the discriminator into believing its images were sampled from the dataset. An optimal generator for a given dataset would also parametrize pdatax, as this would perfectly confuse even the best discriminator.",
    "dataset would also parametrize pdatax, as this would perfectly confuse even the best discriminator. In other words, if the generator parametrizes the exact same distribution as that of the dataset and it is equally likely to sample from either the generator or the dataset, then no discriminator would be able to tell where the query originated from, as both are always equally likely. We formalize this intuition in the next paragraph.",
    "iginated from, as both are always equally likely. We formalize this intuition in the next paragraph. Thinking back to Chapter 2 , given a generator that parametrizes the same distribu‐ tion as the dataset, we have pxy= generator =pxy= dataset ,∀x, where y is a Bernoulli random variable over the two options: generator or dataset. Note that we use pgx and pxy= generator interchangeably, and pdatax and pxy= dataset interchangeably, since they mean the same thing.",
    "erator interchangeably, and pdatax and pxy= dataset interchangeably, since they mean the same thing. The latter option of each allows us to keep in mind we are working with conditional probabilities. Again, assuming that sampling from the generator and sampling from the dataset are equally likely, or py= generator =py= dataset , we can use Bayes’ Rule to obtain the equality: py= generator x=py= dataset x,∀x.",
    "ator =py= dataset , we can use Bayes’ Rule to obtain the equality: py= generator x=py= dataset x,∀x. Since there are only two options, as y is a Bernoulli random variable, we are left with the perfectly confused discriminator alluded to earlier that predicts any image to be sampled from the dataset with probability 1 2. Knowing our end goal, we can now go about designing an objective function for training our generator and discriminator in tandem.",
    "now go about designing an objective function for training our generator and discriminator in tandem. In the original GAN paper, the objective presented was: VG,D=Ex ∼ p dataxlogDx+Ez ∼ pzlog 1 −DGz G(z) represents the mapping from the noise distribution to the data space described earlier, and D(x) represents the score assigned to the input image. D(x) is interpreted as the probability that the input image was drawn from the dataset.",
    "input image. D(x) is interpreted as the probability that the input image was drawn from the dataset. Of course, the discriminator D would like to maximize this objective—this corresponds with Generative Adversarial Networks | 245 --- Page 262 --- assigning high probabilities to images drawn from the dataset rather than images produced by the generator G.",
    "high probabilities to images drawn from the dataset rather than images produced by the generator G. G, on the other hand, would like to minimize this objective, since that corresponds with producing realistic images, or even images that look exactly like those from the dataset, that confuse D and cause it to return a high score for these generator-produced images.",
    "the dataset, that confuse D and cause it to return a high score for these generator-produced images. This idea of maximizing the objective for one network and minimizing the objective for the other is termed minimax, and the optimization procedure looks like this: minGmaxDEx ∼ p dataxlogDx+Ez ∼ pzlog 1 −DGz The paper goes on to show that, for a fixed generator G, the optimal discriminator trained under this objective would output the following score: pdatax pdatax+pgx for a given image x.",
    "rained under this objective would output the following score: pdatax pdatax+pgx for a given image x. First, we consider why this should even describe the behav‐ ior of an optimal discriminator given a fixed generator. Before we get into the “why, ” it’s important to keep in mind that D can be alternatively represented as pθy= dataset x, or the discriminator’s belief that the image was drawn from the dataset. Here θ represents the parameters, or weights, of D.",
    "elief that the image was drawn from the dataset. Here θ represents the parameters, or weights, of D. When we perform an update operation such as gradient descent, θ represents the set of weights that is being updated. It is important to keep in mind that this distribution is distinct from py= dataset x mentioned earlier—the latter is the true probability that a given image was sampled from the dataset.",
    "entioned earlier—the latter is the true probability that a given image was sampled from the dataset. The optimal discriminator can never know the exact origin of the image unless it is impossible for the generator to have produced the image, i.e., pgx= 0. We can quantify the uncertainty in the discriminator’s prediction as a function of the image’s likelihood under the data distribution, or pdatax, and the image’s likelihood under the distribution defined by G, or pgx.",
    "ata distribution, or pdatax, and the image’s likelihood under the distribution defined by G, or pgx. If the image’s likelihood under the distribution defined by the generator is less than that of the data distribution, it makes sense that the optimal discriminator should be swayed accordingly and should score the image closer to one than zero. Note that a quick back-of-the-envelope check shows that this prop‐ erty is true for the score pdatax pdatax+pgx.",
    "quick back-of-the-envelope check shows that this prop‐ erty is true for the score pdatax pdatax+pgx. But why is this the exact proportion by which the property is true? Let’s take a more con‐ crete look at the score pdatax pdatax+pgx and determine why this is the optimal function of the two probabilities.",
    "the score pdatax pdatax+pgx and determine why this is the optimal function of the two probabilities. 246 | Chapter 10: Generative Models --- Page 263 --- Taking some inspiration from our discussion regarding the perfectly confused dis‐ criminator, we can alternatively express the proposed optimal discriminator score in terms of conditional probabilities: pxy= dataset pxy= dataset +pxy= generator Additionally, making the same assumption regarding equal likelihood of sampling from the dataset versus sampling from the generator (py= dataset =py= generator = 0 .",
    "od of sampling from the dataset versus sampling from the generator (py= dataset =py= generator = 0 . 5 ), we can get to a much more interpretable representation of the optimal score: D*x=pxy= dataset pxy= dataset +pxy= generator =pxy= dataset *py= dataset pxy= dataset *py= dataset +pxy= generator *py= generator =px,y= dataset px =py= dataset x The denominator in the third equality is a result of having marginalized out y.",
    "et px =py= dataset x The denominator in the third equality is a result of having marginalized out y. The final result is just the conditional probability of having sampled from the dataset given the input image. It makes sense that the optimal discriminator, pθ*y= dataset x, should strive to match the true probability that the input image was drawn from the dataset, py= dataset x.",
    "strive to match the true probability that the input image was drawn from the dataset, py= dataset x. Now, we consider why the minimax objective defined earlier is maximized by py= dataset x, or the true conditional probability of having drawn from the data‐ set given an image x, under the assumption of a fixed generator.",
    "lity of having drawn from the data‐ set given an image x, under the assumption of a fixed generator. Let’s take a closer look at the objective and try to reformulate it in a more informative manner that may provide us with some insight: VG,D=Ex ∼ p dataxlogDx+Ez ∼ pzlog 1 −DGz =Ex ∼ pxy= dataset logpθy= dataset x +Epφxy= generator log 1 −pθy= dataset x =Ex ∼ pxy= dataset logpθy= dataset x +Epφxy= generator logpθy= generator x As usual, we have formulated the objective in terms of conditional probabilities.",
    "ogpθy= generator x As usual, we have formulated the objective in terms of conditional probabilities. To get from the first equality to the second, we note that taking the expectation with respect to the noise distribution p(z) and then applying a function such as G to each Generative Adversarial Networks | 247 --- Page 264 --- sample is equivalent to just taking the expectation with respect to the distribution over the data space defined by G’s mapping.",
    "taking the expectation with respect to the distribution over the data space defined by G’s mapping. This is similar in spirit to a concept we discussed in Chapter 2 , where random variables can be functions of other random variables. Also note the addition of the letter φ starting from the second line—this letter represents the parameters, or weights, of G.",
    "the letter φ starting from the second line—this letter represents the parameters, or weights, of G. Taking a closer look at the final expression, we start to see an awful lot of similarities between the objective and the concepts of entropy and cross entropy introduced in Chapter 2 .",
    "rities between the objective and the concepts of entropy and cross entropy introduced in Chapter 2 . It turns out that we can manipulate the objective slightly without affecting the best θ here to obtain a sum of the negatives of two cross-entropy terms: θ*= argminθVG,D = argminθEx ∼ pxy= dataset logpθy= dataset x +Epφxy= generator logpθy= generator x = argminθ−Hpx,y= dataset ,pθx,y= dataset −Hpx,y= generator ,pθx,y= generator As discussed in Chapter 2 , the cross entropy between two distributions is mini‐ mized when the two distributions are exactly the same—here we are doing the equivalent by simply maximizing the negative cross entropy instead.",
    "y the same—here we are doing the equivalent by simply maximizing the negative cross entropy instead. Thus, θ ach‐ ieves the optimal set of weights θ* when pθx,y= dataset =px,y= dataset and pθx,y= generator =px,y= generator . As our final step, we’ d like to show that at θ*, pθ*y= dataset x=py= dataset x as promised. We already know that pθ*x,y= dataset =px,y= dataset from our prior work. Dividing by p(x) on both sides leaves us with the desired result.",
    "px,y= dataset from our prior work. Dividing by p(x) on both sides leaves us with the desired result. So far, we have assumed a fixed G, and shown various properties regarding the optimal D. Unfortunately, we can’t assume a fixed G in practice, as we must train the generator as well as the discriminator.",
    "we can’t assume a fixed G in practice, as we must train the generator as well as the discriminator. But now that we have shown some properties regarding the optimal D, we can begin to talk about the properties G must satisfy to achieve the global optimum—a generator that can perfectly confuse even the optimal discriminator.",
    "to achieve the global optimum—a generator that can perfectly confuse even the optimal discriminator. If we assume an optimal discriminator and plug in its score pdatax pdatax+pgx to the objective V(G,D), we obtain an objective that is solely dependent on the parameters, or weights, of G: CG=Ex ∼ p dataxlogpdatax pdatax+pgx+Ex ∼ pgxlog 1 −pdatax pdatax+pgx =Ex ∼ p dataxlogpdatax pdatax+pgx+Ex ∼ pgxlogpgx pdatax+pgx 248 | Chapter 10: Generative Models --- Page 265 --- 2Kingma et al.",
    "ax+pgx+Ex ∼ pgxlogpgx pdatax+pgx 248 | Chapter 10: Generative Models --- Page 265 --- 2Kingma et al. “ Auto-Encoding Variational Bayes. ” arXiv Preprint arXiv :1312.6114. 2014.=Ex ∼ pxy= dataset logpxy= dataset pxy= dataset +pφxy= generator +Ex ∼ pφxy= generator logpφxy= generator pxy= dataset +pφxy= generator We can now minimize this objective by optimizing over the generator weights φ. We refer you to the original GAN paper for the rigorous derivation.",
    "ng over the generator weights φ. We refer you to the original GAN paper for the rigorous derivation. However, as one might expect by now, it turns out that the optimal distribution G represents, or pg*x, is equal to pdatax,∀x. This matches our original intuition regarding the perfectly confused discriminator and shows that the objective function proposed in the original GAN paper does indeed theoretically converge to this global optimum.",
    "nction proposed in the original GAN paper does indeed theoretically converge to this global optimum. Now that we have an optimal generator and discriminator, how do we perform image generation? All we need to do is sample from our noise distribution p(z) and run each sample through the generator. The generator, being optimal, should produce images that look as if they were drawn from the dataset itself.",
    "rator, being optimal, should produce images that look as if they were drawn from the dataset itself. It may come as a surprise to you that the discriminator is no longer needed in this phase—but it has served its purpose. The discriminator played a key role in competing with the generator, each evolving until the latter could produce images that perfectly confused the discriminator.",
    "ator, each evolving until the latter could produce images that perfectly confused the discriminator. Note that unlike the standard interpretation of generative modeling, z does not represent a set of latent variables from which the data is generated. z simply plays the role of being a random variable distributed as one of our standard distributions, such as a uniform distribution or a standard multivariate Gaussian distribution, which are easy to sample from.",
    "niform distribution or a standard multivariate Gaussian distribution, which are easy to sample from. G, when fully trained and optimal, is a complex, differentiable function that transforms samples from p(z) into samples from pdatax, which approximates p(x).",
    "tiable function that transforms samples from p(z) into samples from pdatax, which approximates p(x). In the next section, we will see the parallels between G(z) and the reparametrization trick, which also allows us to sample from a distribution by transforming samples (via a differentiable function) from a distribution that is easier to sample from.",
    "nsforming samples (via a differentiable function) from a distribution that is easier to sample from. Variational Autoencoders In parallel to the introduction of GANs, Kingma and Welling introduced the Varia‐ tional Autoencoder , or V AE for short, in their seminal paper, “ Auto-Encoding Var‐ iational Bayes, ” from 2014.2 The idea behind the V AE is more strongly rooted in probabilistic modeling than the aforementioned GAN.",
    "idea behind the V AE is more strongly rooted in probabilistic modeling than the aforementioned GAN. The V AE assumes there exists a set of unobserved latent variables, which we denote as z, that generate the data we see, which we denote as x. More formally, we say there exists a joint probability distribution p(x,z) over the latent variables z and the observed data x that factors as Variational Autoencoders | 249 --- Page 266 --- pxzpz (see Figure 10-2 ).",
    "ved data x that factors as Variational Autoencoders | 249 --- Page 266 --- pxzpz (see Figure 10-2 ). Thinking back to Chapter 2 , this factorization is quite intuitive. Given the predefined roles of z and x, the universe in which z takes on some value and x is generated from this setting of z makes much more sense than the other way around. Figure 10-2. z represents the latent variables from which every instance of x is generated. The arrow pointing from z to x signifies this relationship.",
    "which every instance of x is generated. The arrow pointing from z to x signifies this relationship. x could represent any sort of continuous or discrete data, including images. We additionally know the domain of x due to our knowledge of the dataset. z, on the other hand, is much more elusive. We have no idea what z looks like, so we make some initial assumptions about it. For example, we may assume that it initially takes the form of a Gaussian distribution, i.e., p(z) is Gaussian.",
    ", we may assume that it initially takes the form of a Gaussian distribution, i.e., p(z) is Gaussian. Again, thinking back to Chapter 2 , we say that p(z), or our prior on z, is Gaussian. Whenever we think about such a data-generation process, some natural probabilistic questions (should) come to mind. For example, what is the distribution pzx, or the posterior of z having known x? As we observe data, our beliefs regarding the underlying parameters often change.",
    "z having known x? As we observe data, our beliefs regarding the underlying parameters often change. Take the coin flip experiment from Chapter 2 as an example. We initially assumed a 50-50 chance of flipping heads, where the 50-50 can be thought of as our latent parameter α—the parameter dictating the data generation procedure of sequences of heads and tails.",
    "t parameter α—the parameter dictating the data generation procedure of sequences of heads and tails. This is a little simplified—in reality, we initially have a distribution over α, the probability of flipping heads, which is our prior distribution. Of course, the domain of the prior is the range [0,1], where it is logical to design the prior pα such that pα= 0 . 5 is larger than all other settings of α. As we observed sequences of flips, we updated our prior via Bayes’ Theorem.",
    "all other settings of α. As we observed sequences of flips, we updated our prior via Bayes’ Theorem. In a similar manner, we initially assume p(z) to be a Gaussian distribution with some mean and variance; but as we observe data, we recalculate our belief in the form of a posterior, p(z|x) (see Figure 10-3 ). Another question naturally comes to mind: what is the distribution p(x|z), or the likelihood of the data x given a certain setting of the latent variables z?",
    "tribution p(x|z), or the likelihood of the data x given a certain setting of the latent variables z? In the coin flip setting, p(x|z) is easy to think about. Due to our complete knowledge of the experiment, we know the probability of any sequence is just the product of the probability of each flip, which is directly defined by z. In more intricate settings such as images, however, we can assume the relationship between the data x and the latent variables z is much more complicated than that.",
    "e the relationship between the data x and the latent variables z is much more complicated than that. For example, when looking at images, it is clear that the value of a given pixel is quite affected by the values of its neighboring pixels and sometimes even by pixels much farther than one might think. The simple independence assumption we have for coin flips will not suffice for our purposes.",
    "think. The simple independence assumption we have for coin flips will not suffice for our purposes. This is just one reason why we can’t simply use a method like Bayes’ 250 | Chapter 10: Generative Models --- Page 267 --- Theorem to learn a posterior over z—it requires much more knowledge regarding the system than what is immediately available to us. Figure 10-3. Here we have the coin flip experiment, where the prior is designed such that 0.5 has the highest likelihood.",
    "have the coin flip experiment, where the prior is designed such that 0.5 has the highest likelihood. Once we see a series of heads and tails, the posterior shifts to the right due to there being more heads than tails. In variational autoencoders, we encode these distributions as neural networks, which can be seen as complex, nonlinear functions that can accurately model the relation‐ ships between latent variables z and the observed data x.",
    "ns that can accurately model the relation‐ ships between latent variables z and the observed data x. We denote the neural network that outputs a distribution over the data given a setting of the latent vari‐ ables, also termed the decoder, as pθxz, where θ represents the weights of the neural network. In other words, the setting of θ, in addition to the predetermined architecture of the neural network, completely define the model’s belief of the true distribution pxz.",
    "chitecture of the neural network, completely define the model’s belief of the true distribution pxz. We optimize θ to achieve a setting that is closest to that of the true distribution. We additionally encode the posterior over z, or p(z|x), as a neural network. We denote this neural network, termed the encoder, as qφzx. Similarly to the decoder, we optimize φ to achieve a setting that is closest to that of the true posterior.",
    "ly to the decoder, we optimize φ to achieve a setting that is closest to that of the true posterior. Kingma and Welling made some key observations that made the variational autoen‐ coder a practical means for generative modeling ( Figure 10-4 ). The first was that the evidence lower bound (ELBO for short), which is a lower bound on the true log likelihood of the data px, could be reformulated in a way that allowed for tractable optimization over the encoder and decoder parameters.",
    "formulated in a way that allowed for tractable optimization over the encoder and decoder parameters. The second was a reparametrization trick that enabled the computation of a low variance estimate of the gradient with respect to the parameters of the encoder, φ. Although this may sound like a lot of jargon right now, we will go into each of these key observations in much more detail and concretely motivate the encoder-decoder architecture.",
    "these key observations in much more detail and concretely motivate the encoder-decoder architecture. Variational Autoencoders | 251 --- Page 268 --- Figure 10-4. The overall VAE architecture presented in Kingma and Welling. Note that both z and the image after the decoder are both samples from the encoder distribution and decoder distribution, respectively. Let’s assume we have observed some data x, where each individual example can be denoted as xi.",
    "vely. Let’s assume we have observed some data x, where each individual example can be denoted as xi. Note that we are still under the assumption that there exist some set of latent variables z generating the data we’ve seen. We split our analysis over the observed data into one over each individual example xi. We know there exists a true posterior over the latent variables pzxi, but we have no idea what that true posterior is.",
    "ts a true posterior over the latent variables pzxi, but we have no idea what that true posterior is. We assume it can be approximated by some distribution over the latent variables qφzxi, where q is a family of distributions in which optimization is much easier but complex enough to accurately model the true posterior. An example would be a multilayer neural network, which, as we’ve already seen, can be efficiently optimized via gradient descent and can represent complex, nonlinear functions.",
    "n, can be efficiently optimized via gradient descent and can represent complex, nonlinear functions. Note that each example xi we’ve seen has some true probability of occurrence, which we can write as pxi.",
    "that each example xi we’ve seen has some true probability of occurrence, which we can write as pxi. We instead work with logpxi, since this allows us to do some convenient decomposition into terms we’ve encountered before and doesn’t affect the verity of the optimization process: logpxi= logpxi,z− logpzxi = logpxi,z− logpzxi+ logqφzxi− logqφzxi =Eqφzxilogpxi,z− logpzxi+ logqφzxi− logqφzxi =Eqφzxilogpxi,z qφzxi+Eqφzxilogqφzxi pzxi = ELBO + KL qφzxipzxi The first step is to express the marginal likelihood of the individual example xi as a function of the example itself and the latent factors z.",
    "ikelihood of the individual example xi as a function of the example itself and the latent factors z. As we learned earlier, the marginal likelihood can be broken down into a quotient of the joint distribution pz,xi and the conditional distribution pzxi. The log function allows us to 252 | Chapter 10: Generative Models --- Page 269 --- separate this quotient into a difference between the logs of the two terms.",
    "Models --- Page 269 --- separate this quotient into a difference between the logs of the two terms. In the second step, we use a little trick that allows us to conveniently insert the approximate posterior into the equality—adding and subtracting the same term shouldn’t affect the equality. In the third step, we insert an expectation with respect to the approx‐ imate posterior. Why is this allowed? Well, a priori we know that logpxi is a constant.",
    "the approx‐ imate posterior. Why is this allowed? Well, a priori we know that logpxi is a constant. It is just the log of the probability of the example occurring under the true distribution, which is fixed. Thus, taking the expectation on both sides doesn’t change anything about the left side of the equation, since the expectation of a constant is just the constant itself.",
    "bout the left side of the equation, since the expectation of a constant is just the constant itself. On the right side, we have now gotten closer to expressing the log of the marginal likelihood in terms that we’ve seen before. In the second to last step, we combine logs back into quotients and use the linearity of expectation to arrive at a sum of two terms: (1) the KL divergence between the approximate posterior and the true posterior, and (2) the ELBO, or the evidence lower bound.",
    "een the approximate posterior and the true posterior, and (2) the ELBO, or the evidence lower bound. By now, you may have noticed that the form of the KL divergence is slightly different than what we encountered in Chapter 2 . Recall the standard KL divergence presented earlier, where the true distribution was p(x) and its approximation was q(x).",
    "L divergence presented earlier, where the true distribution was p(x) and its approximation was q(x). The KL divergence we defined was the difference between the cross entropy of the two distributions and the entropy of the true distribution, which was expressed as follows: Epxlogpx qx We can see that the KL divergence in this derivation is the exact opposite.",
    "as follows: Epxlogpx qx We can see that the KL divergence in this derivation is the exact opposite. The expectation is with respect to the approximate posterior rather than the true poste‐ rior, and the numerator and denominator are flipped. Essentially what we see is Eqxlogqx px instead of Epxlogpx qx. We call this the reverse KL divergence, since the roles of the model and the truth have been switched, and is the quantity we attempt to minimize in V AEs.",
    "of the model and the truth have been switched, and is the quantity we attempt to minimize in V AEs. Although this does not have as clean a physical interpretation as the standard KL, note that the reverse KL divergence is just a type of KL divergence and retains all the properties we discussed in Chapter 2 .",
    "ivergence is just a type of KL divergence and retains all the properties we discussed in Chapter 2 . Thus, optimizing the reverse KL divergence still achieves a unique global minimum of zero when qx=px,∀x, so it is a valid objective to be optimizing over as it reaches its unique minimum when the approximate posterior is exactly the same as the true posterior. The reality, however, is that the true posterior pzxi is still unknown to us.",
    "as the true posterior. The reality, however, is that the true posterior pzxi is still unknown to us. As a result, we can’t directly minimize any KL divergence with the true posterior. This is where the ELBO plays a key role. As we discussed earlier, logpxi is a constant. Thus, minimizing the reverse KL divergence is the same as maximizing the ELBO.",
    "logpxi is a constant. Thus, minimizing the reverse KL divergence is the same as maximizing the ELBO. The name evidence lower bound should make more sense now—as we maximize this term, it provides a better and better lower bound on the true log probability of the example. If we can develop a methodology for maximizing the ELBO efficiently, we Variational Autoencoders | 253 --- Page 270 --- should be well on our way to developing a generative model.",
    "onal Autoencoders | 253 --- Page 270 --- should be well on our way to developing a generative model. Let’s reformulate the ELBO into terms that might be easier to work with: Eqφzxilogpxi,z qφzxi=Eqφzxilogpxi,z− logqφzxi =Eqφzxilogpxiz+ logpz− logqφzxi =Eqφzxilogpxiz+Eqφzxilogpz− logqφzxi =Eqφzxilogpxiz−Eqφzxilogqφzxi pz = −KLqφzxipz+Eqφzxilogpxiz At this point, we can start to see the beginnings of an architecture and an optimiza‐ tion procedure for maximizing the ELBO.",
    "rt to see the beginnings of an architecture and an optimiza‐ tion procedure for maximizing the ELBO. For example, the first term is just the reverse KL divergence between the approximate posterior and the prior, which we already assumed to be a Gaussian distribution. We can use a neural network, or encoder, to represent the approximate posterior.",
    "ssian distribution. We can use a neural network, or encoder, to represent the approximate posterior. The reverse KL divergence acts as a regularization term on the approximate posterior, since maximizing the negative of the reverse KL is the same as minimizing the reverse KL. Regularization prevents the approximate posterior from straying too far from the prior distribution.",
    "Regularization prevents the approximate posterior from straying too far from the prior distribution. This is desirable since we have witnessed only a single example, and thus we don’t want our belief over the latent variables to shift too much from our prior. The second term is the expected true log likelihood of the example given a setting of latent variables z, where z is sampled from the approximate posterior. Wanting to maximize this quantity with respect to φ is intuitively reasonable.",
    "pproximate posterior. Wanting to maximize this quantity with respect to φ is intuitively reasonable. This influences the approximate posterior to assign higher likelihoods to settings of z that, in turn, explain the input example xi as well as possible.",
    "higher likelihoods to settings of z that, in turn, explain the input example xi as well as possible. The balancing act between regularization, which prevents overfitting, and maximum likelihood estimation, which on its own would reach an optimum where qφzxi is just a point mass over the setting of z that best describes xi, is a classic optimization procedure you’ve likely encountered in many data science and machine learning problems.",
    "optimization procedure you’ve likely encountered in many data science and machine learning problems. However, as noted earlier, we unfortunately don’t have access to the true conditional distribution pxz. Instead, we attempt to learn it using a second neural network— the decoder. We denote the parameters of the decoder as θ and let the decoder repre‐ sent the distribution pθxz.",
    "We denote the parameters of the decoder as θ and let the decoder repre‐ sent the distribution pθxz. In summary, we perform the following optimization procedure: φ*,θ*= argmax φ,θ−KLqφzxipz+Eqφzxilogpθxiz 254 | Chapter 10: Generative Models --- Page 271 --- We’ve already discussed why this is a valid optimization procedure for the encoder parameters φ, assuming that pθxz=pxz. Of course, this assumption is not satisfied at the beginning of training.",
    "φ, assuming that pθxz=pxz. Of course, this assumption is not satisfied at the beginning of training. However, as training progresses and θ becomes more and more optimal, we eventually arrive at the desired theoretical optimization. But the question still remains: why is this a valid optimization procedure for θ? If we assume the encoder represents the true posterior distribution, we’ d want to maximize the likelihood of recovering the original example xi from our encoder samples z.",
    "d want to maximize the likelihood of recovering the original example xi from our encoder samples z. Of course, just like the optimization of φ, our assumption about the approximate posterior is not satisfied at the beginning of training—but as training progresses and two networks improve jointly, we hope to eventually reach our goal. This leads us into how to actually carry out the optimization.",
    "we hope to eventually reach our goal. This leads us into how to actually carry out the optimization. For θ, it turns out we can use standard minibatch gradient descent techniques directly: ∇θ−KLqφzxipz+Eqφzxilogpθxiz =∇θ−KLqφzxipz+∇θEqφzxilogpθxiz =∇θEqφzxilogpθxiz =Eqφzxi∇θlogpθxiz ≈1 n∑j= 1n∇θlogpθxiz=zj The first equality arises from the fact that the gradient of a sum of terms is equal to the sum of the gradients of each of the terms.",
    "fact that the gradient of a sum of terms is equal to the sum of the gradients of each of the terms. Since the first term is not a function of θ, its gradient with respect to θ is 0, leading us to the second equality. From there we have the standard minibatch gradient estimate derivation. The optimization with respect to φ is not as simple.",
    "tandard minibatch gradient estimate derivation. The optimization with respect to φ is not as simple. If we try to do the same for φ as we did for θ, we run into an unforeseen issue: ∇φ−KLqφzxipz+Eqφzxilogpθxiz =∇φ−KLqφzxipz+∇φEqφzxilogpθxiz =∇φ−KLqφzxipz+∇φ∫qφzxilogpθxizdz =∇φ−KLqφzxipz+ ∫∇φqφzxilogpθxizdz In the last step, we can’t express the second term as an expectation.",
    "−KLqφzxipz+ ∫∇φqφzxilogpθxizdz In the last step, we can’t express the second term as an expectation. This is because the gradient is with respect to the parameters of the distribution from which we are Variational Autoencoders | 255 --- Page 272 --- sampling. We can’t simply switch the order of the expectation and gradient as we did for θ.",
    "272 --- sampling. We can’t simply switch the order of the expectation and gradient as we did for θ. To get around this, we make the following observation: ∇φqφzxi=∇φqφzxi*qφzxi qφzxi =qφzxi*∇φqφzxi qφzxi =qφzxi∇φlogqφzxi With a bit of calculus and algebra, we have derived an equivalent form for the gradient.",
    "i∇φlogqφzxi With a bit of calculus and algebra, we have derived an equivalent form for the gradient. If we substitute this reformulation into the step we were stuck on: =∇φ−KLqφzxipz+ ∫qφzxi∇φlogqφzxilogpθxizdz =∇φ−KLqφzxipz+Eqφzxi∇φlogqφzxilogpθxiz ≈∇φ−KLqφzxipz+1 n∑j= 1n∇φlogqφz=zjxilogpθxiz=zj We can now use standard minibatch gradient estimation techniques to optimize our objective with respect to φ.",
    "w use standard minibatch gradient estimation techniques to optimize our objective with respect to φ. The observation we made is a well-known technique in the machine learning community termed the log trick. We will see this technique used again later in the chapter on reinforcement learning when we introduce the policy gradient method.",
    "d again later in the chapter on reinforcement learning when we introduce the policy gradient method. Now that we have fully dissected the first observation that Kingma and Welling made, we now move to the second: the computation of a low variance estimate of the gradient with respect to φ. As we mentioned earlier, the log trick allows us to estimate this gradient. However, this estimate has been shown to be of high variance.",
    "k allows us to estimate this gradient. However, this estimate has been shown to be of high variance. This means that if we were to run trials where, in each trial, we draw a few samples zj from the approximate posterior and estimate the gradient with respect to φ, we would expect to see vastly different estimates of the gradient across trials.",
    "with respect to φ, we would expect to see vastly different estimates of the gradient across trials. Of course, this is undesirable, as we’ d like trials for the same input example to be consistent with each other to have any confidence in our training procedure. We could try to ameliorate this by drawing many samples from the approximate posterior for each example, but this becomes computationally prohibitive for relatively little gain.",
    "posterior for each example, but this becomes computationally prohibitive for relatively little gain. Kingma and Welling proposed an alternative method to the log trick for getting around the issue of taking the gradient with respect to the weights of the network parametrizing distribution from which we are sampling. This method is called the reparametrization trick , and it allows us to compute a low variance estimate of the gradient, as opposed to the log trick.",
    ", and it allows us to compute a low variance estimate of the gradient, as opposed to the log trick. Why this is the case is beyond the scope of this 256 | Chapter 10: Generative Models --- Page 273 --- text, but we refer you to the vast amount of academic literature that exists on this and similar topics.",
    ", but we refer you to the vast amount of academic literature that exists on this and similar topics. The reparametrization trick involves assuming the approximate posterior takes on some form, such as a multivariate Gaussian distribution, and then expressing this distribution as a function of another distribution that has no dependence on the weights of the encoder. Let’s assume that qφzxi takes on the form Nz;μφ,σφ2I.",
    "s no dependence on the weights of the encoder. Let’s assume that qφzxi takes on the form Nz;μφ,σφ2I. This represents a multivariate Gaussian distribution where each component zi is independent of all other components and zi∼ Nμφ,i,σφ,i2, ∀i. We use φ in the sub‐ script to explicitly show the approximate posterior’s dependence on the parameters of the encoder through its mean and variance vectors, which are defined by the encoder.",
    "e parameters of the encoder through its mean and variance vectors, which are defined by the encoder. In its current form, we run into the issue of not being able to switch the order of the expectation and the gradient that we encountered earlier. Using the reparametrization trick, we can rewrite the sampling procedure as: z ∼ Nμφ,σφ2Iz=μφ+σφ*ϵ,ϵ ∼ N 0,I We highly encourage you to work out why the sampling procedure can be rewritten in this manner using the definition of the Gaussian distribution.",
    "ampling procedure can be rewritten in this manner using the definition of the Gaussian distribution. It will be easier to consider the univariate case first, where X is a standard Gaussian random variable, and then show Y = c*X is a Gaussian random variable with mean zero and variance c2. Then, consider the general univariate case where X is any Gaussian random variable, and show Y = X + c is a Gaussian random variable with mean E[X] + c and variance Var(X) .",
    "variable, and show Y = X + c is a Gaussian random variable with mean E[X] + c and variance Var(X) . Putting these steps together will get you to the reformulated sampling procedure described previously. In summary, we have expressed the approximate posterior as a function of a distribu‐ tion that is independent of φ, along with a mean vector and a standard deviation vector that are dependent on φ. We term the random variable ϵ an auxiliary random variable.",
    "eviation vector that are dependent on φ. We term the random variable ϵ an auxiliary random variable. Plugging this reformulation into our troublesome gradient expression from earlier: ∇φEqφzxilogpθxiz =∇φEϵ ∼ N 0,Ilogpθxigφϵ =Eϵ ∼ N 0,I∇φlogpθxigφϵ ≈1 n∑j= 1n∇φlogpθxigφϵj Where gφϵ=μφ+σφ*ϵ.",
    "ilogpθxiz =∇φEϵ ∼ N 0,Ilogpθxigφϵ =Eϵ ∼ N 0,I∇φlogpθxigφϵ ≈1 n∑j= 1n∇φlogpθxigφϵj Where gφϵ=μφ+σφ*ϵ. We rewrote z as gφϵ to explicitly show that the depend‐ ence on the encoder parameters is now only through the deterministic function Variational Autoencoders | 257 --- Page 274 --- applied to the sampling distribution, rather than the sampling distribution itself.",
    "--- Page 274 --- applied to the sampling distribution, rather than the sampling distribution itself. This allows us to switch the order of the expectation and the gradient seamlessly, thereby lending it to standard minibatch gradient estimation techniques. How does this change manifest itself in the encoder architecture? Earlier, when using the log trick, we could directly parametrize the approximate posterior via the encoder.",
    ", when using the log trick, we could directly parametrize the approximate posterior via the encoder. Now, we instead have the encoder, for each example xi, output a vector of means μφ, a vector of standard deviations σφ, and sample ϵ from a standard Gaussian distribution that is completely separate from the encoder-decoder V AE architecture.",
    "andard Gaussian distribution that is completely separate from the encoder-decoder V AE architecture. Note that the reparametrization technique comes with its own restrictions—we must assume a form for the approximate posterior, in this case a Gaussian, that allows us to define a differentiable function such as gφ. However, there’s no guarantee the true posterior is Gaussian—it is most likely a complex distribution that cannot be repre‐ sented as functions of our standard distributions.",
    "kely a complex distribution that cannot be repre‐ sented as functions of our standard distributions. This is a trade-off we must make to achieve a low variance gradient estimate for tractable optimization ( Figure 10-5 ). Figure 10-5. What the encoder looks like after the inclusion of reparametrization. It returns a mean and standard deviation vector, which we can combine with ϵ to generate the setting of z.",
    "urns a mean and standard deviation vector, which we can combine with ϵ to generate the setting of z. The purpose of the circle versus rectangles is to show that the only sampling is happening for ϵ, completely independent of the encoder architecture. The mean and standard deviation vectors are produced deterministically from the input image. In addition, z is deterministic once we know the value of ϵ.",
    "deterministically from the input image. In addition, z is deterministic once we know the value of ϵ. Note that the training procedure for a V AE is quite simple—the beast was in the motivation and mathematics behind the architecture and optimization. All we need to do is: 1.Sample an example xi from the dataset. 1. 2.Run xi through the encoder network to generate a vector of means μφ and a 2. vector of standard deviations σφ. 3.Sample ϵ and calculate the result of gφϵ. 3.",
    "f means μφ and a 2. vector of standard deviations σφ. 3.Sample ϵ and calculate the result of gφϵ. 3. 258 | Chapter 10: Generative Models --- Page 275 --- 4.Run the result through the decoder network, which now represents the distribu‐4. tion pθxz=gφϵ. 5.Query this distribution with our initial example xiand take the log of the 5. resulting likelihood. This will be our decoder loss.",
    "ur initial example xiand take the log of the 5. resulting likelihood. This will be our decoder loss. If you took multiple samples of ϵ in step 3, run the above procedure for each sample, and average to get the decoder loss. 6.Sum the decoder loss with −KLqφzxipz, the encoder loss, to get a final 6. loss. Use the negative of the final loss in the next step since we want to maximize it instead of minimize it. 7.Perform classical SGD/minibatch gradient descent to update φ and θ. 7.",
    "it instead of minimize it. 7.Perform classical SGD/minibatch gradient descent to update φ and θ. 7. Now that we have covered how to train a V AE, how do we utilize it as a generative model once it is trained? Note that we initially defined the generative process as pxzpz, where we start with some setting of the latent variables z sampled from the prior distribution and map z to an instance x in the data space via the conditional likelihood.",
    "the prior distribution and map z to an instance x in the data space via the conditional likelihood. We’ve already learned this generative process in the form of pθxz, or the decoder, and assumed the prior distribution pz to be a multivariate standard Gaussian at the beginning. To generate samples from a V AE, we sample zi from the prior distribution p(z), pass this sample through the decoder so it now represents the distribution pθxz=zi, and finally sample xi from pθxz=zi.",
    "rough the decoder so it now represents the distribution pθxz=zi, and finally sample xi from pθxz=zi. Note that we no longer need the approximate posterior at this step—however, it played a key role in the training of the decoder and is still useful in understanding how our latent variable distribution shifts after witnessing an example from the dataset. Implementing a VAE In this section, we will build a V AE from scratch in PyTorch.",
    "from the dataset. Implementing a VAE In this section, we will build a V AE from scratch in PyTorch. We will additionally provide some example training and testing code on the famous MNIST digits dataset.",
    "will additionally provide some example training and testing code on the famous MNIST digits dataset. Before we begin, here is a list of the packages you will need to reproduce this section on your own: import torch from torch.distributions.multivariate_normal \\ import MultivariateNormal import torch.nn as nn from torchvision import datasets, transforms from torchvision.utils import save_image import torch.optim as optim Let’s start with the encoder.",
    "s from torchvision.utils import save_image import torch.optim as optim Let’s start with the encoder. As we discussed in the previous section, the encoder is a neural network that outputs a vector of means and a vector of standard deviations. Each index represents a univariate Gaussian, and the entire vector represents a mul‐ tivariate Gaussian where each component is independent from the others.",
    "ire vector represents a mul‐ tivariate Gaussian where each component is independent from the others. Though Implementing a VAE | 259 --- Page 276 --- we are working with image data, for the sake of simplicity we convert each image into a vector by flattening it at the start. This allows us to apply standard, fully connected layers on the input. Since each image in the MNIST dataset is of size 28 × 28, each resulting representation is a 784-dimensional vector.",
    "in the MNIST dataset is of size 28 × 28, each resulting representation is a 784-dimensional vector. We also need to decide on the number of components, or latent variables, we will use to represent the latent space. We can treat the number of components as a hyperparameter—if we notice that the decoder log likelihoods of input examples are consistently low even after a significant amount of training, this may indicate an approximate posterior that is not expressive enough.",
    "ficant amount of training, this may indicate an approximate posterior that is not expressive enough. Increasing the number of components and retraining in this case is advisable. Here is example code for an encoder: # Encoder layers (Gaussian MLP) D_in, H, D_out = 784, 200, 20 input_layer = nn.Linear(D_in, H) hidden_layer_mean = nn.Linear(H, D_out) hidden_layer_var = nn.Linear(H, D_out) For the sake of simplicity, we leave out nonlinearities between the layers for now.",
    "Linear(H, D_out) For the sake of simplicity, we leave out nonlinearities between the layers for now. Our encoder consists of two levels of layers. The first level operates on the input, embed‐ ding the vector into a lower dimensional representation.",
    "e first level operates on the input, embed‐ ding the vector into a lower dimensional representation. The second level operates on the 200-d representation and consists of two independent layers: one for determining the means of each of the univariate Gaussian components, and one for determining the standard deviations of each of the univariate Gaussian components. Here, we use 20 components.",
    "g the standard deviations of each of the univariate Gaussian components. Here, we use 20 components. As we stated earlier, we assume qφzxi takes the form of a multivariate Gaussian, where each component is independent of the others. Note that attempting to learn a full covariance matrix is computationally prohibitive (amongst other concerns), as its size grows quadratically with the number of components.",
    "prohibitive (amongst other concerns), as its size grows quadratically with the number of components. Here is example code for a decoder: # Decoder layers (Bernoulli MLP for MNIST data) recon_layer = nn.Linear(D_out, H) recon_output = nn.Linear(H, D_in) Again, we leave out the nonlinearities for the sake of simplicity. The decoder operates on the sampled z, which we know is a 20-d vector.",
    "s for the sake of simplicity. The decoder operates on the sampled z, which we know is a 20-d vector. The rest of the decoder architec‐ ture is symmetrical to the encoder, and outputs a distribution over the input data. Although not in the code just yet, there is a final sigmoid layer that will be applied to the output of the recon_output layer which, recall, squashes each input dimension into the range (0,1).",
    "output of the recon_output layer which, recall, squashes each input dimension into the range (0,1). Since we are working with the discrete MNIST dataset where each pixel is represented as either a zero or a one, the output of the final sigmoid layer is used to represent a Bernoulli distribution for each pixel. Recall the Bernoulli distribution from Chapter 2 , represented as Ber(p), where p is the probability of returning a one and 1 – p is the the probability of returning a zero.",
    "where p is the probability of returning a one and 1 – p is the the probability of returning a zero. 260 | Chapter 10: Generative Models --- Page 277 --- More formally, we have that the decoder likelihood distribution pθxz can be rewritten as a product over each pixel: pθxz= ∏j= 1784pθxjz where pxjz=Ber decoderzj Note that decoder( z) represents the 784-d vector after applying the sigmoid layer.",
    "jz=Ber decoderzj Note that decoder( z) represents the 784-d vector after applying the sigmoid layer. For a given pixel xji in the input example xi, we’ d like its corresponding probability p to be close to one if xji= 1, and its corresponding probability p to be close to zero if xji= 0. As you may recall from the previous section, we work with logpθxz, which reduces to ∑j= 1784logpθxjz.",
    "s you may recall from the previous section, we work with logpθxz, which reduces to ∑j= 1784logpθxjz. Now, we can put the encoder and decoder together into a single V AE architecture: class VAE(nn.Module): def __init__(self, D_in, H, D_out): super(VAE, self).__init__() self.D_in, self.H, self.D_out = D_in, H, D_out # Encoder layers (Gaussian MLP) self.input_layer = nn.Linear(D_in, H) self.hidden_layer_mean = nn.Linear(H, D_out) self.hidden_layer_var = nn.Linear(H, D_out) # Decoder layers (Bernoulli MLP for MNIST data) self.recon_layer = nn.Linear(D_out, H) self.recon_output = nn.Linear(H, D_in) self.tanh = nn.Tanh() self.sigmoid = nn.Sigmoid() def encode(self, inp): h_vec = self.input_layer(inp) h_vec = self.sigmoid(h_vec) means = self.hidden_layer_mean(h_vec) log_vars = self.hidden_layer_var(h_vec) return means, log_vars def decode(self, means, log_vars): # Reparametrization trick std_devs = torch.pow(2,log_vars)**0.5 aux = MultivariateNormal(torch.zeros(self.D_out), \\ torch.eye(self.D_out)).sample() sample = means + aux * std_devs # Reconstruction h_vec = self.recon_layer(sample) h_vec = self.tanh(h_vec) output = self.sigmoid(self.recon_output(h_vec)) Implementing a VAE | 261 --- Page 278 --- return output def forward(self, inp): means, log_vars = self.encode(inp) output = self.decode(means, log_vars) return output, means, log_vars def reconstruct(self, sample): h_vec = self.recon_layer(sample) h_vec = self.tanh(h_vec) output = self.sigmoid(self.recon_output(h_vec)) return output The call to encode is followed by the call to decode in the forward function.",
    "(h_vec)) return output The call to encode is followed by the call to decode in the forward function. Note that decode uses only a single sample from the approximate posterior, as we found that a single sample is sufficient for the MNIST dataset, but this can be easily modified to work for multiple samples. To calculate the reverse KL, the forward function returns the results of the encode call in addition to the decoder likelihood distribution.",
    "function returns the results of the encode call in addition to the decoder likelihood distribution. Here is example code for computing the loss: def compute_loss(inp, recon_inp, means, log_vars): # Calculate reverse KL divergence # (formula provided in Kingma and Welling) kl_loss = -0.5 * torch.sum(1 + log_vars - means ** 2 - torch.pow(2,log_vars)) # Calculate BCE loss loss = nn.BCELoss(reduction=\"sum\") recon_loss = loss(recon_inp, inp) return kl_loss + recon_loss We recommend you take a look at the PyTorch documentation for nn.BCELoss and verify that it is indeed computing the negative log likelihood of the input example xi: −∑j= 1784logpθxjiz.",
    "that it is indeed computing the negative log likelihood of the input example xi: −∑j= 1784logpθxjiz. We also recommend you verify that the kl_loss term is the reverse KL divergence between two Gaussian distributions as derived in Kingma and Welling. Returning the sum of the negative log likelihood and the reverse KL divergence as a final loss term gets us to the end of step 6 from the previous section.",
    "e reverse KL divergence as a final loss term gets us to the end of step 6 from the previous section. Finally, for some training code: D_in, H, D_out = 784, 500, 20 vae = VAE(D_in, H, D_out) vae.to(\"cpu\") def train(): vae.train() optimizer = optim.Adam(vae.parameters(), lr=1e-3) train_loader = torch.utils.data.DataLoader( datasets.MNIST('../data', train=True, 262 | Chapter 10: Generative Models --- Page 279 --- download=True, transform=transforms.ToTensor()), batch_size=100, shuffle=True) epochs = 10 for epoch in range(epochs): for batch_idx, (data, _) in enumerate(train_loader): optimizer.zero_grad() data = data.view((100,784)) output, means, log_vars = vae(data) loss = compute_loss(data, output, means, log_vars) loss.backward() optimizer.step() if (batch_idx * len(data)) % 10000 == 0: print( 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}' \\ .format( epoch, batch_idx * len(data), len(train_loader.dataset), 100.",
    "} ({:.0f}%)]\\tLoss: {:.6f}' \\ .format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) torch.save(vae.state_dict(), \"vae.%d\" % epoch) Here, we train the V AE for 10 epochs, saving the state of the V AE at the end of each epoch. Note that we set some hyperparameters fixed here, such as the learning rate of the optimizer and the number of latent variables.",
    "arameters fixed here, such as the learning rate of the optimizer and the number of latent variables. We recommend writing some validation code, in addition to the training code presented here, to select the best hyperparameter settings. Finally, how can we test the generative capabilities of our fully trained V AE?",
    "rparameter settings. Finally, how can we test the generative capabilities of our fully trained V AE? We know that the generative process can be written as p(z)p(x|z), where we first draw a sample zj from our prior, run the sample through the decoder so the decoder’s likelihood distribution now represents pθxz=zj, and sample xj from this distribution.",
    "the decoder’s likelihood distribution now represents pθxz=zj, and sample xj from this distribution. Here is the code that puts this logic into action: def test(): dist = MultivariateNormal(torch.zeros(D_out), torch.eye(D_out)) vae = VAE(D_in, H, D_out) vae.load_state_dict(torch.load(\"vae.%d\" % 9)) vae.eval() outputs = [] for i in range(100): sample = dist.sample() outputs.append(vae.reconstruct(sample).view((1,1,28,28))) outputs = torch.stack(outputs).view(100,1,28,28) save_image(outputs, \"prior_reconstruct_100.png\", nrow=10) The for loop generates 100 samples from the approximate posterior, and for each of those samples, a sample from the corresponding decoder likelihood distribution over Implementing a VAE | 263 --- Page 280 --- the input data.",
    "nding decoder likelihood distribution over Implementing a VAE | 263 --- Page 280 --- the input data. The last couple of lines of code allow us to save the samples in a 10 × 10 grid, depicted in Figure 10-6 . Figure 10-6. 100 samples from a VAE trained on the MNIST dataset for 10 epochs. Though the images are a bit blurry, we can make out digits in most of the samples.",
    "et for 10 epochs. Though the images are a bit blurry, we can make out digits in most of the samples. With more complex architectures such as RNNs, hyperparameter tuning, and longer training times, we will surely see even better results. In the next section, we intro‐ duce a slightly different take on generative models that has recently been achieving popularity.",
    "ro‐ duce a slightly different take on generative models that has recently been achieving popularity. Score-Based Generative Models In this section, we approach generative modeling through a slightly different lens than what we have encountered so far.",
    "approach generative modeling through a slightly different lens than what we have encountered so far. In an optimally trained GAN, we first sample from some noise distribution p(z) and run this sample zi through a generator G, which deterministically transforms zi into a sample xi from the true data distribution (where we approximate the true data distribution p(x) using our dataset, pdatax).",
    "data distribution (where we approximate the true data distribution p(x) using our dataset, pdatax). Though G itself is a deterministic function, G(z) is a random variable distributed as the true data distribution. In summary, we have implicitly defined a distribution over our domain via the generator’s action on samples from p(z), and a way of sampling from the true data distribution via a simpler distribution p(z), such as a multivariate Gaussian distribution.",
    "rue data distribution via a simpler distribution p(z), such as a multivariate Gaussian distribution. 264 | Chapter 10: Generative Models --- Page 281 --- V AEs are more explicit in their probabilistic modeling. We define z to be a set of latent variables that generate the data we see, x. We explicitly learn a conditional distribution over the data pθxz via the decoder, which we can sample from. In an optimally trained V AE, pθxz=pxz, is the true conditional likelihood of the data.",
    "sample from. In an optimally trained V AE, pθxz=pxz, is the true conditional likelihood of the data. To generate data using an optimally trained V AE, we first sample a setting of the latent variables from p(z) and run this sample zi through the decoder, which now parametrizes the distribution pxz=zi. This is an explicit probability distribution we can now sample from.",
    "trizes the distribution pxz=zi. This is an explicit probability distribution we can now sample from. Note that although GANs and V AEs themselves are quite distinct, both of their architectures and actions involve an additional distribution p(z) ( whether that is a noise distribution in GANs or a prior over latent variables in V AEs). Is there a way of sampling from the true data distribution without the additional distribution? Score-based generative models attempt to do just that.",
    "ribution without the additional distribution? Score-based generative models attempt to do just that. One method of sampling from a probability distribution is an iterative process called Langevin dynamics. This process is actually an instance of a class of algo‐ rithms referred to as Markov Chain Monte Carlo (MCMC) algorithms.",
    "ly an instance of a class of algo‐ rithms referred to as Markov Chain Monte Carlo (MCMC) algorithms. Motivating MCMC algorithms and proving why they sample from probability distributions in an unbiased manner are beyond the scope of this section, but we refer you to the vast amount of academic literature that exists on this topic.",
    "this section, but we refer you to the vast amount of academic literature that exists on this topic. Langevin dynamics follows the process defined as follows: xi+ 1=xi+η∇xlogpxi+2ηϵ,ϵ ∼ N 0,I xi here represents a sample from p(x), and this dynamics equation shows us how to generate the next sample xi+ 1 given our current sample.",
    "and this dynamics equation shows us how to generate the next sample xi+ 1 given our current sample. Note that if we were to remove the Gaussian noise component at the end of the dynamics equation, we would just be following the gradient to a maximum of p(x), i.e., performing gradient ascent with some step-size η.",
    "following the gradient to a maximum of p(x), i.e., performing gradient ascent with some step-size η. The intuition behind this dynamics equation is that the addition of the noise component prevents us from simply reaching the maximum x and instead allows us to explore regions with high probability, thereby exploring regions of low probability less ( Figure 10-7 ).",
    "re regions with high probability, thereby exploring regions of low probability less ( Figure 10-7 ). Again, why this produces samples from p(x) in an unbiased manner is beyond the scope of this text, but we highly encourage you to learn more from the academic literature. Score-Based Generative Models | 265 --- Page 282 --- Figure 10-7. We use f here to represent a Gaussian distribution with mean (and also maximum) at the origin. Each of the contours represents locations with equal likelihood.",
    "n (and also maximum) at the origin. Each of the contours represents locations with equal likelihood. As we can see from the diagram, the gradient points directly toward the maximum, but adding a bit of noise allows us to explore and sample from high-density regions without converging to the maximum.",
    "f noise allows us to explore and sample from high-density regions without converging to the maximum. Although we use the gradient of the log probability instead of the gradient of the probability, the value(s) of x that maximizes logpx is the same as the value(s) of x that maximizes p(x) due to the log’s concaveness. More generally, the log’s concaveness also preserves the ordering relationships between all possible values of x, i.e., if px1≥px2, then logpx1≥ logpx2, and vice versa.",
    "lationships between all possible values of x, i.e., if px1≥px2, then logpx1≥ logpx2, and vice versa. For that reason, as we saw in “Implementing a V AE” on page 259 , these sorts of optimization processes tend to not be affected meaningfully by the inclusion of the log. However, the main issue with Langevin dynamics, as we’ve encountered before with other generative models, is that we don’t know p(x), let alone the gradient of its log!",
    "before with other generative models, is that we don’t know p(x), let alone the gradient of its log! But there may be a way to model ∇xlogpx, which we call px’s score function , directly. This would allow us to simply plug the score directly into the Langevin 266 | Chapter 10: Generative Models --- Page 283 --- dynamics equation and draw samples from p(x) as if we knew p(x) all along. This is the idea of score-based generative modeling.",
    "samples from p(x) as if we knew p(x) all along. This is the idea of score-based generative modeling. For a moment, let’s forget the problem of sampling from an unknown distribution p(x) and instead consider the problem of learning p(x). From now until the end of this section, we will consider only the problems of learning and sampling from continuous probability distributions.",
    "will consider only the problems of learning and sampling from continuous probability distributions. In the same vein of explicitly learning approxi‐ mate probability distributions like in V AEs, we can try to approximate p(x) with a learned version pθx, where θ represents the parameters of the learned model. What we envision is a learned function, such as a neural network, that takes as input an example x and outputs a likelihood pθx.",
    "d function, such as a neural network, that takes as input an example x and outputs a likelihood pθx. However, there is no way to ensure that ∫pθxdx= 1, which is a necessary condition of any probability distribution. Instead, we settle for learning what we call an unnormalized probability distribution qθx. This is a function that takes an example x and outputs an unnormalized likeli‐ hood. We can, in theory, represent the normalized probability distribution pθx via qθx Zθ, where Zθ= ∫qθxdx.",
    "can, in theory, represent the normalized probability distribution pθx via qθx Zθ, where Zθ= ∫qθxdx. Unfortunately, this integral is generally intractable and has no closed form solution. Of course, there are exceptions to the rule. For example, Zθ=σ*2π for a univariate Gaussian distribution, where θ=μ,σ are the mean and standard deviation of the Gaussian.",
    "a univariate Gaussian distribution, where θ=μ,σ are the mean and standard deviation of the Gaussian. But if we’ d like to model more expressive distributions via a neural network, for example, it is almost always impossible to tractably calculate Zθ, which we will also refer to as the partition function. How can we go about learning such an unnormalized probability distribution?",
    "the partition function. How can we go about learning such an unnormalized probability distribution? Researchers have presented many approaches for learning qθx throughout the history of machine learning and inference, but one particular method starts to bridge the gap between learning an unnormalized probability distribution and sampling from its normalized version via a process like Langevin dynamics.",
    "bability distribution and sampling from its normalized version via a process like Langevin dynamics. Score matching , or the idea of learning qθx via minimizing the difference between the score function of qθx and the score function of the true distribution px, was first proposed by Hyvarinen in 2005.",
    "of qθx and the score function of the true distribution px, was first proposed by Hyvarinen in 2005. Here, we show that minimizing the difference as stated is equivalent to minimizing the difference between the score function of pθx and the score function of px: ∇xlogqθx=∇xlogpθx*Zθ =∇xlogpθx+∇xlogZθ =∇xlogpθx It turns out that the score function of qθx is the same as the score function of pθx, ∀x.",
    "=∇xlogpθx It turns out that the score function of qθx is the same as the score function of pθx, ∀x. This is because the log first separates the product of qθx and the partition function into a sum of logs, and finally the gradient with respect to x eliminates the Score-Based Generative Models | 267 --- Page 284 --- log of the partition function, since this term is solely dependent on the weights θ and is not a function of x itself.",
    "on function, since this term is solely dependent on the weights θ and is not a function of x itself. Thus, the optimal θ that minimizes the proposed difference is equivalent to the optimal θ that minimizes the difference in scores between pθx and p(x).",
    "ference is equivalent to the optimal θ that minimizes the difference in scores between pθx and p(x). The following is the optimization procedure, which we call explicit score matching: Jθ=Epx1 2∇xlogqθx−∇xlogpx22 θ*= argminθJθ The reason for the leading 1 2 is to simplify the resulting gradient (cancels out with the 2 that will be pulled down from the square of the norm).",
    "he resulting gradient (cancels out with the 2 that will be pulled down from the square of the norm). Note that we have completely removed the dependence on the partition function in our analysis, and we now have a way to (1) learn an unnormalized distribution qθx, and (2) calculate the score of pθxi via our neural network. For item 1, in the case where we find a setting θ that results in Jθ= 0, pθx and p(x) are the same for all x since their gradients are the same for all x.",
    "results in Jθ= 0, pθx and p(x) are the same for all x since their gradients are the same for all x. Of course, in general, two functions that have the same gradients everywhere can still be different functions by being off from each other by a nonzero constant. However, in our case, these two functions cannot be off by a nonzero constant since they are both probability distributions that must sum to one.",
    "not be off by a nonzero constant since they are both probability distributions that must sum to one. Thus, we have a valid optimization procedure for learning an unnormalized distribution that, when normalized, should approximate the true distribution well. To perform item 2, in theory all we would need to do is first run our example xi through our neural network to get qθxi, take the log of qθxi, and backpropagate this result through our network all the way back to the input.",
    "ke the log of qθxi, and backpropagate this result through our network all the way back to the input. We’ve already shown that the resultant score is equivalent to the score of pθxi. Going forward, we will refer to the score function of pθx (and qθx) as Ψθx, and the score function of px as Ψx. Using our new notation, we rewrite the explicit score matching objective as: θ*= argminθEpx1 2Ψθx−Ψx 22 Although we have gotten around the issue of the partition function, we still have no idea what Ψx is.",
    "lthough we have gotten around the issue of the partition function, we still have no idea what Ψx is. Hyvarinen, in 2005, in addition to proposing the notion of explicit score matching, proved an amazing property regarding explicit score matching (satis‐ fied under certain weak regularity conditions): Epx1 2Ψθx−Ψx 22=Epx1 2Ψθx 22+ ∑i= 1d∇xiΨθ,ix+c 268 | Chapter 10: Generative Models --- Page 285 --- Where Ψθ,ix=∇xilogpθx—the score function is just a length d vector (assum‐ ing x is of d dimensions), where each index i corresponds with the partial derivative of the log probability with respect to xi.",
    "here each index i corresponds with the partial derivative of the log probability with respect to xi. c is a constant that has no dependence on θ, so it can simply be ignored during optimization. This is a method the community has come to know as implicit score matching. Note that the equivalent expression has no dependence on the true probability distri‐ bution, and thus we can directly optimize θ, using it as we would any other objective.",
    "ility distri‐ bution, and thus we can directly optimize θ, using it as we would any other objective. Once we learn the optimal θ, all we need to do to perform generative modeling is: 1.Follow the methodology presented earlier for calculating the score of pθxi: 1. run the example through our learned network, take the log of the result, and backpropagate all the way to the input. 2.Sample ϵ from N(0,I). 2. 3.Plug in the results of steps 1 and 2 into the Langevin dynamics equation to obtain3.",
    "om N(0,I). 2. 3.Plug in the results of steps 1 and 2 into the Langevin dynamics equation to obtain3. the next sample, xi+ 1. 4.Repeat steps 1 through 3 with xi+ 1. 4. This procedure allows us to draw samples from pθx, which, as shown earlier, should approximate p(x) well once the network has been trained. Can we do better than implicit score matching?",
    "roximate p(x) well once the network has been trained. Can we do better than implicit score matching? For one, implicit score match‐ ing requires us to calculate second-order gradients, as can be seen from the ∑i= 1d∇xiΨθ,ix term in the implicit score matching objective. This can be quite computationally expensive depending on the size of x.",
    "it score matching objective. This can be quite computationally expensive depending on the size of x. In a framework such as PyTorch, this would require first calculating the first-order gradient through stan‐ dard means such as backpropagation and then looping through each xi manually to compute its second-order gradient. In the next section, we will cover denoising autoencoders and denoising score matching , which modify the objective and allow us to get around these complexity issues.",
    "sing score matching , which modify the objective and allow us to get around these complexity issues. Denoising Autoencoders and Score Matching Before explaining the connection between denoising autoencoders and score match‐ ing, we first motivate the denoising autoencoder architecture. In Chapter 9 , we learned about autoencoders through the lens of representation learning.",
    "hitecture. In Chapter 9 , we learned about autoencoders through the lens of representation learning. We used autoencoders to compress high-dimensional data, such as images, into low- dimensional representations that preserved the information, or useful features, necessary to reconstruct the original data.",
    "ions that preserved the information, or useful features, necessary to reconstruct the original data. We additionally showed, through our experiments on MNIST, that we were able to reconstruct the data quite well and we generally saw, for instances of a given digit, clustering of its low-dimensional representations.",
    "we generally saw, for instances of a given digit, clustering of its low-dimensional representations. This implies that if we were to train a standard classifier on these Denoising Autoencoders and Score Matching | 269 --- Page 286 --- low-dimensional representations, with the label being their original digit categories, we’ d expect to see great accuracy. However, depending on the data we try to compress, it turns out that, at times, our compressions aren’t able to capture useful features.",
    "y to compress, it turns out that, at times, our compressions aren’t able to capture useful features. In other words, when we use our trained autoencoder on real-world images outside of our sample that may be slightly corrupted, rotated, shifted, or captured under various light settings, our ability to classify these images using their low-dimensional representations takes a large dip. Ideally, we would like our learned representations to be invariant to such noise.",
    "takes a large dip. Ideally, we would like our learned representations to be invariant to such noise. In 2008, Vincent proposed denoising autoencoders as a method for combating the issues we see with standard autoencoders. Denoising autoencoders first corrupt the original input data with noise, run the corrupted input through a standard autoencoder, and finally attempt to reconstruct the original input ( Figure 10-8 ).",
    "rough a standard autoencoder, and finally attempt to reconstruct the original input ( Figure 10-8 ). The original paper used a corruption scheme that randomly zeroed out some portion of the input, but acknowledged that a variety of corruption schemes could be used instead. Intuitively, the representations learned from such a procedure should be much more robust to the challenges presented by real-world images.",
    "d from such a procedure should be much more robust to the challenges presented by real-world images. Indeed, the experiments on MNIST by Vincent in 2008 showed that, under various data augmentations such as rotation and background noise, the denoising autoencoder performed significantly better than the standard autoencoder in terms of classification accuracy. Figure 10-8.",
    "significantly better than the standard autoencoder in terms of classification accuracy. Figure 10-8. The denoising autoencoder architecture is the same as that of the standard autoencoder, except instead of minimizing the reconstruction error between y and the input x', we minimize the reconstruction error between y and the original x.",
    "error between y and the input x', we minimize the reconstruction error between y and the original x. Following Vincent 2011, which first noticed the connection between denoising AEs and score matching, we instead define the corruption scheme to be the addition of Gaussian noise to the original data.",
    ", we instead define the corruption scheme to be the addition of Gaussian noise to the original data. Formally, we have that px represents the true distribution of the data, pdatax represents the distribution of the data using our training set, and pσx′x represents the conditional distribution of the corrupted data given the original data.",
    "et, and pσx′x represents the conditional distribution of the corrupted data given the original data. In particular: pσx′x=Nx′;x,σ2I 270 | Chapter 10: Generative Models --- Page 287 --- Where the mean of the distribution is the original data and the subscript σ represents the standard deviation of the Gaussian noise applied to the original data. Note that x’ and x are defined over the same domain (all possible images, for example).",
    "iginal data. Note that x’ and x are defined over the same domain (all possible images, for example). We can now calculate the distribution over the corrupted data: pσx′= ∑xpσx′xpx ≈ ∑xpσx′xpdatax =1 n∑i= 1npσx′x=xi which is the empirical average over the conditional probabilities using each data point from our dataset as the reference.",
    "average over the conditional probabilities using each data point from our dataset as the reference. This follows naturally from letting the true distribution be approximated by the distribution defined by the dataset (same as how this was defined in “Generative Adversarial Networks” on page 244 ). In 2011, Vincent explored the possibility of using pσx′ as the reference instead of px as we do in explicit score matching.",
    "ed the possibility of using pσx′ as the reference instead of px as we do in explicit score matching. The reasoning for this is that pσx′ can be viewed as a continuous approximation to the true distribution px. The approximation defined by pdatax is unbiased, but is unfortunately discontinuous everywhere x is not present in the dataset due to being a uniform distribution over all images in the dataset, with a likelihood of zero everywhere else.",
    "ng a uniform distribution over all images in the dataset, with a likelihood of zero everywhere else. Of course, as σ gets larger, pσx′ is seen as a less and less faithful approximation to px, so we’ d like to work with small σ’s.",
    "pσx′ is seen as a less and less faithful approximation to px, so we’ d like to work with small σ’s. Vincent 2011 first proposed explicit score matching using pσx′ as the reference: Jθ=Epσx′1 2∇x′logpθx′−∇x′logpσx′22 θ*= argminθJθ Note that the same reasoning for why this is a valid optimization procedure for pθx is the same as in the previous section—the only difference here is the reference distribution we are trying to match.",
    "the previous section—the only difference here is the reference distribution we are trying to match. Vincent 2011 actually goes an extra step and shows that this optimization procedure is equivalent to: JDSMθ=Epσx,x′1 2∇x′logpθx′−∇x′logpσx′x22 θDSM* = argminθJDSMθ Although we won’t show the proof here and refer you to Vincent 2011 for the full details, it does utilize the log trick we described in “Implementing a V AE” on page Denoising Autoencoders and Score Matching | 271 --- Page 288 --- 259.",
    "“Implementing a V AE” on page Denoising Autoencoders and Score Matching | 271 --- Page 288 --- 259. We refer to optimizing this objective as denoising score matching, or DSM for short , and as we will show soon, it serves as the connection to denoising AEs. We know that pσx′x=Nx′;x,σ2I, and now compute the gradient of its log: ∇x′logpσx′x=∇x′log1 2πdσ2Ie−x′−xTx′−x 2σ2 =∇x′log1 2πdσ2I+∇x′loge−x′−xTx′−x 2σ2 = −1 2σ2∇x′x′−xTx′−x = −1 2σ2∇x′x′Tx′− 2∇x′x′Tx+∇x′xTx =1 σ2x−x′ Let’s break down the math.",
    "x′−x 2σ2 = −1 2σ2∇x′x′−xTx′−x = −1 2σ2∇x′x′Tx′− 2∇x′x′Tx+∇x′xTx =1 σ2x−x′ Let’s break down the math. The first equality is simply the definition of a Gaussian distribution with mean x and variance σ2I. The second equality is a result of the log breaking up the product into a sum of logs, and the gradient of a sum being the sum of gradients. In the third equality, we see the first term has been removed since it is not a function of x’, and thus its gradient is zero.",
    "see the first term has been removed since it is not a function of x’, and thus its gradient is zero. Additionally, the log of e raised to any power is just the power itself, since log as used here has base e. Finally, we expand out the dot product of x’ – x with itself and apply the gradient to each individual term of the resulting sum. Note that we can simply rewrite −x′Tx−xTx′ as −2x′Tx since the two terms are transposes of each other and result in the same scalar.",
    "−x′Tx−xTx′ as −2x′Tx since the two terms are transposes of each other and result in the same scalar. We refer you to an amazing text called The Matrix Cookbook by KB Petersen and Michael Syskind Pedersen, which can serve as a guide to evaluating these gradients (plus more) and arrive at the final equality. The intuition for the gradient of x′Tx′ is that it is the analog of the derivative of the square of a variable from single-variable calculus.",
    "s that it is the analog of the derivative of the square of a variable from single-variable calculus. For the final step, we will show that optimizing the objective for denoising score matching is equivalent to optimizing the objective for denoising AEs. To recap, a denoising AE has the same architecture as that of a standard AE—the only difference is in the input data and the training objective.",
    "ecture as that of a standard AE—the only difference is in the input data and the training objective. The training objective of the denoising AE looks like: JDAEθ=Epσx,x′decode encodex′−x22 θDAE* = argminθJDAEθ 272 | Chapter 10: Generative Models --- Page 289 --- Note that the parameters, or weights, of both decode() and encode() are encompassed by θ. To summarize, we must show that θDAE* and θDSM* defined earlier are equivalent for some form of the unnormalized likelihood.",
    "ow that θDAE* and θDSM* defined earlier are equivalent for some form of the unnormalized likelihood. Once again, following Vincent 2011, we define the denoising autoencoder as an encoder consisting of a single fully con‐ nected layer followed by a sigmoid layer and a decoder consisting solely of a single fully connected layer. Additionally, we add the constraint that the two fully connected layers are weight-tied so that they are transposes of each other.",
    "raint that the two fully connected layers are weight-tied so that they are transposes of each other. The training objective can now be specified as, where θ=W,b,c: JDAEθ=Epσx,x′WTWx′+b+c−x22 = 2σ4*Epσx,x′1 2σ4WTWx′+b+c−x22 = 2σ4*Epσx,x′1 21 σ2WTWx′+b+c−x′−1 σ2x−x′22 Y ou may notice that our algebraic manipulation has led to the appearance of ∇x′logpσx′x. All we need to do now is find a form for the unnormalized like‐ lihood whose gradient with respect to x’ is 1 σ2WTWx′+b+c−x′.",
    "find a form for the unnormalized like‐ lihood whose gradient with respect to x’ is 1 σ2WTWx′+b+c−x′. As it turns out, if we define the unnormalized likelihood qθx′ to be −1 σ2cTx−1 2x22+ ∑j= 1dsoftplusWjTx+bj and plug in this expression to the denoising score matching objective, we are left with an objective that is just 1 2σ4JDAEθ. We refer you to Vincent 2011 to see why this is the case. Optimizing this new objective with respect to θ is no different from optimizing a denoising autoencoder.",
    "mizing this new objective with respect to θ is no different from optimizing a denoising autoencoder. This is because σ is a positive constant and has no depend‐ ence on θ, thus only scaling the magnitude of the resulting gradient rather than affecting its direction. In summary, we have found that training a denoising AE is the same as optimizing the denoising score matching objective, where the unnormalized likelihood takes the form specified in the previous paragraph.",
    "ing objective, where the unnormalized likelihood takes the form specified in the previous paragraph. More simply, the weights of a trained denoising AE would be the same as those of an unnormalized likelihood specified by −1 σ2cTx−1 2x22+ ∑j= 1dsoftplusWjTx+bj and trained via denoising score matching. All we would need to do to perform generative modeling using a denoising AE is: 1.Fully train the denoising AE by minimizing JDAEθ. 1.",
    "generative modeling using a denoising AE is: 1.Fully train the denoising AE by minimizing JDAEθ. 1. 2.For a given xi, calculate its score by evaluating 1 σ2decode encodexi−xi. 2. 3.Sample ϵ from N(0,I). 3. Denoising Autoencoders and Score Matching | 273 --- Page 290 --- 4.Plug in the results of 2 and 3 into the Langevin dynamics equation to obtain the4. next sample xi+ 1. 5.Repeat steps 2 through 4 with xi+ 1. 5.",
    "gevin dynamics equation to obtain the4. next sample xi+ 1. 5.Repeat steps 2 through 4 with xi+ 1. 5. Though we’ve gotten around the issue of needing to calculate second-order gradients by using this method, there is still the issue of being able to sample only from the noisy approximation of p(x). More recent work builds off of concepts from both implicit score matching and denoising score matching to achieve even stronger and more realistic generative capabilities.",
    "ng and denoising score matching to achieve even stronger and more realistic generative capabilities. We highly recommend you explore the litera‐ ture further, as most of the prerequisite material has been covered in these sections. Summary In summary, we have learned a great deal about generative models. We covered the motivation and mathematics behind GANs, V AEs, and a few forms of score matching, and even implemented a V AE from scratch.",
    "ics behind GANs, V AEs, and a few forms of score matching, and even implemented a V AE from scratch. We also learned about the similarities and differences between these methods. For example, a GAN implicitly models a complex distribution that we can sample from via its generator, while a V AE explicitly learns distributions but is slightly more restrictive in the complexity of distributions it can model.",
    "arns distributions but is slightly more restrictive in the complexity of distributions it can model. Implicit score matching, similarly to GANs, allowed us to sample from complex distributions via Langevin dynamics (without the use of an additional noise distribution p(z)) , but having to compute second-order gradients led us to the development of the denoising score matching and its connection with pre-existing denoising AEs.",
    "the development of the denoising score matching and its connection with pre-existing denoising AEs. Additionally, V AEs took on the strongest probabilistic modeling approach of the three by defining a set of latent variables and explicitly learning an approximate posterior, given an input example, and a likelihood function, given a setting of latent variables. In contrast, for GANs, the additional variable z’s purpose is solely as an intermediate for sampling.",
    "n contrast, for GANs, the additional variable z’s purpose is solely as an intermediate for sampling. Although all of these models tackle generative modeling from distinct perspectives and motivations, they have all produced strong results and have laid a solid groundwork for current and future research. 274 | Chapter 10: Generative Models --- Page 291 --- CHAPTER 11 Methods in Interpretability Overview The field of interpretability is broad and can be uniquely applied to a variety of tasks.",
    "y Overview The field of interpretability is broad and can be uniquely applied to a variety of tasks. Simply put, interpretability defines a model’s ability to “explain” its decision making to a third party. There are many modern architectures that do not have this capability just by construction. A neural network, for example, is a prime example of one of these modern architectures. The term “opaque” is often used to describe neural networks, both in media and in literature.",
    "tures. The term “opaque” is often used to describe neural networks, both in media and in literature. This is because, without post hoc techniques to explain the final classification or regression result of a neural network, the data transformations occurring within the trained model are unclear and difficult for the end user to interpret. All we know is that we fed in an example and out popped a result.",
    "ult for the end user to interpret. All we know is that we fed in an example and out popped a result. Although we can examine the learned weights of a neural network, the composition of all of these weights is an extremely complex function. This makes it difficult to tell what part of the input ended up contributing the most to the final result. A variety of post hoc methodologies have been designed to explain the output of a neural network— saliency mapping is a prime example.",
    "s have been designed to explain the output of a neural network— saliency mapping is a prime example. Saliency mapping measures the gradient of the output of a trained model with respect to the input. By the definition of the gradient, the input positions with the highest magnitude gradients would affect the output value, or class in the case of classification, the most when their values are changed slightly.",
    "tput value, or class in the case of classification, the most when their values are changed slightly. Saliency mapping thus interprets the set of positions (and their respective values) with the highest magnitude gradients as the part of the input that contributes the most to the final result. However, this is not the be-all and end-all of interpretability.",
    "butes the most to the final result. However, this is not the be-all and end-all of interpretability. One issue with saliency mapping is that it can be a bit noisy, especially when we consider the gradient at the individual pixel level for tasks like image classification.",
    "lly when we consider the gradient at the individual pixel level for tasks like image classification. Additionally, if the input is 275 --- Page 292 --- categorical in nature rather than continuous, e.g., one-hot encodings for sentences, the gradient with respect to the input isn’t interpretable in itself since the input space is discontinuous. Further, as mentioned earlier, the task at hand is often key for determining what makes sense as a valid method of interpretability.",
    "he task at hand is often key for determining what makes sense as a valid method of interpretability. We will expound on this more in the sections to come. Oftentimes, interpretability comes at the expense of performance. Building interpret‐ ability into a model often adds some bias (the bias in bias-variance trade-off) by making simplifying model assumptions, e.g., in vanilla linear regression, where we assume a linear relationship between the features and the target variable.",
    "near regression, where we assume a linear relationship between the features and the target variable. These simplifying assumptions, however, are what make the relationship between the input features and the target variable much clearer in a vanilla linear regression as opposed to a complex, neural architecture. This all begs the question: why do we care about interpretability in the first place?",
    "architecture. This all begs the question: why do we care about interpretability in the first place? In a world that is becoming increasingly dominated by technology, complex algorithms, and machine learning, the ability to explain decision making is imperative. Especially in fields such as medicine, where patient’s lives are on the line, or in finance, where peoples’ financial livelihoods are at stake, the ability to explain a model’s decision making is a key step toward widespread adoption.",
    "at stake, the ability to explain a model’s decision making is a key step toward widespread adoption. In the next section, we will cover some classical models that have strong notions of interpretability built into their design. Decision Trees and Tree-Based Algorithms Most classical data science and machine learning methodologies have some built-in form of interpretability. Tree-based algorithms are a clear example of this.",
    "gies have some built-in form of interpretability. Tree-based algorithms are a clear example of this. Decision trees are designed to classify an input based on a series of conditional statements, where each node in the tree is associated with a conditional statement. To understand how a trained tree-based model is making a decision, all we must do for any given input is follow the correct branch at each node in the tree ( Figure 11-1 ).",
    "e must do for any given input is follow the correct branch at each node in the tree ( Figure 11-1 ). 276 | Chapter 11: Methods in Interpretability --- Page 293 --- Figure 11-1. A decision tree trained to classify bird species. Given a set of bird features, follow the right “Yes” or “No” branch at each node to reach a final classification. More complex tree-based algorithms, such as the random forest algorithm, which is composed of an ensemble of large decision trees, are also interpretable.",
    "forest algorithm, which is composed of an ensemble of large decision trees, are also interpretable. For example, in the case of classification, random forest algorithms function by running a given input through each decision tree and then taking the majority output class amongst the decision trees as the final output (or an average in the case of regression). By the algorithm’s construction, we know exactly how random forest came to a final conclusion regarding the input.",
    "hm’s construction, we know exactly how random forest came to a final conclusion regarding the input. In addition to interpretability at the individual example level, decision trees and their more complex ensembles have built-in metrics for feature importance at the global level. For example, when a decision tree is being trained, it must determine which feature to split on and the threshold(s) of that feature at which to split.",
    "it must determine which feature to split on and the threshold(s) of that feature at which to split. In the classification regime, one methodology to do this is to calculate the information gain by splitting on a proposed feature at a proposed threshold. To frame our thinking, let’s think of the possible training labels as the domain of a discrete probability distribution, where the probability of each label is the frequency with which that label appears in the training dataset ( Figure 11-2 ).",
    "f each label is the frequency with which that label appears in the training dataset ( Figure 11-2 ). Decision Trees and Tree-Based Algorithms | 277 --- Page 294 --- Figure 11-2. Label probabilities. Thinking back to Chapter 2 , a metric that summarizes the uncertainty within a probability distribution is the entropy of the distribution.",
    "hat summarizes the uncertainty within a probability distribution is the entropy of the distribution. When given a proposed feature and associated threshold(s) to split on, we can split the training data popula‐ tion into at least two separate groups based on which branch we would follow for each input example.",
    "tion into at least two separate groups based on which branch we would follow for each input example. Each subgroup now has its own distribution over the possible labels, and we take the difference between the training dataset’s entropy and the weighted sum of each subgroup’s entropy to calculate the information gain, where the weight is proportional to the number of elements in each subgroup.",
    "e the information gain, where the weight is proportional to the number of elements in each subgroup. The feature and associated threshold(s) with the highest information gain at each branching point are the optimal split. Why does this work? Although we won’t do a rigorous proof here, consider the problem where we have a molecular dataset with a binary label, for example, indicat‐ ing whether each compound is toxic or not, and we’ d like to build a classifier to predict compound toxicity.",
    "er each compound is toxic or not, and we’ d like to build a classifier to predict compound toxicity. Also assume that one of the features associated with each compound is a binary feature of whether the molecule contains a phenol functional group or not. The phenol functional group is both quite toxic and is a common cause of toxicity in compounds, so splitting on this feature would lead to two well-separated subgroups.",
    "e of toxicity in compounds, so splitting on this feature would lead to two well-separated subgroups. The positive subgroup, which contains compounds with phenol functional groups, is likely to have few false positives due to the phenol’s level of toxicity. The negative subgroup, which contains compounds without phenol functional groups, is likely to have few false negatives due to phenol being a common cause of toxicity.",
    "tional groups, is likely to have few false negatives due to phenol being a common cause of toxicity. Thus, each subgroup’s associated entropy is quite low since the true label distribution over compounds in each subgroup is quite concentrated over a single label. The sum of their weighted entropies removed from the entire dataset’s associated entropy demonstrates a significant information gain ( Figure 11-3 ). 278 | Chapter 11: Methods in Interpretability --- Page 295 --- Figure 11-3.",
    "on gain ( Figure 11-3 ). 278 | Chapter 11: Methods in Interpretability --- Page 295 --- Figure 11-3. The original dataset can be broken down into 30% toxic and 70% nontoxic, where a true label of 1 indicates toxicity and 0 otherwise. Breaking up the n examples into two subgroups based on containing phenol greatly concentrates the true probability over a single label in each subgroup.",
    "on containing phenol greatly concentrates the true probability over a single label in each subgroup. This checks out well with our a priori knowledge of the phenol group—due to both its widespread nature in toxic compounds and its level of toxicity, we would have expected it to be a great feature for toxicity classification. The way we select features and their splits in decision trees is actually the same way we approach greedy algorithms in the more general algorithmic framework.",
    "es is actually the same way we approach greedy algorithms in the more general algorithmic framework. Greedy algorithms select the most optimal local action at each decision point, and depending on the properties of the problem, the composition of these locally optimal actions leads to the global optimum. Decision trees similarly select the feature and split that locally lead to the largest gain in some metric at each decision point.",
    "t the feature and split that locally lead to the largest gain in some metric at each decision point. For example, we just used information gain for toxicity classification, and although we showed the result of just one split, assuming splitting on the phenol trait leads to the highest information gain, we perform this same greedy procedure at each junction of every level in the tree.",
    "information gain, we perform this same greedy procedure at each junction of every level in the tree. However, it turns out that the problem of finding the globally optimal decision tree for a dataset is NP-complete, which for the purposes of this text means that it is computationally very difficult.",
    "is NP-complete, which for the purposes of this text means that it is computationally very difficult. The best we can do to approach the problem in a Decision Trees and Tree-Based Algorithms | 279 --- Page 296 --- tractable manner is the greedy approach, although it does not provably lead to the global optimum. For each feature we split on in a tree, there exists an associated information gain with that feature.",
    "r each feature we split on in a tree, there exists an associated information gain with that feature. The order of importance of each feature is simply a list of the features sorted by their information gain. If we have a random forest rather than a single decision tree, we average the information gain for each feature across all of the trees in the forest and sort using the mean.",
    "the information gain for each feature across all of the trees in the forest and sort using the mean. Note that there is no extra work required in calculating the information gain, since we use information gain to train the individual decision trees in the first place. Thus, we have both example-level interpretability and a global understanding of feature importance for free in tree-based algorithms.",
    "interpretability and a global understanding of feature importance for free in tree-based algorithms. Linear Regression A quick background on linear regression: given a set of features and a target variable, our goal is to find the “best” linear combination of features that approximates the target variable. Implicit in this model is the assumption that the input features are linearly related to the target variable.",
    "in this model is the assumption that the input features are linearly related to the target variable. We define “best” as the set of coefficients that results in the linear combination with the lowest root mean squared error when compared against the ground truth: y=β·x+ϵ,ϵ ∼ N 0,σ2 Where β represents the vector of coefficients. Our built-in, global notion of feature importance follows directly from this.",
    "ector of coefficients. Our built-in, global notion of feature importance follows directly from this. The features that correspond with the coeffi‐ cients with the highest magnitude are, globally, the most important features in the regression. How about an example-level notion of feature importance? Recall that to get a prediction for a given example, we take the dot product between the example and the learned coefficients.",
    "ction for a given example, we take the dot product between the example and the learned coefficients. Logically, the feature associated with the feature-coefficient prod‐ uct that contributes the most, in magnitude, to the final result is the feature that is most important for prediction. Without much effort, we have both an example-level and global-level notion of interpretability more or less built into linear regression.",
    "example-level and global-level notion of interpretability more or less built into linear regression. However, linear regression has some unaddressed issues when considering feature importance. For example, when there exist significant correlations between the fea‐ tures in a multivariate regression, it is often difficult for the model to disentangle the effects of these correlated features on the output.",
    "often difficult for the model to disentangle the effects of these correlated features on the output. In “SHAP” on page 292, we will describe Shapley values, which were designed to measure the marginal, unbiased impact of a given feature on the output in such cases. 280 | Chapter 11: Methods in Interpretability --- Page 297 --- Methods for Evaluating Feature Importance For models where feature importance isn’t built in, researchers have developed a variety of methods over the years to evaluate it.",
    "tance isn’t built in, researchers have developed a variety of methods over the years to evaluate it. In this section, we will discuss a few that are used in the industry, in addition to their benefits and shortcomings. Permutation Feature Importance The idea behind permutation feature importance is quite simple: assume we have a trained neural model f and a set of features U that f has been trained on. We’ d like to understand the impact that an individual feature s has on the predictions of f.",
    "ed on. We’ d like to understand the impact that an individual feature s has on the predictions of f. One way to do this is to randomly rearrange the values that s takes on in the dataset amongst all of the examples and measure the resulting decrease in predictive accuracy. If the feature s did not add much predictive accuracy in the first place, we should see that the predictive accuracy of f decreases minimally when using the permuted samples.",
    "we should see that the predictive accuracy of f decreases minimally when using the permuted samples. Inversely, if feature s was predictive of the output in the first place, we should see a large drop in predictive accuracy upon permuting the values of s in the dataset. In essence, if the feature s was originally strongly correlated with the true labels, randomizing the values of s would break this strong correlation and nullify its effectiveness at predicting the true label.",
    "of s would break this strong correlation and nullify its effectiveness at predicting the true label. Unfortunately, as with all interpretability methods, this one is not perfect. Imagine the scenario in which our target is ice cream sales in a given region and two fea‐ tures in U are the readings of two temperature sensors within a one-mile radius of each other. We’ d expect that each of these features is independently quite predictive of ice cream sales due to the seasonality of our target.",
    "features is independently quite predictive of ice cream sales due to the seasonality of our target. However, if we were to perform the permutation methodology presented previously on this dataset, we’ d counterintuitively get a low feature importance for both of these features. Why is this the case? Although each of these features is strongly predictive of the target, they are also strongly correlated due to the close proximity of the two temperature sensors.",
    "target, they are also strongly correlated due to the close proximity of the two temperature sensors. Additionally, permuting only one of these features at a time to compute its importance means that the other is kept intact, preserving most of the predictive information contained within the two features. Thus, we’ d see little change in the predictive performance of f for both features, leading us to believe that the weather is not predictive of ice cream sales.",
    "of f for both features, leading us to believe that the weather is not predictive of ice cream sales. The moral of the story here is that we must always keep in mind correlations between features in our dataset. It is good data science and machine learning practice to understand the relationships between the features themselves before running these features through any sort of predictive modeling algorithm, simple or complex.",
    "before running these features through any sort of predictive modeling algorithm, simple or complex. One way to do this is to plot each of the z-scored features against each other to get a visual idea of feature correlation. Methods for Evaluating Feature Importance | 281 --- Page 298 --- Partial Dependence Plots Partial dependence plots, or PDPs for short, measure the marginal impact that a subset of features included in the model has on the output.",
    "hort, measure the marginal impact that a subset of features included in the model has on the output. As previously discussed, measuring this marginal impact in an unbiased manner is difficult for complex neural models. In the case of regression, we can represent the trained neural network (or any other complex, uninterpretable model) as a function f that takes as input a set of features U and outputs a value in the reals.",
    "ble model) as a function f that takes as input a set of features U and outputs a value in the reals. Imagine that, as a user of this model, you are looking for an interpretability method that can measure the marginal impact of any subset of features S on the output of f. That is, if we are given an arbitrary setting of feature set S, we would like to calculate the expected output of the function f conditioned on this setting.",
    "set S, we would like to calculate the expected output of the function f conditioned on this setting. The expectation of f is taken over U \\ S , the rest of the features in U (conditioned on the known setting of S). Intuitively, we have marginalized out the feature subset U \\ S and have the output of a new function f’ that takes as input only the feature set S. If we carry out this process for enough settings of S, we can learn the patterns of how f’ changes as the feature set S changes.",
    "for enough settings of S, we can learn the patterns of how f’ changes as the feature set S changes. For example, assume the output is the number of vehicles on the road in a given region and our feature set S consists of a single feature: precipitation levels in that region. The features that make up U \\ S could be variables like time of day, geograph‐ ical location, population density, etc.",
    "make up U \\ S could be variables like time of day, geograph‐ ical location, population density, etc. By running the above process for a range of precipitation levels, we can estimate the number of vehicles we’ d expect to see on the road at each level and observe the trend as precipitation levels get higher or lower. Plotting this trend is what gives us a PDP .",
    "the trend as precipitation levels get higher or lower. Plotting this trend is what gives us a PDP . A couple of important notes: the first is that we do not plan on actually learning f’, but rather estimating it using our trained model f. Learning f’ itself would require retraining for every potential subset S to be explained, which is exponential in the number of features and thus intractable.",
    "ntial subset S to be explained, which is exponential in the number of features and thus intractable. The second is that it is currently unclear how we would go about computing the expectation of f taken over U \\ S . As we will soon see, the PDP methodology addresses this second point. Before diving into the weeds, here is a simple yet concrete mathematical formulation of the process we have just described: fâS=EU ∖SSfU ∖S ,S As stated, the conditional expectation is a bit tricky to estimate.",
    "st described: fâS=EU ∖SSfU ∖S ,S As stated, the conditional expectation is a bit tricky to estimate. So far in the text, we have approximated expectations in an unbiased manner via empirical averages. To estimate a conditional expectation, however, we are further constrained by the fact that we must take only the average over samples that contain the exact setting of S in question.",
    "act that we must take only the average over samples that contain the exact setting of S in question. Unfortunately, the only samples we have from the underlying distribution over U are contained within the dataset we are provided with. And in the common case that the features of U are continuous, the likelihood that we see the exact setting 282 | Chapter 11: Methods in Interpretability --- Page 299 --- of S in question even once in the dataset is exceedingly low.",
    "s in Interpretability --- Page 299 --- of S in question even once in the dataset is exceedingly low. To get around this, PDP makes an independence assumption that allows us to use the entire dataset directly to estimate this expectation: fâS=EU ∖SSfU ∖S ,S=EU ∖SfU ∖S ,S≈1 n∑i= 1nfU ∖Si,S Where n is the number of samples in the dataset. PDP assumes that the features in S are independent from the features in U \\ S .",
    "ples in the dataset. PDP assumes that the features in S are independent from the features in U \\ S . This assumption allows us to use all of the training samples indiscriminately for computing the expectation since, under this assumption, the sampling of U \\ S is independent from the setting of S anyway. We now have a concrete method for estimating the marginal impact of any arbitrary subset of features S on the output of f.",
    "method for estimating the marginal impact of any arbitrary subset of features S on the output of f. If there are significant correlations between the features in S and those in U \\ S , then our generated PDP is likely not reflective of the true marginal effect of S on the output due to bias in our sampling assumption.",
    "ot reflective of the true marginal effect of S on the output due to bias in our sampling assumption. Essentially, we would be taking the average over many samples that are very unlikely to occur, which means that we (1) can’t expect f to generate meaningful outputs on these samples, and (2) are taking the average over the outputs for samples that do not accurately reflect the relationships in the underlying distribution over U.",
    "for samples that do not accurately reflect the relationships in the underlying distribution over U. The second concern is likely pretty clear, but to illustrate the first, imagine that you have trained a neural network to completion on the MNIST dataset. Now, I find an image of a dog online and run this image through the network. By chance, it turns out that the network returns a 9 with high confidence—should we trust these results?",
    "hance, it turns out that the network returns a 9 with high confidence—should we trust these results? Since the input image is completely out of the distribution of images that the model expects to see, we can’t trust the model to generalize to this extent. Although our situation with PDP is a bit less extreme, it is analogous—we have essentially created these unlikely, out-of-distribution “franken-samples” and are expecting f to produce meaningful outputs on these samples.",
    "f-distribution “franken-samples” and are expecting f to produce meaningful outputs on these samples. The independence assumption that PDP makes is an inherent limitation of the method, again since the only samples we have are those from the dataset. Additionally, PDPs are often used to analyze the impact of small subsets of features ( ≤ 2), since humans can interpret only up to three dimensions visually.",
    "f small subsets of features ( ≤ 2), since humans can interpret only up to three dimensions visually. Regardless, PDPs can be an effective method for visualizing trends between subsets of input features and the output of a complex model. Extractive Rationalization Extractive rationalization, or the selection of concise portions of the input that retain most, or all, of the relevant information necessary to predict a property, is a built-in form of interpretability at the example level.",
    "mation necessary to predict a property, is a built-in form of interpretability at the example level. In this section, we will review the Extractive Rationalization | 283 --- Page 300 --- 1Lei et al. “Rationalizing Neural Predictions. ” arXiv Preprint arXiv :1606.04155. 2016.methodology of the paper “Rationalizing Neural Predictions, ”1 which attempts to do this in the natural language space.",
    "paper “Rationalizing Neural Predictions, ”1 which attempts to do this in the natural language space. In this paper, the task at hand is property prediction: given a textual review, predict some properties regarding the text. The paper specifically worked with a beer review dataset, where each review consisted of some text along with an appearance score, a smell score, and a palate score.",
    "ach review consisted of some text along with an appearance score, a smell score, and a palate score. The high-performing but uninterpretable method would be to train a classic property predictor using a recurrent architecture, followed by a vanilla regression neural net‐ work that takes as input the final embedding produced by the recurrent architecture, as shown in Figure 11-4 . Figure 11-4.",
    "t the final embedding produced by the recurrent architecture, as shown in Figure 11-4 . Figure 11-4. Depicted is the classical property predictor, where x is an encoding of the original sentence, h(x) is the hidden state produced by the recurrent architecture after reaching the end of x, and y is the result of a standard feed-forward neural architecture.",
    "ure after reaching the end of x, and y is the result of a standard feed-forward neural architecture. The goal of this paper is to additionally generate rationales, or selected, concise portions of the input text that are most relevant to the property being predicted, while limiting the hit to performance. This is why this method of rationalization is referred to as “extractive”—it works by extracting relevant portions of the input. Y ou might be wondering why there is an emphasis on conciseness.",
    "ing relevant portions of the input. Y ou might be wondering why there is an emphasis on conciseness. If there were no limit or penalty on the conciseness of the rationale produced by the model, there would be no reason for the model to just return the entire input, which is a trivial solution. Of course, all of the information necessary for predicting the output is within the rationale if the rationale is the entire input.",
    "on necessary for predicting the output is within the rationale if the rationale is the entire input. How do we modify the structure of the proposed property predictor to also produce rationales as a built-in mechanism? This paper proposed a two-network approach, where the first network is termed the generator and the second network is termed the encoder.",
    "roach, where the first network is termed the generator and the second network is termed the encoder. The generator is an RNN responsible for selecting the rationale, while the encoder is an RNN responsible for predicting the property given solely the rationale, 284 | Chapter 11: Methods in Interpretability --- Page 301 --- not the entire input.",
    "the rationale, 284 | Chapter 11: Methods in Interpretability --- Page 301 --- not the entire input. The logic behind this is that, given the right objective function, the generator will have to learn to select meaningful portions of the input text to be able to accurately predict the ground truth rating.",
    "lect meaningful portions of the input text to be able to accurately predict the ground truth rating. The generator parameterizes a distribution over all possible binary masks that can be applied to the input, where a 1 indicates that the word should be included in the rationale, and a 0 indicates otherwise.",
    "where a 1 indicates that the word should be included in the rationale, and a 0 indicates otherwise. Figure 11-5 shows the proposed two-step architecture, where the encoder is just the single-step property predictor diagrammed earlier, and z represents a binary mask sampled from the generator. Figure 11-5. The generator parametrizes a distribution over masks z given input x, which we sample from to get the input to the encoder.",
    "zes a distribution over masks z given input x, which we sample from to get the input to the encoder. The encoder follows the same structure as that of the classical property predictor depicted earlier. More formally, we represent the input text x as a vector, where xi represents the token at position i.",
    "re formally, we represent the input text x as a vector, where xi represents the token at position i. The generator parameterizes the distribution p(z|x) , where z is a vector consisting of individual Bernoulli random variables zi, which each take on the value 1 if xi is to be included in the rationale, and 0 otherwise. Note that z is the same length as x, which changes depending on x. How exactly do we represent this distribution?",
    "s the same length as x, which changes depending on x. How exactly do we represent this distribution? A first step is to make a reasonable conditional independence assumption, which is that all zi are mutually independent of each other conditioned on x: pzx= ∏i= 1npziz1, ...,zi− 1,x= ∏i= 1npzix. This is a very reasonable assumption because all of the information regarding whether xi should be in the rationale or not should be contained within x itself (the token xi and its surrounding context).",
    "the rationale or not should be contained within x itself (the token xi and its surrounding context). Converting this to neural net speak, we can implement this by applying a fully connected layer followed by a sigmoid activation to each final hidden state hi of the generator independently to get the probability of zi taking on value 1, as we will see soon. Before going into the specifics of the objective function, we’ll describe the architec‐ tures of the generator and the encoder in more detail.",
    "ective function, we’ll describe the architec‐ tures of the generator and the encoder in more detail. The generator and encoder are both recurrent architectures, where the recurrent unit could be an LSTM or a GRU. As stated in the previous paragraph, the generator produces a hidden unit hi for each Extractive Rationalization | 285 --- Page 302 --- token xi.",
    "rator produces a hidden unit hi for each Extractive Rationalization | 285 --- Page 302 --- token xi. The final embedding for a token consists of two intermediate embeddings: the first intermediate embedding is the result of a forward pass through the tokens, while the second intermediate embedding is the result of a backward pass through the tokens.",
    "tokens, while the second intermediate embedding is the result of a backward pass through the tokens. More formally, we have: ℎi=fℎi− 1,xi ℎi=fℎi+ 1,xi ℎi= concat ℎi,ℎi Where f and f correspond to two independent recurrent units, the former trained on the forward pass and the latter trained on the backward pass. From this formula‐ tion, we can see that the final embedding is bidirectional, incorporating information from the entire context of a token rather than information in solely one direction.",
    "ting information from the entire context of a token rather than information in solely one direction. The paper then applies a single, fully connected layer and sigmoid to each embedding to generate an independent Bernoulli random variable for each token: pzix=σwz·ℎi+bz The encoder is also a recurrent architecture, but is designed to be a regressive architecture due to its purpose of predicting the rating associated with the text.",
    "o be a regressive architecture due to its purpose of predicting the rating associated with the text. For this reason, the encoder can be designed the same way we design the vanilla property predictor alluded to earlier in the section. So, what is the right objective function for training the two networks in tandem? In addition to any constraints we may want to have on the rationales the generator produces, we must also ensure that the predictor is accurate.",
    "o have on the rationales the generator produces, we must also ensure that the predictor is accurate. If the predictor were not accurate, there would be no reason for the generator to produce meaningful rationales. Putting this all together into a mathematical formulation, we have the following objective function: θ*,φ*= argmin θ,φLθ,φ Lθ,φ=∑ x,y∈ DEz ∼ genθxcostx,y,z costx,y,z=λ1*z+λ2*∑ tzt−zt− 1+encφx,z−y22 Where λ1 and λ2 are hyperparameters we can tune during validation.",
    "z=λ1*z+λ2*∑ tzt−zt− 1+encφx,z−y22 Where λ1 and λ2 are hyperparameters we can tune during validation. The cost function used in the paper additionally contains a continuity penalty, which is higher 286 | Chapter 11: Methods in Interpretability --- Page 303 --- when the rationale is interspersed throughout the text rather than one contiguous block. We want to minimize the sum of the expected cost for each training example, where the rationales are drawn according to the generator distribution.",
    "t for each training example, where the rationales are drawn according to the generator distribution. Calculating the expected cost exactly is computationally prohibitive due to the number of config‐ urations of z growing exponentially with the length of x, so we’ d instead like to be able to approximate the gradient of the expected cost via some empirical, sampled estimate.",
    "ke to be able to approximate the gradient of the expected cost via some empirical, sampled estimate. This is feasible for the gradient of the cost function with respect to the parameters of the encoder, but when we try to do this for the generator, we run into a similar issue as we did when we first tried optimizing the V AE encoder: ∇θEz ∼ genθxcostx,y,z= ∑zcostx,y,z*∇θpθzx Note that the cost function is only indirectly a function of θ via sampling from the generator, and thus can be treated as a constant.",
    "y indirectly a function of θ via sampling from the generator, and thus can be treated as a constant. We can’t re-express this as an expectation since the gradient is with respect to the distribution from which we are sampling from.",
    "expectation since the gradient is with respect to the distribution from which we are sampling from. This paper uses the log trick, which we also introduced in the section on V AEs, to resolve this issue: ∑ zcostx,y,z*∇θpθzx =∑ zcostx,y,z*pθzx*∇θlogpθzx =Ez ∼ genθxcostx,y,z*∇θlogpθzx The gradient of the cost function with respect to the parameters of the encoder is just: ∇φEz ∼ genθxcostx,y,z =∑ zpθzx*∇φcostx,y,z =Ez ∼ genθx∇φcostx,y,z Which resembles the standard empirical estimate of the expected gradient when performing SGD or minibatch gradient descent.",
    "ndard empirical estimate of the expected gradient when performing SGD or minibatch gradient descent. How do we go about training these two networks in tandem? It might be easier to consider a single training example for starters. We first select a training example at random from the dataset, where a training example consists of a text review and an associated rating, and feed the text review to the generator.",
    "ample consists of a text review and an associated rating, and feed the text review to the generator. The generator, which now represents a probability distribution over all possible binary masks given the input text review, can be sampled from by sampling each zi independently due to our Extractive Rationalization | 287 --- Page 304 --- 2Ribeiro et al. “Why Should I Trust Y ou? Explaining the Predictions of Any Classifier. ” arXiv Preprint arXiv :1602.04938.",
    "ould I Trust Y ou? Explaining the Predictions of Any Classifier. ” arXiv Preprint arXiv :1602.04938. 2016.conditional independence claim from earlier. Each sampled binary mask represents a possible rationale, which we then feed to the encoder for prediction. After obtaining the result of the encoder for each rationale, we have all the information we need to calculate the cost function for each rationale.",
    "ch rationale, we have all the information we need to calculate the cost function for each rationale. This will suffice for updating the weights of the encoder, but to update the weights of the generator we will also need to keep track of the log likelihood of the rationale, or logpθzkx for each sampled zk. Now that we have a mechanism for training, how do we translate this to validating and testing our model?",
    "that we have a mechanism for training, how do we translate this to validating and testing our model? During the validation and testing phases, instead of sampling binary masks from the generator, we select the most likely binary mask according to the generator probability distribution. To select the most likely binary mask, all we need to do is select the most likely zi for each xi in our input test review x, again due to our conditional independence assumption from earlier.",
    "ch xi in our input test review x, again due to our conditional independence assumption from earlier. This is a very reasonable approach to testing, since this is how we would determine the intended rationale when using this model in the real world. Y ou may have noticed some parallels to the concept of attention.",
    "sing this model in the real world. Y ou may have noticed some parallels to the concept of attention. After all, the generated binary mask can be thought of as a vector of weights we use to multiply the feature vectors that make up the input text review, where these weights are either 0 or 1, rather than some continuous weighting scheme implemented in standard attention.",
    "s are either 0 or 1, rather than some continuous weighting scheme implemented in standard attention. Indeed, the authors of this paper mention that their approach can be viewed as a form of “hard” attention, where we completely mask out or input tokens of the input according to a probability distribution rather than computing a weighted average of the feature vectors in the input.",
    "obability distribution rather than computing a weighted average of the feature vectors in the input. Y ou might be wondering why hard attention makes more sense in this case rather than the “soft” attention schemes presented in the previous section.",
    "more sense in this case rather than the “soft” attention schemes presented in the previous section. In this case, hard attention schemes make more sense because fractional weights on words in a sentence are hard to interpret as a measure of importance, while selecting a strict subset of words in the text as the explanation for a rating is much more interpretable.",
    "ing a strict subset of words in the text as the explanation for a rating is much more interpretable. LIME LIME, or Local Interpretable Model-agnostic Explanations,2 is an interpretability technique that is applied to a trained model rather than a built-in feature of the model itself. LIME is a per-example interpretability method, meaning that it generates a simple, local explanation of the underlying model’s potentially complex behavior.",
    "hat it generates a simple, local explanation of the underlying model’s potentially complex behavior. It is also model agnostic, meaning that the structure of the underlying model itself does not matter when applying LIME. 288 | Chapter 11: Methods in Interpretability --- Page 305 --- Before describing the methodology of LIME, the authors take some time to delineate a few characteristics they believe to be necessary components of any explainer.",
    "me time to delineate a few characteristics they believe to be necessary components of any explainer. The first is that it should be interpretable, meaning that the explainer should provide a “qualitative relationship between input variables and response” that is easy for the user to understand. Even if the features used in the original model are uninterpreta‐ ble, the explainer must use features that a human can interpret.",
    "he original model are uninterpreta‐ ble, the explainer must use features that a human can interpret. For example, in an application of natural language processing, even if the underlying model utilizes a complex word embedding for any given word, the explainer must use features that a human can understand, such as the original words themselves.",
    "the explainer must use features that a human can understand, such as the original words themselves. The second characteristic is local fidelity, which means that the explainer must behave similarly to the underlying model within some vicinity of the chosen example. We might ask, why local and not global fidelity?",
    "g model within some vicinity of the chosen example. We might ask, why local and not global fidelity? Global fidelity, however, as the paper notes, is quite difficult to achieve and would require drastic advances in the field—much of the field of interpretability would be solved if global fidelity could be achieved. Thus, we settle for local fidelity.",
    "retability would be solved if global fidelity could be achieved. Thus, we settle for local fidelity. The third is for the explainer to be model agnostic, which, as we explained earlier, means that the structure of the underlying model itself should not matter. The underlying model can range from a linear regression model to a complex convolu‐ tional neural architecture, and the explainer should still be able to satisfy the other three characteristics.",
    "ral architecture, and the explainer should still be able to satisfy the other three characteristics. Being model agnostic allows for flexibility in the structure of the underlying model, which is desirable as this doesn’t necessitate changes in the structure of the explainer. The fourth and final characteristic is global perspective, which is to select explana‐ tions for a subset of examples that is representative of the model’s behavior. This helps build user trust in the model.",
    "f examples that is representative of the model’s behavior. This helps build user trust in the model. Now we will take some time to develop the methodology of LIME. As stated, the features of the original model may not be interpretable to a human (and usually aren’t for most complex models), so the features used by the explainer will be different from those used by the underlying model.",
    "s), so the features used by the explainer will be different from those used by the underlying model. The features used by the explainer could be indi‐ vidual words in an NLP task, or functional groups in a chemical property prediction task—units, or interpretable components, that the end user can understand easily.",
    "roperty prediction task—units, or interpretable components, that the end user can understand easily. Thus, any example when converted to the feature space of the explainer becomes a binary vector, where each index is associated with a distinct interpretable component (such as a functional group). A one at any index i indicates the presence of the associated interpretable component in the original example, and a zero indicates a lack of that component in the original example.",
    "nent in the original example, and a zero indicates a lack of that component in the original example. Following the notation used in the referenced paper, we denote x ∈ℝd to be the original feature representation of the example to be explained and xâ∈0, 1d′ to be the representation acted upon by the explainer, where d’ is the number of interpretable components.",
    "the representation acted upon by the explainer, where d’ is the number of interpretable components. LIME | 289 --- Page 306 --- Further, the paper defines G to be a class of potentially interpretable models, such as linear regression or random forest, and an explainer to be an instance g ∈ G . g acts on an instance x’ and returns a value in the range of the underlying model.",
    "instance g ∈ G . g acts on an instance x’ and returns a value in the range of the underlying model. We denote the underlying model to be f, which acts on an instance x and is a function from ℝdℝ in the case of regression or a function from ℝd to the range [0,1] in the case of classification, where f returns a probability distribution. Additionally, the paper defines a proximity measure, or kernel, πxz around the instance x.",
    "ribution. Additionally, the paper defines a proximity measure, or kernel, πxz around the instance x. This function can be defined in a multitude of ways—most implementations of LIME use an exponential kernel that attains a maximum value at x and decreases exponentially as one gets farther and farther from x.",
    "hat attains a maximum value at x and decreases exponentially as one gets farther and farther from x. At a high level, LIME attempts to find the explanation g* that minimizes a loss function that looks like: g*= argmin g ∈ GLf,g,x+ωg Where L(f,g,x) is a measure of the unfaithfulness of g in modeling f around the instance in question x, and ωg is a measure of the complexity of g.",
    "ss of g in modeling f around the instance in question x, and ωg is a measure of the complexity of g. Thus, minimiz‐ ing their sum results in an optimal explainer g* that has the desired characteristics of local fidelity and interpretability described earlier. How do we measure the unfaithfulness of a potential explainer?",
    "d interpretability described earlier. How do we measure the unfaithfulness of a potential explainer? The paper’s method‐ ology is to sample an instance z’ from the vicinity of x’, convert z’ back to an example z in the original feature space, and compute the difference between f(z) and g(z’). The difference represents the loss for that sample—if g(z’) is far from f(z), then it is not faithful to the model’s predictions at that point.",
    "sample—if g(z’) is far from f(z), then it is not faithful to the model’s predictions at that point. We can then weight this loss using the kernel πxz, which increasingly discounts the loss as the sample z gets further and further from the original example x. Putting this together, the loss function looks like: Lf,g,x= ∑z,zâπxz*fz−gzâ2 How do we achieve the samples z’ used in this loss function?",
    "on looks like: Lf,g,x= ∑z,zâπxz*fz−gzâ2 How do we achieve the samples z’ used in this loss function? The paper samples from the vicinity of x’ by selecting a subset of the x’ nonzero components, where each subset is chosen uniformly at random, and setting all other indices of the sample to zero ( Figure 11-6 ). 290 | Chapter 11: Methods in Interpretability --- Page 307 --- Figure 11-6.",
    "to zero ( Figure 11-6 ). 290 | Chapter 11: Methods in Interpretability --- Page 307 --- Figure 11-6. x can be thought of as some high-dimensional input such as an image, while each index of x’ is associated with some interpretable feature, where a 1 denotes the existence of that feature in x. The sampling procedure selects some subset of nonzero indices in x’ to keep nonzero in each of w’ and z', which are then mapped back to the original input space.",
    "in x’ to keep nonzero in each of w’ and z', which are then mapped back to the original input space. LIME then maps these samples z’ back to samples z from the original feature space so we can measure the fidelity of the explainer via f(z) – g(z’) . LIME also takes into account the complexity of the explainer via ωg, which enforces the interpretability aspect of viable explainers.",
    "complexity of the explainer via ωg, which enforces the interpretability aspect of viable explainers. In the specific case where G represents the class of linear models, the paper uses a version of ω that places a hard limit on the number of nonzero weights in g: ωg= ∞ * 1 wg0>K Where ωg represents g’s weight vector, the L0 norm counts the number of nonzero elements in ωg, and 1[*] is the indicator function, which evaluates to 1 if the condition within the function is satisfied, and 0 otherwise.",
    "r function, which evaluates to 1 if the condition within the function is satisfied, and 0 otherwise. The result is that ωg attains a value of infinity when ωg has more than K nonzero elements, and is 0 otherwise. This ensures that the chosen ωg will have at most K nonzero elements, LIME | 291 --- Page 308 --- 3Lundberg et al. “ A Unified Approach to Interpreting Model Predictions. ” arXiv Preprint arXiv :1705.07874.",
    "g et al. “ A Unified Approach to Interpreting Model Predictions. ” arXiv Preprint arXiv :1705.07874. 2017.since one can always do better than any proposed ωg that has more than K nonzero elements by simply zeroing out weights until there are at most K nonzero weights. This regularization approach is likely different from regularization approaches you have encountered in the past, such as the L1 or L2 norm on the weight vector.",
    "ization approaches you have encountered in the past, such as the L1 or L2 norm on the weight vector. In fact, to optimize the objective function defined in the paper, the authors utilize an algorithm they term K-LASSO, which involves first selecting K features via LASSO and then performing standard least squares optimization. After performing LIME, we are left with an optimal explainer g, which is a linear model with at most K nonzero weights in this case.",
    "ft with an optimal explainer g, which is a linear model with at most K nonzero weights in this case. Now we must check if g satisfies the goals that the authors set out from the beginning of the paper. First, g must be interpretable. Since we chose a relatively simple class of explainer models G, which were linear models in this example, all we need to explain the behavior of the model around the chosen example x are the values of the (at most) K nonzero weights of g.",
    "ior of the model around the chosen example x are the values of the (at most) K nonzero weights of g. The interpretable components associated with the nonzero weights are considered to be most important for prediction in that locality. In terms of local fidelity, our optimization procedure helps to ensure local fidelity by minimizing the least squares loss between the explainer’s predictions and the model’s predictions.",
    "y minimizing the least squares loss between the explainer’s predictions and the model’s predictions. However, there do exist limitations; for example, the paper notes that if the underlying model is highly nonlinear even within a short vicinity of the example we are explaining, our linear explainer won’t be able to do the model’s local behavior justice. With regards to being model agnostic, note that methodology of LIME does not concern itself with the underlying model’s structure.",
    "nostic, note that methodology of LIME does not concern itself with the underlying model’s structure. All LIME needs to function are predictions f(z) from the underlying model. And finally, to achieve a global perspective, we can select examples that are representative of the model’s behavior and display their explanations to the user. SHAP SHAP , or Shapley Additive Explanations,3 are similarly a per-prediction interpretabil‐ ity method for complex models.",
    "Additive Explanations,3 are similarly a per-prediction interpretabil‐ ity method for complex models. The paper that introduces the methodology of SHAP first provides a framework that the authors feel unifies a variety of interpretability methods in the field.",
    "ovides a framework that the authors feel unifies a variety of interpretability methods in the field. This framework is termed additive feature attribution , where all instances of this framework utilize a linear explanation model that acts on binary variables: gx′=φ0+ ∑i= 1Mφixi′ 292 | Chapter 11: Methods in Interpretability --- Page 309 --- Where M is the number of binary variables.",
    "Chapter 11: Methods in Interpretability --- Page 309 --- Where M is the number of binary variables. For example, LIME, when using a class of linear explainer models, follows this framework exactly, since each example to be explained is first converted to a binary vector over interpretable components. It turns out that, in the additive feature attribution framework, there exists a unique solution in this class that has three desirable properties: local accuracy, missingness, and consistency.",
    "ion in this class that has three desirable properties: local accuracy, missingness, and consistency. Before discussing the unique solution, we will describe the three properties in more detail. The first is local accuracy , which states that the explainer model must match the underlying model exactly at the example being interpreted. This is understandable as a desirable property, since it is reasonable that at least the example being interpreted should be explained perfectly.",
    "y, since it is reasonable that at least the example being interpreted should be explained perfectly. It’s important to note that not all interpretability frame‐ works necessarily follow this property. For example, the explainer that is generated by LIME, as presented in its original paper and described in the previous section, need not be locally accurate in the way that the authors of SHAP define local accuracy. This will be discussed further near the end of this section.",
    "authors of SHAP define local accuracy. This will be discussed further near the end of this section. Mathematically, local accuracy in SHAP is defined as: fx=gxâ=φ0+ ∑i= 1Mφixi′ Note that x’ is a simplified feature vector, where each feature in x’ is a binary variable that represents the presence or absence of a complex feature in the original input space.",
    "y variable that represents the presence or absence of a complex feature in the original input space. The second desirable property is missingness , which states that if x’ contains features equal to zero, the weights associated with those features in the explainer model should also be zero.",
    "qual to zero, the weights associated with those features in the explainer model should also be zero. This is also understandable as a desirable property, since there would be no influence of a feature with value of zero on the output under a linear explainer g, and thus no need to assign a nonzero weight to that feature in the explainer. And finally, the third desirable property is consistency .",
    "weight to that feature in the explainer. And finally, the third desirable property is consistency . This property states that if the underlying model changes such that a feature in the explainer space either increases or keeps its contribution constant, regardless of the values of the other features in the explainer space when compared to the original model, the explainer weight associated with that feature should be larger for the changed underlying model as compared to the original.",
    "ted with that feature should be larger for the changed underlying model as compared to the original. That was a mouthful, so we represent it more precisely in mathematical notation: Iff′ℎxz′−f′ℎxz′∖i≥fℎxz′−fℎxz′∖i,∀z′, thenφif′,x ≥φif,x Where h is the function that maps inputs from the interpretable space back to the original input space. Why is consistency a desirable property?",
    "m the interpretable space back to the original input space. Why is consistency a desirable property? For the new model, the SHAP | 293 --- Page 310 --- delta between the existence of the corresponding, more complex feature in the input space and its absence is greater than or equal to the delta for the old model, regardless of all other feature settings.",
    "e is greater than or equal to the delta for the old model, regardless of all other feature settings. Thus, it makes sense that we should attribute at least as large a weight to it in the explainer for the new model as compared to the old model, since its existence is clearly more important for the new model. As mentioned, for each underlying model f there exists a unique g within the additive attribution framework that also satisfies all three properties listed.",
    "unique g within the additive attribution framework that also satisfies all three properties listed. Although we won’t show this here, this result is one that follows from earlier results in cooperative game theory, where the learned weights are called Shapley values. Shapley values were originally defined to quantify per-example feature importance in multivariate linear regression models where individual features have significant correlations.",
    "ce in multivariate linear regression models where individual features have significant correlations. This is an important problem, especially in the setting of significant correlations due to ambiguity between which features are the most predictive.",
    "setting of significant correlations due to ambiguity between which features are the most predictive. It could be the case that a feature A is correlated with the target y, but when factoring in feature B it turns out that feature A provides only negligible additional value (i.e., predictions don’t change significantly, test statistics remain relatively constant, etc.).",
    "ue (i.e., predictions don’t change significantly, test statistics remain relatively constant, etc.). On the other hand, feature B may provide significant predictive power both in the individual case and in the case where feature A is included. Determining the relative importance of feature A and B in a vanilla multivariate regression that includes both features is difficult due to their non-negligible correla‐ tion.",
    "riate regression that includes both features is difficult due to their non-negligible correla‐ tion. Shapley values tease out these relationships and compute the true marginal impact of a given feature, as we will soon see. Here is the formula for Shapley values, where i represents the feature in question: φi= ∑S ∈ F ∖ iS! *F−S− 1! F!*fS ∪ixS ∪i−fSxS We will now break down this formula.",
    "ature in question: φi= ∑S ∈ F ∖ iS! *F−S− 1! F!*fS ∪ixS ∪i−fSxS We will now break down this formula. Intuitively, the Shapley value for an individual feature is computed by first taking the difference between the predictions for a model trained over a subset of features S plus the feature in question i included, and predictions for a model trained over the same subset of features S but with feature i withheld.",
    "and predictions for a model trained over the same subset of features S but with feature i withheld. The final Shapley value is a weighted sum of these differences over all possible subsets of features S. To find these differences, we can first train a multivariate linear regression model fS that only uses some subset of features S, and then train a second multivariate linear regression model fS ∪i that uses the subset of features S ∪i.",
    "hen train a second multivariate linear regression model fS ∪i that uses the subset of features S ∪i. Let the example we are explaining be denoted as x, and xA denote the portion of x that corresponds to some feature subset A. The difference fS ∪ixS ∪i – fSxS represents how much the prediction changes when we include the feature i.",
    "fference fS ∪ixS ∪i – fSxS represents how much the prediction changes when we include the feature i. Additionally, note that the formula is a sum over all possible feature subsets, which means that if the computed Shapley value for feature i is high, the difference between including feature i and not 294 | Chapter 11: Methods in Interpretability --- Page 311 --- including feature i was likely substantial for a majority of possible feature subsets.",
    "Page 311 --- including feature i was likely substantial for a majority of possible feature subsets. This result signifies that feature i generally adds significant predictive power regard‐ less of the features in S, which is captured by the high Shapley value. In the example provided earlier, we’ d find that feature B has a higher Shapley value than feature A.",
    "n the example provided earlier, we’ d find that feature B has a higher Shapley value than feature A. Additionally, the intuition behind the weighting scheme is that it achieves a more unbiased result for feature importance, since subsets of a given size occur either more or less frequently than subsets of a different size in the set of all subsets. The number of subsets of a given size is computed using the choose function, a concept from counting and probability.",
    "sets of a given size is computed using the choose function, a concept from counting and probability. When this is inverted and used as a weighting scheme, the result of a subset whose size occurs more frequently in the set of all possible subsets is weighted less than, for example, a feature subset consisting of only a single feature other than the feature in question. As stated earlier, we won’t prove why this is unbiased in full, but we hope that this makes the intuition clearer.",
    "ier, we won’t prove why this is unbiased in full, but we hope that this makes the intuition clearer. Y ou may notice that computing exact Shapley regression values for any reasonable number of features is intractable. This would involve training a regression model on all possible subsets of features, where the number of subsets of features (and thus models to train) grows exponentially with the number of features. We instead resort to approximation via sampling to help.",
    "exponentially with the number of features. We instead resort to approximation via sampling to help. Given an example x to explain and a regression model fS trained on some subset of features S, we can compute fS ∖i by taking an expectation of fS with respect to the distribution of feature i conditioned on x’s setting of features S ∖i: fS ∖i=EpxixS ∖ifSxS ∖i,xi Where the bold represents that xS ∖i is being treated as a known entity taken from x, the example being explained, while xi is being treated as an unknown, i.e., is a random variable rather than taking on the value provided by x.",
    "ing treated as an unknown, i.e., is a random variable rather than taking on the value provided by x. As has been a common theme throughout this book, we can approximate expectations like the preceding one in an unbiased manner by sampling and averaging: EpxixS ∖ifSxS ∖i,xi =EpxifSxS ∖i,xi ≈1 n∑ j= 1n fSxS ∖i,xij Y ou may have noticed the parallels between the procedure we just described and the estimation procedure described in “Partial Dependence Plots” on page 282.",
    "we just described and the estimation procedure described in “Partial Dependence Plots” on page 282. In fact, these are doing the exact same thing—notice that we again assume independence SHAP | 295 --- Page 312 --- between feature subset S ∖i and feature i, which allows us to use all samples of feature i from the dataset indiscriminately. The authors propose SHAP values in the general case, where SHAP values are given by the formula: φif,x= ∑z′⊆ x′z′! *M−z′− 1!",
    "values in the general case, where SHAP values are given by the formula: φif,x= ∑z′⊆ x′z′! *M−z′− 1! M!fℎxz′−fℎxz′∖i Where z’ is a subset of the nonzero components of x’. Additionally, z′∖i rep‐ resents setting feature i in the interpretable space equal to zero. Note that if feature i is already zero in the input x’, then the formula outputs zero as well since fℎxz′=fℎxz′∖i,∀z′⊆ x′. This quick check shows that the formula indeed satisfies the property of missingness.",
    "xz′∖i,∀z′⊆ x′. This quick check shows that the formula indeed satisfies the property of missingness. The vector consisting of SHAP values for each feature in x’ completely defines the optimal explainer model g in the additive attribution framework, where optimal signifies that g satisfies all three properties defined earlier: local accuracy, consistency, and missingness.",
    "that g satisfies all three properties defined earlier: local accuracy, consistency, and missingness. Right off the bat, we can see the parallels between the proposed SHAP values and the Shapley values from multivariate regression. Additionally, we can use the same sampling procedure to estimate SHAP values. As discussed, LIME is in the additive attribution framework.",
    "ling procedure to estimate SHAP values. As discussed, LIME is in the additive attribution framework. In the original LIME paper, the optimal explainer model g was selected via a specialized optimization procedure that first selected k features to have a nonzero contribution and then performed standard least squares optimization to achieve the final weights of g.",
    "ntribution and then performed standard least squares optimization to achieve the final weights of g. Due to these heuristics, including the choice of kernel πxz, there is no guarantee that the explainer selected using the procedure presented in the original LIME paper will satisfy the SHAP criteria of local accuracy, missingness, and consistency.",
    "original LIME paper will satisfy the SHAP criteria of local accuracy, missingness, and consistency. However, the optimization procedure presented in the LIME paper does achieve an explainer that satisfies the criteria for explainer models proposed in LIME; recall the concepts of being interpretable, having local fidelity, being model agnostic, and achieving global perspective from the previous section.",
    "ng local fidelity, being model agnostic, and achieving global perspective from the previous section. We point this out specifically to show that different groups of knowledgeable individuals don’t necessarily have the exact same idea of what it means for an explainer to be interpretable, and that interpretability as a concept has evolved over time.",
    "for an explainer to be interpretable, and that interpretability as a concept has evolved over time. It turns out that in LIME, there exists an exact form to the proximity measure πxz, ω, and loss function L such that when minimized, results in an optimal explainer g that satisfies all three SHAP criteria for interpretability: 296 | Chapter 11: Methods in Interpretability --- Page 313 --- ωg= 0 πx′z′=M− 1 Mchoosez′*z′*M−z′ Lf,g,π=∑ z′∈ Zfℎxz′−gz′2*πx′z′ We can optimize this loss function using a weighted least squares optimization to obtain the unique optimal g.",
    "imize this loss function using a weighted least squares optimization to obtain the unique optimal g. Note that the kernel here is distinct in interpretation from the kernel choices presented in the original LIME paper. Instead of the kernel decreasing in value as the samples get farther from the example being explained, the SHAP kernel is symmetric. This can be verified by examining the output of the kernel when |z'| = k and when |z'| = M – k .",
    "ic. This can be verified by examining the output of the kernel when |z'| = k and when |z'| = M – k . In fact, from just looking at the formula, we can see that the value of the kernel isn’t even dependent on x’.",
    "rom just looking at the formula, we can see that the value of the kernel isn’t even dependent on x’. In conclusion, SHAP values unify several existing interpretability methods by first defining the additive attribution framework, which is shared amongst these methods, and second by proving the existence of a unique optimal explainer within this framework that satisfies three desirable properties.",
    "tence of a unique optimal explainer within this framework that satisfies three desirable properties. Summary Although interpretability often comes in a variety of forms, they are all designed with the end goal of being able to explain model behavior. We learned that not every model is interpretable by construction, and even those that are might only be superficially so.",
    "very model is interpretable by construction, and even those that are might only be superficially so. For example, although vanilla linear regression seems to be quite interpretable by design, correlations between features can muddle this initially clear picture. Additionally, we learned about interpretability methods that are built into the model itself, such as extractive rationalization, and post hoc interpretability methods such as LIME and SHAP .",
    "f, such as extractive rationalization, and post hoc interpretability methods such as LIME and SHAP . The right form of interpretability will often depend on the domain—for example, using gradient-based methods for image classification may make sense, but not so much in language problems. The soft attention scheme discussed in previous chapters may not be as desirable for sentiment analysis as, say, the hard selection methodology presented in our section on extractive rationalization.",
    "ysis as, say, the hard selection methodology presented in our section on extractive rationalization. And finally, we learned about how interpretability does not carry the exact same meaning across the board, even in research—note our discussion on the differences between the optimal explainers generated by LIME and SHAP . We hope that this chapter served as a fruitful foray into the vast landscape of interpretability research.",
    "e that this chapter served as a fruitful foray into the vast landscape of interpretability research. Summary | 297 --- Page 315 --- CHAPTER 12 Memory Augmented Neural Networks Mostafa Samir So far we’ve seen how effective an RNN can be at solving a complex problem like machine translation. However, we’re still far from reaching its full potential!",
    "complex problem like machine translation. However, we’re still far from reaching its full potential! In Chapter 9 we mentioned that it’s theoretically proven that the RNN architecture is a universal functional representer; a more precise statement of the same result is that RNNs are Turing complete .",
    "nctional representer; a more precise statement of the same result is that RNNs are Turing complete . This simply means that given proper wiring and adequate parameters, an RNN can learn to solve any computable problem, which is basically any problem that can be solved by a computer algorithm or, equivalently, a Turing machine. Neural Turing Machines Though theoretically possible, it’s extremely difficult to achieve that kind of uni‐ versality in practice.",
    "theoretically possible, it’s extremely difficult to achieve that kind of uni‐ versality in practice. This difficulty stems from the fact that we’re looking at an immensely huge search space of possible wirings and parameter values of RNNs, a space so vastly large for gradient descent to find an appropriate solution for any arbitrary problem. However, in this chapter we’ll start exploring some approaches at the edge of research that will allow us to start tapping into that potential.",
    "ing some approaches at the edge of research that will allow us to start tapping into that potential. Let’s think for a while about a very simple reading comprehension question like the following: Mary travelled to the hallway. She grabbed the milk glass there. Then she travelled to the office, where she found an apple and grabbed it . How many objects is Mary carrying? 299 --- Page 316 --- The answer is so trivial: it’s two.",
    "bed it . How many objects is Mary carrying? 299 --- Page 316 --- The answer is so trivial: it’s two. But what actually happened in our brains that allowed us to come up with the answer so trivially? If we thought about how we could solve that comprehension question using a simple computer program, our approach would probably go like this: 1. allocate a memory location for a counter 2. initialize counter to 0 3. for each word in passage 3.1. if word is 'grabbed' 3.1.1. increment counter 4.",
    "alize counter to 0 3. for each word in passage 3.1. if word is 'grabbed' 3.1.1. increment counter 4. return counter value It turns out that our brains tackle the same task in a similar way to that simple computer program. Once we start reading, we start allocating memory (just as our computer program) and store the pieces of information we receive. We start by storing that location of Mary, which after the first sentence is the hallway.",
    "e receive. We start by storing that location of Mary, which after the first sentence is the hallway. In the second sentence we store the objects Mary is carrying, and by now it’s only a glass of milk. Once we see the third sentence, our brain modifies the first memory location to point to the office. By the end of the fourth sentence, the second memory location is modified to include both the milk and the apple.",
    "the fourth sentence, the second memory location is modified to include both the milk and the apple. When we finally encounter the question, our brains quickly query the second memory location and count the information there, which turns out to be two. In neuroscience and cognitive psychol‐ ogy, such a system of transient storing and manipulation of information is called a working memory , and it’s the main inspiration behind the line of research we’ll be discussing in the rest of this chapter.",
    "’s the main inspiration behind the line of research we’ll be discussing in the rest of this chapter. In 2014, Graves et al. from Google DeepMind started this line of work in a paper called “Neural Turing Machines” in which they introduced a new neural architecture with the same name, a Neural Turing Machine (NTM), that consists of a controller neural network (usually an RNN) with an external memory that resembles the brain’s working memory.",
    "r neural network (usually an RNN) with an external memory that resembles the brain’s working memory. For the close resemblance between the working memory model and the computer model we just saw, Figure 12-1 shows that the same resemblance holds for the NTM architecture, with the external memory in place of the RAM, the read/write heads in place of the read/write buses, and the controller network in place of the CPU, except for the fact that the controller learns its program, unlike the CPU, which is fed its program.",
    "xcept for the fact that the controller learns its program, unlike the CPU, which is fed its program. Figure 12-1 has a single read head and a single write head, but an NTM can have several in practice. 300 | Chapter 12: Memory Augmented Neural Networks --- Page 317 --- Figure 12-1.",
    "everal in practice. 300 | Chapter 12: Memory Augmented Neural Networks --- Page 317 --- Figure 12-1. Comparing the architecture of a modern-day computer, which is fed its program (left) to an NTM that learns its program (right) If we thought about NTMs in light of our earlier discussion of RNN’s Turing com‐ pleteness, we’ll find that augmenting the RNN with an external memory for transient storage prunes a large portion out of that search space, as we now don’t care about exploring RNNs that can both process and store the information; we’re just looking for the RNNs that can process the information stored outside of them.",
    "nformation; we’re just looking for the RNNs that can process the information stored outside of them. This pruning of the search space allows us to start tapping into some of the RNN potentials that were locked away before augmenting it with a memory, evident by the variety of tasks that the NTM could learn: from copying input sequences after seeing them, to emu‐ lating N-gram models, to performing a priority sort on data.",
    "ut sequences after seeing them, to emu‐ lating N-gram models, to performing a priority sort on data. We’ll even see by the end of the chapter how an extension to the NTM can learn to do reading comprehension tasks like the one we saw earlier, with nothing more than a gradient-based search.",
    "ing comprehension tasks like the one we saw earlier, with nothing more than a gradient-based search. Attention-Based Memory Access To be able to train an NTM with a gradient-based search method, we need to make sure that the whole architecture is differentiable so that we can compute the gradient of some output loss with respect to the model’s parameters that process the input. This property is called end-to-end-differentiable , with one end being the inputs and the other the outputs.",
    "perty is called end-to-end-differentiable , with one end being the inputs and the other the outputs. If we attempted to access the NTM’s memory in the same way a digital computer accesses its RAM, via discrete values of addresses, the discreteness of the addresses would introduce discontinuities in gradients of the output, and Attention-Based Memory Access | 301 --- Page 318 --- hence we would lose the ability to train the model with a gradient-based method.",
    "01 --- Page 318 --- hence we would lose the ability to train the model with a gradient-based method. We need a continuous way to access the memory while being able to “focus” on a specific location in it. This kind of continuous focusing can be achieved via attention methods. Instead of generating a discrete memory address, we let each head generate a normal‐ ized softmax attention vector with the same size as the number of memory locations.",
    "nerate a normal‐ ized softmax attention vector with the same size as the number of memory locations. With this attention vector, we’ll be accessing all the memory locations at the same time in a blurry manner, with each value in the vector telling us how much we’re going to focus on the corresponding location, or how likely we’re going to access it.",
    "how much we’re going to focus on the corresponding location, or how likely we’re going to access it. For example, to read a vector at a time step t out of our N×W NTM’s memory matrix denoted by Mt (where N is the number of locations and W is the size of the location), we generate an attention vector, or a weighting vector wt of size N, and our read vector can be calculated via the product: rt=Mt⊤wt where ⊤ denotes the matrix transpose operation.",
    "d vector can be calculated via the product: rt=Mt⊤wt where ⊤ denotes the matrix transpose operation. Figure 12-2 shows how with the weights attending to a specific location, we can retrieve a read vector that approxi‐ mately contains the same information as the content of that memory location. Figure 12-2.",
    "t approxi‐ mately contains the same information as the content of that memory location. Figure 12-2. A demonstration of how a blurry attention-based reading can retrieve a vector containing approximately the same information as in the focused-on location A similar attention weighting method is used for the write head: a weighting vector wt is generated and used for erasing specific information from the memory, as specified by the controller in an erase vector et that has W values between 0 and 1 specifying what to erase and what to keep.",
    "r in an erase vector et that has W values between 0 and 1 specifying what to erase and what to keep. Then we use the same weighting for writing to the erased memory matrix some new information, also specified by the controller in a write vector vt containing W values: 302 | Chapter 12: Memory Augmented Neural Networks --- Page 319 --- Mt=Mt− 1∘E−wtet⊤+wtvt⊤ where E is a matrix of ones and ∘ is element-wise multiplication.",
    "age 319 --- Mt=Mt− 1∘E−wtet⊤+wtvt⊤ where E is a matrix of ones and ∘ is element-wise multiplication. Similar to the reading case, the weighting wt tells us where to focus our erasing (the first term of the equation) and writing operations (the second term).",
    "here to focus our erasing (the first term of the equation) and writing operations (the second term). NTM Memory Addressing Mechanisms Now that we understand how NTMs access their memories in a continuous manner via attention weighting, we’re left with how these weightings are generated and what forms of memory addressing mechanisms they represent.",
    "th how these weightings are generated and what forms of memory addressing mechanisms they represent. We can understand that by exploring what NTMs are expected to do with their memories, and based on the model they are mimicking (the Turning machine), we expect them to be able to access a location by the value it contains, and to be able to go forward or backward from a given location. The first mode of behavior can be achieved with an access mechanism that we’ll call content-based addressing .",
    "mode of behavior can be achieved with an access mechanism that we’ll call content-based addressing . In this form of addressing, the controller emits the value that it’s looking for, which we’ll call a key kt, then it measures its similarity to the information stored in each location and focuses the attention on the most similar one. This kind of weighting can be calculated via: C(M,k, β) = expβDM,k ∑i= 0NexpβDMi,k where D is some similarity measure, like the cosine similarity.",
    "C(M,k, β) = expβDM,k ∑i= 0NexpβDMi,k where D is some similarity measure, like the cosine similarity. The equation is noth‐ ing more than a normalized softmax distribution over the similarity scores. There is, however, an extra parameter β that is used to attenuate the attention weights if needed. We call that the key strength .",
    "rameter β that is used to attenuate the attention weights if needed. We call that the key strength . The main idea behind that parameter is that for some tasks, the key emitted by the controller may not be close to any of the information in the memory, which would result in seemingly uniform attention weights.",
    "to any of the information in the memory, which would result in seemingly uniform attention weights. Figure 12-3 shows how the key strength allows the controller to learn how to attenuate such uniform attention to be more focused on a single location that is the most probable; the controller then learns what value of the strength to emit with each possible key it emits. NTM Memory Addressing Mechanisms | 303 --- Page 320 --- Figure 12-3.",
    "ith each possible key it emits. NTM Memory Addressing Mechanisms | 303 --- Page 320 --- Figure 12-3. An indecisive key with unit strength results in a nearly uniform attention vector; increasing the strength for keys like that focuses the attention on the most probable location To move forward and backward in the memory, we first need to know where we are we standing now, and such information is located in the access weighting from the last time step wt− 1.",
    "standing now, and such information is located in the access weighting from the last time step wt− 1. So to preserve the information about our current location with the new content-based weighting wtc we just got, we interpolate between the two weighting using a scalar gt that lies between 0 and 1: wtg=gtwtc+1 −gtwt− 1 We call gt the interpolation gate , and it’s also emitted by the controller to control the kind of information we want to use in the current time step.",
    "mitted by the controller to control the kind of information we want to use in the current time step. When the gate’s value is close to 1, we favor the addressing given by content lookup. However, when it’s close to 0, we tend to pass the information about our current location through and ignore the content-based addressing.",
    "to pass the information about our current location through and ignore the content-based addressing. The controller learns to use this gate so that, for example, it could set it to 0 when iteration through consecutive locations is desired and information about the current location is crucial. The type of information the controller chooses to gate through is denoted by the gated weighting wtg.",
    "he type of information the controller chooses to gate through is denoted by the gated weighting wtg. To start moving around the memory we need a way to take our current gated weight‐ ing and shift the focus from one location to another. This can be done by convoluting the gated weighting with a shift weighting st, also emitted by the controller.",
    "e done by convoluting the gated weighting with a shift weighting st, also emitted by the controller. This shift 304 | Chapter 12: Memory Augmented Neural Networks --- Page 321 --- weighting is a normalized softmax attention vector of size n+ 1, where n is an even integer specifying the number of possible shifts around the focused-on location in the gated weighting; for example, if it has a size of 3, then there are two possible shifts around a location: one forward and one backward.",
    "has a size of 3, then there are two possible shifts around a location: one forward and one backward. Figure 12-4 shows how a shift weighting can move around the focused-on location in gated weighting. The shifting occurs by convoluting the gated weighting by the shift weighting in pretty much the same way we convoluted images with feature maps back in Chapter 7 . The only exception is how we handle the case when the shift weightings go outside the gated weighting.",
    "e only exception is how we handle the case when the shift weightings go outside the gated weighting. Instead of using padding like we did before, we use a rotational convolution operator where overflown weights get applied to the values at the other end of the gated weighting, as shown in the middle panel of Figure 12-4 . This operation can be expressed element-wise as: wti= ∑j= 0stwtgi+st− 1 2−jmodNstj Figure 12-4.",
    "-4 . This operation can be expressed element-wise as: wti= ∑j= 0stwtgi+st− 1 2−jmodNstj Figure 12-4. A shift weighting focused on the right shifts the gated weighting one location to the right (left). Rotational convolution on a left-focused shift weighting, shifting the gated weighting to the left (middle). A nonsharp centered shift weighting keeps the gated weighting intact but disperses it (right).",
    "dle). A nonsharp centered shift weighting keeps the gated weighting intact but disperses it (right). With the introduction of the shifting operation, our heads’ weightings can now move around the memory freely forward and backward. However, a problem occurs if at any time the shift weighting is not sharp enough.",
    "ward and backward. However, a problem occurs if at any time the shift weighting is not sharp enough. Because of the nature of the convo‐ lution operation, a nonsharp shift weighting (as in the right panel of Figure 12-4 ) disperses the original gated weightings around its surroundings and results in a less-focused shifted weighting. To overcome that blurring effect, we run the shifted weightings through one last operation: a sharpening operation.",
    "t blurring effect, we run the shifted weightings through one last operation: a sharpening operation. The controller emits one last scalar γt≥ 1 that sharpens the shifted weightings via: wt=wtγt ∑i= 0Nwtiγt NTM Memory Addressing Mechanisms | 305 --- Page 322 --- 1Source: Graves et al. “Neural Turing Machines. ” (2014)Starting from interpolation down to the final weighting vector out of sharpening, this process constitutes the second addressing mechanism of NTMs: the location-based mechanism .",
    "ng, this process constitutes the second addressing mechanism of NTMs: the location-based mechanism . Using a combination of both addressing mechanisms, an NTM is able to utilize its memory to learn to solve various tasks. One of these tasks that would allow us to get a deeper look into the NTM in action is the copy task shown in Figure 12-5 . In this task, we present the model with a sequence of random binary vectors that terminate with a special end symbol.",
    "present the model with a sequence of random binary vectors that terminate with a special end symbol. We then request the same input sequence to be copied to the output. Figure 12-5. An NTM trained on the copy task1 The visualization shows how at the input time, the NTM starts writing the inputs step-by-step into consecutive locations in the memory.",
    "input time, the NTM starts writing the inputs step-by-step into consecutive locations in the memory. In the output time, the NTM goes back at the first written vector and iterates through the next locations to read and return the previously written input sequence. The original NTM paper contains several other visualizations of NTMs trained on different problems that are worth checking.",
    "contains several other visualizations of NTMs trained on different problems that are worth checking. These visualizations demonstrate the architecture’s ability to utilize the addressing mechanisms to adapt to and learn to solve various tasks. We’ll suffice with our current understanding of NTMs and skip its implementation.",
    "lve various tasks. We’ll suffice with our current understanding of NTMs and skip its implementation. Instead, we will spend the rest of the chapter exploring the drawbacks of NTMs and how the novel architecture of the differentiable neural computer (DNC) was able to overcome these drawbacks. We’ll conclude our discussion by implementing that novel architecture on simple reading comprehension tasks like the one we saw earlier.",
    "lementing that novel architecture on simple reading comprehension tasks like the one we saw earlier. 306 | Chapter 12: Memory Augmented Neural Networks --- Page 323 --- Differentiable Neural Computers Despite the power of NTMs, they have a few limitations regarding their memory mechanisms. The first of these limitations is that NTMs have no way to ensure that no interference or overlap between written data would occur.",
    "is that NTMs have no way to ensure that no interference or overlap between written data would occur. This is due to the nature of the “differentiable” writing operation in which we write new data everywhere in the memory to some extent specified by the attention. Usually, the attention mechanisms learn to focus the write weightings strongly on a single mem‐ ory location, and the NTM converges to a mostly interference-free behavior, but that’s not guaranteed.",
    "y location, and the NTM converges to a mostly interference-free behavior, but that’s not guaranteed. However, even when the NTM converges to an interference-free behavior, once a memory location has been written to, there’s no way to reuse that location again, even when the data stored in it becomes irrelevant. The inability to free and reuse memory locations is the second limitation of the NTM architecture.",
    ". The inability to free and reuse memory locations is the second limitation of the NTM architecture. This results in new data being written to new locations that are likely to be contiguous, as we saw with the copy task. This contiguous writing fashion is the only way for an NTM to record any temporal information about the data being written: consecutive data is stored in consecutive locations.",
    "poral information about the data being written: consecutive data is stored in consecutive locations. If the write head jumped to another place in the memory while writing some consecutive data, a read head won’t be able to recover the temporal link between the data written before and after the jump: this constitutes the third limitation of NTMs. In October 2016, Graves et al.",
    "re and after the jump: this constitutes the third limitation of NTMs. In October 2016, Graves et al. from DeepMind published in Nature a paper titled, “Hybrid Computing Using a Neural Network with Dynamic External Memory, ” in which they introduced a new memory-augmented neural architecture called differen‐ tiable neural computer (DNC) that improves on NTMs and addresses the limitations we just discussed. Similar to NTMs, DNCs consists of a controller that interacts with an external memory.",
    "st discussed. Similar to NTMs, DNCs consists of a controller that interacts with an external memory. The memory consists of N words of size W, making up an N×W matrix we’ll be calling M. The controller takes in an input vector of size X and the R vectors of size W read from memory in the previous step, where R is the number of read heads.",
    "the R vectors of size W read from memory in the previous step, where R is the number of read heads. The controller then processes them through a neural network and returns two pieces of information: •An interface vector that contains all the necessary information to query the memory (i.e., write and read from it) •A pre-output vector of size Y The external memory then takes in the interface vector, performs the necessary writing through a single write head, then reads R new vectors from the memory.",
    "erforms the necessary writing through a single write head, then reads R new vectors from the memory. It returns the newly read vectors to the controller to be added with the pre-output vector, producing the final output vector of size Y. Differentiable Neural Computers | 307 --- Page 324 --- Figure 12-6 summarizes the operation of the DNC that we just described. We can see that unlike NTMs, DNCs keep other data structures alongside the memory itself to keep track of the state of the memory.",
    "NCs keep other data structures alongside the memory itself to keep track of the state of the memory. As we’ll shortly see, with these data structures and some clever new attention mechanisms, DNCs are able to successfully overcome NTM’s limitations. Figure 12-6. An overview of DNC’s architecture and operation To make the whole architecture differentiable, DNCs access the memory through weight vectors of size N whose elements determine how much the heads focus on each memory location.",
    "weight vectors of size N whose elements determine how much the heads focus on each memory location. There are R weightings for the read heads wtr, 1,⋯, wtr,R where t denotes the time step. On the other hand, there’s one write weighting wtw for the single write head.",
    "denotes the time step. On the other hand, there’s one write weighting wtw for the single write head. Once we obtain these weightings, we can modify the memory matrix and get updated via: Mt=Mt− 1∘E− wtwet⊤+ wtwvt⊤ and et, vt are the erase and write vectors we saw earlier with NTMs, coming from the controller through the interface vector as instructions about what to erase from and write to the memory.",
    "oller through the interface vector as instructions about what to erase from and write to the memory. As soon as we get the updated memory matrix Mt, we can read out the new read vectors rt1, rt2,⋯, rtR using the following equation for each read weighting: rti=Mt⊤wtr,i 308 | Chapter 12: Memory Augmented Neural Networks --- Page 325 --- Up until now, it seems that there’s nothing different from how NTMs write to and read from memory.",
    "- Up until now, it seems that there’s nothing different from how NTMs write to and read from memory. However, the differences will start to show up when we discuss the attention mechanisms DNCs use to obtain their access weightings. While they both share the content-based addressing mechanism C(M, k, β) defined earlier, DNCs use more sophisticated mechanisms to attend more efficiently to the memory.",
    "β) defined earlier, DNCs use more sophisticated mechanisms to attend more efficiently to the memory. Interference-Free Writing in DNCs The first limitation we discussed for NTMs was their inability to ensure an interference-free writing behavior. An intuitive way to address this issue is to design the architecture to focus strongly on a single, free memory location and not wait for NTM to learn to do so.",
    "itecture to focus strongly on a single, free memory location and not wait for NTM to learn to do so. To keep track of which locations are free and which are busy, we need to introduce a new data structure that can hold this kind of information. We’ll call it the usage vector.",
    "troduce a new data structure that can hold this kind of information. We’ll call it the usage vector. The usage vector ut is a vector of size N, where each element holds a value between 0 and 1 that represents how much of the corresponding memory location is used; with 0 indicating a completely free location, and 1 indicating a completely used one. The usage vector initially contains zeros u0=0 and gets updated with the usage information across the steps.",
    "e vector initially contains zeros u0=0 and gets updated with the usage information across the steps. Using this information, it’s clear that the location to which the weights should attend most strongly is the one with the least usage value. To obtain such weighting, we need to first sort the usage vector and obtain the list of location indices in ascending order of the usage; we call such a list a free list and denote it by φt.",
    "cation indices in ascending order of the usage; we call such a list a free list and denote it by φt. Using that free list, we can construct an intermediate weighting called the allocation weighting at that would determine which memory location should be allocated for new data. We calculate at using: atφtj=1 − utφtj∏i= 1j− 1utφti wherej ∈1,⋯,N This equation may look incomprehensible at first glance.",
    "atφtj=1 − utφtj∏i= 1j− 1utφti wherej ∈1,⋯,N This equation may look incomprehensible at first glance. A good way to understand it is to work through it with a numerical example, for example, when ut=1, 0 . 7, 0 . 2, 0 . 4 . We’ll leave the details for you to go through. In the end, you should arrive at the allocation weighting being at=0, 0 . 024, 0 . 8, 0 . 12 .",
    "ough. In the end, you should arrive at the allocation weighting being at=0, 0 . 024, 0 . 8, 0 . 12 . As we go through the calculations, we’ll begin to understand how this formula works: the 1 − utφtjmakes the location weight proportional to how free it is.",
    "stand how this formula works: the 1 − utφtjmakes the location weight proportional to how free it is. By noticing that the product ∏i= 1j− 1utφtj gets smaller and smaller as we iterate through the free list (because we keep multiplying small values between 0 and 1), we can see that this product decreases the location weight even more as we go from the least used location to the most used one, which finally results in the least used location having the largest weight, while the most used one gets the smallest weight.",
    "the least used location having the largest weight, while the most used one gets the smallest weight. So we’re able to guarantee the ability to focus on a single location by design without the the need to Interference-Free Writing in DNCs | 309 --- Page 326 --- hope for the model to learn it on its own from scratch; this means more reliability as well as faster training time.",
    "el to learn it on its own from scratch; this means more reliability as well as faster training time. With the allocation weighting at and lookup weighting ctw we get from the content- based addressing mechanism ctw=CMt− 1,ktw,βtw, where ktw,βtw are the lookup key and the lookup strength we receive through the interface vector, we can now construct our final write weighting: wtw=gtwgtaat+1 −gtactw where gtw and gta are values between 0 and 1 and are called the write and allocation gates, which we also get from the controller through the interface vector.",
    "the write and allocation gates, which we also get from the controller through the interface vector. These gates control the writing operation, with gtw determining if any writing is going to happen in the first place, and gta specifying whether we’ll write to a new location using the allocation weighting or modify an existing value specified by the lookup weighting. DNC Memory Reuse What if while we calculate the allocation weighting we find that all locations are used, or in other words, ut=1?",
    "we calculate the allocation weighting we find that all locations are used, or in other words, ut=1? This means that the allocation weightings will turn out all zeros and no new data can be allocated to memory. This raises the need for the ability to free and reuse the memory. In order to know which locations can be freed and which cannot, we construct a retention vector ψt of size N that specifies how much of each location should be retained and not get freed.",
    "n vector ψt of size N that specifies how much of each location should be retained and not get freed. Each element of this vector takes a value between 0 and 1, with 0 indicating that the corresponding location can be freed, and 1 indicating that it should be retained.",
    "ndicating that the corresponding location can be freed, and 1 indicating that it should be retained. This vector is calculated using: ψt= ∏i= 1R1−ftiwt− 1r,i This equation is basically saying that the degree to which a memory location should be freed is proportional to how much is read from it in the last time steps by the various read heads (represented by the values of the read weightings wt− 1r,i).",
    "st time steps by the various read heads (represented by the values of the read weightings wt− 1r,i). However, continuously freeing a memory location once its data is read is not generally prefera‐ ble as we might still need the data afterward. We let the controller decide when to free and when to retain a location after reading by emitting a set of R free gates ft1,⋯,ftR that have a value between 0 and 1.",
    "ocation after reading by emitting a set of R free gates ft1,⋯,ftR that have a value between 0 and 1. This determines how much freeing should be done based on the fact that the location was just read from. The controller will then learn how to use these gates to achieve the behavior it desires.",
    "read from. The controller will then learn how to use these gates to achieve the behavior it desires. 310 | Chapter 12: Memory Augmented Neural Networks --- Page 327 --- Once the retention vector is obtained, we can use it to update the usage vector to reflect any freeing or retention made via: ut=ut− 1+ wt− 1w− ut− 1∘wt− 1w∘ψt This equation can be read as follows: a location will be used if it has been retained (its value in ψt≈ 1) and either it’s already in use or has just been written to (indicated by its value in ut− 1+ wt− 1w).",
    "nd either it’s already in use or has just been written to (indicated by its value in ut− 1+ wt− 1w). Subtracting the element-wise product ut− 1∘wt− 1w brings the whole expression back between 0 and 1 to be a valid usage value in case the addition between the previous usage got the write weighting past 1. By doing this usage update step before calculating the allocation, we can introduce some free memory for possible new data.",
    "ate step before calculating the allocation, we can introduce some free memory for possible new data. We’re also able to use and reuse a limited amount of memory efficiently and overcome the second limitation of NTMs.",
    "to use and reuse a limited amount of memory efficiently and overcome the second limitation of NTMs. Temporal Linking of DNC Writes With the dynamic memory management mechanisms that DNCs use, each time a memory location is requested for allocation, we’re going to get the most unused location, and there’ll be no positional relation between that location and the location of the previous write.",
    "and there’ll be no positional relation between that location and the location of the previous write. With this type of memory access, NTM’s way of preserving temporal relation with contiguity is not suitable. We’ll need to keep an explicit record of the order of the written data. This explicit recording is achieved in DNCs via two additional data structures along‐ side the memory matrix and the usage vector.",
    "ieved in DNCs via two additional data structures along‐ side the memory matrix and the usage vector. The first is called a precedence vector pt, an N-sized vector considered to be a probability distribution over the memory loca‐ tions, with each value indicating how likely the corresponding location was the last one written to.",
    "tions, with each value indicating how likely the corresponding location was the last one written to. The precedence is initially set to p0=0 and gets updated in the following steps via: pt=1 − ∑i= 1Nwtwipt− 1+ wtw Updating is done by first resetting the previous values of the precedence with a reset factor that is proportionate to how much writing was just made to the memory (indicated by the summation of the write weighting’s components).",
    "riting was just made to the memory (indicated by the summation of the write weighting’s components). Then the value of write weighting is added to the reset value so that a location with a large write weighting (that is the most recent location written to) would also get a large value in the precedence vector. The second data structure we need to record temporal information is the link matrix Lt.",
    "ence vector. The second data structure we need to record temporal information is the link matrix Lt. The link matrix is an N×N matrix in which the element Lti,j has a Temporal Linking of DNC Writes | 311 --- Page 328 --- value between 0,1, indicating how likely it is that location i was written after location j.",
    "328 --- value between 0,1, indicating how likely it is that location i was written after location j. This matrix is also initialized to zeros, and the diagonal elements are kept at zero throughout the time Lti,i= 0, as it’s meaningless to track if a location was written after itself when the previous data has already been overwritten and lost.",
    "if a location was written after itself when the previous data has already been overwritten and lost. However, each other element in the matrix is updated using: Lti,j=1 − wtwi− wtwjLt− 1i,j+ wtwipt− 1j The equation follows the same pattern we saw with other update rules: first the link element is reset by a factor proportional to how much writing had been done on locations i,j.",
    "e link element is reset by a factor proportional to how much writing had been done on locations i,j. Then the link is updated by the correlation (represented here by multiplication) between the write weighting at location i and the previous precedence value of location j. This eliminates NTM’s third limitation; now we can keep track of temporal information no matter how the write head hops around the memory.",
    "; now we can keep track of temporal information no matter how the write head hops around the memory. Understanding the DNC Read Head Once the write head has finished updating the memory matrix and the associated data structures, the read head is now ready to work. Its operation is simple: it needs to be able to look up values in the memory and be able to iterate forward and back‐ ward in temporal ordering between data.",
    "alues in the memory and be able to iterate forward and back‐ ward in temporal ordering between data. The lookup ability can simply be achieved with content-based addressing: for each read head i, we calculate an intermediate weighting ctr,i=CMt,ktr,i,βtr,i, where ktr, 1,⋯,ktr,R and βtr, 1,⋯,βtr,R are two sets of R read keys and strengths received from the controller in the interface vector.",
    "tr,R are two sets of R read keys and strengths received from the controller in the interface vector. To achieve forward and backward iterations, we need to make the weightings go a step ahead or back from the location they recently read from. We can achieve that for the forward iteration by multiplying the link matrix by the last read weightings.",
    "n achieve that for the forward iteration by multiplying the link matrix by the last read weightings. This shifts the weights from the last read location to the location of the last write specified by the link matrix and constructs an intermediate forward weighting for each read head i: fti= Ltwt− 1r,i. Similarly, we construct an intermediate backward weighting by multiplying the transpose of the link matrix by the last read weight‐ ings bti= Lt− 1⊤wt− 1r,i.",
    "g by multiplying the transpose of the link matrix by the last read weight‐ ings bti= Lt− 1⊤wt− 1r,i. We can now construct the new read weightings for each read using the following rule: wtr,i=πti1bti+πti2cti+πti3fti where πt1,⋯,πtR are called the read modes . Each of these are a softmax distribution over three elements that come from the controller on the interface vector.",
    "re a softmax distribution over three elements that come from the controller on the interface vector. Its three values determine the emphasis the read head should put on each read mechanism: 312 | Chapter 12: Memory Augmented Neural Networks --- Page 329 --- backward, lookup, and forward, respectively. The controller learns to use these modes to instruct the memory on how data should be read.",
    "ctively. The controller learns to use these modes to instruct the memory on how data should be read. The DNC Controller Network Now that we’ve figured out the internal workings of the external memory in the DNC architecture, we’re left with understanding how the controller that coordinates all the memory operations work.",
    "e, we’re left with understanding how the controller that coordinates all the memory operations work. The controller’s operation is simple: in its heart there’s a neural network (recurrent or feed-forward) that takes in the input step along with the read-vectors from the last step and outputs a vector whose size depends on the architecture we chose for the network.",
    "the last step and outputs a vector whose size depends on the architecture we chose for the network. Let’s denote that vector by N(χt), where N denotes whatever function is computed by the neural network, and χt denotes the concatenation of the input step and the last read vectors χt=xt; rt− 11;⋯; rt− 1R. This concatenation of the last read vectors serves a similar purpose as the hidden state in a regular LSTM: to condition the output on the past.",
    "serves a similar purpose as the hidden state in a regular LSTM: to condition the output on the past. From that vector emitted from the neural network, we need two pieces of informa‐ tion. The first one is the interface vector ζt. As we saw, the interface vector holds all the information for the memory to carry out its operation. We can look at the ζt vector as a concatenation of the individual elements we encountered before, as depicted in Figure 12-7 . Figure 12-7.",
    "tenation of the individual elements we encountered before, as depicted in Figure 12-7 . Figure 12-7. The interface vector decomposed to its individual components By summing up the sizes along the components, we can consider the ζt vector as one big vector of size R×W+ 3W+ 5R+ 3.",
    "sizes along the components, we can consider the ζt vector as one big vector of size R×W+ 3W+ 5R+ 3. So in order to obtain that vector from the network output, we construct a learnable N ×R×W+ 3W+ 5R+ 3 weights matrix Wζ, where N is the size of the network’s output, such that: ζt=WζNχt Before passing that ζt vector to the memory, we need to make sure that each compo‐ nent has a valid value.",
    "passing that ζt vector to the memory, we need to make sure that each compo‐ nent has a valid value. For example, all the gates as well as the erase vector must have values between 0 and 1, so we pass them through a sigmoid function to ensure that requirement: et=σet,fti=σfti,gta=σgta,gtw=σgtwwhereσz=1 1 +e−z The DNC Controller Network | 313 --- Page 330 --- Also, all the lookup strengths need to have a value larger than or equal to 1, so we pass them through a oneplus function first: βtr,i= oneplus βtr,i,βtw= oneplus βtwwhere oneplus z= 1 + log 1 +ez And finally, the read modes must have a valid softmax distribution: πti= softmax πtiwhere softmax z=ez ∑jezj By these transformations, the interface vector is now ready to be passed to the mem‐ ory; and while it guides the memory in its operations, we’ll be needing a second piece of information from the neural network, the pre-output vector vt.",
    "s, we’ll be needing a second piece of information from the neural network, the pre-output vector vt. This is a vector of the same size of the final output vector, but it’s not the final output vector. By using another learnable N ×Y weights matrix Wy, we can obtain the pre-output via: vt=WyNχt This pre-output vector gives us the ability to condition our final output not just on the network output, but also on the recently read vectors rt from memory.",
    "r final output not just on the network output, but also on the recently read vectors rt from memory. Via a third learnable R×W×Y weights matrix Wr, we can get the final output as: yt=vt+Wrrt1;⋯; rtR Given that the controller knows nothing about the memory except for the word size W, an already learned controller can be scaled to a larger memory with more locations without any need for retraining.",
    "ned controller can be scaled to a larger memory with more locations without any need for retraining. Also, the fact that we didn’t specify any particular structure for the neural network or any particular loss function makes DNC a universal architecture that can be applied to a variety of tasks and learning problems.",
    "makes DNC a universal architecture that can be applied to a variety of tasks and learning problems. Visualizing the DNC in Action One way to see DNC’s operation in action is to train it on a simple task that would allow us to look at the weightings and the parameters’ values and visualize them in an interpretable way. For this simple task, we’ll use the copy problem we already saw with NTMs, but in a slightly modified form.",
    "s simple task, we’ll use the copy problem we already saw with NTMs, but in a slightly modified form. 314 | Chapter 12: Memory Augmented Neural Networks --- Page 331 --- Instead of trying to copy a single sequence of binary vectors, our task here will be to copy a series of such sequences. In Figure 12-8 , (a) shows the single sequence input.",
    "re will be to copy a series of such sequences. In Figure 12-8 , (a) shows the single sequence input. After processing such single sequence input and copying the same sequence to the output, the DNC would have finished its program, and its memory would be reset in a way that will not allow us to see how it can dynamically manage it. Instead we’ll treat a series of such sequences, shown in Figure 12-8 (b), as a single input. Figure 12-8.",
    "ad we’ll treat a series of such sequences, shown in Figure 12-8 (b), as a single input. Figure 12-8. Single sequence input versus a series of input sequences Figure 12-9 shows a visualization of the DNC operation after being trained on a series of length 4 where each sequence contains five binary vectors and an end mark. The DNC used here has only 10 memory locations, so there’s no way it can store all 20 vectors in the input.",
    "used here has only 10 memory locations, so there’s no way it can store all 20 vectors in the input. A feed-forward controller is used to ensure that nothing would be stored in a recurrent state, and only one read head is used to make the visualization more clear. These constraints should force the DNC to learn how to deallocate and reuse memory to successfully copy the whole input, and indeed it does.",
    "o learn how to deallocate and reuse memory to successfully copy the whole input, and indeed it does. We can see in that visualization how the DNC is writing each vector of the five in a sequence into a single memory location. As soon as the end mark is seen, the read head starts reading from these locations in the exact same order of writing. We can see how both the allocation and free gates alternate in activation between writing and reading phases of each sequence in the series.",
    "ree gates alternate in activation between writing and reading phases of each sequence in the series. From the usage vector chart at the bottom, we can also see how after a memory location is written to, its usage becomes exactly 1, and how it drops to 0 just after reading from that location, indicating that it was freed and can be reused again. Visualizing the DNC in Action | 315 --- Page 332 --- Figure 12-9.",
    "was freed and can be reused again. Visualizing the DNC in Action | 315 --- Page 332 --- Figure 12-9. Visualization of the DNC operation on the copy problem This visualization is part of the open source implementation of the DNC architecture by Mostafa Samir . In the next section we’ll learn the important tips and tricks that will allow us to implement a simpler version of DNC on the reading comprehension tasks.",
    "tricks that will allow us to implement a simpler version of DNC on the reading comprehension tasks. 316 | Chapter 12: Memory Augmented Neural Networks --- Page 333 --- Implementing the DNC in PyTorch Implementing the DNC architecture is essentially a direct application of the math we just discussed. So with the full implementation in the code repository associated with the book, we’ll just be focusing on the tricky parts and introduce some new PyTorch practice while we’re at it.",
    "e’ll just be focusing on the tricky parts and introduce some new PyTorch practice while we’re at it. The main part of the implementation resides in the mem_ops.py file where all of the attention and access mechanisms are implemented. This file is then imported to be used with the controller. Two operations that might be a little tricky to implement are the link matrix update and the allocation weighting calculation.",
    "be a little tricky to implement are the link matrix update and the allocation weighting calculation. Both of these opera‐ tions can be naively implemented with for loops, but using for loops in creating a computational graph is generally not a good idea.",
    "with for loops, but using for loops in creating a computational graph is generally not a good idea. Let’s take the link matrix update operation first and see how it looks with a loop-based implementation: def Lt(L, wwt, p, N): L_t = torch.zeros((N,N), dtype=torch.float32) for i in range(N): for j in range(N): if i == j: continue mask = torch.zeros((N,N), dtype=torch.float32) mask[i,j] = 1.0 link_t = (1 - wwt[i] - wwt[j]) * L[i,j] + \\ wwt[i] * p[j] L_t += mask * link_t return L_t After that computational graph is fully defined, it’s then fed with concrete values and executed.",
    "_t After that computational graph is fully defined, it’s then fed with concrete values and executed. With that in mind, we can see, as depicted in Figure 12-10 , how in most of the iterations of the for loop, a new set of nodes representing the loop body gets added in the computational graph. So for N memory locations, we end up with N2−N identical copies of the same nodes, each for each iteration, each taking up a chunk of our RAM and needing its own time to be processed before the next can be.",
    ", each taking up a chunk of our RAM and needing its own time to be processed before the next can be. When N is a small number, say 5, we get 20 identical copies, which is not so bad. However, if we want to use a larger memory, like with N= 256 , we get 65,280 identical copies of the nodes, which is catastrophic for both the memory usage and the execution time. Implementing the DNC in PyTorch | 317 --- Page 334 --- Figure 12-10.",
    "y usage and the execution time. Implementing the DNC in PyTorch | 317 --- Page 334 --- Figure 12-10. The computational graph of the link matrix update operation built with the for loop implementation One possible way to overcome such an issue is vectorization . In vectorization, we take an array operation that is originally defined in terms of individual elements and rewrite it as an operation on the whole array at once.",
    "y defined in terms of individual elements and rewrite it as an operation on the whole array at once. For the link matrix update, we can rewrite the operation as: Lt=1 − wtw⊕wtw∘Lt− 1+ wtwpt− 1∘1 −I Where I is the identity matrix, and the product wtwpt− 1 is an outer product. To achieve this vectorization, we define a new operator, the pairwise-addition of vectors, denoted by ⊕.",
    "chieve this vectorization, we define a new operator, the pairwise-addition of vectors, denoted by ⊕. This new operator is simply defined as: u⊕v =u1+v1⋯ u 1+vn ⋮ ⋱ ⋮ un+v1⋯ un+vn 318 | Chapter 12: Memory Augmented Neural Networks --- Page 335 --- This operator adds a little bit to the memory requirements of the implementation, but not as much as the case in the loop-based implementation.",
    "ry requirements of the implementation, but not as much as the case in the loop-based implementation. With this vectorized reformulation of the update rule, we rewrite a more memory- and time-efficient implementation: def Lt(L, wwt, p, N): \"\"\" returns the updated link matrix given the previous one along with the updated write weightings and the previous precedence vector \"\"\" def pairwise_add(v): \"\"\" returns the matrix of pairs - adding the elements of v to themselves \"\"\" n = v.shape[0] # a NxN matrix of duplicates of u along the columns V = v.repeat(1,n) return V + V # expand dimensions of wwt and p to make matmul behave as outer # product wwt = torch.unsqueeze(wwt, 1) p = torch.unsqueeze(p, 0) I = torch.eye(N, dtype=torch.float32) return (((1 - pairwise_add(wwt)) * L + torch.matmul(wwt, p)) * (1 - I)) A similar process could be made for the allocation weightings rule.",
    "torch.matmul(wwt, p)) * (1 - I)) A similar process could be made for the allocation weightings rule. Instead of having a single rule for each element in the weighting vector, we can decompose it into a few operations that work on the whole vector at once: 1.While sorting the usage vector to get the free list, we also grab the sorted usage vector itself. 2.We calculate the cumulative product vector of the sorted usage.",
    "ab the sorted usage vector itself. 2.We calculate the cumulative product vector of the sorted usage. Each element of that vector is the same as the product term in our original element-wise rule. 3.We multiply the cumulative product vector by (1 – the sorted usage vector). The resulting vector is the allocation weighting but in the sorted order, not the original order of the memory location.",
    "is the allocation weighting but in the sorted order, not the original order of the memory location. 4.For each element of that out-of-order allocation weighting, we take its value and put it in the corresponding index in the free list. The resulting vector is now the correct allocation weighting that we want. Figure 12-11 summarizes this process with a numerical example. Implementing the DNC in PyTorch | 319 --- Page 336 --- Figure 12-11.",
    "ocess with a numerical example. Implementing the DNC in PyTorch | 319 --- Page 336 --- Figure 12-11. The vectorized process of calculating the allocation weightings It may seem that we still need loops for the sorting operation in step 1 and for reor‐ dering the weights in step 4, but fortunately PyTorch provides symbolic operations that would allow us to carry out these operations without the need for a Python loop. For sorting we’ll be using torch.topk .",
    "rry out these operations without the need for a Python loop. For sorting we’ll be using torch.topk . This operation takes a tensor and a number k, and returns both the sorted top k values in descending order and the indices of these values. To get the sorted usage vector in ascending order, we need to get the top N values of the negative of the usage vector.",
    "sage vector in ascending order, we need to get the top N values of the negative of the usage vector. We can bring back the sorted values to their original signs by multiplying the resulting vector by −1: sorted_ut, free_list = torch.topk(-1*ut, N) sorted_ut *= -1 For reordering the allocation weights, we first create an empty tensor array of size N to be the container of the weights in their correct order, and then put the values in their correct places using the instance method scatter(indices, values) .",
    "and then put the values in their correct places using the instance method scatter(indices, values) . This method takes in its second argument a tensor, and scatters the values along its first 320 | Chapter 12: Memory Augmented Neural Networks --- Page 337 --- dimension across the array, with the first argument being a list of indices of the loca‐ tions to which we want to scatter the corresponding values.",
    "ent being a list of indices of the loca‐ tions to which we want to scatter the corresponding values. In our case here, the first argument is the free list, and the second is the out-of-order allocation weightings.",
    "here, the first argument is the free list, and the second is the out-of-order allocation weightings. Once we get the array with the weights in the correct places, we use another instance method, pack() , to wrap up the whole array into a Tensor object: empty_at = torch.empty(N) a_t = empty_at.scatter(0, free_list, out_of_location_at) The last part of the implementation that requires looping is the controller loop itself —the loop that goes over each step of the input sequence to process it.",
    "s the controller loop itself —the loop that goes over each step of the input sequence to process it. Because vectorization works only when operations are defined element-wise, the controller’s loop can’t be vectorized. Fortunately, PyTorch still provides us with a method to escape Python’s for loops and their massive performance hit; this method is the symbolic loop.",
    "od to escape Python’s for loops and their massive performance hit; this method is the symbolic loop. A symbolic loop works like most of our symbolic operations: instead of unrolling the actual loop into the graph, it defines a node that would be executed as a loop when the graph is executed. We’ll leave the symbolic loop implementation in PyTorch up to the reader. More information on how you can use symbolic loops in PyTorch can be found in the torch.fx documentation .",
    "nformation on how you can use symbolic loops in PyTorch can be found in the torch.fx documentation . The TensorFlow implementation of our symbolic loop can be found in the train_babi.py file in the code repository. Teaching a DNC to Read and Comprehend Earlier in the chapter, when we were talking about neural n-grams, we said that it’s not of the complexity of an AI that can answer questions after reading a story.",
    "s, we said that it’s not of the complexity of an AI that can answer questions after reading a story. Now we have reached the point where we can build such a system because this is exactly what DNCs do when applied on the bAbI dataset. The bAbI dataset is a synthetic dataset consisting of 20 sets of stories, questions on those stories, and their answers. Each set represents a specific and unique task of reasoning and inference from text.",
    "their answers. Each set represents a specific and unique task of reasoning and inference from text. In the version we’ll use, each task contains 10,000 questions for training and 1,000 questions for testing. For example, the following story (from which the passage we saw earlier was adapted) is from the lists-and-sets task where the answers to the questions are lists/sets of objects mentioned in the story: 1 Mary took the milk there. 2 Mary went to the office. 3 What is Mary carrying?",
    "tioned in the story: 1 Mary took the milk there. 2 Mary went to the office. 3 What is Mary carrying? milk 1 4 Mary took the apple there. 5 Sandra journeyed to the bedroom. 6 What is Mary carrying? milk,apple 1 4 Teaching a DNC to Read and Comprehend | 321 --- Page 338 --- This is taken directly from the dataset, and as you can see, a story is organized into numbered sentences that start from 1.",
    "rom the dataset, and as you can see, a story is organized into numbered sentences that start from 1. Each question ends with a question mark, and the words that directly follow the question mark are the answers. If an answer consists of more than one word, the words are separated by commas. The numbers that follow the answers are supervisory signals that point to the sentences that contain the answers’ words.",
    "low the answers are supervisory signals that point to the sentences that contain the answers’ words. To make the tasks more challenging, we’ll discard these supervisory signals and let the system learn to read the text and figure out the answer on its own. Following the DNC paper, we’ll preprocess our dataset by removing all the numbers and punc‐ tuation except for “?” and “. ” , bringing all the words to lowercase, and replacing the answer words with dashes “-” in the input sequence.",
    "ng all the words to lowercase, and replacing the answer words with dashes “-” in the input sequence. After this we get 159 unique words and marks (lexicons) across all the tasks, so we’ll encode each lexicon as a one-hot vector of size 159, no embeddings, just the plain words directly. Finally, we combine all of the 200,000 training questions to train the model jointly on them, and we keep each task’s test questions separate to test the trained model afterward on each task individually.",
    "p each task’s test questions separate to test the trained model afterward on each task individually. This whole process is implemented in the preprocess.py file in the code repository. To train the model, we randomly sample a story from the encoded training data, pass it through the DNC with an LSTM controller, and get the corresponding output sequence.",
    "ng data, pass it through the DNC with an LSTM controller, and get the corresponding output sequence. We then measure the loss between the output sequence and the desired sequence using the softmax cross-entropy loss, but only on the steps that contain answers. All the other steps are ignored by weighting the loss with a weights vector that has 1 at the answer’s steps and 0 elsewhere. This process is implemented in the train_babi.py file.",
    "has 1 at the answer’s steps and 0 elsewhere. This process is implemented in the train_babi.py file. After the model is trained, we test its performance on the remaining test questions. Our metric will be the percentage of questions the model failed to answer in each task. An answer to a question is the word with the largest softmax value in the output, or the most probable word. A question is considered to be answered correctly if all of its answer’s words are the correct words.",
    "question is considered to be answered correctly if all of its answer’s words are the correct words. If the model failed to answer more than 5% of a task’s questions, we consider that the model failed on that task. The testing procedure is found in the test_babi.py file. After training the model for about 500,000 iterations (caution—it takes a long time!), we can see that it’s performing pretty well on most of the tasks.",
    "s (caution—it takes a long time!), we can see that it’s performing pretty well on most of the tasks. At the same time, it’s performing badly on more difficult tasks like pathfinding , where the task is to answer questions about how to get from one place to another.",
    "like pathfinding , where the task is to answer questions about how to get from one place to another. The following report compares our model’s results to the mean values reported in the original DNC paper: 322 | Chapter 12: Memory Augmented Neural Networks --- Page 339 --- Task Result Paper's Mean --------------------------------------------------------- single supporting fact 0.00% 9.0±12.6% two supporting facts 11.88% 39.2±20.5% three supporting facts 27.80% 39.6±16.4% two arg relations 1.40% 0.4±0.7% three arg relations 1.70% 1.5±1.0% yes no questions 0.50% 6.9±7.5% counting 4.90% 9.8±7.0% lists sets 2.10% 5.5±5.9% simple negation 0.80% 7.7±8.3% indefinite knowledge 1.70% 9.6±11.4% basic coreference 0.10% 3.3±5.7% conjunction 0.00% 5.0±6.3% compound coreference 0.40% 3.1±3.6% time reasoning 11.80% 11.0±7.5% basic deduction 45.44% 27.2±20.1% basic induction 56.43% 53.6±1.9% positional reasoning 39.02% 32.4±8.0% size reasoning 8.68% 4.2±1.8% path finding 98.21% 64.6±37.4% agents motivations 2.71% 0.0±0.1% --------------------------------------------------------- Mean Err.",
    "gents motivations 2.71% 0.0±0.1% --------------------------------------------------------- Mean Err. 15.78% 16.7±7.6% Failed (err. > 5%) 8 11.2±5.4 Summary In this chapter, we’ve explored the cutting edge of deep learning research with NTMs and DNCs, culminating with the implementation of a model that can solve an involved reading comprehension task. In the final chapter of this book, we’ll begin to explore a very different space of problems known as reinforcement learning.",
    "his book, we’ll begin to explore a very different space of problems known as reinforcement learning. We’ll build an intuition for this new class of tasks and develop an algorithmic foundation to tackle these problems using the deep learning tools we’ve developed thus far. Summary | 323 --- Page 341 --- 1Mnih, Volodymyr, et al. “Human-Level Control Through Deep Reinforcement Learning.",
    "--- Page 341 --- 1Mnih, Volodymyr, et al. “Human-Level Control Through Deep Reinforcement Learning. ” Nature 518.7540 (2015): 529-533.CHAPTER 13 Deep Reinforcement Learning Nicholas Locascio In this chapter, we’ll discuss reinforcement learning, which is a branch of machine learning that deals with learning via interaction and feedback. Reinforcement learn‐ ing is essential to building agents that can not only perceive and interpret the world, but also take action and interact with it.",
    "gents that can not only perceive and interpret the world, but also take action and interact with it. We will discuss how to incorporate deep neural networks into the framework of reinforcement learning and discuss recent advances and improvements in this field.",
    "the framework of reinforcement learning and discuss recent advances and improvements in this field. Deep Reinforcement Learning Masters Atari Games The application of deep neural networks to reinforcement learning had a major breakthrough in 2014, when the London startup DeepMind astonished the machine learning community by unveiling a deep neural network that could learn to play Atari games with superhuman skill.",
    "unity by unveiling a deep neural network that could learn to play Atari games with superhuman skill. This network, termed a deep Q-network (DQN) was the first large-scale successful application of reinforcement learning with deep neural networks. DQN was so remarkable because the same architecture, without any changes, was capable of learning 49 different Atari games, despite each game having different rules, goals, and game-play structure.",
    "49 different Atari games, despite each game having different rules, goals, and game-play structure. To accomplish this feat, Deep‐ Mind brought together many traditional ideas in reinforcement learning while also developing a few novel techniques that proved key to DQN’s success. Later in this chapter, we will implement DQN, as described in the Nature paper, “Human-Level Control Through Deep Reinforcement Learning. ”1 But first, let’s take a dive into reinforcement learning ( Figure 13-1 ).",
    "Reinforcement Learning. ”1 But first, let’s take a dive into reinforcement learning ( Figure 13-1 ). 325 --- Page 342 --- 2This image is from the OpenAI Gym DQN agent that we build in this chapter: Brockman, Greg, et al. “OpenAI Gym. ” arXiv preprint arXiv :1606.01540 (2016). https://gym.openai.com Figure 13-1. A deep reinforcement learning agent playing Breakout2 What Is Reinforcement Learning? Reinforcement learning, at its essentials, is learning by interacting with an environ‐ ment.",
    "arning? Reinforcement learning, at its essentials, is learning by interacting with an environ‐ ment. This learning process involves an agent , an environment , and a reward signal . The agent chooses to take an action in the environment, for which the agent is rewarded accordingly. The way in which an actor chooses actions is called a policy . The agent wants to increase the reward it receives, and so must learn an optimal policy for interacting with the environment ( Figure 13-2 ).",
    "receives, and so must learn an optimal policy for interacting with the environment ( Figure 13-2 ). Reinforcement learning is different from the other types of learning that we have covered thus far. In traditional supervised learning, we are given data and labels, and are tasked with predicting labels given data. In unsupervised learning, we are given just data and are tasked with discovering underlying structure in this data.",
    "learning, we are given just data and are tasked with discovering underlying structure in this data. In 326 | Chapter 13: Deep Reinforcement Learning --- Page 343 --- reinforcement learning, we are given neither data nor labels. Our learning signal is derived from the rewards given to the agent by the environment. Figure 13-2. Reinforcement learning setup Reinforcement learning is exciting to many in the AI community because it is a general-purpose framework for creating intelligent agents.",
    "many in the AI community because it is a general-purpose framework for creating intelligent agents. Given an environment and some rewards, the agent learns to interact with that environment to maximize its total reward. This type of learning is more in line with how humans develop. Y es, we can build a pretty good model to classify dogs from cats with extremely high accuracy by training on thousands of images. But you won’t find this approach used in any elementary schools.",
    "by training on thousands of images. But you won’t find this approach used in any elementary schools. Humans interact with their environment to learn representations of the world that they can use to make decisions. Furthermore, reinforcement learning applications are at the forefront of many cutting-edge technologies, including self-driving cars, robotic motor control, game playing, air-conditioning control, ad placement optimization, and stock market trad‐ ing strategies.",
    "playing, air-conditioning control, ad placement optimization, and stock market trad‐ ing strategies. As an illustrative exercise, we’ll be tackling a simple reinforcement learning and control problem called pole balancing. In this problem, there is a cart with a pole that is connected by a hinge, so the pole can swing around the cart. There is an agent that can control the cart, moving it left or right.",
    "ole can swing around the cart. There is an agent that can control the cart, moving it left or right. There is an environment, which rewards the agent when the pole is pointed upward, and penalizes the agent when the pole falls over ( Figure 13-3 ). What Is Reinforcement Learning? | 327 --- Page 344 --- 3This image is from our OpenAI Gym Policy Gradient agent that we build in this chapter. Figure 13-3.",
    "3This image is from our OpenAI Gym Policy Gradient agent that we build in this chapter. Figure 13-3. A simple reinforcement learning agent: balancing a pole3 Markov Decision Processes Our pole-balancing example has a few important elements, which we formalize as a Markov decision process (MDP). These elements are: State The cart has a range of possible places on the x-plane where it can be. Similarly, the pole has a range of possible angles.",
    "possible places on the x-plane where it can be. Similarly, the pole has a range of possible angles. Action The agent can take action by moving the cart either left or right. State transition When the agent acts, the environment changes: the cart moves and the pole changes angle and velocity. Reward If an agent balances the pole well, it receives a positive reward. If the pole falls, the agent receives a negative reward.",
    "e pole well, it receives a positive reward. If the pole falls, the agent receives a negative reward. An MDP is defined as the following: •S, a finite set of possible states •A, a finite set of actions •Pr,s′s,a, a state transition function •R, reward function MDPs offer a mathematical framework for modeling decision making in a given environment.",
    "rd function MDPs offer a mathematical framework for modeling decision making in a given environment. Figure 13-4 shows an example, with circles representing the states of 328 | Chapter 13: Deep Reinforcement Learning --- Page 345 --- the environment, diamonds representing actions that can be taken, and the edges from diamonds to circles representing the transition from one state to the next.",
    "aken, and the edges from diamonds to circles representing the transition from one state to the next. The numbers along these edges represent the probability of taking a certain action, and the numbers at the end of the arrows represent the reward given to the agent for making the given transition. Figure 13-4. Example of an MDP As an agent takes action in an MDP framework, it forms an episode . An episode consists of series of tuples of states, actions, and rewards.",
    "work, it forms an episode . An episode consists of series of tuples of states, actions, and rewards. Episodes run until the environment reaches a terminal state, like the “Game Over” screen in Atari games, or when the pole hits the ground in our pole-cart example. The following equation shows the variables in an episode: s0,a0,r0,s1,a1,r1, ...sn,an,rn In pole-cart, our environment state can be a tuple of the position of the cart and the angle of the pole, like so: ( xcart, θpole).",
    "tate can be a tuple of the position of the cart and the angle of the pole, like so: ( xcart, θpole). Policy MDP’s aim is to find an optimal policy for our agent. Policies are how our agent acts based on its current state. Formally, policies can be represented as a function π that chooses the action a that the agent will take in state s.",
    "es can be represented as a function π that chooses the action a that the agent will take in state s. Markov Decision Processes | 329 --- Page 346 --- The objective of our MDP is to find a policy to maximize the expected future return: maxπER0+R1+ ...Rtπ In this objective, R represents the future return of each episode. Let’s define exactly what future return means. Future Return Future return is how we consider the rewards of the future.",
    "what future return means. Future Return Future return is how we consider the rewards of the future. Choosing the best action requires consideration of not only the immediate effects of that action, but also the long-term consequences. Sometimes the best action actually has a negative immediate effect, but a better long-term result. For example, a mountain-climbing agent that is rewarded by its altitude may actually have to climb downhill to reach a better path to the mountain’s peak.",
    "d by its altitude may actually have to climb downhill to reach a better path to the mountain’s peak. Therefore, we want our agents to optimize for future return . To do that, the agent must consider the future consequences of its actions. For example, in a game of Pong, the agent receives a reward when the ball passes into the opponent’s goal.",
    "ample, in a game of Pong, the agent receives a reward when the ball passes into the opponent’s goal. However, the actions responsible for this reward (the inputs that position the racquet to strike a scoring hit) happen many time steps before the reward is received. The reward for each of those actions is delayed. We can incorporate delayed rewards into our overall reward signal by constructing a return for each time step that takes into account future rewards as well as immediate rewards.",
    "ing a return for each time step that takes into account future rewards as well as immediate rewards. A naive approach for calculating future return for a time step may be a simple sum like so: Rt= ∑k= 0Trt+k We can calculate all returns, R, where R=R0,R1, ...Ri, ...Rn, with the following code: def calculate_naive_returns(rewards): \"\"\" Calculates a list of naive returns given a list of rewards.\"\"\" total_returns = np.zeros(len(rewards)) total_return = 0.0 for t in range(len(rewards), 0): total_return = total_return + reward total_returns[t] = total_return return total_returns This naive approach successfully incorporates future rewards so the agent can learn an optimal global policy.",
    "e approach successfully incorporates future rewards so the agent can learn an optimal global policy. This approach values future rewards equally to immedi‐ ate rewards. However, this equal consideration of all rewards is problematic. With 330 | Chapter 13: Deep Reinforcement Learning --- Page 347 --- infinite time steps, this expression can diverge to infinity, so we must find a way to bind it.",
    "--- infinite time steps, this expression can diverge to infinity, so we must find a way to bind it. Furthermore, with equal consideration at each time step, the agent can optimize for a future reward, and we would learn a policy that lacks any sense of urgency or time sensitivity in pursuing its rewards. Instead, we should value future rewards slightly less in order to force our agents to learn to get rewards quickly. We accomplish this with a strategy called discounted future return.",
    "to learn to get rewards quickly. We accomplish this with a strategy called discounted future return. Discounted Future Return To implement discounted future return, we scale the reward of a current state by the discount factor, γ, to the power of the current time step. In this way, we penalize agents that take many actions before receiving positive reward. Discounted rewards bias our agent to prefer receiving the reward in the immediate future, which is advantageous to learning a good policy.",
    "refer receiving the reward in the immediate future, which is advantageous to learning a good policy. We can express the reward as follows: Rt= ∑k= 0Tγtrt+k+ 1 The discount factor, γ, represents the level of discounting we want to achieve, and can be between 0 and 1. High γ means little discounting, low γ provides much discounting. A typical γ hyperparameter setting is between 0.99 and 0.97.",
    "nting, low γ provides much discounting. A typical γ hyperparameter setting is between 0.99 and 0.97. We can implement discounted return like so: def discount_rewards(rewards, gamma=0.98): discounted_returns = [0 for _ in rewards] discounted_returns[-1] = rewards[-1] for t in range(len(rewards)-2, -1, -1): # iterate backwards discounted_returns[t] = rewards[t] + discounted_returns[t+1]*gamma return discounted_returns Explore Versus Exploit Reinforcement learning is fundamentally a trial-and-error process.",
    "ed_returns Explore Versus Exploit Reinforcement learning is fundamentally a trial-and-error process. In such a frame‐ work, an agent afraid to make mistakes can prove to be highly problematic. Consider the following scenario. A mouse is placed in the maze shown in Figure 13-5 . Our agent must control the mouse to maximize reward.",
    "se is placed in the maze shown in Figure 13-5 . Our agent must control the mouse to maximize reward. If the mouse gets the water, it receives a reward of +1; if the mouse reaches a poison container (red), it receives a reward of -10; if the mouse gets the cheese, it receives a reward of +100. Upon receiv‐ ing reward, the episode is over. The optimal policy involves the mouse successfully navigating to the cheese and eating it. Explore Versus Exploit | 331 --- Page 348 --- Figure 13-5.",
    "y navigating to the cheese and eating it. Explore Versus Exploit | 331 --- Page 348 --- Figure 13-5. A predicament that many mice find themselves in In the first episode, the mouse takes the left route, steps on a trap, and receives a -10 reward. In the second episode, the mouse avoids the left path, since it resulted in such a negative reward, and drinks the water immediately to its right for a +1 reward. After two episodes, it would seem that the mouse has found a good policy.",
    "its right for a +1 reward. After two episodes, it would seem that the mouse has found a good policy. It continues to follow its learned policy on subsequent episodes and achieves the moderate +1 reward reliably. Since our agent utilizes a greedy strategy—always choosing the mod‐ el’s best action—it is stuck in a policy that is a local maximum .",
    "strategy—always choosing the mod‐ el’s best action—it is stuck in a policy that is a local maximum . To prevent such a situation, it may be useful for the agent to deviate from the model’s recommendation and take a suboptimal action in order to explore more of the envi‐ ronment.",
    "e model’s recommendation and take a suboptimal action in order to explore more of the envi‐ ronment. So instead of taking the immediate right turn to exploit the environment to get water and the reliable +1 reward, our agent may choose to take a left turn and venture into more treacherous areas in search of a more optimal policy. Too much exploration, and our agent fails to optimize any reward. Not enough exploration can result in our agent getting stuck in a local minimum.",
    "ptimize any reward. Not enough exploration can result in our agent getting stuck in a local minimum. This balance of explore versus exploit is crucial to learning a successful policy. 332 | Chapter 13: Deep Reinforcement Learning --- Page 349 --- ϵ-Greedy One strategy for balancing the explore-exploit dilemma is called ϵ-greedy . ϵ-greedy is a simple strategy that involves making a choice at each step to either take the agent’s top recommended action or take a random action.",
    "ing a choice at each step to either take the agent’s top recommended action or take a random action. The probability that the agent takes a random action is the value known as ϵ.",
    "take a random action. The probability that the agent takes a random action is the value known as ϵ. We can implement ϵ-greedy like so: def epsilon_greedy_action(action_distribution, epsilon=1e-1): action_distribution = action_distribution.detach().numpy() if random.random() < epsilon: return np.argmax(np.random.random( action_distribution.shape)) else: return np.argmax(action_distribution) Annealed ϵ-Greedy When training a reinforcement learning model, oftentimes we want to do more exploring in the beginning since our model knows little of the world.",
    "oftentimes we want to do more exploring in the beginning since our model knows little of the world. Later, once our model has seen much of the environment and learned a good policy, we want our agent to trust itself more to further optimize its policy. To accomplish this, we cast aside the idea of a fixed ϵ, and instead anneal it over time, having it start low and increase by a factor after each training episode.",
    "stead anneal it over time, having it start low and increase by a factor after each training episode. Typical settings for annealed ϵ-greedy scenarios include annealing from 0.99 to 0.1 over 10,000 scenarios.",
    "l settings for annealed ϵ-greedy scenarios include annealing from 0.99 to 0.1 over 10,000 scenarios. We can implement annealing like so: def epsilon_greedy_action_annealed(action_distribution, percentage, epsilon_start=1.0, epsilon_end=1e-2): action_distribution = action_distribution.detach().numpy() annealed_epsilon = (epsilon_start*(1.0-percentage) + epsilon_end*percentage) if random.random() < annealed_epsilon: return np.argmax(np.random.random( action_distribution.shape)) else: return np.argmax(action_distribution) Explore Versus Exploit | 333 --- Page 350 --- 4Sutton, Richard S., et al.",
    "rgmax(action_distribution) Explore Versus Exploit | 333 --- Page 350 --- 4Sutton, Richard S., et al. “Policy Gradient Methods for Reinforcement Learning with Function Approxima‐ tion. ” NIPS. Vol. 99. 1999.Policy Versus Value Learning So far we’ve defined the setup of reinforcement learning, discussed discounted future return, and looked at the trade-offs of explore versus exploit. What we haven’t talked about is how we’re actually going to teach an agent to maximize its reward.",
    ". What we haven’t talked about is how we’re actually going to teach an agent to maximize its reward. Approaches to this fall into two broad categories: policy learning and value learning . In policy learning, we are directly learning a policy that maximizes reward. In value learning, we are learning the value of every state + action pair.",
    "cy that maximizes reward. In value learning, we are learning the value of every state + action pair. If you were trying to learn to ride a bike, a policy learning approach would be to think about how pushing on the right pedal while you were falling to the left would course-correct you. If you were trying to learn to ride a bike with a value learning approach, you would assign a score to different bike orientations and actions you can take in those positions.",
    "you would assign a score to different bike orientations and actions you can take in those positions. We’ll be covering both in this chapter, so let’s start with policy learning. In typical supervised learning, we can use stochastic gradient descent to update our parameters to minimize the loss computed from our network’s output and the true label. We are optimizing the expression: arg min θ∑ilogpyi∣ xi;θ In reinforcement learning, we don’t have a true label, only reward signals.",
    "arg min θ∑ilogpyi∣ xi;θ In reinforcement learning, we don’t have a true label, only reward signals. However, we can still use SGD to optimize our weights using something called policy gradients .4 We can use the actions the agent takes, and the returns associated with those actions, to encourage our model weights to take good actions that lead to high reward, and to avoid bad ones that lead to low reward.",
    "eights to take good actions that lead to high reward, and to avoid bad ones that lead to low reward. The expression we optimize for is: arg min θ− ∑iRilogpyi∣ xi;θ where yi is the action taken by the agent at time step t and where Ri is our discounted future return. A In this way, we scale our loss by the value of our return, so if the model chose an action that led to negative return, this would lead to greater loss.",
    "eturn, so if the model chose an action that led to negative return, this would lead to greater loss. Furthermore, if the model is confident in that bad decision, it would get penalized even more, since we are taking into account the log probability of the model choosing that action. With our loss function defined, we can apply SGD to minimize our loss and learn a good policy.",
    "tion. With our loss function defined, we can apply SGD to minimize our loss and learn a good policy. 334 | Chapter 13: Deep Reinforcement Learning --- Page 351 --- Pole-Cart with Policy Gradients We’re going to implement a policy-gradient agent to solve pole-cart, a classic rein‐ forcement learning problem. We will be using an environment from the OpenAI Gym created just for this task. OpenAI Gym The OpenAI Gym is a Python toolkit for developing reinforcement agents.",
    "st for this task. OpenAI Gym The OpenAI Gym is a Python toolkit for developing reinforcement agents. OpenAI Gym provides an easy-to-use interface for interacting with a variety of environments. It contains over one hundred open source implementations of common reinforce‐ ment learning environments. OpenAI Gym speeds up development of reinforcement learning agents by handling everything on the environment simulation side, allowing researchers to focus on their agent and learning algorithms.",
    "e environment simulation side, allowing researchers to focus on their agent and learning algorithms. Another benefit of OpenAI Gym is that researchers can fairly compare and evaluate their results with others because they can all use the same standardized environment for a task. We’ll be using the pole-cart environment from OpenAI Gym to create an agent that can easily interact with this environment.",
    "-cart environment from OpenAI Gym to create an agent that can easily interact with this environment. Creating an Agent To create an agent that can interact with an OpenAI environment, we’ll define a class PGAgent , which will contain our model architecture, model weights, and hyperpara‐ meters: from torch import optim class PGAgent(object): def __init__(self, state_size, num_actions, hidden_size, learning_rate=1e-3, explore_exploit_setting= \\ 'epsilon_greedy_annealed_1.0->0.001'): self.state_size = state_size self.num_actions = num_actions self.hidden_size = hidden_size self.learning_rate = learning_rate self.explore_exploit_setting = \\ explore_exploit_setting self.build_model() def build_model(self): self.model = torch.nn.Sequential( nn.Linear(self.state_size, self.hidden_size), nn.Linear(self.hidden_size, self.hidden_size), nn.Linear(self.hidden_size, self.num_actions), nn.Softmax(dim=0)) Pole-Cart with Policy Gradients | 335 --- Page 352 --- def train(self, state, action_input, reward_input): state = torch.tensor(state).float() action_input = torch.tensor(action_input).long() reward_input = torch.tensor(reward_input).float() self.output = self.model(state) # Select the logits related to the action taken logits_for_actions = self.output.gather(1, action_input.view(-1,1)) self.loss = -torch.mean( torch.log(logits_for_actions) * reward_input) self.loss.backward() self.optimizer = optim.Adam(self.model.parameters()) self.optimizer.step() self.optimizer.zero_grad() return self.loss.item() def sample_action_from_distribution(self, action_distribution, epsilon_percentage): # Choose an action based on the action probability # distribution and an explore vs exploit if self.explore_exploit_setting == 'greedy': action = epsilon_greedy_action(action_distribution, 0.00) elif self.explore_exploit_setting == 'epsilon_greedy_0.05': action = epsilon_greedy_action(action_distribution, 0.05) elif self.explore_exploit_setting == 'epsilon_greedy_0.25': action = epsilon_greedy_action(action_distribution, 0.25) elif self.explore_exploit_setting == 'epsilon_greedy_0.50': action = epsilon_greedy_action(action_distribution, 0.50) elif self.explore_exploit_setting == 'epsilon_greedy_0.90': action = epsilon_greedy_action(action_distribution, 0.90) elif self.explore_exploit_setting == \\ 'epsilon_greedy_annealed_1.0->0.001': action = epsilon_greedy_action_annealed( action_distribution, epsilon_percentage, 1.0,0.001) elif self.explore_exploit_setting == \\ 'epsilon_greedy_annealed_0.5->0.001': action = epsilon_greedy_action_annealed( action_distribution, epsilon_percentage, 0.5, 0.001) elif self.explore_exploit_setting == \\ 'epsilon_greedy_annealed_0.25->0.001': action = epsilon_greedy_action_annealed( 336 | Chapter 13: Deep Reinforcement Learning --- Page 353 --- action_distribution, epsilon_percentage, 0.25, 0.001) return action def predict_action(self, state, epsilon_percentage): action_distribution = self.model( torch.from_numpy(state).float()) action = self.sample_action_from_distribution( action_distribution, epsilon_percentage) return action Building the Model and Optimizer Let’s break down some important functions.",
    "ercentage) return action Building the Model and Optimizer Let’s break down some important functions. In build_model() , we define our model architecture as a three-layer neural network. The model returns a layer of three nodes, each representing the model’s action probability distribution. In build_train ing() , we implement our policy gradient optimizer.",
    "action probability distribution. In build_train ing() , we implement our policy gradient optimizer. We express our objective loss as we talked about, scaling the model’s prediction probability for an action with the return received for taking that action, and summing these all up to form a minibatch. With our objective defined, we can use torch.optim.AdamOptimizer , which will adjust our weights according to the gradient to minimize our loss.",
    ".optim.AdamOptimizer , which will adjust our weights according to the gradient to minimize our loss. Sampling Actions We define the predict_action function, which samples an action based on the mod‐ el’s action probability distribution output. We support the various sampling strategies that we talked about to balance explore versus exploit, including greedy, ϵ greedy, and ϵ greedy annealing.",
    "talked about to balance explore versus exploit, including greedy, ϵ greedy, and ϵ greedy annealing. Keeping Track of History We’ll be aggregating our gradients from multiple episode runs, so it will be useful to keep track of state, action, and reward tuples.",
    "from multiple episode runs, so it will be useful to keep track of state, action, and reward tuples. To this end, we implement an episode history and memory: class EpisodeHistory(object): def __init__(self): self.states = [] self.actions = [] self.rewards = [] self.state_primes = [] self.discounted_returns = [] def add_to_history(self, state, action, reward, state_prime): self.states.append(state) self.actions.append(action) Pole-Cart with Policy Gradients | 337 --- Page 354 --- self.rewards.append(reward) self.state_primes.append(state_prime) class Memory(object): def __init__(self): self.states = [] self.actions = [] self.rewards = [] self.state_primes = [] self.discounted_returns = [] def reset_memory(self): self.states = [] self.actions = [] self.rewards = [] self.state_primes = [] self.discounted_returns = [] def add_episode(self, episode): self.states += episode.states self.actions += episode.actions self.rewards += episode.rewards self.discounted_returns += episode.discounted_returns Policy Gradient Main Function Let’s put this all together in our main function, which will create an OpenAI Gym environment for CartPole, make an instance of our agent, and have our agent interact with and train on the CartPole environment: # Configure Settings #total_episodes = 5000 total_episodes = 16 total_steps_max = 10000 epsilon_stop = 3000 train_frequency = 8 max_episode_length = 500 render_start = -1 should_render = False explore_exploit_setting = 'epsilon_greedy_annealed_1.0->0.001' env = gym.make('CartPole-v0') state_size = env.observation_space.shape[0] # 4 for # CartPole-v0 num_actions = env.action_space.n # 2 for CartPole-v0 solved = False agent = PGAgent(state_size=state_size, num_actions=num_actions, 338 | Chapter 13: Deep Reinforcement Learning --- Page 355 --- hidden_size=16, explore_exploit_setting= \\ explore_exploit_setting) episode_rewards = [] batch_losses = [] global_memory = Memory() steps = 0 for i in range(total_episodes): state = env.reset() episode_reward = 0.0 episode_history = EpisodeHistory() epsilon_percentage = float(min(i/float(epsilon_stop), 1.0)) for j in range(max_episode_length): action = agent.predict_action(state, epsilon_percentage) state_prime, reward, terminal, _ = env.step(action) episode_history.add_to_history( state, action, reward, state_prime) state = state_prime episode_reward += reward steps += 1 if j == (max_episode_length - 1): terminal = True if terminal: episode_history.discounted_returns = \\ discount_rewards(episode_history.rewards) global_memory.add_episode(episode_history) # every 8th episode train the NN # train on all actions from episodes in memory, # then reset memory if np.mod(i, train_frequency) == 0: reward_input = global_memory.discounted_returns action_input = global_memory.actions state = global_memory.states # train step batch_loss = agent.train(state, action_input, reward_input) # print(f'Batch loss: {batch_loss}') # batch_losses.append(batch_loss) global_memory.reset_memory() episode_rewards.append(episode_reward) if i % 10 == 0: Pole-Cart with Policy Gradients | 339 --- Page 356 --- mean_rewards = torch.mean(torch.tensor( episode_rewards[:-10])) if mean_rewards > 10.0: solved = True else: solved = False print(f'Solved: {solved} Mean Reward: {mean_rewards}') break # stop playing if terminal print(f'Episode[{i}]: {len(episode_history.actions)} \\ actions {episode_reward} reward') This code will train a CartPole agent to successfully and consistently balance the pole.",
    "d} reward') This code will train a CartPole agent to successfully and consistently balance the pole. PGAgent Performance on Pole-Cart Figure 13-6 is a chart of the average reward of our agent at each step of training. We try out 8 different sampling methods, and achieve best results with ϵ greedy annealing from 1.0 to 0.001. Figure 13-6.",
    "t sampling methods, and achieve best results with ϵ greedy annealing from 1.0 to 0.001. Figure 13-6. Explore-exploit configurations affect how fast and how well learning occurs 340 | Chapter 13: Deep Reinforcement Learning --- Page 357 --- Notice how, across the board, standard ϵ greedy does very poorly. Let’s talk about why this might be. With a high ϵ set to 0.9, we are taking a random action 90% of the time.",
    "lk about why this might be. With a high ϵ set to 0.9, we are taking a random action 90% of the time. Even if the model learns to execute the perfect actions, we’ll still be using these only 10% of the time. On the other end, with a low ϵ of 0.05, we are taking what our model believes to be optimal actions the vast majority of the time. This performance is a bit better, but gets stuck in a local reward minimum because it lacks the ability to explore other strategies.",
    ", but gets stuck in a local reward minimum because it lacks the ability to explore other strategies. So neither ϵ greedy of 0.05 nor 0.9 gives us great results. The former places too much emphasis on exploration, and the latter, too little. This is why ϵ annealing is such a powerful sampling strategy. It allows the model to explore early and exploit late, which is crucial to learning good policies.",
    ". It allows the model to explore early and exploit late, which is crucial to learning good policies. Trust-Region Policy Optimization Trust-region policy optimization , or TRPO for short, is a framework that ensures policy improvement while preventing the policy from shifting too much during each training step.",
    "res policy improvement while preventing the policy from shifting too much during each training step. TRPO has been empirically shown to outperform many of its fellow policy gradient and policy iteration methods, allowing researchers to effectively learn complex, nonlinear policies (often parametrized by large neural networks) that weren’t previously possible through gradient-based methods. In this section, we will motivate TRPO and describe its objective in more detail.",
    "ent-based methods. In this section, we will motivate TRPO and describe its objective in more detail. The idea of preventing the policy from shifting too much during each training step is not a new one—most regularized optimization procedures do this indirectly by penalizing the norm of the parameters, for example, globally ensuring the norm of the parameters doesn’t get too high.",
    "m of the parameters, for example, globally ensuring the norm of the parameters doesn’t get too high. Of course, in cases where regularized optimiza‐ tion can also be formulated as constrained optimization (where there are explicit bounds on the norm of the parameter vector), such as L2-regularized linear regres‐ sion, we have a direct equivalence to the idea of preventing the policy from shifting too much during each training step.",
    "t equivalence to the idea of preventing the policy from shifting too much during each training step. The per-step change in the norm of the param‐ eters is bounded by the range of the constraint, since all possible parameter values must fall in this range. For those interested, I would recommend looking further into the equivalence between Tikhonov and Ivanov regularization in linear regression.",
    "ooking further into the equivalence between Tikhonov and Ivanov regularization in linear regression. Preventing the policy from shifting too much during each training step has the standard effect of regularized optimization: it promotes stability in training, which is ideal in preventing overfitting to new data. How do we define a shift in the policy?",
    "ining, which is ideal in preventing overfitting to new data. How do we define a shift in the policy? Policies are simply discrete probability distributions over the action space given a state, πθas, for which we can use notions of dissimilarity, introduced in Chapter 2 . The original TRPO paper introduced a bound on the average KL divergence (over all possible states) between the current policy and the new policy.",
    "the average KL divergence (over all possible states) between the current policy and the new policy. Now that we’ve introduced the constraint portion of TRPO’s constrained optimiza‐ tion, we will motivate and define the objective function.",
    "nt portion of TRPO’s constrained optimiza‐ tion, we will motivate and define the objective function. Trust-Region Policy Optimization | 341 --- Page 358 --- Let’s recap and introduce some terminology: ηπ= Es0,a0,s1,a1, ...∑t= 0∞γtrst Qπst,at= Est+ 1,at+ 1, ...∑l= 0∞γlrst+l Vπst =Eat,st+ 1,at+ 1...∑l= 0∞γlrst+l Aπs,a= Qπs,a−Vπs ρπs= ∑i= 0∞γiPst=s The first term is ηπ, which represents the expected discounted reward .",
    "Qπs,a−Vπs ρπs= ∑i= 0∞γiPst=s The first term is ηπ, which represents the expected discounted reward . We saw the finite-time horizon version of the term inside the expectation earlier when we discussed future discounted reward. Instead of looking at a single trajectory here, we take the expectation over all possible trajectories as defined by our policy π. As usual, we can estimate this expectation via an empirical average by sampling trajectories using π.",
    "s usual, we can estimate this expectation via an empirical average by sampling trajectories using π. The second term, which will be discussed in more detail in “Q-Learning and Deep Q-Networks” on page 347, is the Q-function Qπst,at, which looks very similar to the previous term but is instead defined as the expected discounted return from time t, given we are in some state st and perform a defined action at in that state. We again calculate the expectation using our policy π.",
    "nd perform a defined action at in that state. We again calculate the expectation using our policy π. Note that the time t doesn’t actually matter all too much since we only consider an infinite time horizon and the expected discounted return from t rather than from the beginning of the trajectory. The third term is Vπst, or the value function at a particular state at time t.",
    "ng of the trajectory. The third term is Vπst, or the value function at a particular state at time t. The value function can actually be more concisely written as Vπst=EatQπst,at, or the expectation of the Q-function with respect to πatst. In essence, the Q-function supposes that we take a defined action at in state st, while the value function leaves at as a variable.",
    "oses that we take a defined action at in state st, while the value function leaves at as a variable. Thus, to get the value function, all we need to do is take the expectation of the Q-function with respect to the distribution over at knowing the current state st. The result is the weighted average of the Q-function, where the weights are πatst. In essence, this term captures the average future discounted return we’ d expect to see starting in some state st.",
    "is term captures the average future discounted return we’ d expect to see starting in some state st. The fourth term is Aπs,a, or the advantage function . Note that we have dropped the time t by now for the reasons mentioned earlier.",
    "advantage function . Note that we have dropped the time t by now for the reasons mentioned earlier. The intuition for the advantage function is that it quantifies, under a fixed policy π, the benefit of letting trajectories play out after taking a particular action a in the current state s, over simply letting trajectories play out from the current state s completely unconstrained.",
    "tate s, over simply letting trajectories play out from the current state s completely unconstrained. Even more concisely, it defines how much better, or worse, in the long run it is to initially take action a in state s compared to the average. 342 | Chapter 13: Deep Reinforcement Learning --- Page 359 --- The final term, or the unnormalized discounted visitation frequency, reintroduces the time term t.",
    "- The final term, or the unnormalized discounted visitation frequency, reintroduces the time term t. This term is a function of the probability of being in state s at each time t from the start to infinity. This term will be important in our definition of the objective function.",
    "from the start to infinity. This term will be important in our definition of the objective function. The original TRPO paper chose to optimize the model parameters by maximizing this objective function: Lθoldθ= ∑sρθolds∑aπθasAθolds,a θnew= argmaxθLθoldθ Although we won’t fully show the derivation behind this objective as it is quite math‐ ematically involved and beyond the scope of this text, we provide some intuition. Let’s first examine this term: ∑aπθasAθolds,a, assuming a fixed state s.",
    "we provide some intuition. Let’s first examine this term: ∑aπθasAθolds,a, assuming a fixed state s. For the sake of argument, let’s replace θ with θold as our proposed policy’s parameters, which also represents our current policy’s parameters: ∑aπθoldasAπθolds,a=Ea ∼ πθoldasAπθolds,a =Ea ∼ πθoldasQπθolds,a−Ea ∼ πθoldasVπθolds =Ea ∼ πθoldasQπθolds,a−Vπθolds =Vπθolds−Vπθolds = 0 What have we shown here?",
    "s,a−Ea ∼ πθoldasVπθolds =Ea ∼ πθoldasQπθolds,a−Vπθolds =Vπθolds−Vπθolds = 0 What have we shown here? Earlier, we talked about how Aπθolds,a defines how much better or worse it is, under the current policy, to take action a in state s compared to what we expect to see starting from state s unconstrained.",
    ", to take action a in state s compared to what we expect to see starting from state s unconstrained. Here, we showed that if we average each of these advantages weighted by the current policy’s distribution, we are left with zero average advantage over the current policy—this makes a lot of intuitive sense, since the proposed policy and the current policy are the exact same. We don’t expect to see any performance gain by replacing the current policy with itself.",
    "exact same. We don’t expect to see any performance gain by replacing the current policy with itself. Now, if we replace θ with a different proposed policy’s parameters θalt, the above derivation leads us to: Ea ∼ πθaltasQπθolds,a−Vπθolds Trust-Region Policy Optimization | 343 --- Page 360 --- This is as far as simplification will take us, since the actions in the first term are no longer distributed as the current policy and we can’t make the simplification that led us to the penultimate step.",
    "uted as the current policy and we can’t make the simplification that led us to the penultimate step. If we evaluate this expression and we receive a positive result, we can interpret the result as representing a positive average advantage from following the proposed policy compared to following the current policy, directly translating to a performance gain for this specific state s by replacing the current policy with the proposed policy.",
    "performance gain for this specific state s by replacing the current policy with the proposed policy. Note that we have only been considering a specific state s. But even if we see a performance gain for some state, it might be the case that that state only rarely shows up. This leads us to the inclusion of the term ∑sρθolds, which quantifies how often we see a given state.",
    "his leads us to the inclusion of the term ∑sρθolds, which quantifies how often we see a given state. We can actually rewrite this as an expectation even though this is an unnormalized distribution—all we’ d need to do is factor out the normalizing constant, which is also a constant from the perspective of θ since the normalizing constant is solely a function of θold.",
    "so a constant from the perspective of θ since the normalizing constant is solely a function of θold. Keep in mind that ∑sρθolds is evaluated using the current policy rather than the proposed policy; this is because, as noted in the paper, the complex dependency this introduces on θ when optimizing the alternative objective (which uses ∑sρθs) with respect to θ makes the optimization process difficult.",
    "alternative objective (which uses ∑sρθs) with respect to θ makes the optimization process difficult. Additionally, the paper proves that the first-order gradient matches that of the alternative objective anyway, allowing us to make this substitution without introducing a biased gradient estimate. We won’t show this here, however, as it is beyond the scope of the text. Putting everything together, we have the following constrained optimization objective: θ*= argmaxθ∑sρθolds∑aπθasAθolds,a s.t. Avg.",
    "e have the following constrained optimization objective: θ*= argmaxθ∑sρθolds∑aπθasAθolds,a s.t. Avg. KL θold,θ≤δ where the average KL divergence denotes the expected KL divergence between policies over all states. This is what we call the trust region, and it represents the parameter settings that lie close enough to the current parameter setting, mitigate training instability, and mitigate overfitting. How do we go about optimizing this objective?",
    "tigate training instability, and mitigate overfitting. How do we go about optimizing this objective? The inner summation looks like an expectation with respect to πθa,s, but all we have is our current setting of parameter values θold. In the standard setting, or on-policy setting, we are sampling from the same policy we are optimizing, so we can use classic policy gradient optimization for this.",
    "from the same policy we are optimizing, so we can use classic policy gradient optimization for this. However, TRPO can be modified to work in the off-policy setting as well, where the policy we are sampling from is different from the policy we are optimizing.",
    "tting as well, where the policy we are sampling from is different from the policy we are optimizing. Generally, the reason for this 344 | Chapter 13: Deep Reinforcement Learning --- Page 361 --- distinction is that we may have a behavior policy, the policy we are sampling from that may be more exploratory in nature, while we learn the target policy, which is to be optimized.",
    "that may be more exploratory in nature, while we learn the target policy, which is to be optimized. In the off-policy setting, since we are sampling actions from a different distribution q(a|s) (the behavior policy) from πθas (the target policy), we instead use the following constrained optimization objective: θ*= argmaxθ∑sρθolds∑aπθas qasAθolds,a s.t. Avg. KL θold,θ≤δ The addition of q(a|s) accounts for the fact that we are sampling from a separate behavior policy.",
    "δ The addition of q(a|s) accounts for the fact that we are sampling from a separate behavior policy. We can think about this more concretely in terms of expectations: ∑aπθasAθolds,a= ∑aqas qasπθasAθolds,a = ∑aqasπθas qasAθolds,a =Eqasπθas qasAθolds,a Note that the left side of the first equality can be written as an expectation of the advantage with respect to the target policy.",
    "first equality can be written as an expectation of the advantage with respect to the target policy. In a few algebraic manipulations, we were able to convert our original objective into an equivalent objective, but with an expectation that is taken with respect to the behavior policy.",
    "an equivalent objective, but with an expectation that is taken with respect to the behavior policy. This is ideal because we are sampling from the behavior policy and can thus use standard minibatch gradient descent techniques to optimize this objective (adding in the constraint on the KL divergence makes this a bit more complicated than just standard gradient descent).",
    "straint on the KL divergence makes this a bit more complicated than just standard gradient descent). And finally, we have already seen methods for sampling from the unnormalized probability distribution ρolds for the outer expectation, amongst others that exist in academic literature. Proximal Policy Optimization One issue with TRPO is that its optimization is relatively complicated due to the inclusion of the average KL divergence term and involves second-order optimization to perform.",
    "o the inclusion of the average KL divergence term and involves second-order optimization to perform. Proximal policy optimization , or PPO for short, is an algorithm that tries to retain the benefits of TRPO without the complicated optimization.",
    "ort, is an algorithm that tries to retain the benefits of TRPO without the complicated optimization. PPO proposes the following objective instead: Jθ=Eminπθas πθoldasAθolds,a, clipπθas πθoldas, 1 −ϵ, 1 +ϵAθolds,a Proximal Policy Optimization | 345 --- Page 362 --- θ*= argmaxθJθ Note that we no longer have a complex constraint, but rather an extra term built into the optimization objective.",
    "no longer have a complex constraint, but rather an extra term built into the optimization objective. The clip function represents an upper limit and a lower limit on the ratio between the target policy and the behavior policy, where any ratio above or below these limits is set equal to the corresponding limit. Note the inclusion of the minimum between the original and the clipped, which prevents us from making extreme updates and keeps us from overfitting.",
    "iginal and the clipped, which prevents us from making extreme updates and keeps us from overfitting. As stated in the paper introducing PPO, it’s important to notice that the objective for TRPO and PPO have the same gradient at θ=θold. This is the case in at least the on-policy setting, where we have a single policy from which we are sampling and optimizing (i.e., no distinction between the behavior and target policy). Let’s take a closer look at why this is the case.",
    "stinction between the behavior and target policy). Let’s take a closer look at why this is the case. To do this, we first need to reformulate TRPO’s constrained optimization objective as an equivalent regularized optimization objective (recall from early in the previous section), which we can do according to the theory.",
    "tion objective (recall from early in the previous section), which we can do according to the theory. The objective looks like: JTRPOθ=Eπθas πθoldasAs,a−β* KLπθoldasπθas Notice that we can separate the expression within the expectation into a difference of expectations due to the linearity of expectation. If we first consider the second expectation, or the KL term, we’ll notice that this term is minimized at θ=θold, since the reference distribution is parametrized using θold.",
    "that this term is minimized at θ=θold, since the reference distribution is parametrized using θold. Thus, the gradient at this setting is zero, since we have already reached the global minimum. We are left with only the gradient of the first expectation: ∇θEπθas πθoldasAs,a Looking to the objective for PPO, we notice that at θ=θold, the ratio between the two policies is one, eliminating the need for the clip term.",
    "e that at θ=θold, the ratio between the two policies is one, eliminating the need for the clip term. Thus, we are left with a minimum over two equivalent terms, which simplifies to the expectation over a single term. The gradient comes out to exactly what we just saw for the TRPO objective: ∇θEπθas πθoldasAs,a We have shown that PPO has the same gradient as TRPO in the select on-policy setting, and is additionally much easier to optimize in practice.",
    "nt as TRPO in the select on-policy setting, and is additionally much easier to optimize in practice. PPO has also shown 346 | Chapter 13: Deep Reinforcement Learning --- Page 363 --- strong empirical results on a variety of tasks, and has become widely used in the field of deep RL. Q-Learning and Deep Q-Networks Q-learning is in the category of reinforcement learning called value learning. Instead of directly learning a policy, we will be learning the value of states and actions.",
    "earning. Instead of directly learning a policy, we will be learning the value of states and actions. Q-learning involves learning a function, a Q-function , which represents the quality of a state, action pair. The Q-function, defined Q(s, a) , is a function that calculates the maximum discounted future return when action a is performed in state s.",
    "function that calculates the maximum discounted future return when action a is performed in state s. The Q-value represents our expected long-term rewards, given we are at a state, and take an action, and then take every subsequent action perfectly (to maximize expected future reward). This can be expressed formally as: Q*st,at=maxπE∑i=tTγiri A question you may be asking is, how can we know Q-values?",
    "essed formally as: Q*st,at=maxπE∑i=tTγiri A question you may be asking is, how can we know Q-values? It is difficult, even for humans, to know how good an action is, because you need to know how you are going to act in the future. Our expected future returns depend on what our long-term strategy is going to be. This seems to be a bit of a chicken-and-egg problem. In order to value a state, action pair, you need to know all the perfect subsequent actions.",
    "roblem. In order to value a state, action pair, you need to know all the perfect subsequent actions. And in order to know the best actions, you need to have accurate values for a state and action. The Bellman Equation We solve this dilemma by defining our Q-values as a function of future Q-values.",
    "he Bellman Equation We solve this dilemma by defining our Q-values as a function of future Q-values. This relation is called the Bellman equation , and it states that the maximum future reward for taking action is the current reward plus the next step’s max future reward from taking the next action a’: Q*st,at=Ert+γmaxa′Q*st+ 1,a′ This recursive definition allows us to relate between Q-values.",
    "ion a’: Q*st,at=Ert+γmaxa′Q*st+ 1,a′ This recursive definition allows us to relate between Q-values. And since we can now relate between Q-values past and future, this equation conven‐ iently defines an update rule. Namely, we can update past Q-values to be based on future Q-values. This is powerful because there exists a Q-value we know is correct: the Q-value of the very last action before the episode is over.",
    "exists a Q-value we know is correct: the Q-value of the very last action before the episode is over. For this last state, we know exactly that the next action led to the next reward, so we can perfectly set the Q-values for that state. We can use the update rule, then, to propagate that Q-value to the previous time step: Q-Learning and Deep Q-Networks | 347 --- Page 364 --- QjQj+ 1Qj+ 2 ...Q* This updating of the Q-value is known as value iteration .",
    "| 347 --- Page 364 --- QjQj+ 1Qj+ 2 ...Q* This updating of the Q-value is known as value iteration . Our first Q-value starts out completely wrong, but this is perfectly acceptable. With each iteration, we can update our Q-value via the correct one from the future. After one iteration, the last Q-value is accurate, since it is just the reward from the last state and action before episode termination. Then we perform our Q-value update, which sets the second-to-last Q-value.",
    "fore episode termination. Then we perform our Q-value update, which sets the second-to-last Q-value. In our next iteration, we can guarantee that the last two Q-values are correct, and so on and so forth. Through value iteration, we will be guaranteed convergence on the ultimate optimal Q-value. Issues with Value Iteration Value iteration produces a mapping between state and action pairs with correspond‐ ing Q-values, and we are constructing a table of these mappings, or a Q-table .",
    "irs with correspond‐ ing Q-values, and we are constructing a table of these mappings, or a Q-table . Let’s briefly talk about the size of this Q-table. Value iteration is an exhaustive process that requires a full traversal of the entire space of state, action pairs.",
    "is an exhaustive process that requires a full traversal of the entire space of state, action pairs. In a game like Breakout, with 100 bricks that can be either present or not, with 50 positions for the paddle to be in, and 250 positions for the ball to be in, and 3 actions, we have already constructed a space that is far, far larger than the sum of all computational capacity of humanity.",
    "constructed a space that is far, far larger than the sum of all computational capacity of humanity. Furthermore, in stochastic environments, the space of our Q-table would be even larger, and possibly infinite. With such a large space, it will be intractable for us to find all of the Q-values for every state, action pair. Clearly this approach is not going to work. How else are we going to do Q-learning?",
    "te, action pair. Clearly this approach is not going to work. How else are we going to do Q-learning? Approximating the Q-Function The size of our Q-table makes the naive approach intractable for any nontoy prob‐ lem. However, what if we relax our requirement for an optimal Q-function? If instead, we learn approximations of the Q-function, we can use a model to estimate our Q-function.",
    "f instead, we learn approximations of the Q-function, we can use a model to estimate our Q-function. Instead of having to experience every state, action pair to update our Q-table, we can learn a function that approximates this table, and even generalizes outside of its own experience. This means we won’t have to perform an exhaustive search through all possible Q-values to learn a Q-function. Deep Q-Network This was the main motivation behind DeepMind’s work on deep Q-network (DQN).",
    "unction. Deep Q-Network This was the main motivation behind DeepMind’s work on deep Q-network (DQN). DQN uses a deep neural network that takes an image (the state) in to estimate the Q-value for all possible actions. 348 | Chapter 13: Deep Reinforcement Learning --- Page 365 --- Training DQN We would like to train our network to approximate the Q-function.",
    "ning --- Page 365 --- Training DQN We would like to train our network to approximate the Q-function. We express this Q-function approximation as a function of our model’s parameters, like this: Qθs,a ∣ θ∼ Q *s,a Remember, Q-learning is a value-learning algorithm. We are not learning a policy directly, but rather we are learning the values of each state, action pair, regardless if they are good or not.",
    "ut rather we are learning the values of each state, action pair, regardless if they are good or not. We have expressed our model’s Q-function approximation as Qtheta, and we would like this to be close to the future expected reward.",
    "-function approximation as Qtheta, and we would like this to be close to the future expected reward. Using the Bellman equation from earlier, we can express this future expected reward as: Rt*=rt+γmaxa′Qst+ 1,a′θ Our objective is to minimize the difference between our Q’s approximation, and the next Q value: minθ∑e ∈ E ∑t= 0TQst,atθ−Rt* Expanding this expression gives us our full objective: minθ∑e ∈ E ∑t= 0TQst,atθ−rt+γmaxa′Qst+ 1,a′θ This objective is fully differentiable as a function of our model parameters, and we can find gradients to use in stochastic gradient descent to minimize this loss.",
    "l parameters, and we can find gradients to use in stochastic gradient descent to minimize this loss. Learning Stability One issue you may have noticed is that we are defining our loss function based on the difference of our model’s predicted Q-value of this step and the predicted Q-value of the next step. In this way, our loss is doubly dependent on our model parameters. With each parameter update, the Q-values are constantly shifting, and we are using shifting Q-values to do further updates.",
    "ate, the Q-values are constantly shifting, and we are using shifting Q-values to do further updates. This high correlation of updates can lead to feedback loops and instability in our learning, where our parameters may oscillate and make the loss diverge. We can employ a couple of simple engineering hacks to remedy this correlation problem: namely, target Q-network and experience replay.",
    "ngineering hacks to remedy this correlation problem: namely, target Q-network and experience replay. Q-Learning and Deep Q-Networks | 349 --- Page 366 --- Target Q-Network Instead of updating a single network frequently with respect to itself, we can reduce this codependence by introducing a second network, called the target network . Our loss function features to instances of the Q-function, Qst,atθ and Qst+ 1,a′θ.",
    "target network . Our loss function features to instances of the Q-function, Qst,atθ and Qst+ 1,a′θ. We are going to have the first Q be represented as our prediction network, and our second Q will be produced by the target Q-network. The target Q-network is a copy of our prediction network that lags in its parameter updates. We update the target Q-network to equal the prediction network only every few batches.",
    "eter updates. We update the target Q-network to equal the prediction network only every few batches. This provides much needed stability to our Q-values, and we can now properly learn a good Q-function. Experience Replay There is yet another source of irksome instability to our learning: the high correla‐ tions of recent experiences. If we train our DQN with batches drawn from recent experience, these action, state pairs are all going to be related to one another.",
    "drawn from recent experience, these action, state pairs are all going to be related to one another. This is harmful because we want our batch gradients to be representative of the entire gradient, and if our data is not representative of the data distribution, our batch gradient will not be an accurate estimate of the true gradient. So, we have to break up this correlation of data in our batches. We can do this using something called experience replay .",
    "p this correlation of data in our batches. We can do this using something called experience replay . In experience replay, we store all of the agent’s experiences as a table, and to construct a batch, we randomly sample from these experiences. We store these experiences in a table as si,ai,ri,si+ 1 tuples. From these four values, we can compute our loss function, and thus our gradient to optimize our network. This experience replay table is more of a queue than a table.",
    "our gradient to optimize our network. This experience replay table is more of a queue than a table. The experiences an agent sees early in training may not be representative of the experiences a trained agent finds itself in later, so it is useful to remove old experiences from our table. From Q-Function to Policy Q-learning is a value learning paradigm, not a policy learning algorithm. This means we are not directly learning a policy for acting in our environment.",
    "learning algorithm. This means we are not directly learning a policy for acting in our environment. But can’t we construct a policy from what our Q-function tells us? If we have learned a good Q-function approximation, this means we know the value of every action for every state. We could then trivially construct an optimal policy in the following way: look at our Q-function for all actions in our current state, choose the action with the max Q-value, enter a new state, and repeat.",
    "actions in our current state, choose the action with the max Q-value, enter a new state, and repeat. If our Q-function is optimal, our policy derived from it will be optimal.",
    "r a new state, and repeat. If our Q-function is optimal, our policy derived from it will be optimal. With this in mind, we can express the optimal policy as follows: 350 | Chapter 13: Deep Reinforcement Learning --- Page 367 --- πs;θ= arg max a′Q*s,a′;θ We can also use the sampling techniques we discussed earlier to make a stochastic policy that sometime deviates from the Q-function recommendations to vary the amount of exploration our agent does.",
    "etime deviates from the Q-function recommendations to vary the amount of exploration our agent does. DQN and the Markov Assumption DQN is still a Markov decision process that relies on the Markov assumption , which assumes that the next state si+ 1 depends only on the current state si and action ai, and not on any previous states or actions. This assumption doesn’t hold true for many environments where the game’s state cannot be summed up in a single frame.",
    "oesn’t hold true for many environments where the game’s state cannot be summed up in a single frame. For example, in Pong, the ball’s velocity (an important factor in successful game play) is not captured in any single game frame. The Markov assumption makes modeling decision processes much simpler and reliable, but often at a loss of modeling power. DQN’s Solution to the Markov Assumption DQN solves this problem by utilizing state history .",
    "power. DQN’s Solution to the Markov Assumption DQN solves this problem by utilizing state history . Instead of processing one game frame as the game’s state, DQN considers the past four game frames as the game’s current state. This allows DQN to utilize time-dependent information. This is a bit of an engineering hack, and we will discuss better ways of dealing with sequences of states at the end of this chapter.",
    "ack, and we will discuss better ways of dealing with sequences of states at the end of this chapter. Playing Breakout with DQN Let’s pull all of what we learned together and actually go about implementing DQN to play Breakout.",
    "Let’s pull all of what we learned together and actually go about implementing DQN to play Breakout. We start out by defining our DQNAgent : # DQNAgent class DQNAgent(object): def __init__(self, num_actions, learning_rate=1e-3, history_length=4, screen_height=84, screen_width=84, gamma=0.99): self.num_actions = num_actions self.learning_rate = learning_rate self.history_length = history_length self.screen_height = screen_height self.screen_width = screen_width self.gamma = gamma self.build_prediction_network() self.build_target_network() #self.build_training() Q-Learning and Deep Q-Networks | 351 --- Page 368 --- def build_prediction_network(self): self.model_predict = nn.Sequential( nn.Conv2d(4, 32, kernel_size=8 , stride=4), nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.Flatten(), nn.Linear(3136, 512), nn.Linear(512, self.num_actions) ) def build_target_network(self): self.model_target = nn.Sequential( nn.Conv2d(4, 32, kernel_size=8 , stride=4), nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.Flatten(), nn.Linear(3136, 512), nn.Linear(512, self.num_actions) ) def sample_and_train_pred(self, replay_table, batch_size): s_t, action, reward, s_t_plus_1, terminal = \\ replay_table.sample_batch(batch_size) # given state_t, find q_t (predict_model) and # q_t+1 (target_model) # do it in batches # Find q_t_plus_1 input_t = torch.from_numpy(s_t_plus_1).float() model_t = self.model_target.float() q_t_plus_1 = model_t(input_t) terminal = torch.tensor(terminal).float() max_q_t_plus_1, _ = torch.max(q_t_plus_1, dim=1) reward = torch.from_numpy(reward).float() target_q_t = (1.",
    "_plus_1, _ = torch.max(q_t_plus_1, dim=1) reward = torch.from_numpy(reward).float() target_q_t = (1. - terminal) * self.gamma * \\ max_q_t_plus_1 + reward # Find q_t, and q_of_action input_p = torch.from_numpy(s_t).float() model_p = self.model_predict.float() q_t = model_p(input_p) action = torch.from_numpy(action) action_one_hot = nn.functional.one_hot(action, self.num_actions) q_of_action = torch.sum(q_t * action_one_hot) # Compute loss self.delta = (target_q_t - q_of_action) self.loss = torch.mean(self.delta) 352 | Chapter 13: Deep Reinforcement Learning --- Page 369 --- # Update predict_model gradients (only) self.optimizer = optim.Adam(self.model_predict.parameters(), lr = self.learning_rate) self.loss.backward() self.optimizer.step() return q_t def predict_action(self, state, epsilon_percentage): input_p = torch.from_numpy(state).float().unsqueeze(dim=0) model_p = self.model_predict.float() action_distribution = model_p(input_p) # sample from action distribution action = epsilon_greedy_action_annealed( action_distribution.detach(), epsilon_percentage) return action def process_state_into_stacked_frames(self, frame, past_frames, past_state=None): full_state = np.zeros((self.history_length, self.screen_width, self.screen_height)) if past_state is not None: for i in range(len(past_state)-1): full_state[i, :, :] = past_state[i+1, :, :] full_state[-1, :, :] = self.preprocess_frame(frame, (self.screen_width, self.screen_height) ) else: all_frames = past_frames + [frame] for i, frame_f in enumerate(all_frames): full_state[i, :, :] = self.preprocess_frame(frame_f, (self.screen_width, self.screen_height) ) return full_state def to_grayscale(self, x): return np.dot(x[...,:3], [0.299, 0.587, 0.114]) def preprocess_frame(self, im, shape): cropped = im[16:201,:] # (185, 160, 3) grayscaled = self.to_grayscale(cropped) # (185, 160) # resize to (84,84) resized = np.array(Image.fromarray(grayscaled).resize(shape)) mean, std = 40.45, 64.15 Q-Learning and Deep Q-Networks | 353 --- Page 370 --- frame = (resized-mean)/std return frame There is a lot going on in this class, so let’s break it down in the following sections.",
    "eturn frame There is a lot going on in this class, so let’s break it down in the following sections. Building Our Architecture We build our two Q-networks: the prediction network and the target Q-network. Notice how they have the same architecture definition, since they are the same network, with the target Q just having delayed parameter updates. Since we are learning to play Breakout from pure pixel input, our game state is an array of pixels.",
    "Since we are learning to play Breakout from pure pixel input, our game state is an array of pixels. We pass this image through three convolution layers, and then two fully connected layers to produce our Q-values for each of our potential actions. Stacking Frames Y ou may notice that our state input is actually of size [None, self.history_length, self.screen_height, self.screen_width] .",
    "state input is actually of size [None, self.history_length, self.screen_height, self.screen_width] . Remember, in order to model and capture time-dependent state variables like speed, DQN uses not just one image, but a group of consecutive images, also known as a history . Each of these consec‐ utive images is treated as a separate channel. We construct these stacked frames with the helper function process_state _into_stacked_frames(self, frame, past_frames, past_state=None) .",
    "the helper function process_state _into_stacked_frames(self, frame, past_frames, past_state=None) . Setting Up Training Operations Our loss function is derived from our objective expression from earlier in this chapter: minθ∑e ∈ E ∑t= 0TQst,atθ−rt+γmaxa′Qst+ 1,a′θ We want our prediction network to equal our target network, plus the return at the current time step.",
    "e want our prediction network to equal our target network, plus the return at the current time step. We can express this in pure PyTorch code as the difference between the output of our prediction network and the output of our target network. We use this gradient to update and train our prediction network, using AdamOptimizer . Updating Our Target Q-Network To ensure a stable learning environment, we update our target Q-network only once every four batches.",
    "o ensure a stable learning environment, we update our target Q-network only once every four batches. Our update rule for the target Q-network is pretty simple: we just set its weights equal to the prediction network. We do this in the function update_target_q_network(self) . The optimizer_predict.step() function sets the target Q-network’s weights equal to those of the prediction network.",
    "redict.step() function sets the target Q-network’s weights equal to those of the prediction network. 354 | Chapter 13: Deep Reinforcement Learning --- Page 371 --- Implementing Experience Replay We’ve discussed how experience replay can help de-correlate our gradient batch updates to improve the quality of our Q-learning and subsequent derived policy. Let’s walk though a simple implementation of experience replay.",
    "rning and subsequent derived policy. Let’s walk though a simple implementation of experience replay. We expose a method add_episode (self, episode) , which takes an entire episode (an EpisodeHistory object) and adds it to the ExperienceReplayTable. It then checks if the table is full and removes the oldest experiences from the table.",
    "eReplayTable. It then checks if the table is full and removes the oldest experiences from the table. When it comes time to sample from this table, we can call sample_batch(self, batch_size) to randomly construct a batch from our table of experiences: class ExperienceReplayTable(object): def __init__(self, table_size=50000): self.states = [] self.actions = [] self.rewards = [] self.state_primes = [] self.terminals = [] self.table_size = table_size def add_episode(self, episode): self.states += episode.states self.actions += episode.actions self.rewards += episode.rewards self.state_primes += episode.state_primes self.terminals += episode.terminals self.purge_old_experiences() def purge_old_experiences(self): while len(self.states) > self.table_size: self.states.pop(0) self.actions.pop(0) self.rewards.pop(0) self.state_primes.pop(0) def sample_batch(self, batch_size): s_t, action, reward, s_t_plus_1, terminal = [], [], [], [], [] rands = np.arange(len(self.states)) np.random.shuffle(rands) rands = rands[:batch_size] for r_i in rands: s_t.append(self.states[r_i]) action.append(self.actions[r_i]) reward.append(self.rewards[r_i]) s_t_plus_1.append(self.state_primes[r_i]) Q-Learning and Deep Q-Networks | 355 --- Page 372 --- terminal.append(self.terminals[r_i]) return (np.array(s_t), np.array(action), np.array(reward), np.array(s_t_plus_1), np.array(terminal)) DQN Main Loop Let’s put this all together in our main function, which will create an OpenAI Gym environment for Breakout, make an instance of our DQNAgent , and have our agent interact with and train to play Breakout successfully: learn_start = 4 total_episodes = 32 epsilon_stop = 32 train_frequency = 2 target_frequency = 4 batch_size = 4 max_episode_length = 1000 env = gym.make('Breakout-v4') num_actions = env.action_space.n solved = False agent = DQNAgent(num_actions=num_actions, learning_rate=1e-4, history_length=4, gamma=0.98) episode_rewards = [] q_t_list = [] batch_losses = [] past_frames_last_time = None replay_table = ExperienceReplayTable() global_step_counter = 0 for i in range(total_episodes): # Get initial frame -> state frame = env.reset() # np.array of shape (210, 160, 3) # past_frames is a list of past 3 frames (np.arrays) past_frames = [copy.deepcopy(frame) for _ in range( agent.history_length-1)] state = agent.process_state_into_stacked_frames( frame, past_frames, past_state=None) # state is (4,84,84) # initialize episode history (s_t, a, r, s_t+1, terminal) episode_reward = 0.0 episode_history = EpisodeHistory() epsilon_percentage = float(min(i/float(epsilon_stop), 1.0)) for j in range(max_episode_length): # predict action or choose random action at first if global_step_counter < learn_start: 356 | Chapter 13: Deep Reinforcement Learning --- Page 373 --- action = np.argmax(np.random.random((agent.num_actions))) else: action = agent.predict_action(state, epsilon_percentage) # take action, get next frame (-> next state), reward, # and terminal reward = 0 frame_prime, reward, terminal, _ = env.step(action) if terminal == True: reward -= 1 # get next state from next frame and past frames state_prime = agent.process_state_into_stacked_frames( frame_prime, past_frames, past_state=state) # Update past_frames with frame_prime for next time past_frames.append(frame_prime) past_frames = past_frames[len(past_frames)- \\ agent.history_length:] past_frames_last_time = past_frames # Add to episode history (state, action, reward, # state_prime, terminal) episode_history.add_to_history( state, action, reward, state_prime, terminal) state = state_prime episode_reward += reward global_step_counter += 1 # Do not train predict_model until we have enough # episodes in episode history if global_step_counter > learn_start: if global_step_counter % train_frequency == 0: if(len(replay_table.actions) != 0): q_t = agent.sample_and_train_pred(replay_table, batch_size) q_t_list.append(q_t) if global_step_counter % target_frequency == 0: agent.model_target.load_state_dict( agent.model_predict.state_dict()) # If terminal or max episodes reached, # add episode_history to replay table if j == (max_episode_length - 1): terminal = True if terminal: replay_table.add_episode(episode_history) episode_rewards.append(episode_reward) break Q-Learning and Deep Q-Networks | 357 --- Page 374 --- print(f'Episode[{i}]: {len(episode_history.actions)} \\ actions {episode_reward} reward') DQNAgent Results on Breakout We train our DQNAgent for one thousand episodes to see the learning curve.",
    "Agent Results on Breakout We train our DQNAgent for one thousand episodes to see the learning curve. To obtain superhuman results on Atari, typical training time runs up to several days. However, we can see a general upward trend in reward pretty quickly, as shown in Figure 13-7 . Figure 13-7.",
    ", we can see a general upward trend in reward pretty quickly, as shown in Figure 13-7 . Figure 13-7. Our DQNAgent gets increasingly better at Breakout during training as it learns a good value function and also acts less stochastically due to ϵ-greedy annealing Improving and Moving Beyond DQN DQN did a pretty good job back in 2013 in solving Atari tasks, but had some serious shortcomings.",
    "QN DQN did a pretty good job back in 2013 in solving Atari tasks, but had some serious shortcomings. DQN’s many weaknesses include that it takes very long to train, doesn’t work well on certain types of games, and requires retraining for every new game. Much of the deep reinforcement learning research of the past few years has been in addressing these various weaknesses. 358 | Chapter 13: Deep Reinforcement Learning --- Page 375 --- 5Sorokin, Ivan, et al. “Deep Attention Recurrent Q-Network.",
    "Reinforcement Learning --- Page 375 --- 5Sorokin, Ivan, et al. “Deep Attention Recurrent Q-Network. ” arXiv preprint arXiv :1512.01693 (2015). 6Mnih, Volodymyr, et al. “ Asynchronous Methods for Deep Reinforcement Learning. ” International Conference on Machine Learning . 2016.Deep Recurrent Q-Networks Remember the Markov assumption? The one that states that the next state relies only on the previous state and the action taken by the agent?",
    "that states that the next state relies only on the previous state and the action taken by the agent? DQN’s solution to the Markov assumption problem, stacking four consecutive frames as separate channels, sidesteps this issue and is a bit of an ad hoc engineering hack. Why 4 frames and not 10? This imposed frames history hyperparameter limits the model’s generality. How do we deal with arbitrary sequences of related data?",
    "perparameter limits the model’s generality. How do we deal with arbitrary sequences of related data? That’s right: we can use what we learned back in Chapter 8 on RNNs to model sequences with deep recurrent Q-networks (DRQNs). DRQN uses a recurrent layer to transfer a latent knowledge of state from one time step to the next.",
    "DRQN uses a recurrent layer to transfer a latent knowledge of state from one time step to the next. In this way, the model itself can learn how many frames are informa‐ tive to include in its state and can even learn to throw away noninformative ones or remember things from long ago. DRQN has even been extended to include neural attention mechanism, as shown in Sorokin et al.",
    "g ago. DRQN has even been extended to include neural attention mechanism, as shown in Sorokin et al. ’s 2015 paper, “Deep Attention Recurrent Q-Network” (DAQRN).5 Since DRQN is dealing with sequences of data, it can attend to certain parts of the sequence. This ability to attend to certain parts of the image both improves performance and provides model interpretability by producing a rationale for the action taken.",
    "roves performance and provides model interpretability by producing a rationale for the action taken. DRQN has shown to be better than DQN at playing first-person shooter (FPS) games like DOOM , as well as improving performance on certain Atari games with long time dependencies, like Seaquest .",
    "as well as improving performance on certain Atari games with long time dependencies, like Seaquest . Asynchronous Advantage Actor-Critic Agent Asynchronous advantage actor-critic (A3C) is a new approach to deep reinforcement learning introduced in the 2016 DeepMind paper, “ Asynchronous Methods for Deep Reinforcement Learning. ”6 Let’s discuss what it is and why it improves upon DQN.",
    "s Methods for Deep Reinforcement Learning. ”6 Let’s discuss what it is and why it improves upon DQN. A3C is asynchronous , which means we can parallelize our agent across many threads, which means orders of magnitude faster training by speeding up our environment simulation. A3C runs many environments at once to gather experiences.",
    "by speeding up our environment simulation. A3C runs many environments at once to gather experiences. Beyond the speed increase, this approach presents another significant advantage in that it further decorrelates the experiences in our batches, because the batch is being filled with the experiences of numerous agents in different scenarios simultaneously. Improving and Moving Beyond DQN | 359 --- Page 376 --- 7Konda, Vijay R., and John N. Tsitsiklis. “ Actor-Critic Algorithms. ” NIPS . Vol. 13.",
    "Page 376 --- 7Konda, Vijay R., and John N. Tsitsiklis. “ Actor-Critic Algorithms. ” NIPS . Vol. 13. 1999. 8Jaderberg, Max, et al. “Reinforcement Learning with Unsupervised Auxiliary Tasks. ” arXiv preprint arXiv :1611.05397 (2016).A3C uses an actor-critic method.7 Actor-critic methods involve learning both a value function Vst (the critic) and also a policy πst (the actor).",
    "c methods involve learning both a value function Vst (the critic) and also a policy πst (the actor). Early in this chapter, we delineated two different approaches to reinforcement learning: value learning and policy learning. A3C combines the strengths of each, using the critic’s value function to improve the actor’s policy. A3C uses an advantage function instead of a pure discounted future return.",
    "prove the actor’s policy. A3C uses an advantage function instead of a pure discounted future return. When doing policy learning, we want to penalize the agent when it chooses an action that leads to a bad reward. A3C aims to achieve this same goal, but uses advantage instead of reward as its criterion. Advantage represents the difference between the model’s prediction of the quality of the action taken versus the actual quality of the action taken. We can express advantage as: At=Q*st,at−Vst.",
    "on taken versus the actual quality of the action taken. We can express advantage as: At=Q*st,at−Vst. A3C has a value function, V(t), but it does not express a Q-function. Instead, A3C estimates the advantage by using the discounted future reward as an approximation for the Q-function: At=Rt−Vst These three techniques proved key to A3C’s takeover of most deep reinforcement learning benchmarks.",
    "These three techniques proved key to A3C’s takeover of most deep reinforcement learning benchmarks. A3C agents can learn to play Atari Breakout in less than 12 hours, whereas DQN agents may take 3 to 4 days. UNsupervised REinforcement and Auxiliary Learning UNREAL is an improvement on A3C introduced in “Reinforcement learning with unsupervised auxiliary tasks\" by Jaderberg et al.,8 who, you guessed it, are from DeepMind. UNREAL addresses the problem of reward sparsity.",
    "rg et al.,8 who, you guessed it, are from DeepMind. UNREAL addresses the problem of reward sparsity. Reinforcement learning is so difficult because our agent just receives rewards, and it is hard to determine exactly why rewards increase or decrease, which makes learning difficult. Additionally, in reinforcement learning, we must learn a good representation of the world as well as a good policy to achieve reward.",
    "arning, we must learn a good representation of the world as well as a good policy to achieve reward. Doing all of this with a weak learning signal like sparse rewards is quite a tall order. 360 | Chapter 13: Deep Reinforcement Learning --- Page 377 --- UNREAL asks the question, what can we learn from the world without rewards? It aims to learn a useful world representation in an unsupervised matter. Specifically, UNREAL adds some additional unsupervised auxiliary tasks to its overall objective.",
    "er. Specifically, UNREAL adds some additional unsupervised auxiliary tasks to its overall objective. The first task involves the UNREAL agent learning about how its actions affect the environment. The agent is tasked with controlling pixel values on the screen by taking actions. To produce a set of pixel values in the next frame, the agent must take a specific action in this frame.",
    "roduce a set of pixel values in the next frame, the agent must take a specific action in this frame. In this way, the agent learns how its actions affect the world around it, enabling it to learn a representation of the world that takes into account its own actions. The second task involves the UNREAL agent learning reward prediction. Given a sequence of states, the agent is tasked with predicting the value of the next reward received.",
    "ven a sequence of states, the agent is tasked with predicting the value of the next reward received. The intuition behind this is that if an agent can predict the next reward, it probably has a pretty good model of the future state of the environment, which will be useful when constructing a policy. As a result of these unsupervised auxiliary tasks, UNREAL is able to learn around 10 times faster than A3C in the Labyrynth game environment.",
    "ry tasks, UNREAL is able to learn around 10 times faster than A3C in the Labyrynth game environment. UNREAL highlights the importance of learning good world representations and how unsupervised learning can aid in weak learning signal or low-resource learning problems like reinforcement learning. Summary In this chapter, we covered the fundamentals of reinforcement learning, including MDPs, maximum discounted future rewards, and explore versus exploit.",
    "inforcement learning, including MDPs, maximum discounted future rewards, and explore versus exploit. We also covered various approaches to deep reinforcement learning, including policy gradi‐ ents and deep Q-networks, and touched on some recent improvements on DQN and new developments in deep reinforcement learning. Reinforcement learning is essential to building agents that can not only perceive and interpret the world, but also take action and interact with it.",
    "gents that can not only perceive and interpret the world, but also take action and interact with it. Deep reinforcement learning has made major advancements toward this goal, successfully producing agents capable of mastering Atari games, safely driving automobiles, trading stocks profitably, controlling robots, and more.",
    "ng Atari games, safely driving automobiles, trading stocks profitably, controlling robots, and more. Summary | 361 --- Page 379 --- Index Symbols 2D convolutions, 130 __init__ method, 134 ϵ-Greedy strategy, 333 A action, 328 actor-critic method, 360 AdaDelta optimizer, 115 AdaGrad algorithm, 111 Adam algorithm, 113-115, 135, 143 additive feature attribution, 292 advantage function, 342, 360 affine transformation, 139 agent, 326 AlexNet, 120, 147 algebra (see linear algebra) algorithms AdaDelta algorithm, 115 AdaGrad algorithm, 111 Adam algorithm, 113-115, 135, 143 Broyden–Fletcher–Goldfarb–Shanno algo‐ rithm, 110 greedy algorithms, 279 Markov Chain Monte Carlo algorithm, 265 neural style algorithm, 152 random forest algorithm, 277 tree-based algorithms, 276-280 allocation weightings, 309, 317 annealed ϵ-Greedy strategy, 333 arc-standard system, 199 artificial neural networks, 48 artistic styles, replicating, 152asynchronous advantage actor-critic agent (A3C), 359 Atari games, 325 attention-based memory access, 301-303 attentional mechanisms, 227-230 audiograms, 154 autoencoders architecture, 160, 178 denoising, 171-173, 269-274 k-Sparse autoencoders, 176 in PyTorch, 161-170 sparsity, 174-177 Variational Autoencoders, 249-259 auxiliary random variables, 257 axons, 46 B bAbI dataset, 321 backpropagation, 61-63 backward() function, 84, 86 basis, 9 batch gradient descent, 63 batch normalization, 137-139 BatchNorm1d constructor, 139 BatchNorm2d class, 138 Bayes' Theorem, 27, 35 Bayesian view, 18 beam searches, 203-205 Bellman equation, 347 BFGS (Broyden–Fletcher–Goldfarb–Shanno) algorithm, 110 bias, 46 blurry attention-based reading, 302 Breakout, 351, 358 363 --- Page 380 --- broadcasting, 81 Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm, 110 bucketing, 231 build_vocab_from_iterator function, 220 C CBOW (Continuous Bag of Words model), 179 cell bodies, 46 Central Limit Theorem (CLT), 34 characteristic polynomial, 14 CIFAR-10 challenge, 141 classification, 20 classifiers, 20 closed under scalar multiplication and closed under addition, 9 CLT (Central Limit Theorem), 34 code, 160 code examples, obtaining and using, x column space, 7-10 column vector, 6 column vector interpretation of matrix multi‐ plication, 4 comments and questions, xi complement, 18 Compose transform, 137 compressive embeddings, 160, 175 computer programs, 40 conda package management system, 77 conditional probability, 20-21 conjugate gradient descent, 109 consistency, 293 content-based addressing, 303 Continuous Bag of Words (CBOW) model, 179 continuous probability distributions, 32-36 convolutional filters, 152 convolutional neural networks (CNNs) applying to other domains, 154 batch normalization, 137-139 CIFAR-10 challenge, 141 convolutional filters, 152 feature selection, 118-120 filters and feature maps, 122-127 full architectural description of, 132-133 full description of, 127-131 group normalization, 139 image preprocessing pipelines, 136 inception of, 120 max pooling, 131MNIST classifier using, 134-135 neurons in human vision, 117 residual learning and skip connections, 147-149 residual networks with superhuman vision, 149-152 scaling problems, 121 space required for, 133 visualizing learning in, 143-146 convolutions, 124 copy.deepcopy method, 99 covariance, 27 critical point, 102 cross entropy, 31, 36 cross-entropy loss metric, 86 D DAQRN (Deep Attention Recurrent Q- Network), 359 data matrix, 74 data structures and operations matrix arrays, 1 matrix operations, 3-6 matrix-vector multiplication, 7 vector operations, 6 DataLoader class (PyTorch), 88-89, 221 Dataset class (PyTorch), 87-89 dataset normalization, 136 decision trees, 276-280 decoders, 161, 224, 251 Deep Attention Recurrent Q-Network (DAQRN), 359 deep learning approach to learning, x approach to problem solving, 68 core concepts of, 1 definition of term, 39 prerequisites to learning, ix stateful deep learning models, 206 success of, 45 deep Q-networks (DQNs) (see also Q-learning) building architecture, 354 DQNAgent results on Breakout, 358 experience replay, 350 from Q-function to policy, 350 implementing experience replay, 355 learning stability, 349 main loop, 356 Markov assumption and, 351 364 | Index --- Page 381 --- motivation behind, 348 playing Breakout with, 351 remarkable achievements of, 325 setting up training operations, 354 solution to Markov assumption, 351 stacking frames, 354 target Q-network, 350 training DQNs, 349 updating target Q-network, 354 deep recurrent Q-networks (DRQNs), 359 deep reinforcement learning basics of, 326 explore-exploit dilemma, 331-333 Markov decision process, 328-331 moving beyond DQNs asynchronous advantage actor-critic agent, 359 deep recurrent Q-networks, 359 UNREAL, 360 pole-cart problem agent creation, 335 history, 337 model and optimizer, 337 OpenAI Gym, 335 PGAgent performance, 340 policy gradient main function, 338 sampling actions, 337 policy versus value learning, 334 proximal policy optimization, 345 Q-learning and deep Q-networks approximating Q-function, 348 Bellman equation, 347 building architecture, 354 deep Q-networks, 348 DQN and Markov assumption, 351 DQN main loop, 356 DQNAgent results on Breakout, 358 DQNs solution to Markov assumption, 351 experience replay, 350 from Q-function to policy, 350 implementing experience replay, 355 learning stability, 349 playing Breakout with DQN, 351 Q-functions and Q-values, 347 setting up training operations, 354 stacking frames, 354 target Q-network, 350 training DQNs, 349updating target Q-network, 354 value iteration, 348 trust-region policy optimization, 341-345 DeepMind, 325 delta rule, 59 dendrites, 46 denoising, 171-173, 269 denoising score matching (DSM), 272 density, 175 dependency parsing, 197-202 determinant, 14 differentiable neural computers (DNCs) basics of, 307-309 controller network, 313 implementing in PyTorch, 317-321 interference-free writing in, 309 memory reuse, 310 read heads, 312 teaching to read and comprehend, 321-322 temporal linking of writes, 311 visualizing in action, 314-316 dimension, 4, 9 dimensionality reduction alternatives to, 177 drawbacks of PCA for, 159 discounted future return, 331 discrete space, 17 discriminative models, 243 discriminators, 244 divergence, 176 DNC (see differentiable neural computers) DOOM, 359 dot product, 6 dot product interpretation of matrix multiplica‐ tion, 4 downsampler function, 150 DQN (see deep Q-networks) dropout, 23, 73 DRQNs (deep recurrent Q-networks), 359 E eigendecomposition, 187 eigenvalues, 13-14 eigenvectors, 13-14 ELBO (evidence lower bound), 251 embedding and representation learning autoencoder architecture, 160 autoencoders in PyTorch, 161-170 context and, 177-179 Index | 365 --- Page 382 --- definition of embeddings, 157 denoising, 171-173 lower-dimensional representations, 157 PCA and SVD, 187-188 principal component analysis, 158-159 Skip-Gram model, 182-186 sparsity, 174-177 Word2Vec framework, 179-182 empirical risk, 75 encoders, 160, 224, 251 end-of-sequence (EOS) tokens, 225 end-to-end-differentiability, 301 entropy, 29-32, 36 environment, 326 epochs, 69 epsilon greedy strategy, 333 erase vector, 302, 308 error surfaces challenges of spurious local minima, 98-101 critical challenges to optimization, 104-106 flat regions in, 101-103 learning rate adaptation accumulating historical gradients, 111 choosing correct learning rate, 111 combining momentum and RMSProp, 113-115 exponentially weighted moving average of gradients, 112 local minima in, 96 model identifiability, 97 momentum-based optimization, 106-109 philosophy behind optimizer selection, 115 second-order methods, 109 events and probability, 17-20 evidence lower bound (ELBO), 251 expansion step, 204 expectation and variance, 24-27 expected discounted reward, 342 experience replay, 350, 355 explainers, 275 (see also interpretability meth‐ ods) explicit score matching, 268 explore-exploit dilemma, 331-333 exponentially weighted decay, 106 extractive rationalization, 283-288 F fast-food problem, 55 feature extraction, 119feature importance, evaluating partial dependence plots, 282 permutation feature importance, 281 feature maps, 123-127 feature selection automating with embeddings, 157 shortcomings of, 118-120 feature vectors, 119 features, 41 feed-forward neural networks basics of, 48-51 sequence analysis and, 189 training backpropagation, 61-63 delta rule and learning rates, 58 fast-food problem, 55 gradient descent, 57 gradient descent with sigmoidal neu‐ rons, 60 preventing overfitting, 71-73 stochastic and minibatch gradient descent, 63 test sets, validation sets, and overfitting, 65-71 filters, 123-127, 152 first moment, 113 first-person shooter (FPS) games, 359 for loops, 317 fractional max pooling, 132 free list, 309 frequentist view, 18 fundamental spaces column space, 7-10 null space, 10-13 future return, 330 G GANs (Generative Adversarial Networks), 244-249 garden path sentences, 203 Gated Recurrent Unit (GRU), 218 gated weighting, 304 Gaussian distributions, 34, 250 generalization, 66 Generative Adversarial Networks (GANs), 244-249 generative models basics of, 243 366 | Index --- Page 383 --- denoising autoencoders and score match‐ ing, 269-274 Generative Adversarial Networks, 244-249 score-based generative models, 264-269 Variational Autoencoders basics, 249-259 Variational Autoencoders implementation, 259-264 generators, 244 gensim package, 192 global normalization, 205 GO tokens, 232 Google Colab, 192, 219 Google News, 192 gradient descent batch, 63 challenges of using, 95 challenges with vanishing gradients, 210-212 conjugate, 109 learning parameter vectors through, 45 minibatch, 65 with sigmoidal neurons, 60 stochastic, 64 training feed-forward networks, 57 gradients, 57, 104-106, 334 Gram matrix, 153 greedy algorithms, 279 greedy pretraining, 95, 203 grid searches, 69 group normalization, 139 GRU (Gated Recurrent Unit), 218 H Hessian matrix (H), 105 hidden layers, 49 history, 354 human vision, 117 hyperparameter optimization, 69 I identity matrix, 5 iff (if and only if), 6 ill-conditioning, 105 image preprocessing pipelines, 136 image recognition (see convolutional neural networks) image whitening, 136 ImageNet challenge, 120 implicit score matching, 269independence, 21 intelligent agents, 327 intelligent machines, 39 intensity detectors, 119 interface vector, 307 interpolation gate, 304 interpretability, 174 interpretability methods classic models decision trees and tree-based algorithms, 276-280 linear regression, 280 evaluating feature importance partial dependence plots, 282 permutation feature importance, 281 extractive rationalization, 283-288 LIME, 288-292 overview of, 275 SHAP, 292-297 intersection, 19 inversion, 3 inverted dropout, 73 iterable-style datasets, 220 J joint probability distribution, 27 K k-Sparse autoencoders, 176 keep gates, 213 kernel_size argument, 131, 150 key strength, 303 Kullback-Leibler (KL) divergence, 31, 36, 176, 253 L L-BFGS, 110 L1 regularization, 72 L2 regularization, 72, 139 Langevin dynamics, 265 language translation, 230-239 Large Movie Review Dataset, 219 layer normalization, 140 learning rate, 58, 139 learning rate adaptation AdaGrad algorithm, 111 Adam algorithm, 113-115 choosing correct learning rate, 111 Index | 367 --- Page 384 --- RMSProp optimizer, 112 LevelDB database, 192 likelihood, 33 LIME (Local Interpretable Model-agnostic Explanations), 288-292 linear algebra data structures and operations, 1-7 eigenvectors and eigenvalues, 13-14 fundamental spaces, 7-13 relationship to deep learning, 1 linear independence, 9 linear neurons, 51 linear perceptrons definition of term, 43 expressing as neurons, 47 linear regression, 280 link matrix, 311 load_state_dict, 99 local accuracy, 293 Local Interpretable Model-agnostic Explana‐ tions (LIME), 288-292 local maxima, 102 local minima, 96 local normalization, 205 locally invariance, 132 location-based mechanism, 306 logit, 46 long short-term memory (LSTM) architecture, 213-217 loop-based implementation, 317 loss metric, 86 lower-dimensional representations, 157 (see also embedding and representational learn‐ ing) LSTM (long short-term memory) architecture, 213-217 M machine learning (ML), 41-45 manifolds, 172 Markov assumption, 351 Markov Chain Monte Carlo (MCMC) algo‐ rithms, 265 Markov decision process (MDP) discounted future return, 331 future return, 330 overview of, 328 policy, 329 matrix arrays, 1matrix inverse, 12 matrix operations, 3-6 matrix properties, 7-13 matrix-vector multiplication, 7 max future reward, 347 max norm constraints, 73 max pooling layer, 131 MCMC (Markov Chain Monte Carlo) algo‐ rithms, 265 MDP (see Markov decision process) memory cells, 213 memory constrained learning, 139 memory-augmented neural networks (see also neural networks) differentiable neural computers basics of, 307-309 controller network, 313 implementing in PyTorch, 317-321 interference-free writing in, 309 memory reuse, 310 read heads, 312 teaching to read and comprehend, 321-322 temporal linking of writes, 311 visualizing in action, 314-316 Neural Turing Machines attention-based memory access, 301-303 basics of, 299-301 memory addressing mechanisms, 303-306 mem_ops.py file, 317 minibatch gradient descent, 65 missingness, 293 ML (machine learning), 41-45 MNIST classifiers, 89-92, 134-135 model identifiability, 97 model.parameters() method, 99, 108 model.state_dict() function, 99 models, 41 momentum, 107 momentum value, 139 momentum-based optimization, 106-109 N NCE (noise-contrastive estimation), 180 Nesterov momentum, 109 neural n-gram strategy, 190 neural net learning theory, 74 368 | Index --- Page 385 --- neural networks (see also memory augmented neural networks) artificially intelligent machines, 39 feed-forward neural networks, 48-51, 55-73, 189 linear neurons, 51 linear perceptrons expressed as neurons, 47 machine learning, 41-45 neurons, 45-47 scaling problems, 121 sigmoid, tanh, and ReLU neurons, 51-52 softmax output layers, 54 traditional computer programs, 40 neural style algorithm, 152 neural translation networks, 230-239 Neural Turing Machines (NTMs) attention-based memory access, 301-303 basics of, 299-301 memory addressing mechanisms, 303-306 neurons artificial, 46 biological, 45 in human vision, 117 linear neurons, 51 linear perceptrons expressed as, 47 sigmoid, tanh, and ReLU neurons, 51-52 nn module (PyTorch), 84-87, 134 nn.BCELoss, 262 noise-contrastive estimation (NCE), 180 nonidentifiability, 97 normalization batch normalization, 137-139 dataset normalization, 136 global normalization, 205 group normalization, 139 local normalization, 205 Normalize transform, 136 NTM (see Neural Turing Machines) null space, 10-13 num_features argument, 138 O off-policy setting, 344 on-policy setting, 344 one-hot vector representations, 178 oneplus function, 314 OpenAI Gym, 335 optimization critical challenges to, 104-106definition of term, 44 learning rate adaptation, 111-115 momentum-based, 106-109 philosophy behind optimizer selection, 115 primary challenge in, 96 second-order methods, 109 opt_state_dict, 99 orthogonality, 6 out_channels argument, 131 overfitting preventing, 71-73 problem of, 65-71 P pack() method, 321 padding, 231 padding argument, 131 parameter vectors, 55 parameters() function, 86 Parsey McParseface, 202 part-of-speech (POS) tags implementing taggers, 192-196 producing with neural n-grams, 190 partial dependence plots (PDPs), 282 partition function, 267 pathfinding, 322 PCA (see principal component analysis) PDPs (partial dependence plots), 282 perceptrons definition of term, 43 expressing linear as neurons, 47 permutation feature importance, 281 PGAgent class, 335 pole-cart problem agent creation, 335 history, 337 model and optimizer, 337 OpenAI Gym, 335 PGAgent performance, 340 policy gradient main function, 338 sampling actions, 337 policy, 326 policy gradients, 334 policy learning, 334 policy optimization proximal policy optimization, 345 trust-region policy optimization , 341-345 population risk, 75 posterior, 18 Index | 369 --- Page 386 --- PPO (proximal policy optimization), 345 pre-output vector, 307, 314 precedence vector, 311 prefix codes, 30 pretraining greedy pretraining, 95, 203 image preprocessing pipelines, 136 principal component analysis (PCA), 14, 158-159, 187-188 prior, 18 probability Bayes' Theorem, 27 conditional probability, 20-21 continuous probability distributions, 32-36 entropy, cross entropy, and KL divergence, 29-32 events and probability, 17-20 expectation and variance, 24-27 random variables, 22-23 probability density functions (PDFs), 32 probability distribution, 17 proximal policy optimization (PPO), 345 pruning step, 204 PyTorch 2D convolution, 130 accessing model parameters, 99 autoencoders in, 161-170 batch normalization, 138 converting data to tensors, 137 Datasets and DataLoaders, 87-89 gradients in, 83 group normalization, 141 image whitening, 136 implementing DNCs in, 317-321 installing, 77 MNIST classifier, 89-92 NCE implementation, 181 nn module, 84-87, 134 optimization AdaDelta optimizer, 115 AdaGrad algorithm, 111 Adam optimizer, 115 momentum-based, 108 Nerestov momentum, 109 RMSProp optimizer, 113 support for second-order methods, 110 origins and benefits of, 77 primitives for RNN models, 218 residual block for ResNet34, 149Skip-Gram model, 182-186 tensors basics of, 78 tensor attributes, 79 tensor init, 78 tensor operations, 80-83 torchvision library, 136, 149 Q Q-learning (see also deep Q-networks) approximating Q-function, 348 Bellman equation, 347 Q-functions and Q-values, 347 Q-tables, 348 value iteration, 348 questions and comments, xi R random forest algorithm, 277 random variables, 22-23 random walk, 107 rand_state_dict, 100 read modes, 312 reading and comprehension, 321-322 Rectified Linear Unit (ReLU) neurons, 52 recurrent neural networks (RNNs) for sentiment analysis, 219-224 for seq2seq tasks, 224-227 for sequence analysis, 207-209 regression, 20, 280 regularization, 71 reinforcement learning (RL), 325 (see also deep reinforcement learning) ReLU (Rectified Linear Unit) neuron, 52 reparameterization trick, 251, 256 representation learning (see embedding and representation learning) residual learning superhuman vision, 149-152 for very deep networks, 147-149 ResNet34 architecture, 147 ResNet34 model, 149, 149 retention vector, 310 return, 330 reverse KL divergence, 253 reward, 328 reward prediction, 361 reward signal, 326 RL (reinforcement learning), 325 370 | Index --- Page 387 --- (see also deep reinforcement learning) RMSProp optimizer, 112 rotational invariance, 187 row vector, 6 S saddle points, 102 saliency mapping, 275 sample space, 17 scalar-matrix multiplication, 3 scaled dot product attention, 239 scaling problems, 121 scikit-learn library, 168, 185 score function, 266 score matching, 267, 270-274 score-based generative models, 264-269 Seaquest, 359 second-order methods, 109 self-attention, 239-241 sentiment analysis models, 219-224 seq2seq problems solving with RNNs, 224-227 tackling with neural n-grams, 190 sequence analysis applying to multiple fields, 242 attentional mechanisms, 227-230 beam search and global normalization, 203-205 dependency parsing and SyntaxNet, 197-202 long short-term memory units, 213-217 neural n-grams, 190 neural translation networks, 230-239 part-of-speech taggers, 192-196 PyTorch primitives for RNN models, 218 recurrent neural networks, 207-209 self-attention and transformers, 239-241 sentiment analysis models, 219-224 seq2seq tasks with RNNs, 224-227 stateful deep learning models, 206 vanishing gradients, 210-212 variable-length inputs, 189 SGD (stochastic gradient descent), 64 SHAP (Shapley Additive Explanations), 292-297 shape, 4 Shapley Additive Explanations (SHAP), 292-297 shift weighting, 304sigmoid neurons, 51, 60 singular value decomposition (SVD), 187-188 singular values, 187 singularity, 12 skip connections, 147-149 Skip-Gram model, 182-186 skip-thought vectors, 225 softmax layer, 54 span, 9 spanning list, 9 sparsity, 174-177 spatial extent, 131 spurious local minima, 98-101 square, 4 state, 328 state history, 351 state transition, 328 stateful deep learning models, 206 state_dict method, 99 steepest descent, 109 stochastic gradient descent (SGD), 64 stride argument, 131 supervised learning, 326 SVD (singular value decomposition), 187-188 symbolic loops, 321 SyntaxNet, 202-205 T t-Distributed Stochastic Neighbor Embedding (t-SNE), 145, 185 tanh neurons, 52 target network, 350 TensorBoard, 164 tensors (PyTorch) basics of, 78 tensor attributes, 79 tensor init, 78 tensor operations, 80-83 test sets, 65-71 text_pipeline function, 221 tokenization, 230 tokenizers, 219 torch.load, 99 torch.nn.GroupNorm class, 141 torch.nn.RNNCell objects, 218 torch.optim module (PyTorch), 87 torch.topk operation, 320 torch.utils.tensorboard.SummaryWriter, 163 torchaudio library, 78 Index | 371 --- Page 388 --- Torchtext library, 219, 233 torchtext.datasets.IMDB class, 221 torchvision library image whitening with, 136 installing, 78 ResNet34 model, 149 training (see also error surfaces; feed-forward neural networks) batch normalization, 137-139 definition of term, 55 greedy pretraining, 95, 203 training sets, 68 transformers, 239-241 translation networks, 230-239 tree-based algorithms, 276-280 TRPO (trust-region policy optimization), 341-345 trust region, 344 trust-region policy optimization (TRPO), 341-345 Turing complete, 299 U uniform distribution, 33 union, 19 unnormalized discounted visitation frequency, 343 unsupervised learning, 326 UNsupervised REinforcement and Auxiliary Learning (UNREAL), 360 usage vector, 309 V validation sets, 65-71 value function, 342value iteration, 348 value learning, 334 vanishing gradients, 210-212 variance, 24-27 Variational Autoencoders (V AEs) about, 249-259 implementing, 259-264 vector operations, 6 vector space, 8 vectorization, 50, 178, 318 vectors, 2 VGGNet architecture, 133, 153 vision, 117 (see also convolutional neural net‐ works) visualizations comparing PCA to autoencoders, 169 using autoencoders, 168 using CNNs, 143-146 using t-SNE, 185 using TensorBoard, 164, 164 W weight decay, 72, 106 weighting vector, 302 weights, 55 whitening, 136 word embeddings (see also embedding and representational learning) Skip-Gram model, 182-186 Word2Vec framework for, 179-182 word-level tokenization, 231 Word2Vec framework, 179-182 working memory, 300 write gates, 214 write vector, 302, 308 372 | Index --- Page 389 --- About the Authors Nithin Buduma is one of the first machine learning engineers at XY .ai, a startup based out of Harvard and Stanford working to help healthcare companies leverage their massive datasets.",
    "ed out of Harvard and Stanford working to help healthcare companies leverage their massive datasets. Nikhil Buduma is the cofounder and chief scientist of Remedy, a San Francisco- based company that is building a new system for data-driven primary healthcare. At the age of 16, he managed a drug discovery laboratory at San Jose State Univer‐ sity and developed novel low-cost screening methodologies for resource-constrained communities.",
    "ver‐ sity and developed novel low-cost screening methodologies for resource-constrained communities. By the age of 19, he was a two-time gold medalist at the International Biology Olympiad. He later attended MIT, where he focused on developing large- scale data systems to impact healthcare delivery, mental health, and medical research.",
    "loping large- scale data systems to impact healthcare delivery, mental health, and medical research. At MIT, he cofounded Lean On Me, a national nonprofit organization that provides an anonymous text hotline to enable effective peer support on college campuses, and leverages data to effect positive mental health and wellness outcomes.",
    "port on college campuses, and leverages data to effect positive mental health and wellness outcomes. Today, Nikhil spends his free time investing in hard technology and data companies through his venture fund, Q Venture Partners, and managing a data analytics team for the Milwaukee Brewers baseball team. Joe Papa is the author of the PyTorch Pocket Reference (O’Reilly) and founder of PyTorch Academy and TeachMe.AI.",
    "the author of the PyTorch Pocket Reference (O’Reilly) and founder of PyTorch Academy and TeachMe.AI. He has over 25 years of experience in research and development and currently leads AI projects as Chief AI Engineer at Mobile Insights. He holds an MSEE and has led AI research teams with PyTorch at Booz Allen Hamilton and Perspecta Labs. Joe has mentored hundreds of data scientists and has taught more than 7,000 students across the world on Udemy, Packt, and O’Reilly Learning.",
    "sts and has taught more than 7,000 students across the world on Udemy, Packt, and O’Reilly Learning. Colophon The animal on the cover of Fundamentals of Deep Learning is a North Pacific crestfish (Lophotus capellei ), also known as the unicornfish. It’s part of the Lophotidae family and lives in the deep waters of the Atlantic and Pacific oceans. Because of their seclusion from researchers, little is known about this fish. Some have been caught, however, that are six feet in length.",
    "chers, little is known about this fish. Some have been caught, however, that are six feet in length. Many of the animals on O’Reilly covers are endangered; all of them are important to the world. To learn more about how you can help, go to animals.oreilly.com . The cover image is by Karen Montgomery, based on a black and white engraving from Lydekker’s Royal Natural History . The cover fonts are Gilroy Semibold and Guardian Sans.",
    "aving from Lydekker’s Royal Natural History . The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono. --- Page 390 --- Learn from experts. Become one yourself. Books | Live online courses Instant Answers | Virtual events Videos | Interactive learning Get started at oreilly.com. ©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc.",
    "t oreilly.com. ©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175 --- Page 391 --- Typical CNN Layer Input Convolutional Stage :Affine Transform Detector Stage: Nonline arity Pooling Stage Normalization Stage (Optional) Output: Feature Map --- Page 392 --- A simple CNN structure CONV: Convolutional kernel layer RELU: Activation function POOL: Dimension reduction layer FC: Fully connection layer --- Page 395 --- Example: 200x200 image 40K hidden units ~2B parameters !!!",
    ": Fully connection layer --- Page 395 --- Example: 200x200 image 40K hidden units ~2B parameters !!! -Spatial correlation islocal -Waste of resources + we have notenoughFullyConnected Layer training samples anyway.. --- Page 396 --- LocallyConnected Layer Example :200x200 image 40Khidden units Filter size:10x10 4Mparameters Note: This parameterization is good when input image is registered (e.g., face recognition ).",
    "meters Note: This parameterization is good when input image is registered (e.g., face recognition ). --- Page 397 --- The C onvolution operation --- Page 398 --- Convolutional kernel Padding on the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input --- Page 399 --- Example of'valid' 2-Dconvolution (without kernel flipping) where a 3x4matrix convolved with a2x2 kernel tooutput a2x3matrix --- Page 401 --- Convolutional Layer Share the same parameters across different locations (assuming input is stationary): Convolutions with learned kernels --- Page 402 --- Convolutional Layer --- Page 403 --- Convolutional Layer --- Page 404 --- Convolutional Layer --- Page 405 --- Convolutional Layer --- Page 406 --- Convolutional Layer --- Page 407 --- Convolutional Layer --- Page 408 --- Convolutional Layer --- Page 409 --- Convolutional Layer --- Page 410 --- Convolutional Layer --- Page 411 --- Convolutional Layer --- Page 412 --- Convolutional Layer --- Page 413 --- Convolutional Layer --- Page 414 --- Convolutional Layer --- Page 415 --- Convolutional Layer --- Page 416 --- Convolutional Layer --- Page 417 --- Convolutional Layer --- Page 418 --- Convolutional Layer *-1 01 -1 01 -1 01 = --- Page 419 --- Learn multiple filters.",
    "r --- Page 418 --- Convolutional Layer *-1 01 -1 01 -1 01 = --- Page 419 --- Learn multiple filters. E.g.: 200x200 image 100 Filters Filter size: 10x10 10K parameters Convolutional Layer --- Page 420 --- Reason 1 : Sparse Connectivity --- Page 421 --- •Receptive fields of units in deeper layers larger than shallow layers •T h o u g h direct connections are very sparse, d e e p e r layers indirectly connected to m o s t of t h e input image •Effect increases with strided convolution or pooling.",
    "tly connected to m o s t of t h e input image •Effect increases with strided convolution or pooling. --- Page 422 --- Input n e u r o n s r e p r e s e n t i n g a 2 8 x 2 8 i m a g e ( s u c h asf r o m MNIST d a t a s e t ) 32 --- Page 423 --- Ev e r y h i d d e n l a y e r n e u r o n h a s a l o c a l r e c e p t i v e f i e l d o f r e g i o n 5 x 5 p i x e l s 33 --- Page 424 --- A n d soo n , t h e f i rs t h i d d e n l a y e r isbuilt!",
    "n 5 x 5 p i x e l s 33 --- Page 424 --- A n d soo n , t h e f i rs t h i d d e n l a y e r isbuilt! ( 2 8 -5+1)=24x24n e u r o n s int h e h i d d e n l aye r on'valid' c o n v o l u t i o n Si ze oft h e h i d d e n l aye r c a n bec h a n g e d u s i n g a n o t h e r va r i a n t ofc o n v o l u t i o n 34 --- Page 425 --- Reason 2 : Parameter sharing --- Page 426 --- A closer look at spatial dimensions: 32 332x32x3 image 5x5x3 filter 32activation map 12828 convolve (slide) over all spatial locations --- Page 427 --- 32 3 628activation maps 32 28 Convolution LayerFor example, if we had 6 5x5 filters, we’ll get 6 separate activation maps: We stack these up to get a “new image” of size28x28x6!",
    "ilters, we’ll get 6 separate activation maps: We stack these up to get a “new image” of size28x28x6! --- Page 429 --- 7 7x7 input (spatially) assume 3x3filter 7A closer look at spatial dimensions: --- Page 430 --- 7 7x7 input (spatially) assume 3x3filter 7A closer look at spatial dimensions: --- Page 431 --- 7 7x7 input (spatially) assume 3x3filter 7A closer look at spatial dimensions: --- Page 432 --- 7 7x7 input (spatially) assume 3x3filter 7A closer look at spatial dimensions: --- Page 433 --- => 5x5output7 7x7 input (spatially) assume 3x3filter 7A closer look at spatial dimensions: --- Page 434 --- 7x7 input (spatially) assume 3x3 filter applied with stride 27 7A closer look at spatial dimensions: --- Page 435 --- 7x7 input (spatially) assume 3x3 filter applied with stride 27 7A closer look at spatial dimensions: --- Page 436 --- 7x7 input (spatially) assume 3x3 filter applied with stride 2 => 3x3output!7 7A closer look at spatial dimensions: --- Page 437 --- 7x7 input (spatially) assume 3x3 filter applied with stride 3?7 7A closer look at spatial dimensions: --- Page 438 --- 7x7 input (spatially) assume 3x3 filter applied with stride 3?7 7A closer look at spatial dimensions: doesn’t fit!",
    "tially) assume 3x3 filter applied with stride 3?7 7A closer look at spatial dimensions: doesn’t fit! cannot apply 3x3 filter on 7x7 input with stride 3. --- Page 439 --- NF FN Output size: (N -F) / stride +1 e.g. N = 7, F =3: stride 1 => (7 -3)/1 + 1 = 5 stride 2 => (7 -3)/2 + 1 = 3 stride 3 => (7 -3)/3 + 1 = 2.33 :\\ --- Page 441 --- 000000 0 0 0 0e.g. input 7x7 3x3 filter, applied with stride 1 pad with 1 pixel border => what is the output?",
    "0 0 0e.g. input 7x7 3x3 filter, applied with stride 1 pad with 1 pixel border => what is the output? (recall:) (N -F) / stride +1Zero -Padding: common to the border --- Page 442 --- e.g. input 7x7 3x3 filter, applied with stride 1 pad with 1 pixel border => what is the output? 7x7output!000000 0 0 0 0Zero -Padding: common to the border --- Page 443 --- e.g. input 7x7 3x3 filter, applied with stride 1 pad with 1 pixel border => what is the output? 7x7output!",
    "nput 7x7 3x3 filter, applied with stride 1 pad with 1 pixel border => what is the output? 7x7output! in general, common to see CONV layers with stride 1, filters of size FxF , and zero-padding with (F-1)/2. (will preserve sizespatially) e.g.F=3=>zero padwith1 F=5=>zero padwith2 F=7=>zero padwith3000000 0 0 0 0Zero -Padding: common to the border --- Page 444 --- Examples time: Input volume: 32x32x3 10 5x5 filters with stride 1, pad 2 Output volume size: ?",
    "4 --- Examples time: Input volume: 32x32x3 10 5x5 filters with stride 1, pad 2 Output volume size: ? --- Page 445 --- Examples time: Input volume: 32x32 x3 10 5x5 filters with stride 1, pad2 Output volume size: (32+2*2-5)/1+1 = 32 spatially, so 32x32x 10 --- Page 446 --- Examples time: Input volume: 32x32x3 10 5x5 filters with stride 1, pad 2 Number of parameters in this layer?",
    "time: Input volume: 32x32x3 10 5x5 filters with stride 1, pad 2 Number of parameters in this layer? --- Page 447 --- Examples time: Input volume: 32x32 x3 10 5x5 filters with stride 1, pad2 (+1 for bias)Number of parameters in this layer? each filter has 5*5*3 + 1 = 76params => 76*10 =760 --- Page 450 --- Common settings: K = (powers of 2, e.g. 32, 64, 128, 512) -F = 3, S = 1, P =1 -F = 5, S = 1, P =2 -F = 5, S = 2, P = ?",
    "= (powers of 2, e.g. 32, 64, 128, 512) -F = 3, S = 1, P =1 -F = 5, S = 1, P =2 -F = 5, S = 2, P = ? (whatever fits) -F = 1, S = 1, P =0 --- Page 452 --- Let us assume filter is an “eye” detector. Q.: how can we make the detection robust to the exact location of the eye?Pooling Layer --- Page 453 --- By “pooling ” (e.g., taking max) filter responses at different locations we gain robustness to the exact spatial location of features.",
    "ilter responses at different locations we gain robustness to the exact spatial location of features. Pooling Layer --- Page 454 --- Pooling layer --- Page 456 --- Eﬀect = invariance to small translations of theinput Pooling --- Page 458 --- -makes the representations smaller and more manageable -operates over each activation map independently --- Page 459 --- 11 2 4 56 7 8 32 1 0 12 3 4Single depth slice x ymax pool with 2x2filters and stride 268 34 Max Pooling --- Page 461 --- PoolingLayer:Recep tive FieldSize Conv.",
    "th 2x2filters and stride 268 34 Max Pooling --- Page 461 --- PoolingLayer:Recep tive FieldSize Conv. layerhn−1 hnhn 1 Pool. layer If convolutional filters have size KxK and stride 1, and pooling layer has pools of size PxP , then each unit in the pooling layer depends upon a patch (at the input of the preceding conv. layer) of size: (P+K -1)x(P+K -1) --- Page 462 --- PoolingLayer:Recep tive FieldSize Conv. layerhn−1 hnhn 1 Pool.",
    "e: (P+K -1)x(P+K -1) --- Page 462 --- PoolingLayer:Recep tive FieldSize Conv. layerhn−1 hnhn 1 Pool. layer If convolutional filters have size KxK and stride 1, and pooling layer has pools of size PxP , then each unit in the pooling layer depends upon a patch (at the input of the preceding conv. layer) of size: (P+K -1)x(P+K -1) --- Page 463 --- 7 8ConvNets: Typical Stage One stage (zoom) Convol. Pooling --- Page 464 --- ConvNets: Typical Stage One stage (zoom) Convol.",
    "e One stage (zoom) Convol. Pooling --- Page 464 --- ConvNets: Typical Stage One stage (zoom) Convol. Pooling Conceptually similar to: SIFT, HoG, etc. --- Page 465 --- Fully Conn. Layers 1ststage 2ndstage 3rdstageInput ImageClass Labels ConvNets: Typical Architecture One stage (zoom) Convol. Pooling Whole system --- Page 466 --- Conceptually similar to: SIFT → K-Means → Pyramid Pooling →SVM Lazebnik et al. “...Spatial Pyramid Matching...” CVPR 2006 SIFT → Fisher Vect. → Pooling →SVM Sanchez et al.",
    "et al. “...Spatial Pyramid Matching...” CVPR 2006 SIFT → Fisher Vect. → Pooling →SVM Sanchez et al. “Image classifcation with F.V.: Theory and practice” IJCV 2012 Fully Conn. LayersWhole system 1ststage 2ndstage 3rdstageInput ImageClass LabelsConvNets: Typical Architecture --- Page 468 --- 85 NxMxM, M small H hidden units / Hx1x1 feature maps Fully conn. layer / Conv. layer (H kernels of sizeNxMxM) --- Page 469 --- H hidden units / Hx1x1 feature maps NxMxM, M small Fully conn. layer / Conv.",
    "NxMxM) --- Page 469 --- H hidden units / Hx1x1 feature maps NxMxM, M small Fully conn. layer / Conv. layer (H kernels of sizeNxMxM) Fully conn. layer / Conv. layer (K kernels of size Hx186x1)K hidden units / Kx1x1 feature maps --- Page 470 --- 87Viewing fully connected layers asconvolutional layers enables efficient use ofconvnets onbigger images (no need toslide windows but unroll network over space asneeded tore-usecomputation) .",
    "bigger images (no need toslide windows but unroll network over space asneeded tore-usecomputation) . CNNInput Image CNNInput ImageInput Image TRAINING TIME TEST TIME xy --- Page 471 --- ConvNets: Test At test time, run only is forward mode (FPROP).",
    "NG TIME TEST TIME xy --- Page 471 --- ConvNets: Test At test time, run only is forward mode (FPROP). --- Page 472 --- CONV NETS: EXAMPLES -OCR/House number &Trafficsign classification --- Page 473 --- CONV NETS: EXAMPLES -Texture classification --- Page 474 --- CONV NETS: EXAMPLES -Pedestrian detection --- Page 475 --- CONV NETS: EXAMPLES -Scene Parsing --- Page 476 --- CONV NETS: EXAMPLES -Segmentation 3D volumetric images --- Page 477 --- 102CONV NETS: EXAMPLES -Action recognition from videos --- Page 478 --- CONV NETS: EXAMPLES -Object detection --- Page 479 --- Architecture for Classification input imagelabel Conv.",
    ": EXAMPLES -Object detection --- Page 479 --- Architecture for Classification input imagelabel Conv. layer: 3x3filters Max pooling layer: 2x2, stride 2 Fully connected layer: 4096 hiddens 24 Layers intotal!!! 10964 128 256 512 512 --- Page 480 --- Architecture for Classification input imagelabel 0.1G 20G} FLOPS: 20G 90TOTAL --- Page 481 --- Architecture for Classification input imagelabel 123M 21M} Nr.",
    "} FLOPS: 20G 90TOTAL --- Page 481 --- Architecture for Classification input imagelabel 123M 21M} Nr. ofparameters: 144M 91TOTAL --- Page 482 --- Architecture for Classification input imagelabel 123M 21M} Nr.",
    "meters: 144M 91TOTAL --- Page 482 --- Architecture for Classification input imagelabel 123M 21M} Nr. ofparameters: 144M 92TOTAL Data augmentation is key to improve generalization: -random translation -left/right flipping -scaling --- Page 483 --- Optimization SGD with momentum : Learning rate = 0.01 Momentum =0.9 Improving generalization by: Weight sharing (convolution) Input distortions Dropout =0.5 Weight decay =0.0005 --- Page 484 --- ChoosingThe Architectu re Task dependent Cross -validation [Convolution → pooling]* + fully connected layer The more data: the more layers and the more kernels Look at the number of parameters at each layer Look at the number of flops at each layer Computational resources Be creative :) --- Page 485 --- How ToOptimize SGD (with momentum) usually works very well Pick learning rate by running on a subset of thedata Bottou “Stochastic Gradient Tricks” Neural Networks 2012 Start with large learning rate and divide by 2 until loss does not diverge Decay learning rate by a factor of ~1000 or more by the end oftraining Use non-linearity Initialize parameters so that each feature across layers has similar variance.",
    "ing Use non-linearity Initialize parameters so that each feature across layers has similar variance. Avoid units in saturation. --- Page 486 --- Improving Generalization Weight sharing (greatly reduce the number ofparameters) Data augmentation (e.g., jittering, noise injection, etc.) Dropout Hinton et al.",
    "umber ofparameters) Data augmentation (e.g., jittering, noise injection, etc.) Dropout Hinton et al. “Improving Nns by preventing co -adaptation of feature detectors” arxiv 2012 Weight decay (L2, L1) Sparsity in the hidden units Multi -task (unsupervised learning) --- Page 487 --- Good ToKnow Check gradients numerically by finite differences Visualize features (feature maps need to be uncorrelated) and have high variance.",
    "finite differences Visualize features (feature maps need to be uncorrelated) and have high variance. samples hidden unit Good training: hidden units are sparse across samples and across features. --- Page 488 --- Check gradients numerically by finite differences Visualize features (feature maps need to be uncorrelated) and have high variance. samples hidden unit Bad training: many hidden units ignore the input and/or exhibit strong correlations.",
    "les hidden unit Bad training: many hidden units ignore the input and/or exhibit strong correlations. Good ToKnow --- Page 489 --- Check gradients numerically by finite differences Visualize features (feature maps need to be uncorrelated) and have high variance.",
    "finite differences Visualize features (feature maps need to be uncorrelated) and have high variance. Visualize parameters toonoisy toocorrelated lack structure Good training: learned filters exhibit structure and areuncorrelated.GOOD BAD BAD BAD Good ToKnow --- Page 490 --- Check gradients numerically by finite differences Visualize features (feature maps need to be uncorrelated) and have high variance. Visualize parameters Measure error on both training and validation set.",
    "ted) and have high variance. Visualize parameters Measure error on both training and validation set. Test on a small subset of the data and check the error →0. Good ToKnow --- Page 491 --- WhatIfItDoesNotWork? Training diverges: Learning rate may be too large → decrease learning rate BPROP is buggy → numerical gradient checking Parameters collapse / loss is minimized but accuracy is low Check loss function: Is it appropriate for the task you want tosolve? Does it have degenerate solutions?",
    "k loss function: Is it appropriate for the task you want tosolve? Does it have degenerate solutions? Check “pull -up” term. Network is underperforming Compute flops and nr.params. → if too small, make net larger Visualize hidden units/params → fixoptmization Network is tooslow Compute flops and nr. params. → GPU,distrib.",
    "hidden units/params → fixoptmization Network is tooslow Compute flops and nr. params. → GPU,distrib. framework, make net smaller --- Page 494 --- •By applying convolution and pooling, important features will be obtained (extracted) and it can reduce the complexity of dimension also the computational speed. •These will be applied to ANN for classification. --- Page 497 --- •Recurrent neural networks (RNNs) are tailored to the processing of sequential data.",
    "-- Page 497 --- •Recurrent neural networks (RNNs) are tailored to the processing of sequential data. •An RNN processes asequence ofdata by processing each element inthesequence one at time . •AnRNN network only hasasingle hidden layer, butit also has amemory buffer that stores the output of this hidden layer forone input and feeds itback into the hidden layer along with the next input from the sequence .",
    "er forone input and feeds itback into the hidden layer along with the next input from the sequence . --- Page 498 --- •This recurrent flow ofinformation means that the network processes each input within the context generated by processing the previous input, which inturn was processed inthecontext oftheinput preceding it.",
    "d by processing the previous input, which inturn was processed inthecontext oftheinput preceding it. •Inthis way, theinformation that flows through therecurrent loop encodes contextual information from (potentially) allof the preceding inputs inthe sequence .This allows the network tomaintain amemory ofwhat ithas seen previously inthesequence tohelp itdecide what todowith thecurrent input .",
    "amemory ofwhat ithas seen previously inthesequence tohelp itdecide what todowith thecurrent input . --- Page 499 --- •The depth of an RNN arises from thefact that thememory vector ispropagated forward and evolved through each input inthe sequence ;asaresult anRNN network is considered asdeep asasequence islong .",
    "through each input inthe sequence ;asaresult anRNN network is considered asdeep asasequence islong . •Figure 5.2illustrates the architecture ofanRNN and shows how information flows through thenetwork asitprocesses a sequence .Ateach time step, the network inthis figure receives avector containing two elements asinput .The schematic ontheleftoffigure 5.2(time step= 1.0)shows the flow ofinformation inthenetwork when itreceives thefirst input inthesequence .",
    "e step= 1.0)shows the flow ofinformation inthenetwork when itreceives thefirst input inthesequence . --- Page 500 --- •This input vector isfedforward into thethree neurons inthe hidden layer ofthenetwork .Atthesame time these neurons also receive whatever information isstored inthe memory buffer .Because this istheinitial input, thememory buffer will only contain default initialization values .Each oftheneurons inthe hidden layer will process the input and generate an activation .",
    "ion values .Each oftheneurons inthe hidden layer will process the input and generate an activation . •The schematic inthe middle offigure 5.2(time step= 1.5) shows how this activation flows onthrough thenetwork :the activation ofeach neuron ispassed totheoutput layer where itisprocessed togenerate theoutput ofthenetwork, and itis also stored inthe memory buffer (overwriting whatever information was stored there) .",
    "work, and itis also stored inthe memory buffer (overwriting whatever information was stored there) . --- Page 501 --- •The elements ofthe memory buffer simply store the information written tothem ;they donottransform itinany way.Asaresult, there are noweights onthe edges going from the hidden units tothe buffer .There are, however, weights onallthe other edges inthe network, including those from the memory buffer units tothe neurons inthe hidden layer .Attime step 2,the network receives the next input from the sequence, and this ispassed tothe hidden layer neurons along with the information stored inthe buffer .This time the buffer contains the activations that were generated bythe hidden neurons inresponse tothe first input .",
    "fer contains the activations that were generated bythe hidden neurons inresponse tothe first input . --- Page 505 --- •The hidden layer generates avector ofactivations that is passed totheoutput layer and isalso propagated forward to the next time step along the horizontal arrows connecting thehidden states .",
    "o propagated forward to the next time step along the horizontal arrows connecting thehidden states . •Although RNNs can process asequence ofinputs, they struggle with the problem ofvanishing gradients .This is because training anRNN toprocess asequence ofinputs requires theerror tobebackpropagated through theentire length of the sequence .",
    "s asequence ofinputs requires theerror tobebackpropagated through theentire length of the sequence . --- Page 506 --- •backpropagating theerror through allthehidden layers, which in turn involves repeatedly multiplying theerror bytheweights on the connections feeding activations from one hidden layer forward tothenext hidden layer .Aparticular problem with this process isthat itisthesame setofweights that areused onall the connections between the hidden layers :each horizontal arrow represents the same set ofconnections between the memory buffer and the hidden layer, and the weights onthese connections arestationary through time (i.e.,theydon’t change from one time step tothenext during theprocessing ofagiven sequence ofinputs) .",
    ".e.,theydon’t change from one time step tothenext during theprocessing ofagiven sequence ofinputs) . --- Page 507 --- •Consequently, backpropogating anerror through ktime steps involves (among other multiplications) multiplying the error gradient bythesame setofweights ktimes .",
    "nvolves (among other multiplications) multiplying the error gradient bythesame setofweights ktimes . •This isequivalent tomultiplying each error gradient bya weight raised tothepower ofk.Ifthis weight isless than 1, then when itisraised toapower, itdiminishes atan exponential rate, and consequently, the error gradient also tends todiminish atanexponential rate with respect tothe length ofthesequence —and vanish .",
    "ent also tends todiminish atanexponential rate with respect tothe length ofthesequence —and vanish . --- Page 515 --- •Long Short -Term Memory Networks (LSTM) isadeep learning, sequential neural network that allows information topersist . •Itisarecurrent neural network (RNN) architecture widely used inDeep Learning .Itexcels atcapturing long-term dependencies, making itideal forsequence prediction tasks .",
    "Learning .Itexcels atcapturing long-term dependencies, making itideal forsequence prediction tasks . •Itisaspecial type ofRecurrent Neural Network which iscapable ofhandling thevanishing gradient problem faced byRNN . •LSTM was designed byHochreiter and Schmidhuber that resolves the problem caused bytraditional rnns and machine learning algorithms . •Ex:while watching avideo weremember theprevious scene ….",
    "nal rnns and machine learning algorithms . •Ex:while watching avideo weremember theprevious scene …. •While reading books weremember theprevious chapter … --- Page 516 --- •RNNs work likethis:they remember theprevious information and useitforprocessing thecurrent input . •The shortcoming ofRNN isthey cannot remember long-term dependencies duetovanishing gradient . •LSTMs are explicitly designed toavoid long-term dependency problems .",
    "ies duetovanishing gradient . •LSTMs are explicitly designed toavoid long-term dependency problems . •Unlike traditional neural networks, LSTM incorporates feedback connections, allowing ittoprocess entire sequences ofdata, not justindividual data points . •This makes ithighly effective inunderstanding and predicting patterns insequential data liketime series, text, andspeech . --- Page 517 --- •The LSTM network architecture consists ofthree parts, andeach part performs anindividual function .",
    "•The LSTM network architecture consists ofthree parts, andeach part performs anindividual function . •These three parts ofanLSTM unit areknown asgates .They control theflow ofinformation inandoutofthememory cellor lstm cell.The first gate iscalled Forget gate, thesecond gate is known astheInput gate, andthelastoneistheOutput gate . --- Page 518 --- •The first part chooses whether theinformation coming from the previous timestamp istoberemembered orisirrelevant andcan beforgotten .",
    "einformation coming from the previous timestamp istoberemembered orisirrelevant andcan beforgotten . •Inthesecond part, thecelltries tolearn new information from theinput tothiscell. •Inthethird part, thecellpasses theupdated information from thecurrent timestamp tothenext timestamp .This one cycle of LSTM isconsidered asingle -time step.",
    "om thecurrent timestamp tothenext timestamp .This one cycle of LSTM isconsidered asingle -time step. •AnLSTM unit that consists ofthese three gates and amemory cell orlstm cell can beconsidered asalayer ofneurons in traditional feedforward neural network, with each neuron having ahidden layer andacurrent state .",
    "in traditional feedforward neural network, with each neuron having ahidden layer andacurrent state . --- Page 519 --- •AsRNN, anLSTM also hasahidden state where H(t-1)represents thehidden state oftheprevious timestamp and Htisthehidden state ofthecurrent timestamp .Inaddition tothat, LSTM also hasa cellstate represented byC(t-1)andC(t)fortheprevious andcurrent timestamps, respectively .",
    "also hasa cellstate represented byC(t-1)andC(t)fortheprevious andcurrent timestamps, respectively . •Here thehidden state isknown asShort term memory, and thecell state isknown asLong term memory .Refer tothefollowing image . --- Page 521 --- •Anexample tounderstand how LSTM works .Forexample wehave two sentences separated byafullstop . •The first sentence is “Bob isanice person,” . andthesecond sentence is “Dan, ontheOther hand, isevil”.",
    "e first sentence is “Bob isanice person,” . andthesecond sentence is “Dan, ontheOther hand, isevil”. Itisvery clear, inthefirst sentence, wearetalking about Bob, andas soon asweencounter thefullstop( .),westarted talking about Dan. •Aswemove from thefirst sentence tothesecond sentence, our network should realize that wearenomore talking about Bob.Now oursubject isDan. •Here, theForget gate ofthenetwork allows ittoforget about it.",
    "lking about Bob.Now oursubject isDan. •Here, theForget gate ofthenetwork allows ittoforget about it. --- Page 522 --- Case I “Bob isanice person,” .“Dan, ontheOther hand, isevil”. Case II “Bob knows swimming .Hetold meover thephone that hehad served thenavy forfour long years .” Case III Bob single -handedly fought theenemy and died forhiscountry . Forhiscontributions, hewas given abrave______ .” Forhiscontributions, hewas known asabrave ______ .",
    "Forhiscontributions, hewas given abrave______ .” Forhiscontributions, hewas known asabrave ______ . --- Page 523 --- •Forget Gate •InacelloftheLSTM neural network, thefirst step istodecide whether weshould keep theinformation from theprevious time step orforget it.Here istheequation forforget gate. •Xt:input tothecurrent timestamp .",
    "previous time step orforget it.Here istheequation forforget gate. •Xt:input tothecurrent timestamp . •Uf:weight associated with theinput •Ht-1:The hidden state oftheprevious timestamp •Wf:Itistheweight matrix associated with thehidden state --- Page 524 --- •Later, asigmoid function isapplied toit.That will make fta number between 0and 1.This ftislater multiplied with thecell state oftheprevious timestamp, asshown below .",
    "between 0and 1.This ftislater multiplied with thecell state oftheprevious timestamp, asshown below . --- Page 525 --- •Input Gate •The input gate is used to quantify the importance of the new information carried by the input. Here is the equation of the input gate.",
    "the importance of the new information carried by the input. Here is the equation of the input gate. •Xt:Input atthecurrent timestamp t •Ui:weight matrix ofinput •Ht-1:Ahidden state attheprevious timestamp •Wi:Weight matrix ofinput associated with hidden state Again wehave applied thesigmoid function over it.Asaresult, thevalue ofIattimestamp twill bebetween 0and 1. --- Page 526 --- •Let’s take another example .",
    "sult, thevalue ofIattimestamp twill bebetween 0and 1. --- Page 526 --- •Let’s take another example . •“Bob knows swimming .Hetold meover the phone that hehad served thenavy forfour long years .” •So,inboth these sentences, wearetalking about Bob.However, both give different kinds ofinformation about Bob.Inthefirst sentence, we get the information that heknows swimming .Whereas the second sentence tells, heuses thephone and served inthenavy for four years .",
    "s swimming .Whereas the second sentence tells, heuses thephone and served inthenavy for four years . •Now just think about it,based onthe context given inthe first sentence, which information inthesecond sentence iscritical?",
    "t,based onthe context given inthe first sentence, which information inthesecond sentence iscritical? First, heused thephone totell,orheserved inthenavy .Inthiscontext, it doesn’t matter whether heused thephone orany other medium of communication topass ontheinformation .The fact that hewas in thenavy isimportant information, andthisissomething wewant our model toremember forfuture computation .This isthetask ofthe Input gate.",
    "dthisissomething wewant our model toremember forfuture computation .This isthetask ofthe Input gate. --- Page 527 --- •New Information •Thenew information that needed tobepassed tothecellstate isafunction ofahidden state attheprevious timestamp t-1and input xattimestamp t.",
    "ssed tothecellstate isafunction ofahidden state attheprevious timestamp t-1and input xattimestamp t. •The activation function here istanh .Due tothetanh function, thevalue ofnew information willbebetween -1and 1.Ifthe value ofNtisnegative, theinformation issubtracted from the cellstate, andifthevalue ispositive, theinformation isadded to thecellstate atthecurrent timestamp . --- Page 528 --- •However, the Ntwon’t be added directly to the cell state.",
    "atthecurrent timestamp . --- Page 528 --- •However, the Ntwon’t be added directly to the cell state. Here comes the updated equation: Here, Ct -1 is the cell state at the current timestamp, and the others are the values we have calculated previously. --- Page 529 --- •Output Gate •Its value will also lie between 0 and 1 because of this sigmoid function. Now to calculate the current hidden state, we will use Ot and tanh of the updated cell state. As shown below.",
    "lculate the current hidden state, we will use Ot and tanh of the updated cell state. As shown below. It turns out that the hidden state is a function of Long term memory (Ct) and the current output. --- Page 530 --- •Here the token with the maximum score in the output is the prediction. If you need to take the output of the current timestamp, just apply the SoftMax activation on hidden state Ht. --- Page 531 --- •“Bob single -handedly fought theenemy anddied forhiscountry .",
    "on hidden state Ht. --- Page 531 --- •“Bob single -handedly fought theenemy anddied forhiscountry . Forhiscontributions, brave______ .” •During this task, wehave tocomplete the second sentence . Now, theminute weseetheword brave, weknow that weare talking about aperson . •Inthesentence, only Bob isbrave, wecannotsaytheenemy is brave, orthe country isbrave .Sobased onthe current expectation, wehave togive arelevant word tofillintheblank .",
    "country isbrave .Sobased onthe current expectation, wehave togive arelevant word tofillintheblank . •That word isouroutput, and this isthefunction ofourOutput gate."
  ],
  "metadata": [
    {
      "source": "Introduction_to_Deep_Feedforward_Networks",
      "chunk_id": 0,
      "total_chunks": 8
    },
    {
      "source": "Introduction_to_Deep_Feedforward_Networks",
      "chunk_id": 1,
      "total_chunks": 8
    },
    {
      "source": "Introduction_to_Deep_Feedforward_Networks",
      "chunk_id": 2,
      "total_chunks": 8
    },
    {
      "source": "Introduction_to_Deep_Feedforward_Networks",
      "chunk_id": 3,
      "total_chunks": 8
    },
    {
      "source": "Introduction_to_Deep_Feedforward_Networks",
      "chunk_id": 4,
      "total_chunks": 8
    },
    {
      "source": "Introduction_to_Deep_Feedforward_Networks",
      "chunk_id": 5,
      "total_chunks": 8
    },
    {
      "source": "Introduction_to_Deep_Feedforward_Networks",
      "chunk_id": 6,
      "total_chunks": 8
    },
    {
      "source": "Introduction_to_Deep_Feedforward_Networks",
      "chunk_id": 7,
      "total_chunks": 8
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 0,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 1,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 2,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 3,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 4,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 5,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 6,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 7,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 8,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 9,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 10,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 11,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 12,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 13,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 14,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 15,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 16,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 17,
      "total_chunks": 19
    },
    {
      "source": "Introduction_to_Neural_Networks",
      "chunk_id": 18,
      "total_chunks": 19
    },
    {
      "source": "Introduction to Convolutional Neural Networks (CNNs)",
      "chunk_id": 0,
      "total_chunks": 8
    },
    {
      "source": "Introduction to Convolutional Neural Networks (CNNs)",
      "chunk_id": 1,
      "total_chunks": 8
    },
    {
      "source": "Introduction to Convolutional Neural Networks (CNNs)",
      "chunk_id": 2,
      "total_chunks": 8
    },
    {
      "source": "Introduction to Convolutional Neural Networks (CNNs)",
      "chunk_id": 3,
      "total_chunks": 8
    },
    {
      "source": "Introduction to Convolutional Neural Networks (CNNs)",
      "chunk_id": 4,
      "total_chunks": 8
    },
    {
      "source": "Introduction to Convolutional Neural Networks (CNNs)",
      "chunk_id": 5,
      "total_chunks": 8
    },
    {
      "source": "Introduction to Convolutional Neural Networks (CNNs)",
      "chunk_id": 6,
      "total_chunks": 8
    },
    {
      "source": "Introduction to Convolutional Neural Networks (CNNs)",
      "chunk_id": 7,
      "total_chunks": 8
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 0,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 3,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 4,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 5,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 6,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 7,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 8,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 9,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 10,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 11,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 12,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 13,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 14,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 15,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 16,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 17,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 18,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 19,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 20,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 21,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 22,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 23,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 24,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 25,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 26,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 27,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 28,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 29,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 30,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 31,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 32,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 33,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 34,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 35,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 36,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 37,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 38,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 39,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 40,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 41,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 42,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 43,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 44,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 45,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 46,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 47,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 48,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 49,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 50,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 51,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 52,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 53,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 54,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 55,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 56,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 57,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 58,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 59,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 60,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 61,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 62,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 63,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 64,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 65,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 66,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 67,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 68,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 69,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 70,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 71,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 72,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 73,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 74,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 75,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 76,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 77,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 78,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 79,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 80,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 81,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 82,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 83,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 84,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 85,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 86,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 87,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 88,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 89,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 90,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 91,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 92,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 93,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 94,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 95,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 96,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 97,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 98,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 99,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 100,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 101,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 102,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 103,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 104,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 105,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 106,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 107,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 108,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 109,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 110,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 111,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 112,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 113,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 114,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 115,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 116,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 117,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 118,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 119,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 120,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 121,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 122,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 123,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 124,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 125,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 126,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 127,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 128,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 129,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 130,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 131,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 132,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 133,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 134,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 135,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 136,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 137,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 138,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 139,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 140,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 141,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 142,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 143,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 144,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 145,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 146,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 147,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 148,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 149,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 150,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 151,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 152,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 153,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 154,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 155,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 156,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 157,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 158,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 159,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 160,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 161,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 162,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 163,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 164,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 165,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 166,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 167,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 168,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 169,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 170,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 171,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 172,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 173,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 174,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 175,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 176,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 177,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 178,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 179,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 180,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 181,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 182,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 183,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 184,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 185,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 186,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 187,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 188,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 189,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 190,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 191,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 192,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 193,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 194,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 195,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 196,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 197,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 198,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 199,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 200,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 201,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 202,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 203,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 204,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 205,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 206,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 207,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 208,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 209,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 210,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 211,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 212,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 213,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 214,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 215,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 216,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 217,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 218,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 219,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 220,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 221,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 222,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 223,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 224,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 225,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 226,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 227,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 228,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 229,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 230,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 231,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 232,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 233,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 234,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 235,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 236,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 237,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 238,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 239,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 240,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 241,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 242,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 243,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 244,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 245,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 246,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 247,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 248,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 249,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 250,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 251,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 252,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 253,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 254,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 255,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 256,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 257,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 258,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 259,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 260,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 261,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 262,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 263,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 264,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 265,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 266,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 267,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 268,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 269,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 270,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 271,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 272,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 273,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 274,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 275,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 276,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 277,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 278,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 279,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 280,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 281,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 282,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 283,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 284,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 285,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 286,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 287,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 288,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 289,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 290,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 291,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 292,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 293,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 294,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 295,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 296,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 297,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 298,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 299,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 300,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 301,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 302,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 303,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 304,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 305,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 306,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 307,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 308,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 309,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 310,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 311,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 312,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 313,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 314,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 315,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 316,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 317,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 318,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 319,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 320,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 321,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 322,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 323,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 324,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 325,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 326,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 327,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 328,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 329,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 330,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 331,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 332,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 333,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 334,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 335,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 336,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 337,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 338,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 339,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 340,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 341,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 342,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 343,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 344,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 345,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 346,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 347,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 348,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 349,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 350,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 351,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 352,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 353,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 354,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 355,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 356,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 357,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 358,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 359,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 360,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 361,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 362,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 363,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 364,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 365,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 366,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 367,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 368,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 369,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 370,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 371,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 372,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 373,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 374,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 375,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 376,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 377,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 378,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 379,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 380,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 381,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 382,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 383,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 384,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 385,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 386,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 387,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 388,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 389,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 390,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 391,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 392,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 393,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 394,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 395,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 396,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 397,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 398,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 399,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 400,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 401,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 402,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 403,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 404,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 405,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 406,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 407,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 408,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 409,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 410,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 411,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 412,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 413,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 414,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 415,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 416,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 417,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 418,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 419,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 420,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 421,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 422,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 423,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 424,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 425,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 426,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 427,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 428,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 429,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 430,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 431,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 432,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 433,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 434,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 435,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 436,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 437,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 438,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 439,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 440,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 441,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 442,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 443,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 444,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 445,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 446,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 447,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 448,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 449,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 450,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 451,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 452,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 453,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 454,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 455,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 456,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 457,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 458,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 459,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 460,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 461,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 462,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 463,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 464,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 465,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 466,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 467,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 468,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 469,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 470,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 471,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 472,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 473,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 474,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 475,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 476,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 477,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 478,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 479,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 480,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 481,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 482,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 483,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 484,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 485,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 486,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 487,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 488,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 489,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 490,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 491,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 492,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 493,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 494,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 495,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 496,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 497,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 498,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 499,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 500,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 501,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 502,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 503,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 504,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 505,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 506,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 507,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 508,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 509,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 510,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 511,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 512,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 513,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 514,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 515,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 516,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 517,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 518,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 519,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 520,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 521,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 522,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 523,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 524,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 525,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 526,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 527,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 528,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 529,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 530,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 531,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 532,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 533,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 534,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 535,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 536,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 537,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 538,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 539,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 540,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 541,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 542,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 543,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 544,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 545,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 546,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 547,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 548,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 549,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 550,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 551,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 552,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 553,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 554,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 555,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 556,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 557,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 558,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 559,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 560,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 561,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 562,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 563,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 564,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 565,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 566,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 567,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 568,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 569,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 570,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 571,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 572,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 573,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 574,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 575,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 576,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 577,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 578,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 579,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 580,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 581,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 582,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 583,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 584,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 585,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 586,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 587,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 588,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 589,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 590,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 591,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 592,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 593,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 594,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 595,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 596,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 597,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 598,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 599,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 600,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 601,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 602,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 603,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 604,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 605,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 606,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 607,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 608,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 609,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 610,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 611,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 612,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 613,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 614,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 615,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 616,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 617,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 618,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 619,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 620,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 621,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 622,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 623,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 624,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 625,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 626,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 627,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 628,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 629,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 630,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 631,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 632,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 633,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 634,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 635,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 636,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 637,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 638,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 639,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 640,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 641,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 642,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 643,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 644,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 645,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 646,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 647,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 648,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 649,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 650,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 651,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 652,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 653,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 654,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 655,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 656,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 657,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 658,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 659,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 660,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 661,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 662,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 663,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 664,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 665,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 666,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 667,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 668,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 669,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 670,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 671,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 672,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 673,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 674,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 675,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 676,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 677,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 678,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 679,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 680,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 681,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 682,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 683,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 684,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 685,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 686,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 687,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 688,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 689,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 690,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 691,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 692,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 693,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 694,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 695,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 696,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 697,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 698,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 699,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 700,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 701,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 702,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 703,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 704,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 705,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 706,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 707,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 708,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 709,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 710,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 711,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 712,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 713,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 714,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 715,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 716,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 717,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 718,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 719,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 720,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 721,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 722,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 723,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 724,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 725,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 726,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 727,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 728,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 729,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 730,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 731,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 732,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 733,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 734,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 735,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 736,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 737,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 738,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 739,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 740,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 741,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 742,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 743,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 744,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 745,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 746,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 747,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 748,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 749,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 750,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 751,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 752,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 753,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 754,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 755,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 756,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 757,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 758,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 759,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 760,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 761,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 762,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 763,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 764,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 765,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 766,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 767,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 768,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 769,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 770,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 771,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 772,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 773,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 774,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 775,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 776,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 777,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 778,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 779,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 780,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 781,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 782,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 783,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 784,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 785,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 786,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 787,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 788,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 789,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 790,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 791,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 792,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 793,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 794,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 795,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 796,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 797,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 798,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 799,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 800,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 801,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 802,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 803,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 804,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 805,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 806,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 807,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 808,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 809,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 810,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 811,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 812,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 813,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 814,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 815,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 816,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 817,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 818,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 819,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 820,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 821,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 822,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 823,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 824,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 825,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 826,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 827,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 828,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 829,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 830,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 831,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 832,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 833,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 834,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 835,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 836,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 837,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 838,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 839,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 840,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 841,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 842,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 843,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 844,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 845,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 846,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 847,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 848,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 849,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 850,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 851,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 852,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 853,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 854,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 855,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 856,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 857,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 858,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 859,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 860,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 861,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 862,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 863,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 864,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 865,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 866,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 867,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 868,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 869,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 870,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 871,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 872,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 873,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 874,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 875,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 876,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 877,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 878,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 879,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 880,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 881,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 882,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 883,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 884,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 885,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 886,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 887,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 888,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 889,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 890,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 891,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 892,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 893,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 894,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 895,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 896,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 897,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 898,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 899,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 900,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 901,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 902,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 903,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 904,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 905,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 906,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 907,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 908,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 909,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 910,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 911,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 912,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 913,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 914,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 915,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 916,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 917,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 918,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 919,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 920,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 921,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 922,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 923,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 924,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 925,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 926,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 927,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 928,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 929,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 930,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 931,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 932,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 933,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 934,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 935,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 936,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 937,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 938,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 939,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 940,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 941,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 942,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 943,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 944,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 945,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 946,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 947,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 948,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 949,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 950,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 951,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 952,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 953,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 954,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 955,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 956,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 957,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 958,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 959,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 960,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 961,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 962,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 963,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 964,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 965,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 966,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 967,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 968,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 969,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 970,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 971,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 972,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 973,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 974,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 975,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 976,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 977,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 978,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 979,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 980,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 981,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 982,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 983,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 984,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 985,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 986,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 987,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 988,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 989,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 990,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 991,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 992,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 993,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 994,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 995,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 996,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 997,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 998,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 999,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1000,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1001,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1002,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1003,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1004,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1005,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1006,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1007,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1008,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1009,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1010,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1011,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1012,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1013,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1014,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1015,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1016,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1017,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1018,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1019,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1020,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1021,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1022,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1023,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1024,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1025,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1026,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1027,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1028,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1029,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1030,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1031,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1032,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1033,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1034,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1035,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1036,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1037,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1038,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1039,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1040,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1041,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1042,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1043,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1044,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1045,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1046,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1047,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1048,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1049,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1050,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1051,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1052,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1053,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1054,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1055,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1056,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1057,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1058,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1059,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1060,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1061,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1062,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1063,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1064,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1065,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1066,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1067,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1068,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1069,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1070,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1071,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1072,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1073,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1074,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1075,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1076,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1077,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1078,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1079,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1080,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1081,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1082,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1083,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1084,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1085,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1086,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1087,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1088,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1089,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1090,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1091,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1092,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1093,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1094,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1095,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1096,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1097,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1098,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1099,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1100,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1101,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1102,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1103,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1104,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1105,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1106,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1107,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1108,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1109,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1110,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1111,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1112,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1113,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1114,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1115,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1116,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1117,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1118,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1119,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1120,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1121,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1122,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1123,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1124,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1125,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1126,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1127,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1128,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1129,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1130,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1131,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1132,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1133,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1134,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1135,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1136,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1137,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1138,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1139,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1140,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1141,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1142,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1143,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1144,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1145,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1146,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1147,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1148,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1149,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1150,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1151,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1152,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1153,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1154,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1155,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1156,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1157,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1158,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1159,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1160,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1161,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1162,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1163,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1164,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1165,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1166,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1167,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1168,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1169,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1170,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1171,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1172,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1173,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1174,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1175,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1176,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1177,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1178,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1179,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1180,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1181,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1182,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1183,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1184,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1185,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1186,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1187,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1188,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1189,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1190,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1191,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1192,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1193,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1194,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1195,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1196,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1197,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1198,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1199,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1200,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1201,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1202,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1203,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1204,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1205,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1206,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1207,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1208,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1209,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1210,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1211,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1212,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1213,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1214,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1215,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1216,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1217,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1218,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1219,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1220,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1221,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1222,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1223,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1224,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1225,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1226,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1227,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1228,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1229,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1230,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1231,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1232,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1233,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1234,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1235,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1236,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1237,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1238,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1239,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1240,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1241,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1242,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1243,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1244,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1245,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1246,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1247,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1248,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1249,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1250,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1251,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1252,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1253,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1254,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1255,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1256,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1257,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1258,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1259,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1260,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1261,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1262,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1263,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1264,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1265,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1266,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1267,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1268,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1269,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1270,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1271,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1272,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1273,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1274,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1275,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1276,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1277,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1278,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1279,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1280,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1281,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1282,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1283,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1284,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1285,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1286,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1287,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1288,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1289,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1290,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1291,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1292,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1293,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1294,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1295,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1296,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1297,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1298,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1299,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1300,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1301,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1302,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1303,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1304,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1305,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1306,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1307,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1308,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1309,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1310,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1311,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1312,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1313,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1314,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1315,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1316,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1317,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1318,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1319,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1320,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1321,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1322,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1323,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1324,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1325,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1326,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1327,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1328,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1329,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1330,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1331,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1332,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1333,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1334,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1335,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1336,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1337,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1338,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1339,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1340,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1341,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1342,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1343,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1344,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1345,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1346,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1347,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1348,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1349,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1350,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1351,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1352,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1353,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1354,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1355,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1356,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1357,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1358,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1359,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1360,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1361,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1362,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1363,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1364,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1365,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1366,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1367,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1368,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1369,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1370,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1371,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1372,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1373,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1374,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1375,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1376,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1377,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1378,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1379,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1380,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1381,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1382,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1383,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1384,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1385,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1386,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1387,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1388,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1389,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1390,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1391,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1392,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1393,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1394,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1395,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1396,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1397,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1398,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1399,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1400,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1401,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1402,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1403,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1404,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1405,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1406,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1407,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1408,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1409,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1410,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1411,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1412,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1413,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1414,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1415,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1416,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1417,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1418,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1419,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1420,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1421,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1422,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1423,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1424,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1425,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1426,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1427,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1428,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1429,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1430,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1431,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1432,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1433,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1434,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1435,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1436,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1437,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1438,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1439,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1440,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1441,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1442,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1443,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1444,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1445,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1446,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1447,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1448,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1449,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1450,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1451,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1452,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1453,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1454,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1455,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1456,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1457,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1458,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1459,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1460,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1461,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1462,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1463,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1464,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1465,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1466,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1467,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1468,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1469,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1470,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1471,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1472,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1473,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1474,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1475,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1476,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1477,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1478,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1479,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1480,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1481,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1482,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1483,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1484,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1485,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1486,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1487,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1488,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1489,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1490,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1491,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1492,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1493,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1494,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1495,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1496,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1497,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1498,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1499,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1500,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1501,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1502,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1503,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1504,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1505,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1506,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1507,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1508,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1509,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1510,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1511,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1512,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1513,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1514,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1515,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1516,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1517,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1518,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1519,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1520,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1521,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1522,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1523,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1524,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1525,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1526,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1527,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1528,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1529,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1530,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1531,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1532,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1533,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1534,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1535,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1536,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1537,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1538,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1539,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1540,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1541,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1542,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1543,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1544,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1545,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1546,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1547,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1548,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1549,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1550,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1551,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1552,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1553,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1554,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1555,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1556,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1557,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1558,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1559,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1560,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1561,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1562,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1563,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1564,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1565,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1566,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1567,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1568,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1569,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1570,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1571,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1572,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1573,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1574,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1575,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1576,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1577,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1578,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1579,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1580,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1581,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1582,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1583,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1584,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1585,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1586,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1587,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1588,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1589,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1590,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1591,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1592,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1593,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1594,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1595,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1596,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1597,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1598,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1599,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1600,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1601,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1602,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1603,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1604,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1605,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1606,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1607,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1608,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1609,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1610,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1611,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1612,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1613,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1614,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1615,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1616,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1617,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1618,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1619,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1620,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1621,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1622,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1623,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1624,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1625,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1626,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1627,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1628,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1629,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1630,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1631,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1632,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1633,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1634,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1635,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1636,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1637,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1638,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1639,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1640,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1641,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1642,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1643,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1644,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1645,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1646,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1647,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1648,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1649,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1650,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1651,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1652,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1653,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1654,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1655,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1656,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1657,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1658,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1659,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1660,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1661,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1662,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1663,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1664,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1665,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1666,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1667,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1668,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1669,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1670,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1671,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1672,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1673,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1674,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1675,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1676,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1677,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1678,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1679,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1680,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1681,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1682,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1683,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1684,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1685,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1686,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1687,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1688,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1689,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1690,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1691,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1692,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1693,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1694,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1695,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1696,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1697,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1698,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1699,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1700,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1701,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1702,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1703,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1704,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1705,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1706,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1707,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1708,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1709,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1710,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1711,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1712,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1713,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1714,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1715,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1716,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1717,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1718,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1719,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1720,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1721,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1722,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1723,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1724,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1725,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1726,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1727,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1728,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1729,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1730,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1731,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1732,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1733,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1734,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1735,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1736,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1737,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1738,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1739,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1740,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1741,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1742,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1743,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1744,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1745,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1746,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1747,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1748,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1749,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1750,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1751,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1752,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1753,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1754,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1755,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1756,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1757,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1758,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1759,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1760,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1761,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1762,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1763,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1764,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1765,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1766,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1767,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1768,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1769,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1770,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1771,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1772,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1773,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1774,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1775,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1776,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1777,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1778,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1779,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1780,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1781,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1782,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1783,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1784,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1785,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1786,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1787,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1788,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1789,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1790,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1791,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1792,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1793,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1794,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1795,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1796,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1797,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1798,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1799,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1800,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1801,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1802,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1803,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1804,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1805,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1806,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1807,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1808,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1809,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1810,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1811,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1812,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1813,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1814,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1815,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1816,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1817,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1818,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1819,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1820,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1821,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1822,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1823,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1824,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1825,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1826,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1827,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1828,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1829,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1830,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1831,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1832,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1833,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1834,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1835,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1836,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1837,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1838,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1839,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1840,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1841,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1842,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1843,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1844,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1845,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1846,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1847,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1848,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1849,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1850,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1851,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1852,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1853,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1854,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1855,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1856,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1857,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1858,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1859,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1860,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1861,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1862,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1863,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1864,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1865,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1866,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1867,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1868,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1869,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1870,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1871,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1872,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1873,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1874,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1875,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1876,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1877,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1878,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1879,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1880,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1881,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1882,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1883,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1884,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1885,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1886,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1887,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1888,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1889,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1890,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1891,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1892,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1893,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1894,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1895,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1896,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1897,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1898,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1899,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1900,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1901,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1902,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1903,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1904,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1905,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1906,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1907,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1908,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1909,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1910,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1911,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1912,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1913,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1914,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1915,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1916,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1917,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1918,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1919,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1920,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1921,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1922,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1923,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1924,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1925,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1926,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1927,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1928,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1929,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1930,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1931,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1932,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1933,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1934,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1935,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1936,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1937,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1938,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1939,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1940,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1941,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1942,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1943,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1944,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1945,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1946,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1947,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1948,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1949,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1950,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1951,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1952,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1953,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1954,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1955,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1956,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1957,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1958,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1959,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1960,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1961,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1962,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1963,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1964,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1965,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1966,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1967,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1968,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1969,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1970,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1971,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1972,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1973,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1974,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1975,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1976,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1977,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1978,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1979,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1980,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1981,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1982,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1983,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1984,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1985,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1986,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1987,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1988,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1989,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1990,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1991,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1992,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1993,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1994,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1995,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1996,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1997,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1998,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 1999,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2000,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2001,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2002,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2003,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2004,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2005,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2006,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2007,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2008,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2009,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2010,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2011,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2012,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2013,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2014,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2015,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2016,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2017,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2018,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2019,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2020,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2021,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2022,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2023,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2024,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2025,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2026,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2027,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2028,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2029,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2030,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2031,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2032,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2033,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2034,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2035,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2036,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2037,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2038,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2039,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2040,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2041,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2042,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2043,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2044,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2045,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2046,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2047,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2048,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2049,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2050,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2051,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2052,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2053,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2054,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2055,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2056,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2057,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2058,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2059,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2060,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2061,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2062,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2063,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2064,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2065,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2066,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2067,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2068,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2069,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2070,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2071,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2072,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2073,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2074,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2075,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2076,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2077,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2078,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2079,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2080,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2081,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2082,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2083,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2084,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2085,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2086,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2087,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2088,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2089,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2090,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2091,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2092,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2093,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2094,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2095,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2096,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2097,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2098,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2099,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2100,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2101,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2102,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2103,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2104,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2105,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2106,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2107,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2108,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2109,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2110,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2111,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2112,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2113,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2114,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2115,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2116,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2117,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2118,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2119,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2120,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2121,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2122,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2123,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2124,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2125,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2126,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2127,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2128,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2129,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2130,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2131,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2132,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2133,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2134,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2135,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2136,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2137,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2138,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2139,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2140,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2141,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2142,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2143,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2144,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2145,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2146,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2147,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2148,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2149,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2150,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2151,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2152,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2153,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2154,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2155,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2156,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2157,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2158,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2159,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2160,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2161,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2162,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2163,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2164,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2165,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2166,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2167,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2168,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2169,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2170,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2171,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2172,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2173,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2174,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2175,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2176,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2177,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2178,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2179,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2180,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2181,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2182,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2183,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2184,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2185,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2186,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2187,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2188,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2189,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2190,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2191,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2192,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2193,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2194,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2195,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2196,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2197,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2198,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2199,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2200,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2201,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2202,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2203,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2204,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2205,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2206,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2207,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2208,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2209,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2210,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2211,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2212,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2213,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2214,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2215,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2216,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2217,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2218,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2219,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2220,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2221,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2222,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2223,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2224,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2225,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2226,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2227,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2228,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2229,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2230,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2231,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2232,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2233,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2234,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2235,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2236,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2237,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2238,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2239,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2240,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2241,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2242,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2243,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2244,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2245,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2246,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2247,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2248,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2249,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2250,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2251,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2252,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2253,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2254,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2255,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2256,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2257,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2258,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2259,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2260,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2261,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2262,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2263,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2264,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2265,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2266,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2267,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2268,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2269,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2270,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2271,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2272,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2273,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2274,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2275,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2276,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2277,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2278,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2279,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2280,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2281,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2282,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2283,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2284,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2285,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2286,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2287,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2288,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2289,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2290,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2291,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2292,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2293,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2294,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2295,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2296,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2297,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2298,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2299,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2300,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2301,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2302,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2303,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2304,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2305,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2306,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2307,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2308,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2309,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2310,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2311,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2312,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2313,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2314,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2315,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2316,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2317,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2318,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2319,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2320,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2321,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2322,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2323,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2324,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2325,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2326,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2327,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2328,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2329,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2330,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2331,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2332,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2333,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2334,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2335,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2336,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2337,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2338,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2339,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2340,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2341,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2342,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2343,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2344,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2345,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2346,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2347,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2348,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2349,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2350,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2351,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2352,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2353,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2354,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2355,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2356,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2357,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2358,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2359,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2360,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2361,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2362,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2363,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2364,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2365,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2366,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2367,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2368,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2369,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2370,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2371,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2372,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2373,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2374,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2375,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2376,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2377,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2378,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2379,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2380,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2381,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2382,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2383,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2384,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2385,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2386,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2387,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2388,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2389,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2390,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2391,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2392,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2393,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2394,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2395,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2396,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2397,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2398,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2399,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2400,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2401,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2402,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2403,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2404,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2405,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2406,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2407,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2408,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2409,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2410,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2411,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2412,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2413,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2414,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2415,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2416,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2417,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2418,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2419,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2420,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2421,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2422,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2423,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2424,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2425,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2426,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2427,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2428,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2429,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2430,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2431,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2432,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2433,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2434,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2435,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2436,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2437,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2438,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2439,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2440,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2441,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2442,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2443,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2444,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2445,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2446,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2447,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2448,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2449,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2450,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2451,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2452,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2453,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2454,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2455,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2456,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2457,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2458,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2459,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2460,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2461,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2462,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2463,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2464,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2465,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2466,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2467,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2468,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2469,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2470,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2471,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2472,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2473,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2474,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2475,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2476,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2477,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2478,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2479,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2480,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2481,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2482,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2483,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2484,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2485,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2486,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2487,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2488,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2489,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2490,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2491,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2492,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2493,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2494,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2495,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2496,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2497,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2498,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2499,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2500,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2501,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2502,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2503,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2504,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2505,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2506,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2507,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2508,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2509,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2510,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2511,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2512,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2513,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2514,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2515,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2516,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2517,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2518,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2519,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2520,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2521,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2522,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2523,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2524,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2525,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2526,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2527,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2528,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2529,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2530,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2531,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2532,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2533,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2534,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2535,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2536,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2537,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2538,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2539,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2540,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2541,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2542,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2543,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2544,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2545,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2546,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2547,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2548,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2549,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2550,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2551,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2552,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2553,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2554,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2555,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2556,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2557,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2558,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2559,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2560,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2561,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2562,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2563,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2564,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2565,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2566,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2567,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2568,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2569,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2570,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2571,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2572,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2573,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2574,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2575,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2576,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2577,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2578,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2579,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2580,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2581,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2582,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2583,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2584,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2585,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2586,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2587,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2588,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2589,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2590,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2591,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2592,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2593,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2594,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2595,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2596,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2597,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2598,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2599,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2600,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2601,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2602,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2603,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2604,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2605,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2606,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2607,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2608,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2609,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2610,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2611,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2612,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2613,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2614,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2615,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2616,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2617,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2618,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2619,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2620,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2621,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2622,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2623,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2624,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2625,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2626,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2627,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2628,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2629,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2630,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2631,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2632,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2633,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2634,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2635,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2636,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2637,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2638,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2639,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2640,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2641,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2642,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2643,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2644,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2645,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2646,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2647,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2648,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2649,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2650,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2651,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2652,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2653,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2654,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2655,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2656,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2657,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2658,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2659,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2660,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2661,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2662,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2663,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2664,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2665,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2666,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2667,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2668,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2669,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2670,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2671,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2672,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2673,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2674,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2675,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2676,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2677,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2678,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2679,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2680,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2681,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2682,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2683,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2684,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2685,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2686,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2687,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2688,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2689,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2690,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2691,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2692,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2693,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2694,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2695,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2696,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2697,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2698,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2699,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2700,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2701,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2702,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2703,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2704,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2705,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2706,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2707,
      "total_chunks": 2709
    },
    {
      "source": "(Wiley Series in Probability and Statistics) Alvin C. Rencher, G. Bruce Schaalje - Linear Models in Statistics-Wiley-Interscience (2008)",
      "chunk_id": 2708,
      "total_chunks": 2709
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 0,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 3,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 4,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 5,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 6,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 7,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 8,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 9,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 10,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 11,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 12,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 13,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 14,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 15,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 16,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 17,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 18,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 19,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 20,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 21,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 22,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 23,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 24,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 25,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 26,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 27,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 28,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 29,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 30,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 31,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 32,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 33,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 34,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 35,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 36,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 37,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 38,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 39,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 40,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 41,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 42,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 43,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 44,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 45,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 46,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 47,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 48,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 49,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 50,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 51,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 52,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 53,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 54,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 55,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 56,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 57,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 58,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 59,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 60,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 61,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 62,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 63,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 64,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 65,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 66,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 67,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 68,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 69,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 70,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 71,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 72,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 73,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 74,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 75,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 76,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 77,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 78,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 79,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 80,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 81,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 82,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 83,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 84,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 85,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 86,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 87,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 88,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 89,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 90,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 91,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 92,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 93,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 94,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 95,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 96,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 97,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 98,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 99,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 100,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 101,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 102,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 103,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 104,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 105,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 106,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 107,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 108,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 109,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 110,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 111,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 112,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 113,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 114,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 115,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 116,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 117,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 118,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 119,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 120,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 121,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 122,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 123,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 124,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 125,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 126,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 127,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 128,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 129,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 130,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 131,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 132,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 133,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 134,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 135,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 136,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 137,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 138,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 139,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 140,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 141,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 142,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 143,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 144,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 145,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 146,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 147,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 148,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 149,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 150,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 151,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 152,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 153,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 154,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 155,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 156,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 157,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 158,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 159,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 160,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 161,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 162,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 163,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 164,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 165,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 166,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 167,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 168,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 169,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 170,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 171,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 172,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 173,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 174,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 175,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 176,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 177,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 178,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 179,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 180,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 181,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 182,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 183,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 184,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 185,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 186,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 187,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 188,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 189,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 190,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 191,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 192,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 193,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 194,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 195,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 196,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 197,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 198,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 199,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 200,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 201,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 202,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 203,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 204,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 205,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 206,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 207,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 208,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 209,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 210,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 211,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 212,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 213,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 214,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 215,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 216,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 217,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 218,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 219,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 220,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 221,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 222,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 223,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 224,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 225,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 226,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 227,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 228,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 229,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 230,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 231,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 232,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 233,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 234,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 235,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 236,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 237,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 238,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 239,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 240,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 241,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 242,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 243,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 244,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 245,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 246,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 247,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 248,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 249,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 250,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 251,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 252,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 253,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 254,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 255,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 256,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 257,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 258,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 259,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 260,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 261,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 262,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 263,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 264,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 265,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 266,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 267,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 268,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 269,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 270,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 271,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 272,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 273,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 274,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 275,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 276,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 277,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 278,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 279,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 280,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 281,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 282,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 283,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 284,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 285,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 286,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 287,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 288,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 289,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 290,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 291,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 292,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 293,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 294,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 295,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 296,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 297,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 298,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 299,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 300,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 301,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 302,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 303,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 304,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 305,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 306,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 307,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 308,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 309,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 310,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 311,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 312,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 313,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 314,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 315,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 316,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 317,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 318,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 319,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 320,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 321,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 322,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 323,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 324,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 325,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 326,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 327,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 328,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 329,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 330,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 331,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 332,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 333,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 334,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 335,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 336,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 337,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 338,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 339,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 340,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 341,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 342,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 343,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 344,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 345,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 346,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 347,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 348,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 349,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 350,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 351,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 352,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 353,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 354,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 355,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 356,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 357,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 358,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 359,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 360,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 361,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 362,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 363,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 364,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 365,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 366,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 367,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 368,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 369,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 370,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 371,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 372,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 373,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 374,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 375,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 376,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 377,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 378,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 379,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 380,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 381,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 382,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 383,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 384,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 385,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 386,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 387,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 388,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 389,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 390,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 391,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 392,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 393,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 394,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 395,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 396,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 397,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 398,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 399,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 400,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 401,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 402,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 403,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 404,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 405,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 406,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 407,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 408,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 409,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 410,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 411,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 412,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 413,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 414,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 415,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 416,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 417,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 418,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 419,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 420,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 421,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 422,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 423,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 424,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 425,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 426,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 427,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 428,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 429,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 430,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 431,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 432,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 433,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 434,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 435,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 436,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 437,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 438,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 439,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 440,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 441,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 442,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 443,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 444,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 445,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 446,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 447,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 448,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 449,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 450,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 451,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 452,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 453,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 454,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 455,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 456,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 457,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 458,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 459,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 460,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 461,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 462,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 463,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 464,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 465,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 466,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 467,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 468,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 469,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 470,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 471,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 472,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 473,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 474,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 475,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 476,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 477,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 478,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 479,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 480,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 481,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 482,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 483,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 484,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 485,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 486,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 487,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 488,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 489,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 490,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 491,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 492,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 493,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 494,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 495,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 496,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 497,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 498,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 499,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 500,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 501,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 502,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 503,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 504,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 505,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 506,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 507,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 508,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 509,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 510,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 511,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 512,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 513,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 514,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 515,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 516,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 517,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 518,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 519,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 520,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 521,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 522,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 523,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 524,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 525,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 526,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 527,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 528,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 529,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 530,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 531,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 532,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 533,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 534,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 535,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 536,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 537,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 538,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 539,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 540,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 541,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 542,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 543,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 544,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 545,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 546,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 547,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 548,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 549,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 550,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 551,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 552,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 553,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 554,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 555,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 556,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 557,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 558,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 559,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 560,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 561,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 562,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 563,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 564,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 565,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 566,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 567,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 568,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 569,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 570,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 571,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 572,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 573,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 574,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 575,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 576,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 577,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 578,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 579,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 580,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 581,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 582,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 583,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 584,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 585,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 586,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 587,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 588,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 589,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 590,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 591,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 592,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 593,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 594,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 595,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 596,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 597,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 598,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 599,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 600,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 601,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 602,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 603,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 604,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 605,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 606,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 607,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 608,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 609,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 610,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 611,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 612,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 613,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 614,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 615,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 616,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 617,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 618,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 619,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 620,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 621,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 622,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 623,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 624,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 625,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 626,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 627,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 628,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 629,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 630,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 631,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 632,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 633,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 634,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 635,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 636,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 637,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 638,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 639,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 640,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 641,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 642,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 643,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 644,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 645,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 646,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 647,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 648,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 649,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 650,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 651,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 652,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 653,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 654,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 655,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 656,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 657,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 658,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 659,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 660,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 661,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 662,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 663,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 664,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 665,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 666,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 667,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 668,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 669,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 670,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 671,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 672,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 673,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 674,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 675,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 676,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 677,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 678,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 679,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 680,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 681,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 682,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 683,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 684,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 685,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 686,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 687,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 688,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 689,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 690,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 691,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 692,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 693,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 694,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 695,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 696,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 697,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 698,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 699,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 700,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 701,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 702,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 703,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 704,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 705,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 706,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 707,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 708,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 709,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 710,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 711,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 712,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 713,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 714,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 715,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 716,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 717,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 718,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 719,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 720,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 721,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 722,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 723,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 724,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 725,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 726,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 727,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 728,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 729,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 730,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 731,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 732,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 733,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 734,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 735,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 736,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 737,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 738,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 739,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 740,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 741,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 742,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 743,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 744,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 745,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 746,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 747,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 748,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 749,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 750,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 751,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 752,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 753,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 754,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 755,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 756,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 757,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 758,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 759,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 760,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 761,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 762,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 763,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 764,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 765,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 766,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 767,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 768,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 769,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 770,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 771,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 772,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 773,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 774,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 775,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 776,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 777,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 778,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 779,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 780,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 781,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 782,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 783,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 784,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 785,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 786,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 787,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 788,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 789,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 790,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 791,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 792,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 793,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 794,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 795,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 796,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 797,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 798,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 799,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 800,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 801,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 802,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 803,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 804,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 805,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 806,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 807,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 808,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 809,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 810,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 811,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 812,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 813,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 814,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 815,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 816,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 817,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 818,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 819,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 820,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 821,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 822,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 823,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 824,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 825,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 826,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 827,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 828,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 829,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 830,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 831,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 832,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 833,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 834,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 835,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 836,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 837,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 838,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 839,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 840,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 841,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 842,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 843,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 844,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 845,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 846,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 847,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 848,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 849,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 850,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 851,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 852,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 853,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 854,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 855,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 856,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 857,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 858,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 859,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 860,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 861,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 862,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 863,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 864,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 865,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 866,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 867,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 868,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 869,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 870,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 871,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 872,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 873,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 874,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 875,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 876,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 877,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 878,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 879,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 880,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 881,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 882,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 883,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 884,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 885,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 886,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 887,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 888,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 889,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 890,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 891,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 892,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 893,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 894,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 895,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 896,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 897,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 898,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 899,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 900,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 901,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 902,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 903,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 904,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 905,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 906,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 907,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 908,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 909,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 910,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 911,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 912,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 913,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 914,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 915,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 916,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 917,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 918,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 919,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 920,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 921,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 922,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 923,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 924,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 925,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 926,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 927,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 928,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 929,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 930,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 931,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 932,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 933,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 934,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 935,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 936,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 937,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 938,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 939,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 940,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 941,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 942,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 943,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 944,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 945,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 946,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 947,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 948,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 949,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 950,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 951,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 952,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 953,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 954,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 955,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 956,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 957,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 958,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 959,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 960,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 961,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 962,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 963,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 964,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 965,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 966,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 967,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 968,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 969,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 970,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 971,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 972,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 973,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 974,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 975,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 976,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 977,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 978,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 979,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 980,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 981,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 982,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 983,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 984,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 985,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 986,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 987,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 988,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 989,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 990,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 991,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 992,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 993,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 994,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 995,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 996,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 997,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 998,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 999,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1000,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1001,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1002,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1003,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1004,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1005,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1006,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1007,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1008,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1009,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1010,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1011,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1012,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1013,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1014,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1015,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1016,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1017,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1018,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1019,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1020,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1021,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1022,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1023,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1024,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1025,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1026,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1027,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1028,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1029,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1030,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1031,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1032,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1033,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1034,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1035,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1036,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1037,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1038,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1039,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1040,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1041,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1042,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1043,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1044,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1045,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1046,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1047,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1048,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1049,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1050,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1051,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1052,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1053,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1054,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1055,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1056,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1057,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1058,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1059,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1060,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1061,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1062,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1063,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1064,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1065,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1066,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1067,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1068,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1069,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1070,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1071,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1072,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1073,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1074,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1075,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1076,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1077,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1078,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1079,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1080,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1081,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1082,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1083,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1084,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1085,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1086,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1087,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1088,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1089,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1090,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1091,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1092,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1093,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1094,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1095,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1096,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1097,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1098,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1099,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1100,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1101,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1102,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1103,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1104,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1105,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1106,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1107,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1108,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1109,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1110,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1111,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1112,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1113,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1114,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1115,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1116,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1117,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1118,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1119,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1120,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1121,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1122,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1123,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1124,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1125,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1126,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1127,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1128,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1129,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1130,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1131,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1132,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1133,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1134,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1135,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1136,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1137,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1138,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1139,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1140,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1141,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1142,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1143,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1144,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1145,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1146,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1147,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1148,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1149,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1150,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1151,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1152,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1153,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1154,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1155,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1156,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1157,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1158,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1159,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1160,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1161,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1162,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1163,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1164,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1165,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1166,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1167,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1168,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1169,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1170,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1171,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1172,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1173,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1174,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1175,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1176,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1177,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1178,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1179,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1180,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1181,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1182,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1183,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1184,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1185,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1186,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1187,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1188,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1189,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1190,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1191,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1192,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1193,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1194,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1195,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1196,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1197,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1198,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1199,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1200,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1201,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1202,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1203,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1204,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1205,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1206,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1207,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1208,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1209,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1210,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1211,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1212,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1213,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1214,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1215,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1216,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1217,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1218,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1219,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1220,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1221,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1222,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1223,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1224,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1225,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1226,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1227,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1228,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1229,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1230,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1231,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1232,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1233,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1234,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1235,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1236,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1237,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1238,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1239,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1240,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1241,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1242,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1243,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1244,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1245,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1246,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1247,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1248,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1249,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1250,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1251,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1252,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1253,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1254,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1255,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1256,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1257,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1258,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1259,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1260,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1261,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1262,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1263,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1264,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1265,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1266,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1267,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1268,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1269,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1270,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1271,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1272,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1273,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1274,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1275,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1276,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1277,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1278,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1279,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1280,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1281,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1282,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1283,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1284,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1285,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1286,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1287,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1288,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1289,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1290,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1291,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1292,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1293,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1294,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1295,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1296,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1297,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1298,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1299,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1300,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1301,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1302,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1303,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1304,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1305,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1306,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1307,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1308,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1309,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1310,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1311,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1312,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1313,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1314,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1315,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1316,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1317,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1318,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1319,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1320,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1321,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1322,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1323,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1324,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1325,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1326,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1327,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1328,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1329,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1330,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1331,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1332,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1333,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1334,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1335,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1336,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1337,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1338,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1339,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1340,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1341,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1342,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1343,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1344,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1345,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1346,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1347,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1348,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1349,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1350,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1351,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1352,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1353,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1354,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1355,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1356,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1357,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1358,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1359,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1360,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1361,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1362,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1363,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1364,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1365,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1366,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1367,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1368,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1369,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1370,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1371,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1372,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1373,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1374,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1375,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1376,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1377,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1378,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1379,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1380,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1381,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1382,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1383,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1384,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1385,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1386,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1387,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1388,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1389,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1390,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1391,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1392,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1393,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1394,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1395,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1396,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1397,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1398,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1399,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1400,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1401,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1402,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1403,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1404,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1405,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1406,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1407,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1408,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1409,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1410,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1411,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1412,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1413,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1414,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1415,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1416,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1417,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1418,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1419,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1420,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1421,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1422,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1423,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1424,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1425,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1426,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1427,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1428,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1429,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1430,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1431,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1432,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1433,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1434,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1435,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1436,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1437,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1438,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1439,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1440,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1441,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1442,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1443,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1444,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1445,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1446,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1447,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1448,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1449,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1450,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1451,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1452,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1453,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1454,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1455,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1456,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1457,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1458,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1459,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1460,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1461,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1462,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1463,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1464,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1465,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1466,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1467,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1468,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1469,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1470,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1471,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1472,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1473,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1474,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1475,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1476,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1477,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1478,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1479,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1480,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1481,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1482,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1483,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1484,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1485,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1486,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1487,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1488,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1489,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1490,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1491,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1492,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1493,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1494,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1495,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1496,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1497,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1498,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1499,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1500,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1501,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1502,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1503,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1504,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1505,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1506,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1507,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1508,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1509,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1510,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1511,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1512,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1513,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1514,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1515,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1516,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1517,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1518,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1519,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1520,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1521,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1522,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1523,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1524,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1525,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1526,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1527,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1528,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1529,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1530,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1531,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1532,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1533,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1534,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1535,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1536,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1537,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1538,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1539,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1540,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1541,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1542,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1543,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1544,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1545,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1546,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1547,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1548,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1549,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1550,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1551,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1552,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1553,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1554,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1555,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1556,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1557,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1558,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1559,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1560,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1561,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1562,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1563,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1564,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1565,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1566,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1567,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1568,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1569,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1570,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1571,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1572,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1573,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1574,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1575,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1576,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1577,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1578,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1579,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1580,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1581,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1582,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1583,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1584,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1585,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1586,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1587,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1588,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1589,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1590,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1591,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1592,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1593,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1594,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1595,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1596,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1597,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1598,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1599,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1600,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1601,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1602,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1603,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1604,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1605,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1606,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1607,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1608,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1609,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1610,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1611,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1612,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1613,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1614,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1615,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1616,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1617,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1618,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1619,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1620,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1621,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1622,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1623,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1624,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1625,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1626,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1627,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1628,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1629,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1630,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1631,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1632,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1633,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1634,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1635,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1636,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1637,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1638,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1639,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1640,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1641,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1642,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1643,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1644,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1645,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1646,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1647,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1648,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1649,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1650,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1651,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1652,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1653,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1654,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1655,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1656,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1657,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1658,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1659,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1660,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1661,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1662,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1663,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1664,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1665,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1666,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1667,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1668,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1669,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1670,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1671,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1672,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1673,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1674,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1675,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1676,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1677,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1678,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1679,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1680,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1681,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1682,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1683,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1684,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1685,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1686,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1687,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1688,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1689,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1690,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1691,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1692,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1693,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1694,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1695,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1696,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1697,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1698,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1699,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1700,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1701,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1702,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1703,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1704,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1705,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1706,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1707,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1708,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1709,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1710,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1711,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1712,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1713,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1714,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1715,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1716,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1717,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1718,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1719,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1720,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1721,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1722,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1723,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1724,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1725,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1726,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1727,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1728,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1729,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1730,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1731,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1732,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1733,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1734,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1735,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1736,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1737,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1738,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1739,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1740,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1741,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1742,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1743,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1744,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1745,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1746,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1747,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1748,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1749,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1750,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1751,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1752,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1753,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1754,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1755,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1756,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1757,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1758,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1759,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1760,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1761,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1762,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1763,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1764,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1765,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1766,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1767,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1768,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1769,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1770,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1771,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1772,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1773,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1774,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1775,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1776,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1777,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1778,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1779,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1780,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1781,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1782,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1783,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1784,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1785,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1786,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1787,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1788,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1789,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1790,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1791,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1792,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1793,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1794,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1795,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1796,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1797,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1798,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1799,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1800,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1801,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1802,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1803,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1804,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1805,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1806,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1807,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1808,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1809,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1810,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1811,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1812,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1813,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1814,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1815,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1816,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1817,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1818,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1819,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1820,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1821,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1822,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1823,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1824,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1825,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1826,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1827,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1828,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1829,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1830,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1831,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1832,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1833,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1834,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1835,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1836,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1837,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1838,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1839,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1840,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1841,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1842,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1843,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1844,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1845,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1846,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1847,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1848,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1849,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1850,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1851,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1852,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1853,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1854,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1855,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1856,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1857,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1858,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1859,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1860,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1861,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1862,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1863,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1864,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1865,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1866,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1867,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1868,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1869,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1870,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1871,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1872,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1873,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1874,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1875,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1876,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1877,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1878,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1879,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1880,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1881,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1882,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1883,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1884,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1885,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1886,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1887,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1888,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1889,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1890,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1891,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1892,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1893,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1894,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1895,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1896,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1897,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1898,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1899,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1900,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1901,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1902,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1903,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1904,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1905,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1906,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1907,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1908,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1909,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1910,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1911,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1912,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1913,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1914,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1915,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1916,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1917,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1918,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1919,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1920,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1921,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1922,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1923,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1924,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1925,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1926,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1927,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1928,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1929,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1930,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1931,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1932,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1933,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1934,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1935,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1936,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1937,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1938,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1939,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1940,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1941,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1942,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1943,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1944,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1945,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1946,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1947,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1948,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1949,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1950,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1951,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1952,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1953,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1954,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1955,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1956,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1957,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1958,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1959,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1960,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1961,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1962,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1963,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1964,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1965,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1966,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1967,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1968,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1969,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1970,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1971,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1972,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1973,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1974,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1975,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1976,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1977,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1978,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1979,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1980,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1981,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1982,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1983,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1984,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1985,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1986,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1987,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1988,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1989,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1990,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1991,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1992,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1993,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1994,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1995,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1996,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1997,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1998,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 1999,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2000,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2001,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2002,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2003,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2004,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2005,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2006,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2007,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2008,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2009,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2010,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2011,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2012,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2013,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2014,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2015,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2016,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2017,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2018,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2019,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2020,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2021,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2022,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2023,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2024,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2025,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2026,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2027,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2028,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2029,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2030,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2031,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2032,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2033,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2034,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2035,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2036,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2037,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2038,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2039,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2040,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2041,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2042,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2043,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2044,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2045,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2046,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2047,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2048,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2049,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2050,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2051,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2052,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2053,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2054,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2055,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2056,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2057,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2058,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2059,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2060,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2061,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2062,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2063,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2064,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2065,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2066,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2067,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2068,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2069,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2070,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2071,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2072,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2073,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2074,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2075,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2076,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2077,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2078,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2079,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2080,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2081,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2082,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2083,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2084,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2085,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2086,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2087,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2088,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2089,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2090,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2091,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2092,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2093,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2094,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2095,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2096,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2097,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2098,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2099,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2100,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2101,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2102,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2103,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2104,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2105,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2106,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2107,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2108,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2109,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2110,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2111,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2112,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2113,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2114,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2115,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2116,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2117,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2118,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2119,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2120,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2121,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2122,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2123,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2124,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2125,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2126,
      "total_chunks": 2128
    },
    {
      "source": "deep-learning-material",
      "chunk_id": 2127,
      "total_chunks": 2128
    }
  ],
  "is_ready": true
}